I0817 06:45:05.954600      21 e2e.go:126] Starting e2e run "cc861678-4c64-437f-95a3-e3501ec4e99d" on Ginkgo node 1
Aug 17 06:45:05.968: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1692254705 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 17 06:45:06.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 06:45:06.061: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 17 06:45:06.073: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 17 06:45:06.090: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 17 06:45:06.090: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Aug 17 06:45:06.090: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 17 06:45:06.093: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 17 06:45:06.093: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 17 06:45:06.093: INFO: e2e test version: v1.26.6
Aug 17 06:45:06.094: INFO: kube-apiserver version: v1.26.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 17 06:45:06.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 06:45:06.096: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.037 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 17 06:45:06.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 06:45:06.061: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 17 06:45:06.073: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 17 06:45:06.090: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 17 06:45:06.090: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Aug 17 06:45:06.090: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 17 06:45:06.093: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Aug 17 06:45:06.093: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Aug 17 06:45:06.093: INFO: e2e test version: v1.26.6
    Aug 17 06:45:06.094: INFO: kube-apiserver version: v1.26.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 17 06:45:06.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 06:45:06.096: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:45:06.117
Aug 17 06:45:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 06:45:06.118
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:06.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:06.155
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-869f02ba-1d77-4803-a374-7d85f4597d16 08/17/23 06:45:06.181
STEP: Creating the pod 08/17/23 06:45:06.185
Aug 17 06:45:06.192: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e" in namespace "configmap-4984" to be "running"
Aug 17 06:45:06.195: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680927ms
Aug 17 06:45:08.198: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006047266s
Aug 17 06:45:10.198: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005558212s
Aug 17 06:45:12.199: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006472823s
Aug 17 06:45:14.200: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007366575s
Aug 17 06:45:16.240: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047798998s
Aug 17 06:45:18.236: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Running", Reason="", readiness=true. Elapsed: 12.04341518s
Aug 17 06:45:18.236: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e" satisfied condition "running"
STEP: Waiting for pod with text data 08/17/23 06:45:18.236
STEP: Waiting for pod with binary data 08/17/23 06:45:18.249
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 06:45:18.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4984" for this suite. 08/17/23 06:45:18.256
------------------------------
â€¢ [SLOW TEST] [12.143 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:45:06.117
    Aug 17 06:45:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 06:45:06.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:06.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:06.155
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-869f02ba-1d77-4803-a374-7d85f4597d16 08/17/23 06:45:06.181
    STEP: Creating the pod 08/17/23 06:45:06.185
    Aug 17 06:45:06.192: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e" in namespace "configmap-4984" to be "running"
    Aug 17 06:45:06.195: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680927ms
    Aug 17 06:45:08.198: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006047266s
    Aug 17 06:45:10.198: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005558212s
    Aug 17 06:45:12.199: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006472823s
    Aug 17 06:45:14.200: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007366575s
    Aug 17 06:45:16.240: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047798998s
    Aug 17 06:45:18.236: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e": Phase="Running", Reason="", readiness=true. Elapsed: 12.04341518s
    Aug 17 06:45:18.236: INFO: Pod "pod-configmaps-4c2e5e16-be07-497a-a3eb-15dfd2e1659e" satisfied condition "running"
    STEP: Waiting for pod with text data 08/17/23 06:45:18.236
    STEP: Waiting for pod with binary data 08/17/23 06:45:18.249
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:45:18.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4984" for this suite. 08/17/23 06:45:18.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:45:18.261
Aug 17 06:45:18.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename ephemeral-containers-test 08/17/23 06:45:18.262
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:18.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:18.275
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/17/23 06:45:18.28
Aug 17 06:45:18.393: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4407" to be "running and ready"
Aug 17 06:45:18.395: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26461ms
Aug 17 06:45:18.395: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 17 06:45:20.399: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005709476s
Aug 17 06:45:20.399: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 17 06:45:20.399: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/17/23 06:45:20.401
Aug 17 06:45:20.411: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4407" to be "container debugger running"
Aug 17 06:45:20.413: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.290855ms
Aug 17 06:45:22.447: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.036053295s
Aug 17 06:45:22.447: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/17/23 06:45:22.447
Aug 17 06:45:22.447: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4407 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 06:45:22.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 06:45:22.448: INFO: ExecWithOptions: Clientset creation
Aug 17 06:45:22.448: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4407/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 17 06:45:22.505: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 06:45:22.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4407" for this suite. 08/17/23 06:45:22.538
------------------------------
â€¢ [4.304 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:45:18.261
    Aug 17 06:45:18.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/17/23 06:45:18.262
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:18.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:18.275
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/17/23 06:45:18.28
    Aug 17 06:45:18.393: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4407" to be "running and ready"
    Aug 17 06:45:18.395: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26461ms
    Aug 17 06:45:18.395: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 06:45:20.399: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005709476s
    Aug 17 06:45:20.399: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 17 06:45:20.399: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/17/23 06:45:20.401
    Aug 17 06:45:20.411: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4407" to be "container debugger running"
    Aug 17 06:45:20.413: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.290855ms
    Aug 17 06:45:22.447: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.036053295s
    Aug 17 06:45:22.447: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/17/23 06:45:22.447
    Aug 17 06:45:22.447: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4407 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 06:45:22.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 06:45:22.448: INFO: ExecWithOptions: Clientset creation
    Aug 17 06:45:22.448: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4407/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 17 06:45:22.505: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:45:22.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4407" for this suite. 08/17/23 06:45:22.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:45:22.566
Aug 17 06:45:22.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename gc 08/17/23 06:45:22.567
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:22.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:22.591
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/17/23 06:45:22.654
STEP: delete the rc 08/17/23 06:45:27.664
STEP: wait for the rc to be deleted 08/17/23 06:45:27.668
Aug 17 06:45:28.680: INFO: 80 pods remaining
Aug 17 06:45:28.680: INFO: 80 pods has nil DeletionTimestamp
Aug 17 06:45:28.680: INFO: 
Aug 17 06:45:29.682: INFO: 71 pods remaining
Aug 17 06:45:29.682: INFO: 71 pods has nil DeletionTimestamp
Aug 17 06:45:29.682: INFO: 
Aug 17 06:45:30.685: INFO: 60 pods remaining
Aug 17 06:45:30.685: INFO: 60 pods has nil DeletionTimestamp
Aug 17 06:45:30.685: INFO: 
Aug 17 06:45:31.677: INFO: 40 pods remaining
Aug 17 06:45:31.677: INFO: 40 pods has nil DeletionTimestamp
Aug 17 06:45:31.677: INFO: 
Aug 17 06:45:32.695: INFO: 31 pods remaining
Aug 17 06:45:32.695: INFO: 31 pods has nil DeletionTimestamp
Aug 17 06:45:32.695: INFO: 
Aug 17 06:45:33.675: INFO: 20 pods remaining
Aug 17 06:45:33.675: INFO: 20 pods has nil DeletionTimestamp
Aug 17 06:45:33.675: INFO: 
STEP: Gathering metrics 08/17/23 06:45:34.677
Aug 17 06:45:35.302: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
Aug 17 06:45:35.305: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 2.284757ms
Aug 17 06:45:35.305: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
Aug 17 06:45:35.305: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
Aug 17 06:45:35.760: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 06:45:35.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9398" for this suite. 08/17/23 06:45:35.763
------------------------------
â€¢ [SLOW TEST] [13.201 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:45:22.566
    Aug 17 06:45:22.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename gc 08/17/23 06:45:22.567
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:22.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:22.591
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/17/23 06:45:22.654
    STEP: delete the rc 08/17/23 06:45:27.664
    STEP: wait for the rc to be deleted 08/17/23 06:45:27.668
    Aug 17 06:45:28.680: INFO: 80 pods remaining
    Aug 17 06:45:28.680: INFO: 80 pods has nil DeletionTimestamp
    Aug 17 06:45:28.680: INFO: 
    Aug 17 06:45:29.682: INFO: 71 pods remaining
    Aug 17 06:45:29.682: INFO: 71 pods has nil DeletionTimestamp
    Aug 17 06:45:29.682: INFO: 
    Aug 17 06:45:30.685: INFO: 60 pods remaining
    Aug 17 06:45:30.685: INFO: 60 pods has nil DeletionTimestamp
    Aug 17 06:45:30.685: INFO: 
    Aug 17 06:45:31.677: INFO: 40 pods remaining
    Aug 17 06:45:31.677: INFO: 40 pods has nil DeletionTimestamp
    Aug 17 06:45:31.677: INFO: 
    Aug 17 06:45:32.695: INFO: 31 pods remaining
    Aug 17 06:45:32.695: INFO: 31 pods has nil DeletionTimestamp
    Aug 17 06:45:32.695: INFO: 
    Aug 17 06:45:33.675: INFO: 20 pods remaining
    Aug 17 06:45:33.675: INFO: 20 pods has nil DeletionTimestamp
    Aug 17 06:45:33.675: INFO: 
    STEP: Gathering metrics 08/17/23 06:45:34.677
    Aug 17 06:45:35.302: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
    Aug 17 06:45:35.305: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 2.284757ms
    Aug 17 06:45:35.305: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
    Aug 17 06:45:35.305: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
    Aug 17 06:45:35.760: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:45:35.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9398" for this suite. 08/17/23 06:45:35.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:45:35.768
Aug 17 06:45:35.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 06:45:35.769
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:35.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:35.779
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-f2707d84-11f4-4c48-894c-11ac844e6517 08/17/23 06:45:35.802
STEP: Creating a pod to test consume configMaps 08/17/23 06:45:35.815
Aug 17 06:45:35.830: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd" in namespace "projected-1140" to be "Succeeded or Failed"
Aug 17 06:45:35.839: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.133273ms
Aug 17 06:45:37.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012757033s
Aug 17 06:45:39.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013040492s
Aug 17 06:45:41.842: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011615974s
Aug 17 06:45:43.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012853167s
STEP: Saw pod success 08/17/23 06:45:43.843
Aug 17 06:45:43.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd" satisfied condition "Succeeded or Failed"
Aug 17 06:45:43.844: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd container agnhost-container: <nil>
STEP: delete the pod 08/17/23 06:45:43.848
Aug 17 06:45:43.853: INFO: Waiting for pod pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd to disappear
Aug 17 06:45:43.855: INFO: Pod pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 06:45:43.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1140" for this suite. 08/17/23 06:45:43.858
------------------------------
â€¢ [SLOW TEST] [8.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:45:35.768
    Aug 17 06:45:35.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 06:45:35.769
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:35.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:35.779
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-f2707d84-11f4-4c48-894c-11ac844e6517 08/17/23 06:45:35.802
    STEP: Creating a pod to test consume configMaps 08/17/23 06:45:35.815
    Aug 17 06:45:35.830: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd" in namespace "projected-1140" to be "Succeeded or Failed"
    Aug 17 06:45:35.839: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.133273ms
    Aug 17 06:45:37.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012757033s
    Aug 17 06:45:39.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013040492s
    Aug 17 06:45:41.842: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011615974s
    Aug 17 06:45:43.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012853167s
    STEP: Saw pod success 08/17/23 06:45:43.843
    Aug 17 06:45:43.843: INFO: Pod "pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd" satisfied condition "Succeeded or Failed"
    Aug 17 06:45:43.844: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 06:45:43.848
    Aug 17 06:45:43.853: INFO: Waiting for pod pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd to disappear
    Aug 17 06:45:43.855: INFO: Pod pod-projected-configmaps-dcb91a25-beca-4b59-9124-14495b2b85dd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:45:43.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1140" for this suite. 08/17/23 06:45:43.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:45:43.863
Aug 17 06:45:43.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 06:45:43.864
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:43.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:43.875
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1112 08/17/23 06:45:43.877
STEP: creating service affinity-clusterip-transition in namespace services-1112 08/17/23 06:45:43.877
STEP: creating replication controller affinity-clusterip-transition in namespace services-1112 08/17/23 06:45:43.954
I0817 06:45:43.958260      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1112, replica count: 3
I0817 06:45:47.009501      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 06:45:50.011655      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 06:45:53.011842      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 06:45:56.012842      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 06:45:56.016: INFO: Creating new exec pod
Aug 17 06:45:56.020: INFO: Waiting up to 5m0s for pod "execpod-affinityhcx6d" in namespace "services-1112" to be "running"
Aug 17 06:45:56.021: INFO: Pod "execpod-affinityhcx6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.872511ms
Aug 17 06:45:58.024: INFO: Pod "execpod-affinityhcx6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003951933s
Aug 17 06:45:58.024: INFO: Pod "execpod-affinityhcx6d" satisfied condition "running"
Aug 17 06:45:59.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 17 06:45:59.152: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 17 06:45:59.152: INFO: stdout: ""
Aug 17 06:45:59.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c nc -v -z -w 2 10.111.20.255 80'
Aug 17 06:45:59.277: INFO: stderr: "+ nc -v -z -w 2 10.111.20.255 80\nConnection to 10.111.20.255 80 port [tcp/http] succeeded!\n"
Aug 17 06:45:59.277: INFO: stdout: ""
Aug 17 06:45:59.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.20.255:80/ ; done'
Aug 17 06:45:59.470: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n"
Aug 17 06:45:59.470: INFO: stdout: "\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-fx5xg"
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
Aug 17 06:45:59.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.20.255:80/ ; done'
Aug 17 06:45:59.652: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n"
Aug 17 06:45:59.653: INFO: stdout: "\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h"
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
Aug 17 06:45:59.653: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1112, will wait for the garbage collector to delete the pods 08/17/23 06:45:59.661
Aug 17 06:45:59.717: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.932851ms
Aug 17 06:45:59.818: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.880858ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 06:46:02.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1112" for this suite. 08/17/23 06:46:02.472
------------------------------
â€¢ [SLOW TEST] [18.612 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:45:43.863
    Aug 17 06:45:43.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 06:45:43.864
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:45:43.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:45:43.875
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1112 08/17/23 06:45:43.877
    STEP: creating service affinity-clusterip-transition in namespace services-1112 08/17/23 06:45:43.877
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1112 08/17/23 06:45:43.954
    I0817 06:45:43.958260      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1112, replica count: 3
    I0817 06:45:47.009501      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0817 06:45:50.011655      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0817 06:45:53.011842      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0817 06:45:56.012842      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 06:45:56.016: INFO: Creating new exec pod
    Aug 17 06:45:56.020: INFO: Waiting up to 5m0s for pod "execpod-affinityhcx6d" in namespace "services-1112" to be "running"
    Aug 17 06:45:56.021: INFO: Pod "execpod-affinityhcx6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.872511ms
    Aug 17 06:45:58.024: INFO: Pod "execpod-affinityhcx6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003951933s
    Aug 17 06:45:58.024: INFO: Pod "execpod-affinityhcx6d" satisfied condition "running"
    Aug 17 06:45:59.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 17 06:45:59.152: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 17 06:45:59.152: INFO: stdout: ""
    Aug 17 06:45:59.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c nc -v -z -w 2 10.111.20.255 80'
    Aug 17 06:45:59.277: INFO: stderr: "+ nc -v -z -w 2 10.111.20.255 80\nConnection to 10.111.20.255 80 port [tcp/http] succeeded!\n"
    Aug 17 06:45:59.277: INFO: stdout: ""
    Aug 17 06:45:59.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.20.255:80/ ; done'
    Aug 17 06:45:59.470: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n"
    Aug 17 06:45:59.470: INFO: stdout: "\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-fx5xg\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-zzfsn\naffinity-clusterip-transition-fx5xg"
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-zzfsn
    Aug 17 06:45:59.470: INFO: Received response from host: affinity-clusterip-transition-fx5xg
    Aug 17 06:45:59.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1112 exec execpod-affinityhcx6d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.20.255:80/ ; done'
    Aug 17 06:45:59.652: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.20.255:80/\n"
    Aug 17 06:45:59.653: INFO: stdout: "\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h\naffinity-clusterip-transition-s258h"
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Received response from host: affinity-clusterip-transition-s258h
    Aug 17 06:45:59.653: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1112, will wait for the garbage collector to delete the pods 08/17/23 06:45:59.661
    Aug 17 06:45:59.717: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.932851ms
    Aug 17 06:45:59.818: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.880858ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:46:02.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1112" for this suite. 08/17/23 06:46:02.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:46:02.476
Aug 17 06:46:02.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename events 08/17/23 06:46:02.476
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:46:02.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:46:02.491
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/17/23 06:46:02.493
STEP: get a list of Events with a label in the current namespace 08/17/23 06:46:02.517
STEP: delete a list of events 08/17/23 06:46:02.52
Aug 17 06:46:02.520: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/17/23 06:46:02.529
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 17 06:46:02.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2859" for this suite. 08/17/23 06:46:02.535
------------------------------
â€¢ [0.063 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:46:02.476
    Aug 17 06:46:02.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename events 08/17/23 06:46:02.476
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:46:02.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:46:02.491
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/17/23 06:46:02.493
    STEP: get a list of Events with a label in the current namespace 08/17/23 06:46:02.517
    STEP: delete a list of events 08/17/23 06:46:02.52
    Aug 17 06:46:02.520: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/17/23 06:46:02.529
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:46:02.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2859" for this suite. 08/17/23 06:46:02.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:46:02.54
Aug 17 06:46:02.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-preemption 08/17/23 06:46:02.541
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:46:02.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:46:02.562
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 06:46:02.583: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 06:47:02.633: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:47:02.634
Aug 17 06:47:02.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 06:47:02.635
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:02.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:02.653
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 17 06:47:02.671: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 17 06:47:02.674: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 17 06:47:02.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 06:47:02.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5600" for this suite. 08/17/23 06:47:02.721
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3298" for this suite. 08/17/23 06:47:02.725
------------------------------
â€¢ [SLOW TEST] [60.189 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:46:02.54
    Aug 17 06:46:02.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 06:46:02.541
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:46:02.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:46:02.562
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 06:46:02.583: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 06:47:02.633: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:47:02.634
    Aug 17 06:47:02.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 06:47:02.635
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:02.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:02.653
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 17 06:47:02.671: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 17 06:47:02.674: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:47:02.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:47:02.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5600" for this suite. 08/17/23 06:47:02.721
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3298" for this suite. 08/17/23 06:47:02.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:47:02.73
Aug 17 06:47:02.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 06:47:02.73
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:02.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:02.743
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/17/23 06:47:02.745
Aug 17 06:47:02.758: INFO: Waiting up to 5m0s for pod "pod-ttdr6" in namespace "pods-9793" to be "running"
Aug 17 06:47:02.760: INFO: Pod "pod-ttdr6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980453ms
Aug 17 06:47:04.762: INFO: Pod "pod-ttdr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.003818568s
Aug 17 06:47:04.762: INFO: Pod "pod-ttdr6" satisfied condition "running"
STEP: patching /status 08/17/23 06:47:04.762
Aug 17 06:47:04.767: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 06:47:04.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9793" for this suite. 08/17/23 06:47:04.77
------------------------------
â€¢ [2.043 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:47:02.73
    Aug 17 06:47:02.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 06:47:02.73
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:02.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:02.743
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/17/23 06:47:02.745
    Aug 17 06:47:02.758: INFO: Waiting up to 5m0s for pod "pod-ttdr6" in namespace "pods-9793" to be "running"
    Aug 17 06:47:02.760: INFO: Pod "pod-ttdr6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980453ms
    Aug 17 06:47:04.762: INFO: Pod "pod-ttdr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.003818568s
    Aug 17 06:47:04.762: INFO: Pod "pod-ttdr6" satisfied condition "running"
    STEP: patching /status 08/17/23 06:47:04.762
    Aug 17 06:47:04.767: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:47:04.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9793" for this suite. 08/17/23 06:47:04.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:47:04.775
Aug 17 06:47:04.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 06:47:04.776
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:04.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:04.787
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/17/23 06:47:04.789
Aug 17 06:47:04.811: INFO: Waiting up to 5m0s for pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6" in namespace "projected-2026" to be "running and ready"
Aug 17 06:47:04.814: INFO: Pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068067ms
Aug 17 06:47:04.814: INFO: The phase of Pod labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 06:47:06.826: INFO: Pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014542885s
Aug 17 06:47:06.826: INFO: The phase of Pod labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6 is Running (Ready = true)
Aug 17 06:47:06.826: INFO: Pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6" satisfied condition "running and ready"
Aug 17 06:47:07.340: INFO: Successfully updated pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 06:47:11.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2026" for this suite. 08/17/23 06:47:11.358
------------------------------
â€¢ [SLOW TEST] [6.587 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:47:04.775
    Aug 17 06:47:04.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 06:47:04.776
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:04.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:04.787
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/17/23 06:47:04.789
    Aug 17 06:47:04.811: INFO: Waiting up to 5m0s for pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6" in namespace "projected-2026" to be "running and ready"
    Aug 17 06:47:04.814: INFO: Pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068067ms
    Aug 17 06:47:04.814: INFO: The phase of Pod labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 06:47:06.826: INFO: Pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014542885s
    Aug 17 06:47:06.826: INFO: The phase of Pod labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6 is Running (Ready = true)
    Aug 17 06:47:06.826: INFO: Pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6" satisfied condition "running and ready"
    Aug 17 06:47:07.340: INFO: Successfully updated pod "labelsupdate3967624d-8479-4e0c-a7f2-b5b2ac689fc6"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:47:11.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2026" for this suite. 08/17/23 06:47:11.358
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:47:11.363
Aug 17 06:47:11.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename ingress 08/17/23 06:47:11.363
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:11.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:11.398
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/17/23 06:47:11.401
STEP: getting /apis/networking.k8s.io 08/17/23 06:47:11.403
STEP: getting /apis/networking.k8s.iov1 08/17/23 06:47:11.404
STEP: creating 08/17/23 06:47:11.405
STEP: getting 08/17/23 06:47:11.42
STEP: listing 08/17/23 06:47:11.424
STEP: watching 08/17/23 06:47:11.426
Aug 17 06:47:11.427: INFO: starting watch
STEP: cluster-wide listing 08/17/23 06:47:11.428
STEP: cluster-wide watching 08/17/23 06:47:11.433
Aug 17 06:47:11.433: INFO: starting watch
STEP: patching 08/17/23 06:47:11.434
STEP: updating 08/17/23 06:47:11.438
Aug 17 06:47:11.444: INFO: waiting for watch events with expected annotations
Aug 17 06:47:11.444: INFO: saw patched and updated annotations
STEP: patching /status 08/17/23 06:47:11.444
STEP: updating /status 08/17/23 06:47:11.449
STEP: get /status 08/17/23 06:47:11.455
STEP: deleting 08/17/23 06:47:11.458
STEP: deleting a collection 08/17/23 06:47:11.467
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 17 06:47:11.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-9798" for this suite. 08/17/23 06:47:11.478
------------------------------
â€¢ [0.120 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:47:11.363
    Aug 17 06:47:11.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename ingress 08/17/23 06:47:11.363
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:11.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:11.398
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/17/23 06:47:11.401
    STEP: getting /apis/networking.k8s.io 08/17/23 06:47:11.403
    STEP: getting /apis/networking.k8s.iov1 08/17/23 06:47:11.404
    STEP: creating 08/17/23 06:47:11.405
    STEP: getting 08/17/23 06:47:11.42
    STEP: listing 08/17/23 06:47:11.424
    STEP: watching 08/17/23 06:47:11.426
    Aug 17 06:47:11.427: INFO: starting watch
    STEP: cluster-wide listing 08/17/23 06:47:11.428
    STEP: cluster-wide watching 08/17/23 06:47:11.433
    Aug 17 06:47:11.433: INFO: starting watch
    STEP: patching 08/17/23 06:47:11.434
    STEP: updating 08/17/23 06:47:11.438
    Aug 17 06:47:11.444: INFO: waiting for watch events with expected annotations
    Aug 17 06:47:11.444: INFO: saw patched and updated annotations
    STEP: patching /status 08/17/23 06:47:11.444
    STEP: updating /status 08/17/23 06:47:11.449
    STEP: get /status 08/17/23 06:47:11.455
    STEP: deleting 08/17/23 06:47:11.458
    STEP: deleting a collection 08/17/23 06:47:11.467
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:47:11.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-9798" for this suite. 08/17/23 06:47:11.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:47:11.483
Aug 17 06:47:11.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 06:47:11.484
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:11.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:11.504
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/17/23 06:47:11.507
Aug 17 06:47:11.517: INFO: Waiting up to 2m0s for pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" in namespace "var-expansion-8038" to be "running"
Aug 17 06:47:11.519: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119289ms
Aug 17 06:47:13.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004781238s
Aug 17 06:47:15.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004622526s
Aug 17 06:47:17.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004268795s
Aug 17 06:47:19.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004587527s
Aug 17 06:47:21.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004674839s
Aug 17 06:47:23.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004801002s
Aug 17 06:47:25.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005694924s
Aug 17 06:47:27.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004583494s
Aug 17 06:47:29.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004829182s
Aug 17 06:47:31.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005669204s
Aug 17 06:47:33.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00494101s
Aug 17 06:47:35.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006726765s
Aug 17 06:47:37.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004776577s
Aug 17 06:47:39.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005178248s
Aug 17 06:47:41.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005468838s
Aug 17 06:47:43.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004663054s
Aug 17 06:47:45.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006083871s
Aug 17 06:47:47.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004135877s
Aug 17 06:47:49.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005077161s
Aug 17 06:47:51.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00538138s
Aug 17 06:47:53.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004817347s
Aug 17 06:47:55.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006636713s
Aug 17 06:47:57.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004737185s
Aug 17 06:47:59.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004703268s
Aug 17 06:48:01.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005577691s
Aug 17 06:48:03.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00486134s
Aug 17 06:48:05.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005787297s
Aug 17 06:48:07.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00545004s
Aug 17 06:48:09.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006409009s
Aug 17 06:48:11.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005471019s
Aug 17 06:48:13.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005237636s
Aug 17 06:48:15.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004901834s
Aug 17 06:48:17.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004107864s
Aug 17 06:48:19.524: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006944675s
Aug 17 06:48:21.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.00581963s
Aug 17 06:48:23.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006475919s
Aug 17 06:48:25.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005175789s
Aug 17 06:48:27.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004611769s
Aug 17 06:48:29.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004794592s
Aug 17 06:48:31.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006087953s
Aug 17 06:48:33.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004301279s
Aug 17 06:48:35.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004789215s
Aug 17 06:48:37.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004298856s
Aug 17 06:48:39.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00480727s
Aug 17 06:48:41.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004608725s
Aug 17 06:48:43.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.003989468s
Aug 17 06:48:45.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004850153s
Aug 17 06:48:47.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005673266s
Aug 17 06:48:49.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00637153s
Aug 17 06:48:51.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005059866s
Aug 17 06:48:53.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005520914s
Aug 17 06:48:55.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005005181s
Aug 17 06:48:57.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00464216s
Aug 17 06:48:59.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004998378s
Aug 17 06:49:01.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004699733s
Aug 17 06:49:03.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00464174s
Aug 17 06:49:05.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004437195s
Aug 17 06:49:07.580: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.06357912s
Aug 17 06:49:09.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004714394s
Aug 17 06:49:11.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005282125s
Aug 17 06:49:11.524: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007527761s
STEP: updating the pod 08/17/23 06:49:11.524
Aug 17 06:49:12.035: INFO: Successfully updated pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd"
STEP: waiting for pod running 08/17/23 06:49:12.035
Aug 17 06:49:12.035: INFO: Waiting up to 2m0s for pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" in namespace "var-expansion-8038" to be "running"
Aug 17 06:49:12.037: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103313ms
Aug 17 06:49:14.041: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005870895s
Aug 17 06:49:14.041: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" satisfied condition "running"
STEP: deleting the pod gracefully 08/17/23 06:49:14.041
Aug 17 06:49:14.041: INFO: Deleting pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" in namespace "var-expansion-8038"
Aug 17 06:49:14.045: INFO: Wait up to 5m0s for pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 06:49:46.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8038" for this suite. 08/17/23 06:49:46.054
------------------------------
â€¢ [SLOW TEST] [154.575 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:47:11.483
    Aug 17 06:47:11.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 06:47:11.484
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:47:11.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:47:11.504
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/17/23 06:47:11.507
    Aug 17 06:47:11.517: INFO: Waiting up to 2m0s for pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" in namespace "var-expansion-8038" to be "running"
    Aug 17 06:47:11.519: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119289ms
    Aug 17 06:47:13.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004781238s
    Aug 17 06:47:15.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004622526s
    Aug 17 06:47:17.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004268795s
    Aug 17 06:47:19.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004587527s
    Aug 17 06:47:21.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004674839s
    Aug 17 06:47:23.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004801002s
    Aug 17 06:47:25.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005694924s
    Aug 17 06:47:27.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004583494s
    Aug 17 06:47:29.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004829182s
    Aug 17 06:47:31.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005669204s
    Aug 17 06:47:33.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00494101s
    Aug 17 06:47:35.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006726765s
    Aug 17 06:47:37.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004776577s
    Aug 17 06:47:39.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005178248s
    Aug 17 06:47:41.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005468838s
    Aug 17 06:47:43.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004663054s
    Aug 17 06:47:45.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006083871s
    Aug 17 06:47:47.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004135877s
    Aug 17 06:47:49.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005077161s
    Aug 17 06:47:51.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00538138s
    Aug 17 06:47:53.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004817347s
    Aug 17 06:47:55.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006636713s
    Aug 17 06:47:57.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004737185s
    Aug 17 06:47:59.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004703268s
    Aug 17 06:48:01.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005577691s
    Aug 17 06:48:03.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00486134s
    Aug 17 06:48:05.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005787297s
    Aug 17 06:48:07.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00545004s
    Aug 17 06:48:09.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006409009s
    Aug 17 06:48:11.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005471019s
    Aug 17 06:48:13.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005237636s
    Aug 17 06:48:15.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004901834s
    Aug 17 06:48:17.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004107864s
    Aug 17 06:48:19.524: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006944675s
    Aug 17 06:48:21.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.00581963s
    Aug 17 06:48:23.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006475919s
    Aug 17 06:48:25.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005175789s
    Aug 17 06:48:27.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004611769s
    Aug 17 06:48:29.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004794592s
    Aug 17 06:48:31.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006087953s
    Aug 17 06:48:33.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004301279s
    Aug 17 06:48:35.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004789215s
    Aug 17 06:48:37.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004298856s
    Aug 17 06:48:39.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00480727s
    Aug 17 06:48:41.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004608725s
    Aug 17 06:48:43.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.003989468s
    Aug 17 06:48:45.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004850153s
    Aug 17 06:48:47.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005673266s
    Aug 17 06:48:49.523: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00637153s
    Aug 17 06:48:51.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005059866s
    Aug 17 06:48:53.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005520914s
    Aug 17 06:48:55.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005005181s
    Aug 17 06:48:57.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00464216s
    Aug 17 06:48:59.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004998378s
    Aug 17 06:49:01.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004699733s
    Aug 17 06:49:03.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00464174s
    Aug 17 06:49:05.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004437195s
    Aug 17 06:49:07.580: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.06357912s
    Aug 17 06:49:09.521: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004714394s
    Aug 17 06:49:11.522: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005282125s
    Aug 17 06:49:11.524: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007527761s
    STEP: updating the pod 08/17/23 06:49:11.524
    Aug 17 06:49:12.035: INFO: Successfully updated pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd"
    STEP: waiting for pod running 08/17/23 06:49:12.035
    Aug 17 06:49:12.035: INFO: Waiting up to 2m0s for pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" in namespace "var-expansion-8038" to be "running"
    Aug 17 06:49:12.037: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103313ms
    Aug 17 06:49:14.041: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005870895s
    Aug 17 06:49:14.041: INFO: Pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" satisfied condition "running"
    STEP: deleting the pod gracefully 08/17/23 06:49:14.041
    Aug 17 06:49:14.041: INFO: Deleting pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" in namespace "var-expansion-8038"
    Aug 17 06:49:14.045: INFO: Wait up to 5m0s for pod "var-expansion-a920553a-613a-40c0-a6c6-4d46044d0dbd" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:49:46.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8038" for this suite. 08/17/23 06:49:46.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:49:46.059
Aug 17 06:49:46.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 06:49:46.06
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:46.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:46.077
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/17/23 06:49:46.118
Aug 17 06:49:46.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6" in namespace "projected-124" to be "Succeeded or Failed"
Aug 17 06:49:46.143: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034156ms
Aug 17 06:49:48.147: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006083066s
Aug 17 06:49:50.145: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004712806s
STEP: Saw pod success 08/17/23 06:49:50.145
Aug 17 06:49:50.145: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6" satisfied condition "Succeeded or Failed"
Aug 17 06:49:50.147: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6 container client-container: <nil>
STEP: delete the pod 08/17/23 06:49:50.156
Aug 17 06:49:50.162: INFO: Waiting for pod downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6 to disappear
Aug 17 06:49:50.164: INFO: Pod downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 06:49:50.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-124" for this suite. 08/17/23 06:49:50.167
------------------------------
â€¢ [4.110 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:49:46.059
    Aug 17 06:49:46.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 06:49:46.06
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:46.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:46.077
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/17/23 06:49:46.118
    Aug 17 06:49:46.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6" in namespace "projected-124" to be "Succeeded or Failed"
    Aug 17 06:49:46.143: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034156ms
    Aug 17 06:49:48.147: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006083066s
    Aug 17 06:49:50.145: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004712806s
    STEP: Saw pod success 08/17/23 06:49:50.145
    Aug 17 06:49:50.145: INFO: Pod "downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6" satisfied condition "Succeeded or Failed"
    Aug 17 06:49:50.147: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6 container client-container: <nil>
    STEP: delete the pod 08/17/23 06:49:50.156
    Aug 17 06:49:50.162: INFO: Waiting for pod downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6 to disappear
    Aug 17 06:49:50.164: INFO: Pod downwardapi-volume-89055808-3922-47e2-b228-4df7c24215f6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:49:50.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-124" for this suite. 08/17/23 06:49:50.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:49:50.17
Aug 17 06:49:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename endpointslice 08/17/23 06:49:50.17
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:50.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:50.182
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 17 06:49:50.201: INFO: Endpoints addresses: [10.60.200.175] , ports: [6443]
Aug 17 06:49:50.201: INFO: EndpointSlices addresses: [10.60.200.175] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 06:49:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6049" for this suite. 08/17/23 06:49:50.208
------------------------------
â€¢ [0.041 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:49:50.17
    Aug 17 06:49:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename endpointslice 08/17/23 06:49:50.17
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:50.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:50.182
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 17 06:49:50.201: INFO: Endpoints addresses: [10.60.200.175] , ports: [6443]
    Aug 17 06:49:50.201: INFO: EndpointSlices addresses: [10.60.200.175] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:49:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6049" for this suite. 08/17/23 06:49:50.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:49:50.211
Aug 17 06:49:50.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replication-controller 08/17/23 06:49:50.212
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:50.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:50.235
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-jwl52" 08/17/23 06:49:50.237
Aug 17 06:49:50.245: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
Aug 17 06:49:51.248: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
Aug 17 06:49:51.250: INFO: Found 1 replicas for "e2e-rc-jwl52" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-jwl52" 08/17/23 06:49:51.25
STEP: Updating a scale subresource 08/17/23 06:49:51.252
STEP: Verifying replicas where modified for replication controller "e2e-rc-jwl52" 08/17/23 06:49:51.255
Aug 17 06:49:51.255: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
Aug 17 06:49:52.257: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
Aug 17 06:49:52.259: INFO: Found 2 replicas for "e2e-rc-jwl52" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 06:49:52.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1732" for this suite. 08/17/23 06:49:52.261
------------------------------
â€¢ [2.053 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:49:50.211
    Aug 17 06:49:50.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replication-controller 08/17/23 06:49:50.212
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:50.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:50.235
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-jwl52" 08/17/23 06:49:50.237
    Aug 17 06:49:50.245: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
    Aug 17 06:49:51.248: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
    Aug 17 06:49:51.250: INFO: Found 1 replicas for "e2e-rc-jwl52" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-jwl52" 08/17/23 06:49:51.25
    STEP: Updating a scale subresource 08/17/23 06:49:51.252
    STEP: Verifying replicas where modified for replication controller "e2e-rc-jwl52" 08/17/23 06:49:51.255
    Aug 17 06:49:51.255: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
    Aug 17 06:49:52.257: INFO: Get Replication Controller "e2e-rc-jwl52" to confirm replicas
    Aug 17 06:49:52.259: INFO: Found 2 replicas for "e2e-rc-jwl52" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:49:52.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1732" for this suite. 08/17/23 06:49:52.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:49:52.265
Aug 17 06:49:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 06:49:52.265
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:52.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:52.326
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 06:49:52.328
Aug 17 06:49:52.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-2760 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 17 06:49:52.437: INFO: stderr: ""
Aug 17 06:49:52.437: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/17/23 06:49:52.437
Aug 17 06:49:52.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-2760 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 17 06:49:54.234: INFO: stderr: ""
Aug 17 06:49:54.234: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 06:49:54.234
Aug 17 06:49:54.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-2760 delete pods e2e-test-httpd-pod'
Aug 17 06:49:59.170: INFO: stderr: ""
Aug 17 06:49:59.170: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 06:49:59.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2760" for this suite. 08/17/23 06:49:59.173
------------------------------
â€¢ [SLOW TEST] [6.911 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:49:52.265
    Aug 17 06:49:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 06:49:52.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:52.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:52.326
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 06:49:52.328
    Aug 17 06:49:52.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-2760 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 17 06:49:52.437: INFO: stderr: ""
    Aug 17 06:49:52.437: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/17/23 06:49:52.437
    Aug 17 06:49:52.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-2760 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 17 06:49:54.234: INFO: stderr: ""
    Aug 17 06:49:54.234: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 06:49:54.234
    Aug 17 06:49:54.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-2760 delete pods e2e-test-httpd-pod'
    Aug 17 06:49:59.170: INFO: stderr: ""
    Aug 17 06:49:59.170: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:49:59.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2760" for this suite. 08/17/23 06:49:59.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:49:59.177
Aug 17 06:49:59.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 06:49:59.177
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:59.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:59.187
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-9385 08/17/23 06:49:59.189
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[] 08/17/23 06:49:59.261
Aug 17 06:49:59.264: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 17 06:50:00.304: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9385 08/17/23 06:50:00.304
Aug 17 06:50:00.308: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9385" to be "running and ready"
Aug 17 06:50:00.309: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.431435ms
Aug 17 06:50:00.309: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 06:50:02.312: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004287805s
Aug 17 06:50:02.312: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 17 06:50:02.312: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[pod1:[80]] 08/17/23 06:50:02.315
Aug 17 06:50:02.320: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/17/23 06:50:02.321
Aug 17 06:50:02.321: INFO: Creating new exec pod
Aug 17 06:50:02.323: INFO: Waiting up to 5m0s for pod "execpodnrqhc" in namespace "services-9385" to be "running"
Aug 17 06:50:02.325: INFO: Pod "execpodnrqhc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.517802ms
Aug 17 06:50:04.327: INFO: Pod "execpodnrqhc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003334231s
Aug 17 06:50:04.327: INFO: Pod "execpodnrqhc" satisfied condition "running"
Aug 17 06:50:05.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 17 06:50:05.452: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 17 06:50:05.452: INFO: stdout: ""
Aug 17 06:50:05.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 10.106.64.103 80'
Aug 17 06:50:05.586: INFO: stderr: "+ nc -v -z -w 2 10.106.64.103 80\nConnection to 10.106.64.103 80 port [tcp/http] succeeded!\n"
Aug 17 06:50:05.586: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-9385 08/17/23 06:50:05.586
Aug 17 06:50:05.590: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9385" to be "running and ready"
Aug 17 06:50:05.592: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089333ms
Aug 17 06:50:05.592: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 06:50:07.594: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004424305s
Aug 17 06:50:07.594: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 17 06:50:07.594: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[pod1:[80] pod2:[80]] 08/17/23 06:50:07.596
Aug 17 06:50:07.602: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/17/23 06:50:07.602
Aug 17 06:50:08.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 17 06:50:08.726: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 17 06:50:08.726: INFO: stdout: ""
Aug 17 06:50:08.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 10.106.64.103 80'
Aug 17 06:50:08.847: INFO: stderr: "+ nc -v -z -w 2 10.106.64.103 80\nConnection to 10.106.64.103 80 port [tcp/http] succeeded!\n"
Aug 17 06:50:08.847: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9385 08/17/23 06:50:08.847
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[pod2:[80]] 08/17/23 06:50:08.856
Aug 17 06:50:08.865: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/17/23 06:50:08.865
Aug 17 06:50:09.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 17 06:50:10.005: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 17 06:50:10.005: INFO: stdout: ""
Aug 17 06:50:10.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 10.106.64.103 80'
Aug 17 06:50:10.132: INFO: stderr: "+ nc -v -z -w 2 10.106.64.103 80\nConnection to 10.106.64.103 80 port [tcp/http] succeeded!\n"
Aug 17 06:50:10.132: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-9385 08/17/23 06:50:10.132
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[] 08/17/23 06:50:10.142
Aug 17 06:50:10.148: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:10.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9385" for this suite. 08/17/23 06:50:10.168
------------------------------
â€¢ [SLOW TEST] [10.997 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:49:59.177
    Aug 17 06:49:59.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 06:49:59.177
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:49:59.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:49:59.187
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-9385 08/17/23 06:49:59.189
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[] 08/17/23 06:49:59.261
    Aug 17 06:49:59.264: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Aug 17 06:50:00.304: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9385 08/17/23 06:50:00.304
    Aug 17 06:50:00.308: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9385" to be "running and ready"
    Aug 17 06:50:00.309: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.431435ms
    Aug 17 06:50:00.309: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 06:50:02.312: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004287805s
    Aug 17 06:50:02.312: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 17 06:50:02.312: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[pod1:[80]] 08/17/23 06:50:02.315
    Aug 17 06:50:02.320: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/17/23 06:50:02.321
    Aug 17 06:50:02.321: INFO: Creating new exec pod
    Aug 17 06:50:02.323: INFO: Waiting up to 5m0s for pod "execpodnrqhc" in namespace "services-9385" to be "running"
    Aug 17 06:50:02.325: INFO: Pod "execpodnrqhc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.517802ms
    Aug 17 06:50:04.327: INFO: Pod "execpodnrqhc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003334231s
    Aug 17 06:50:04.327: INFO: Pod "execpodnrqhc" satisfied condition "running"
    Aug 17 06:50:05.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 17 06:50:05.452: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 17 06:50:05.452: INFO: stdout: ""
    Aug 17 06:50:05.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 10.106.64.103 80'
    Aug 17 06:50:05.586: INFO: stderr: "+ nc -v -z -w 2 10.106.64.103 80\nConnection to 10.106.64.103 80 port [tcp/http] succeeded!\n"
    Aug 17 06:50:05.586: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-9385 08/17/23 06:50:05.586
    Aug 17 06:50:05.590: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9385" to be "running and ready"
    Aug 17 06:50:05.592: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089333ms
    Aug 17 06:50:05.592: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 06:50:07.594: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004424305s
    Aug 17 06:50:07.594: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 17 06:50:07.594: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[pod1:[80] pod2:[80]] 08/17/23 06:50:07.596
    Aug 17 06:50:07.602: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/17/23 06:50:07.602
    Aug 17 06:50:08.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 17 06:50:08.726: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 17 06:50:08.726: INFO: stdout: ""
    Aug 17 06:50:08.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 10.106.64.103 80'
    Aug 17 06:50:08.847: INFO: stderr: "+ nc -v -z -w 2 10.106.64.103 80\nConnection to 10.106.64.103 80 port [tcp/http] succeeded!\n"
    Aug 17 06:50:08.847: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9385 08/17/23 06:50:08.847
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[pod2:[80]] 08/17/23 06:50:08.856
    Aug 17 06:50:08.865: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/17/23 06:50:08.865
    Aug 17 06:50:09.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 17 06:50:10.005: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 17 06:50:10.005: INFO: stdout: ""
    Aug 17 06:50:10.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9385 exec execpodnrqhc -- /bin/sh -x -c nc -v -z -w 2 10.106.64.103 80'
    Aug 17 06:50:10.132: INFO: stderr: "+ nc -v -z -w 2 10.106.64.103 80\nConnection to 10.106.64.103 80 port [tcp/http] succeeded!\n"
    Aug 17 06:50:10.132: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-9385 08/17/23 06:50:10.132
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9385 to expose endpoints map[] 08/17/23 06:50:10.142
    Aug 17 06:50:10.148: INFO: successfully validated that service endpoint-test2 in namespace services-9385 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:10.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9385" for this suite. 08/17/23 06:50:10.168
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:10.174
Aug 17 06:50:10.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename security-context-test 08/17/23 06:50:10.175
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:10.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:10.187
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 17 06:50:10.197: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34" in namespace "security-context-test-7771" to be "Succeeded or Failed"
Aug 17 06:50:10.201: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798439ms
Aug 17 06:50:12.203: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006341377s
Aug 17 06:50:14.205: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008216172s
Aug 17 06:50:16.204: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00695544s
Aug 17 06:50:18.204: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007444897s
Aug 17 06:50:18.204: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:18.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7771" for this suite. 08/17/23 06:50:18.21
------------------------------
â€¢ [SLOW TEST] [8.039 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:10.174
    Aug 17 06:50:10.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename security-context-test 08/17/23 06:50:10.175
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:10.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:10.187
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 17 06:50:10.197: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34" in namespace "security-context-test-7771" to be "Succeeded or Failed"
    Aug 17 06:50:10.201: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798439ms
    Aug 17 06:50:12.203: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006341377s
    Aug 17 06:50:14.205: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008216172s
    Aug 17 06:50:16.204: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00695544s
    Aug 17 06:50:18.204: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007444897s
    Aug 17 06:50:18.204: INFO: Pod "alpine-nnp-false-f81ffd33-8961-4725-88fa-7a20fd2afe34" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:18.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7771" for this suite. 08/17/23 06:50:18.21
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:18.215
Aug 17 06:50:18.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replicaset 08/17/23 06:50:18.216
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:18.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:18.232
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 17 06:50:18.256: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 06:50:23.260: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 06:50:23.26
STEP: Scaling up "test-rs" replicaset  08/17/23 06:50:23.26
Aug 17 06:50:23.265: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/17/23 06:50:23.265
W0817 06:50:23.273200      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 17 06:50:23.274: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 06:50:23.278: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 06:50:23.286: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 06:50:23.290: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 06:50:24.160: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 2, AvailableReplicas 2
Aug 17 06:50:31.364: INFO: observed Replicaset test-rs in namespace replicaset-4315 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:31.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4315" for this suite. 08/17/23 06:50:31.366
------------------------------
â€¢ [SLOW TEST] [13.157 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:18.215
    Aug 17 06:50:18.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replicaset 08/17/23 06:50:18.216
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:18.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:18.232
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 17 06:50:18.256: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 06:50:23.260: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 06:50:23.26
    STEP: Scaling up "test-rs" replicaset  08/17/23 06:50:23.26
    Aug 17 06:50:23.265: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/17/23 06:50:23.265
    W0817 06:50:23.273200      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 17 06:50:23.274: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 06:50:23.278: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 06:50:23.286: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 06:50:23.290: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 06:50:24.160: INFO: observed ReplicaSet test-rs in namespace replicaset-4315 with ReadyReplicas 2, AvailableReplicas 2
    Aug 17 06:50:31.364: INFO: observed Replicaset test-rs in namespace replicaset-4315 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:31.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4315" for this suite. 08/17/23 06:50:31.366
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:31.372
Aug 17 06:50:31.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 06:50:31.373
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:31.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:31.384
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 17 06:50:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/17/23 06:50:33.388
Aug 17 06:50:33.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
Aug 17 06:50:34.678: INFO: stderr: ""
Aug 17 06:50:34.678: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 17 06:50:34.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 delete e2e-test-crd-publish-openapi-5099-crds test-foo'
Aug 17 06:50:34.761: INFO: stderr: ""
Aug 17 06:50:34.761: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 17 06:50:34.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 apply -f -'
Aug 17 06:50:35.053: INFO: stderr: ""
Aug 17 06:50:35.053: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 17 06:50:35.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 delete e2e-test-crd-publish-openapi-5099-crds test-foo'
Aug 17 06:50:35.134: INFO: stderr: ""
Aug 17 06:50:35.134: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/17/23 06:50:35.134
Aug 17 06:50:35.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
Aug 17 06:50:35.428: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/17/23 06:50:35.428
Aug 17 06:50:35.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
Aug 17 06:50:35.722: INFO: rc: 1
Aug 17 06:50:35.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 apply -f -'
Aug 17 06:50:36.023: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/17/23 06:50:36.023
Aug 17 06:50:36.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
Aug 17 06:50:36.322: INFO: rc: 1
Aug 17 06:50:36.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 apply -f -'
Aug 17 06:50:36.622: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/17/23 06:50:36.622
Aug 17 06:50:36.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds'
Aug 17 06:50:37.016: INFO: stderr: ""
Aug 17 06:50:37.016: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/17/23 06:50:37.016
Aug 17 06:50:37.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.metadata'
Aug 17 06:50:37.320: INFO: stderr: ""
Aug 17 06:50:37.320: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 17 06:50:37.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.spec'
Aug 17 06:50:37.625: INFO: stderr: ""
Aug 17 06:50:37.625: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 17 06:50:37.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.spec.bars'
Aug 17 06:50:37.922: INFO: stderr: ""
Aug 17 06:50:37.922: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/17/23 06:50:37.922
Aug 17 06:50:37.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.spec.bars2'
Aug 17 06:50:38.227: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:40.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8598" for this suite. 08/17/23 06:50:40.234
------------------------------
â€¢ [SLOW TEST] [8.866 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:31.372
    Aug 17 06:50:31.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 06:50:31.373
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:31.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:31.384
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 17 06:50:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/17/23 06:50:33.388
    Aug 17 06:50:33.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
    Aug 17 06:50:34.678: INFO: stderr: ""
    Aug 17 06:50:34.678: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 17 06:50:34.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 delete e2e-test-crd-publish-openapi-5099-crds test-foo'
    Aug 17 06:50:34.761: INFO: stderr: ""
    Aug 17 06:50:34.761: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 17 06:50:34.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 apply -f -'
    Aug 17 06:50:35.053: INFO: stderr: ""
    Aug 17 06:50:35.053: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 17 06:50:35.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 delete e2e-test-crd-publish-openapi-5099-crds test-foo'
    Aug 17 06:50:35.134: INFO: stderr: ""
    Aug 17 06:50:35.134: INFO: stdout: "e2e-test-crd-publish-openapi-5099-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/17/23 06:50:35.134
    Aug 17 06:50:35.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
    Aug 17 06:50:35.428: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/17/23 06:50:35.428
    Aug 17 06:50:35.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
    Aug 17 06:50:35.722: INFO: rc: 1
    Aug 17 06:50:35.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 apply -f -'
    Aug 17 06:50:36.023: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/17/23 06:50:36.023
    Aug 17 06:50:36.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 create -f -'
    Aug 17 06:50:36.322: INFO: rc: 1
    Aug 17 06:50:36.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 --namespace=crd-publish-openapi-8598 apply -f -'
    Aug 17 06:50:36.622: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/17/23 06:50:36.622
    Aug 17 06:50:36.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds'
    Aug 17 06:50:37.016: INFO: stderr: ""
    Aug 17 06:50:37.016: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/17/23 06:50:37.016
    Aug 17 06:50:37.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.metadata'
    Aug 17 06:50:37.320: INFO: stderr: ""
    Aug 17 06:50:37.320: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 17 06:50:37.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.spec'
    Aug 17 06:50:37.625: INFO: stderr: ""
    Aug 17 06:50:37.625: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 17 06:50:37.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.spec.bars'
    Aug 17 06:50:37.922: INFO: stderr: ""
    Aug 17 06:50:37.922: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5099-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/17/23 06:50:37.922
    Aug 17 06:50:37.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-8598 explain e2e-test-crd-publish-openapi-5099-crds.spec.bars2'
    Aug 17 06:50:38.227: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:40.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8598" for this suite. 08/17/23 06:50:40.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:40.24
Aug 17 06:50:40.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svcaccounts 08/17/23 06:50:40.241
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:40.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:40.268
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/17/23 06:50:40.271
Aug 17 06:50:40.290: INFO: Waiting up to 5m0s for pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c" in namespace "svcaccounts-7362" to be "Succeeded or Failed"
Aug 17 06:50:40.295: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.731717ms
Aug 17 06:50:42.298: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007732415s
Aug 17 06:50:44.297: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007012416s
STEP: Saw pod success 08/17/23 06:50:44.297
Aug 17 06:50:44.297: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c" satisfied condition "Succeeded or Failed"
Aug 17 06:50:44.299: INFO: Trying to get logs from node yst-node2 pod test-pod-117633b5-179a-4668-8e04-0035ab65904c container agnhost-container: <nil>
STEP: delete the pod 08/17/23 06:50:44.303
Aug 17 06:50:44.342: INFO: Waiting for pod test-pod-117633b5-179a-4668-8e04-0035ab65904c to disappear
Aug 17 06:50:44.343: INFO: Pod test-pod-117633b5-179a-4668-8e04-0035ab65904c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:44.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7362" for this suite. 08/17/23 06:50:44.346
------------------------------
â€¢ [4.109 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:40.24
    Aug 17 06:50:40.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 06:50:40.241
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:40.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:40.268
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/17/23 06:50:40.271
    Aug 17 06:50:40.290: INFO: Waiting up to 5m0s for pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c" in namespace "svcaccounts-7362" to be "Succeeded or Failed"
    Aug 17 06:50:40.295: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.731717ms
    Aug 17 06:50:42.298: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007732415s
    Aug 17 06:50:44.297: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007012416s
    STEP: Saw pod success 08/17/23 06:50:44.297
    Aug 17 06:50:44.297: INFO: Pod "test-pod-117633b5-179a-4668-8e04-0035ab65904c" satisfied condition "Succeeded or Failed"
    Aug 17 06:50:44.299: INFO: Trying to get logs from node yst-node2 pod test-pod-117633b5-179a-4668-8e04-0035ab65904c container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 06:50:44.303
    Aug 17 06:50:44.342: INFO: Waiting for pod test-pod-117633b5-179a-4668-8e04-0035ab65904c to disappear
    Aug 17 06:50:44.343: INFO: Pod test-pod-117633b5-179a-4668-8e04-0035ab65904c no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:44.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7362" for this suite. 08/17/23 06:50:44.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:44.352
Aug 17 06:50:44.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 06:50:44.353
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:44.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:44.373
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 06:50:44.375
Aug 17 06:50:44.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3361 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 17 06:50:44.451: INFO: stderr: ""
Aug 17 06:50:44.451: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 06:50:44.451
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 17 06:50:44.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3361 delete pods e2e-test-httpd-pod'
Aug 17 06:50:46.302: INFO: stderr: ""
Aug 17 06:50:46.302: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:46.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3361" for this suite. 08/17/23 06:50:46.305
------------------------------
â€¢ [1.956 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:44.352
    Aug 17 06:50:44.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 06:50:44.353
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:44.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:44.373
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 06:50:44.375
    Aug 17 06:50:44.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3361 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 17 06:50:44.451: INFO: stderr: ""
    Aug 17 06:50:44.451: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 06:50:44.451
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 17 06:50:44.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3361 delete pods e2e-test-httpd-pod'
    Aug 17 06:50:46.302: INFO: stderr: ""
    Aug 17 06:50:46.302: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:46.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3361" for this suite. 08/17/23 06:50:46.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:46.308
Aug 17 06:50:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 06:50:46.309
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:46.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:46.318
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 17 06:50:46.335: INFO: Waiting up to 5m0s for pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85" in namespace "pods-4746" to be "running and ready"
Aug 17 06:50:46.341: INFO: Pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378899ms
Aug 17 06:50:46.341: INFO: The phase of Pod server-envvars-a01ee543-97af-499d-bafa-0c642245dd85 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 06:50:48.345: INFO: Pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85": Phase="Running", Reason="", readiness=true. Elapsed: 2.009835801s
Aug 17 06:50:48.345: INFO: The phase of Pod server-envvars-a01ee543-97af-499d-bafa-0c642245dd85 is Running (Ready = true)
Aug 17 06:50:48.345: INFO: Pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85" satisfied condition "running and ready"
Aug 17 06:50:48.358: INFO: Waiting up to 5m0s for pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c" in namespace "pods-4746" to be "Succeeded or Failed"
Aug 17 06:50:48.360: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866506ms
Aug 17 06:50:50.363: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005102661s
Aug 17 06:50:52.363: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005225918s
STEP: Saw pod success 08/17/23 06:50:52.363
Aug 17 06:50:52.363: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c" satisfied condition "Succeeded or Failed"
Aug 17 06:50:52.365: INFO: Trying to get logs from node yst-node2 pod client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c container env3cont: <nil>
STEP: delete the pod 08/17/23 06:50:52.37
Aug 17 06:50:52.376: INFO: Waiting for pod client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c to disappear
Aug 17 06:50:52.377: INFO: Pod client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:52.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4746" for this suite. 08/17/23 06:50:52.38
------------------------------
â€¢ [SLOW TEST] [6.075 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:46.308
    Aug 17 06:50:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 06:50:46.309
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:46.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:46.318
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 17 06:50:46.335: INFO: Waiting up to 5m0s for pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85" in namespace "pods-4746" to be "running and ready"
    Aug 17 06:50:46.341: INFO: Pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378899ms
    Aug 17 06:50:46.341: INFO: The phase of Pod server-envvars-a01ee543-97af-499d-bafa-0c642245dd85 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 06:50:48.345: INFO: Pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85": Phase="Running", Reason="", readiness=true. Elapsed: 2.009835801s
    Aug 17 06:50:48.345: INFO: The phase of Pod server-envvars-a01ee543-97af-499d-bafa-0c642245dd85 is Running (Ready = true)
    Aug 17 06:50:48.345: INFO: Pod "server-envvars-a01ee543-97af-499d-bafa-0c642245dd85" satisfied condition "running and ready"
    Aug 17 06:50:48.358: INFO: Waiting up to 5m0s for pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c" in namespace "pods-4746" to be "Succeeded or Failed"
    Aug 17 06:50:48.360: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866506ms
    Aug 17 06:50:50.363: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005102661s
    Aug 17 06:50:52.363: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005225918s
    STEP: Saw pod success 08/17/23 06:50:52.363
    Aug 17 06:50:52.363: INFO: Pod "client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c" satisfied condition "Succeeded or Failed"
    Aug 17 06:50:52.365: INFO: Trying to get logs from node yst-node2 pod client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c container env3cont: <nil>
    STEP: delete the pod 08/17/23 06:50:52.37
    Aug 17 06:50:52.376: INFO: Waiting for pod client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c to disappear
    Aug 17 06:50:52.377: INFO: Pod client-envvars-8258d8c6-9b86-4deb-8fee-e40b753f6b6c no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:52.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4746" for this suite. 08/17/23 06:50:52.38
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:52.383
Aug 17 06:50:52.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 06:50:52.384
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:52.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:52.394
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/17/23 06:50:52.397
Aug 17 06:50:52.413: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d" in namespace "projected-3843" to be "Succeeded or Failed"
Aug 17 06:50:52.415: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.979884ms
Aug 17 06:50:54.417: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004013448s
Aug 17 06:50:56.417: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00475036s
STEP: Saw pod success 08/17/23 06:50:56.417
Aug 17 06:50:56.418: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d" satisfied condition "Succeeded or Failed"
Aug 17 06:50:56.419: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d container client-container: <nil>
STEP: delete the pod 08/17/23 06:50:56.422
Aug 17 06:50:56.429: INFO: Waiting for pod downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d to disappear
Aug 17 06:50:56.430: INFO: Pod downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:56.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3843" for this suite. 08/17/23 06:50:56.433
------------------------------
â€¢ [4.052 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:52.383
    Aug 17 06:50:52.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 06:50:52.384
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:52.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:52.394
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/17/23 06:50:52.397
    Aug 17 06:50:52.413: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d" in namespace "projected-3843" to be "Succeeded or Failed"
    Aug 17 06:50:52.415: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.979884ms
    Aug 17 06:50:54.417: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004013448s
    Aug 17 06:50:56.417: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00475036s
    STEP: Saw pod success 08/17/23 06:50:56.417
    Aug 17 06:50:56.418: INFO: Pod "downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d" satisfied condition "Succeeded or Failed"
    Aug 17 06:50:56.419: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d container client-container: <nil>
    STEP: delete the pod 08/17/23 06:50:56.422
    Aug 17 06:50:56.429: INFO: Waiting for pod downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d to disappear
    Aug 17 06:50:56.430: INFO: Pod downwardapi-volume-f7dfd150-f060-451f-a02c-93811bd2e59d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:56.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3843" for this suite. 08/17/23 06:50:56.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:56.437
Aug 17 06:50:56.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename endpointslice 08/17/23 06:50:56.438
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:56.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:56.452
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/17/23 06:50:56.454
STEP: getting /apis/discovery.k8s.io 08/17/23 06:50:56.457
STEP: getting /apis/discovery.k8s.iov1 08/17/23 06:50:56.459
STEP: creating 08/17/23 06:50:56.461
STEP: getting 08/17/23 06:50:56.489
STEP: listing 08/17/23 06:50:56.491
STEP: watching 08/17/23 06:50:56.493
Aug 17 06:50:56.493: INFO: starting watch
STEP: cluster-wide listing 08/17/23 06:50:56.494
STEP: cluster-wide watching 08/17/23 06:50:56.498
Aug 17 06:50:56.498: INFO: starting watch
STEP: patching 08/17/23 06:50:56.498
STEP: updating 08/17/23 06:50:56.502
Aug 17 06:50:56.506: INFO: waiting for watch events with expected annotations
Aug 17 06:50:56.506: INFO: saw patched and updated annotations
STEP: deleting 08/17/23 06:50:56.506
STEP: deleting a collection 08/17/23 06:50:56.512
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 06:50:56.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2761" for this suite. 08/17/23 06:50:56.52
------------------------------
â€¢ [0.086 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:56.437
    Aug 17 06:50:56.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename endpointslice 08/17/23 06:50:56.438
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:56.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:56.452
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/17/23 06:50:56.454
    STEP: getting /apis/discovery.k8s.io 08/17/23 06:50:56.457
    STEP: getting /apis/discovery.k8s.iov1 08/17/23 06:50:56.459
    STEP: creating 08/17/23 06:50:56.461
    STEP: getting 08/17/23 06:50:56.489
    STEP: listing 08/17/23 06:50:56.491
    STEP: watching 08/17/23 06:50:56.493
    Aug 17 06:50:56.493: INFO: starting watch
    STEP: cluster-wide listing 08/17/23 06:50:56.494
    STEP: cluster-wide watching 08/17/23 06:50:56.498
    Aug 17 06:50:56.498: INFO: starting watch
    STEP: patching 08/17/23 06:50:56.498
    STEP: updating 08/17/23 06:50:56.502
    Aug 17 06:50:56.506: INFO: waiting for watch events with expected annotations
    Aug 17 06:50:56.506: INFO: saw patched and updated annotations
    STEP: deleting 08/17/23 06:50:56.506
    STEP: deleting a collection 08/17/23 06:50:56.512
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:50:56.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2761" for this suite. 08/17/23 06:50:56.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:50:56.524
Aug 17 06:50:56.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename subpath 08/17/23 06:50:56.524
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:56.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:56.536
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 06:50:56.542
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-vksz 08/17/23 06:50:56.556
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 06:50:56.556
Aug 17 06:50:56.562: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vksz" in namespace "subpath-6526" to be "Succeeded or Failed"
Aug 17 06:50:56.564: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913949ms
Aug 17 06:50:58.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004712794s
Aug 17 06:51:00.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 4.004506318s
Aug 17 06:51:02.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 6.004901883s
Aug 17 06:51:04.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 8.00600835s
Aug 17 06:51:06.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 10.006047056s
Aug 17 06:51:08.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 12.006385377s
Aug 17 06:51:10.567: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 14.005722428s
Aug 17 06:51:12.567: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 16.005720181s
Aug 17 06:51:14.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 18.00595268s
Aug 17 06:51:16.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 20.004521714s
Aug 17 06:51:18.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=false. Elapsed: 22.004387258s
Aug 17 06:51:20.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004298621s
STEP: Saw pod success 08/17/23 06:51:20.566
Aug 17 06:51:20.566: INFO: Pod "pod-subpath-test-downwardapi-vksz" satisfied condition "Succeeded or Failed"
Aug 17 06:51:20.568: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-downwardapi-vksz container test-container-subpath-downwardapi-vksz: <nil>
STEP: delete the pod 08/17/23 06:51:20.571
Aug 17 06:51:20.577: INFO: Waiting for pod pod-subpath-test-downwardapi-vksz to disappear
Aug 17 06:51:20.579: INFO: Pod pod-subpath-test-downwardapi-vksz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vksz 08/17/23 06:51:20.579
Aug 17 06:51:20.579: INFO: Deleting pod "pod-subpath-test-downwardapi-vksz" in namespace "subpath-6526"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 06:51:20.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6526" for this suite. 08/17/23 06:51:20.583
------------------------------
â€¢ [SLOW TEST] [24.062 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:50:56.524
    Aug 17 06:50:56.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename subpath 08/17/23 06:50:56.524
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:50:56.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:50:56.536
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 06:50:56.542
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-vksz 08/17/23 06:50:56.556
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 06:50:56.556
    Aug 17 06:50:56.562: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vksz" in namespace "subpath-6526" to be "Succeeded or Failed"
    Aug 17 06:50:56.564: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913949ms
    Aug 17 06:50:58.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004712794s
    Aug 17 06:51:00.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 4.004506318s
    Aug 17 06:51:02.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 6.004901883s
    Aug 17 06:51:04.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 8.00600835s
    Aug 17 06:51:06.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 10.006047056s
    Aug 17 06:51:08.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 12.006385377s
    Aug 17 06:51:10.567: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 14.005722428s
    Aug 17 06:51:12.567: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 16.005720181s
    Aug 17 06:51:14.568: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 18.00595268s
    Aug 17 06:51:16.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=true. Elapsed: 20.004521714s
    Aug 17 06:51:18.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Running", Reason="", readiness=false. Elapsed: 22.004387258s
    Aug 17 06:51:20.566: INFO: Pod "pod-subpath-test-downwardapi-vksz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004298621s
    STEP: Saw pod success 08/17/23 06:51:20.566
    Aug 17 06:51:20.566: INFO: Pod "pod-subpath-test-downwardapi-vksz" satisfied condition "Succeeded or Failed"
    Aug 17 06:51:20.568: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-downwardapi-vksz container test-container-subpath-downwardapi-vksz: <nil>
    STEP: delete the pod 08/17/23 06:51:20.571
    Aug 17 06:51:20.577: INFO: Waiting for pod pod-subpath-test-downwardapi-vksz to disappear
    Aug 17 06:51:20.579: INFO: Pod pod-subpath-test-downwardapi-vksz no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-vksz 08/17/23 06:51:20.579
    Aug 17 06:51:20.579: INFO: Deleting pod "pod-subpath-test-downwardapi-vksz" in namespace "subpath-6526"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:51:20.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6526" for this suite. 08/17/23 06:51:20.583
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:51:20.586
Aug 17 06:51:20.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename podtemplate 08/17/23 06:51:20.586
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:20.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:20.612
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/17/23 06:51:20.615
STEP: Replace a pod template 08/17/23 06:51:20.62
Aug 17 06:51:20.627: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 17 06:51:20.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-494" for this suite. 08/17/23 06:51:20.63
------------------------------
â€¢ [0.047 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:51:20.586
    Aug 17 06:51:20.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename podtemplate 08/17/23 06:51:20.586
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:20.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:20.612
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/17/23 06:51:20.615
    STEP: Replace a pod template 08/17/23 06:51:20.62
    Aug 17 06:51:20.627: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:51:20.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-494" for this suite. 08/17/23 06:51:20.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:51:20.634
Aug 17 06:51:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 06:51:20.634
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:20.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:20.643
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/17/23 06:51:20.646
Aug 17 06:51:20.666: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee" in namespace "downward-api-130" to be "Succeeded or Failed"
Aug 17 06:51:20.670: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.652971ms
Aug 17 06:51:22.674: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee": Phase="Running", Reason="", readiness=false. Elapsed: 2.008021101s
Aug 17 06:51:24.673: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007305851s
STEP: Saw pod success 08/17/23 06:51:24.673
Aug 17 06:51:24.673: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee" satisfied condition "Succeeded or Failed"
Aug 17 06:51:24.675: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee container client-container: <nil>
STEP: delete the pod 08/17/23 06:51:24.678
Aug 17 06:51:24.684: INFO: Waiting for pod downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee to disappear
Aug 17 06:51:24.686: INFO: Pod downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 06:51:24.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-130" for this suite. 08/17/23 06:51:24.689
------------------------------
â€¢ [4.058 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:51:20.634
    Aug 17 06:51:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 06:51:20.634
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:20.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:20.643
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/17/23 06:51:20.646
    Aug 17 06:51:20.666: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee" in namespace "downward-api-130" to be "Succeeded or Failed"
    Aug 17 06:51:20.670: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.652971ms
    Aug 17 06:51:22.674: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee": Phase="Running", Reason="", readiness=false. Elapsed: 2.008021101s
    Aug 17 06:51:24.673: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007305851s
    STEP: Saw pod success 08/17/23 06:51:24.673
    Aug 17 06:51:24.673: INFO: Pod "downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee" satisfied condition "Succeeded or Failed"
    Aug 17 06:51:24.675: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee container client-container: <nil>
    STEP: delete the pod 08/17/23 06:51:24.678
    Aug 17 06:51:24.684: INFO: Waiting for pod downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee to disappear
    Aug 17 06:51:24.686: INFO: Pod downwardapi-volume-70c74b03-9c4f-41cc-9736-bd71430453ee no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:51:24.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-130" for this suite. 08/17/23 06:51:24.689
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:51:24.692
Aug 17 06:51:24.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 06:51:24.692
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:24.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:24.704
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 06:51:24.706
Aug 17 06:51:24.749: INFO: Waiting up to 5m0s for pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f" in namespace "emptydir-1486" to be "Succeeded or Failed"
Aug 17 06:51:24.751: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.892236ms
Aug 17 06:51:26.754: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004713521s
Aug 17 06:51:28.753: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004329807s
STEP: Saw pod success 08/17/23 06:51:28.753
Aug 17 06:51:28.753: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f" satisfied condition "Succeeded or Failed"
Aug 17 06:51:28.755: INFO: Trying to get logs from node yst-node2 pod pod-6e4d24b1-e85a-474c-980a-137a42f8a83f container test-container: <nil>
STEP: delete the pod 08/17/23 06:51:28.759
Aug 17 06:51:28.764: INFO: Waiting for pod pod-6e4d24b1-e85a-474c-980a-137a42f8a83f to disappear
Aug 17 06:51:28.765: INFO: Pod pod-6e4d24b1-e85a-474c-980a-137a42f8a83f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 06:51:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1486" for this suite. 08/17/23 06:51:28.768
------------------------------
â€¢ [4.080 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:51:24.692
    Aug 17 06:51:24.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 06:51:24.692
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:24.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:24.704
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 06:51:24.706
    Aug 17 06:51:24.749: INFO: Waiting up to 5m0s for pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f" in namespace "emptydir-1486" to be "Succeeded or Failed"
    Aug 17 06:51:24.751: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.892236ms
    Aug 17 06:51:26.754: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004713521s
    Aug 17 06:51:28.753: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004329807s
    STEP: Saw pod success 08/17/23 06:51:28.753
    Aug 17 06:51:28.753: INFO: Pod "pod-6e4d24b1-e85a-474c-980a-137a42f8a83f" satisfied condition "Succeeded or Failed"
    Aug 17 06:51:28.755: INFO: Trying to get logs from node yst-node2 pod pod-6e4d24b1-e85a-474c-980a-137a42f8a83f container test-container: <nil>
    STEP: delete the pod 08/17/23 06:51:28.759
    Aug 17 06:51:28.764: INFO: Waiting for pod pod-6e4d24b1-e85a-474c-980a-137a42f8a83f to disappear
    Aug 17 06:51:28.765: INFO: Pod pod-6e4d24b1-e85a-474c-980a-137a42f8a83f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:51:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1486" for this suite. 08/17/23 06:51:28.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:51:28.772
Aug 17 06:51:28.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-runtime 08/17/23 06:51:28.773
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:28.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:28.787
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/17/23 06:51:28.807
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/17/23 06:51:47.858
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/17/23 06:51:47.859
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/17/23 06:51:47.862
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/17/23 06:51:47.863
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/17/23 06:51:47.876
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/17/23 06:51:50.886
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/17/23 06:51:52.893
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/17/23 06:51:52.897
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/17/23 06:51:52.897
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/17/23 06:51:52.905
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/17/23 06:51:53.909
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/17/23 06:51:56.918
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/17/23 06:51:56.921
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/17/23 06:51:56.921
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 06:51:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9056" for this suite. 08/17/23 06:51:56.933
------------------------------
â€¢ [SLOW TEST] [28.164 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:51:28.772
    Aug 17 06:51:28.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-runtime 08/17/23 06:51:28.773
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:28.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:28.787
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/17/23 06:51:28.807
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/17/23 06:51:47.858
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/17/23 06:51:47.859
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/17/23 06:51:47.862
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/17/23 06:51:47.863
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/17/23 06:51:47.876
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/17/23 06:51:50.886
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/17/23 06:51:52.893
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/17/23 06:51:52.897
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/17/23 06:51:52.897
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/17/23 06:51:52.905
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/17/23 06:51:53.909
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/17/23 06:51:56.918
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/17/23 06:51:56.921
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/17/23 06:51:56.921
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:51:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9056" for this suite. 08/17/23 06:51:56.933
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:51:56.936
Aug 17 06:51:56.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 06:51:56.937
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:56.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:56.956
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6 in namespace container-probe-6863 08/17/23 06:51:56.958
Aug 17 06:51:56.975: INFO: Waiting up to 5m0s for pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6" in namespace "container-probe-6863" to be "not pending"
Aug 17 06:51:56.979: INFO: Pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.680656ms
Aug 17 06:51:58.981: INFO: Pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005868424s
Aug 17 06:51:58.981: INFO: Pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6" satisfied condition "not pending"
Aug 17 06:51:58.981: INFO: Started pod test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6 in namespace container-probe-6863
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 06:51:58.981
Aug 17 06:51:58.983: INFO: Initial restart count of pod test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6 is 0
STEP: deleting the pod 08/17/23 06:55:59.444
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 06:55:59.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6863" for this suite. 08/17/23 06:55:59.456
------------------------------
â€¢ [SLOW TEST] [242.525 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:51:56.936
    Aug 17 06:51:56.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 06:51:56.937
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:51:56.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:51:56.956
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6 in namespace container-probe-6863 08/17/23 06:51:56.958
    Aug 17 06:51:56.975: INFO: Waiting up to 5m0s for pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6" in namespace "container-probe-6863" to be "not pending"
    Aug 17 06:51:56.979: INFO: Pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.680656ms
    Aug 17 06:51:58.981: INFO: Pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005868424s
    Aug 17 06:51:58.981: INFO: Pod "test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6" satisfied condition "not pending"
    Aug 17 06:51:58.981: INFO: Started pod test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6 in namespace container-probe-6863
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 06:51:58.981
    Aug 17 06:51:58.983: INFO: Initial restart count of pod test-webserver-44a7c3b8-f26b-4f50-8cc3-b82aa01798c6 is 0
    STEP: deleting the pod 08/17/23 06:55:59.444
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:55:59.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6863" for this suite. 08/17/23 06:55:59.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:55:59.465
Aug 17 06:55:59.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename daemonsets 08/17/23 06:55:59.466
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:55:59.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:55:59.487
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 08/17/23 06:55:59.571
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 06:55:59.575
Aug 17 06:55:59.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 06:55:59.581: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 06:56:00.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 06:56:00.588: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 06:56:01.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 06:56:01.588: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 08/17/23 06:56:01.59
Aug 17 06:56:01.593: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/17/23 06:56:01.593
Aug 17 06:56:01.601: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/17/23 06:56:01.601
Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: ADDED
Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.603: INFO: Found daemon set daemon-set in namespace daemonsets-7692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 06:56:01.603: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/17/23 06:56:01.603
STEP: watching for the daemon set status to be patched 08/17/23 06:56:01.608
Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: ADDED
Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.611: INFO: Observed daemon set daemon-set in namespace daemonsets-7692 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 06:56:01.611: INFO: Found daemon set daemon-set in namespace daemonsets-7692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 17 06:56:01.611: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/17/23 06:56:01.613
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7692, will wait for the garbage collector to delete the pods 08/17/23 06:56:01.613
Aug 17 06:56:01.669: INFO: Deleting DaemonSet.extensions daemon-set took: 4.316744ms
Aug 17 06:56:01.769: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.167325ms
Aug 17 06:56:04.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 06:56:04.372: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 06:56:04.374: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26997629"},"items":null}

Aug 17 06:56:04.375: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26997629"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 06:56:04.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7692" for this suite. 08/17/23 06:56:04.392
------------------------------
â€¢ [4.930 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:55:59.465
    Aug 17 06:55:59.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename daemonsets 08/17/23 06:55:59.466
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:55:59.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:55:59.487
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 08/17/23 06:55:59.571
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 06:55:59.575
    Aug 17 06:55:59.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 06:55:59.581: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 06:56:00.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 06:56:00.588: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 06:56:01.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 06:56:01.588: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 08/17/23 06:56:01.59
    Aug 17 06:56:01.593: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/17/23 06:56:01.593
    Aug 17 06:56:01.601: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/17/23 06:56:01.601
    Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: ADDED
    Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.603: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.603: INFO: Found daemon set daemon-set in namespace daemonsets-7692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 06:56:01.603: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/17/23 06:56:01.603
    STEP: watching for the daemon set status to be patched 08/17/23 06:56:01.608
    Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: ADDED
    Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.611: INFO: Observed daemon set daemon-set in namespace daemonsets-7692 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 06:56:01.611: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 06:56:01.611: INFO: Found daemon set daemon-set in namespace daemonsets-7692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 17 06:56:01.611: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 06:56:01.613
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7692, will wait for the garbage collector to delete the pods 08/17/23 06:56:01.613
    Aug 17 06:56:01.669: INFO: Deleting DaemonSet.extensions daemon-set took: 4.316744ms
    Aug 17 06:56:01.769: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.167325ms
    Aug 17 06:56:04.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 06:56:04.372: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 06:56:04.374: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26997629"},"items":null}

    Aug 17 06:56:04.375: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26997629"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:56:04.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7692" for this suite. 08/17/23 06:56:04.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:56:04.396
Aug 17 06:56:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 06:56:04.397
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:56:04.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:56:04.409
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 06:56:04.411
Aug 17 06:56:04.445: INFO: Waiting up to 5m0s for pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1" in namespace "emptydir-6637" to be "Succeeded or Failed"
Aug 17 06:56:04.451: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960159ms
Aug 17 06:56:06.455: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00915909s
Aug 17 06:56:08.454: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008973501s
STEP: Saw pod success 08/17/23 06:56:08.454
Aug 17 06:56:08.454: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1" satisfied condition "Succeeded or Failed"
Aug 17 06:56:08.456: INFO: Trying to get logs from node yst-node2 pod pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1 container test-container: <nil>
STEP: delete the pod 08/17/23 06:56:08.469
Aug 17 06:56:08.476: INFO: Waiting for pod pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1 to disappear
Aug 17 06:56:08.479: INFO: Pod pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 06:56:08.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6637" for this suite. 08/17/23 06:56:08.483
------------------------------
â€¢ [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:56:04.396
    Aug 17 06:56:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 06:56:04.397
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:56:04.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:56:04.409
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 06:56:04.411
    Aug 17 06:56:04.445: INFO: Waiting up to 5m0s for pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1" in namespace "emptydir-6637" to be "Succeeded or Failed"
    Aug 17 06:56:04.451: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960159ms
    Aug 17 06:56:06.455: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00915909s
    Aug 17 06:56:08.454: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008973501s
    STEP: Saw pod success 08/17/23 06:56:08.454
    Aug 17 06:56:08.454: INFO: Pod "pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1" satisfied condition "Succeeded or Failed"
    Aug 17 06:56:08.456: INFO: Trying to get logs from node yst-node2 pod pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1 container test-container: <nil>
    STEP: delete the pod 08/17/23 06:56:08.469
    Aug 17 06:56:08.476: INFO: Waiting for pod pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1 to disappear
    Aug 17 06:56:08.479: INFO: Pod pod-51b2ef3a-2017-4958-af93-7f1c8f6575c1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:56:08.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6637" for this suite. 08/17/23 06:56:08.483
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:56:08.491
Aug 17 06:56:08.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 06:56:08.492
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:56:08.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:56:08.506
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7760 08/17/23 06:56:08.509
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-7760 08/17/23 06:56:08.537
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7760 08/17/23 06:56:08.543
Aug 17 06:56:08.545: INFO: Found 0 stateful pods, waiting for 1
Aug 17 06:56:18.549: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/17/23 06:56:18.549
Aug 17 06:56:18.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:56:18.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:56:18.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:56:18.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:56:18.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 17 06:56:28.712: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:56:28.712: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:56:28.739: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Aug 17 06:56:28.740: INFO: ss-0  yst-node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  }]
Aug 17 06:56:28.740: INFO: 
Aug 17 06:56:28.740: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 17 06:56:29.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996932375s
Aug 17 06:56:30.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993105324s
Aug 17 06:56:31.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989668052s
Aug 17 06:56:32.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986208881s
Aug 17 06:56:33.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983013209s
Aug 17 06:56:34.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980019481s
Aug 17 06:56:35.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976736915s
Aug 17 06:56:36.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973163041s
Aug 17 06:56:37.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.082447ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7760 08/17/23 06:56:38.776
Aug 17 06:56:38.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 06:56:38.923: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 06:56:38.923: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 06:56:38.923: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 06:56:38.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 06:56:39.096: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 17 06:56:39.096: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 06:56:39.096: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 06:56:39.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 06:56:39.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 17 06:56:39.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 06:56:39.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 06:56:39.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 17 06:56:49.338: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 06:56:49.338: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 06:56:49.338: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/17/23 06:56:49.338
Aug 17 06:56:49.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:56:49.500: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:56:49.500: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:56:49.500: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:56:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:56:49.655: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:56:49.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:56:49.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:56:49.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 06:56:49.784: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 06:56:49.784: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 06:56:49.784: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 06:56:49.784: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:56:49.787: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 17 06:56:59.792: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:56:59.792: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:56:59.792: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 06:56:59.799: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Aug 17 06:56:59.799: INFO: ss-0  yst-node2   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  }]
Aug 17 06:56:59.799: INFO: ss-1  yst-node1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  }]
Aug 17 06:56:59.799: INFO: ss-2  yst-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  }]
Aug 17 06:56:59.799: INFO: 
Aug 17 06:56:59.799: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 17 06:57:00.824: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Aug 17 06:57:00.824: INFO: ss-0  yst-node2   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  }]
Aug 17 06:57:00.824: INFO: ss-2  yst-master  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  }]
Aug 17 06:57:00.824: INFO: 
Aug 17 06:57:00.824: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 17 06:57:01.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.971930586s
Aug 17 06:57:02.830: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.969256762s
Aug 17 06:57:03.832: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.966654204s
Aug 17 06:57:04.835: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.964285698s
Aug 17 06:57:05.837: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.961977688s
Aug 17 06:57:06.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.958913114s
Aug 17 06:57:07.843: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.95635961s
Aug 17 06:57:08.845: INFO: Verifying statefulset ss doesn't scale past 0 for another 953.899819ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7760 08/17/23 06:57:09.846
Aug 17 06:57:09.965: INFO: Scaling statefulset ss to 0
Aug 17 06:57:09.994: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 06:57:09.996: INFO: Deleting all statefulset in ns statefulset-7760
Aug 17 06:57:09.997: INFO: Scaling statefulset ss to 0
Aug 17 06:57:10.005: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:57:10.006: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 06:57:10.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7760" for this suite. 08/17/23 06:57:10.036
------------------------------
â€¢ [SLOW TEST] [61.573 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:56:08.491
    Aug 17 06:56:08.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 06:56:08.492
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:56:08.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:56:08.506
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7760 08/17/23 06:56:08.509
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-7760 08/17/23 06:56:08.537
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7760 08/17/23 06:56:08.543
    Aug 17 06:56:08.545: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 06:56:18.549: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/17/23 06:56:18.549
    Aug 17 06:56:18.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 06:56:18.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 06:56:18.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 06:56:18.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 06:56:18.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 17 06:56:28.712: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 06:56:28.712: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 06:56:28.739: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
    Aug 17 06:56:28.740: INFO: ss-0  yst-node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  }]
    Aug 17 06:56:28.740: INFO: 
    Aug 17 06:56:28.740: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 17 06:56:29.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996932375s
    Aug 17 06:56:30.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993105324s
    Aug 17 06:56:31.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989668052s
    Aug 17 06:56:32.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986208881s
    Aug 17 06:56:33.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983013209s
    Aug 17 06:56:34.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980019481s
    Aug 17 06:56:35.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976736915s
    Aug 17 06:56:36.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973163041s
    Aug 17 06:56:37.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.082447ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7760 08/17/23 06:56:38.776
    Aug 17 06:56:38.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 06:56:38.923: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 06:56:38.923: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 06:56:38.923: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 06:56:38.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 06:56:39.096: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 17 06:56:39.096: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 06:56:39.096: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 06:56:39.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 06:56:39.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 17 06:56:39.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 06:56:39.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 06:56:39.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 17 06:56:49.338: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 06:56:49.338: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 06:56:49.338: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/17/23 06:56:49.338
    Aug 17 06:56:49.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 06:56:49.500: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 06:56:49.500: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 06:56:49.500: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 06:56:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 06:56:49.655: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 06:56:49.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 06:56:49.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 06:56:49.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-7760 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 06:56:49.784: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 06:56:49.784: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 06:56:49.784: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 06:56:49.784: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 06:56:49.787: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Aug 17 06:56:59.792: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 06:56:59.792: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 06:56:59.792: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 06:56:59.799: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Aug 17 06:56:59.799: INFO: ss-0  yst-node2   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  }]
    Aug 17 06:56:59.799: INFO: ss-1  yst-node1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  }]
    Aug 17 06:56:59.799: INFO: ss-2  yst-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  }]
    Aug 17 06:56:59.799: INFO: 
    Aug 17 06:56:59.799: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 17 06:57:00.824: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Aug 17 06:57:00.824: INFO: ss-0  yst-node2   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:08 +0000 UTC  }]
    Aug 17 06:57:00.824: INFO: ss-2  yst-master  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 06:56:28 +0000 UTC  }]
    Aug 17 06:57:00.824: INFO: 
    Aug 17 06:57:00.824: INFO: StatefulSet ss has not reached scale 0, at 2
    Aug 17 06:57:01.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.971930586s
    Aug 17 06:57:02.830: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.969256762s
    Aug 17 06:57:03.832: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.966654204s
    Aug 17 06:57:04.835: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.964285698s
    Aug 17 06:57:05.837: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.961977688s
    Aug 17 06:57:06.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.958913114s
    Aug 17 06:57:07.843: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.95635961s
    Aug 17 06:57:08.845: INFO: Verifying statefulset ss doesn't scale past 0 for another 953.899819ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7760 08/17/23 06:57:09.846
    Aug 17 06:57:09.965: INFO: Scaling statefulset ss to 0
    Aug 17 06:57:09.994: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 06:57:09.996: INFO: Deleting all statefulset in ns statefulset-7760
    Aug 17 06:57:09.997: INFO: Scaling statefulset ss to 0
    Aug 17 06:57:10.005: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 06:57:10.006: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:57:10.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7760" for this suite. 08/17/23 06:57:10.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:57:10.065
Aug 17 06:57:10.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename gc 08/17/23 06:57:10.065
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:10.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:10.128
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/17/23 06:57:10.227
STEP: create the rc2 08/17/23 06:57:10.239
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/17/23 06:57:15.342
STEP: delete the rc simpletest-rc-to-be-deleted 08/17/23 06:57:15.843
STEP: wait for the rc to be deleted 08/17/23 06:57:15.851
Aug 17 06:57:20.865: INFO: 75 pods remaining
Aug 17 06:57:20.865: INFO: 75 pods has nil DeletionTimestamp
Aug 17 06:57:20.865: INFO: 
STEP: Gathering metrics 08/17/23 06:57:25.868
Aug 17 06:57:26.260: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
Aug 17 06:57:26.273: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 12.720071ms
Aug 17 06:57:26.273: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
Aug 17 06:57:26.273: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
Aug 17 06:57:26.331: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 17 06:57:26.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-2228s" in namespace "gc-6815"
Aug 17 06:57:26.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-2crmw" in namespace "gc-6815"
Aug 17 06:57:26.349: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l6qh" in namespace "gc-6815"
Aug 17 06:57:26.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nvlh" in namespace "gc-6815"
Aug 17 06:57:26.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tfsz" in namespace "gc-6815"
Aug 17 06:57:26.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-464jp" in namespace "gc-6815"
Aug 17 06:57:26.487: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p7vm" in namespace "gc-6815"
Aug 17 06:57:26.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-55lrf" in namespace "gc-6815"
Aug 17 06:57:26.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-56d74" in namespace "gc-6815"
Aug 17 06:57:26.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-58dv9" in namespace "gc-6815"
Aug 17 06:57:26.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qfqf" in namespace "gc-6815"
Aug 17 06:57:26.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qp77" in namespace "gc-6815"
Aug 17 06:57:26.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-664g2" in namespace "gc-6815"
Aug 17 06:57:26.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-79glz" in namespace "gc-6815"
Aug 17 06:57:26.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xmq9" in namespace "gc-6815"
Aug 17 06:57:26.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-856bm" in namespace "gc-6815"
Aug 17 06:57:26.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b79j" in namespace "gc-6815"
Aug 17 06:57:26.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jwt2" in namespace "gc-6815"
Aug 17 06:57:26.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l7th" in namespace "gc-6815"
Aug 17 06:57:26.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lvj4" in namespace "gc-6815"
Aug 17 06:57:26.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-976nl" in namespace "gc-6815"
Aug 17 06:57:26.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f54g" in namespace "gc-6815"
Aug 17 06:57:26.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nfnr" in namespace "gc-6815"
Aug 17 06:57:26.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk46h" in namespace "gc-6815"
Aug 17 06:57:26.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn4pj" in namespace "gc-6815"
Aug 17 06:57:27.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz5qg" in namespace "gc-6815"
Aug 17 06:57:27.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjl87" in namespace "gc-6815"
Aug 17 06:57:27.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnftm" in namespace "gc-6815"
Aug 17 06:57:27.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct6fm" in namespace "gc-6815"
Aug 17 06:57:27.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5jjn" in namespace "gc-6815"
Aug 17 06:57:27.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcmbc" in namespace "gc-6815"
Aug 17 06:57:27.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg5g4" in namespace "gc-6815"
Aug 17 06:57:27.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlnlr" in namespace "gc-6815"
Aug 17 06:57:27.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmm2c" in namespace "gc-6815"
Aug 17 06:57:27.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-dt27k" in namespace "gc-6815"
Aug 17 06:57:27.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxbjh" in namespace "gc-6815"
Aug 17 06:57:27.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxgm9" in namespace "gc-6815"
Aug 17 06:57:27.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9rvh" in namespace "gc-6815"
Aug 17 06:57:27.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffbxh" in namespace "gc-6815"
Aug 17 06:57:27.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl4pk" in namespace "gc-6815"
Aug 17 06:57:27.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-flsjq" in namespace "gc-6815"
Aug 17 06:57:27.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-glx9k" in namespace "gc-6815"
Aug 17 06:57:27.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-grn9w" in namespace "gc-6815"
Aug 17 06:57:27.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4wks" in namespace "gc-6815"
Aug 17 06:57:27.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8lfg" in namespace "gc-6815"
Aug 17 06:57:27.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8tgt" in namespace "gc-6815"
Aug 17 06:57:27.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-jczc5" in namespace "gc-6815"
Aug 17 06:57:27.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-jlnpt" in namespace "gc-6815"
Aug 17 06:57:27.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvgnv" in namespace "gc-6815"
Aug 17 06:57:27.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2l26" in namespace "gc-6815"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 06:57:27.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6815" for this suite. 08/17/23 06:57:27.732
------------------------------
â€¢ [SLOW TEST] [17.673 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:57:10.065
    Aug 17 06:57:10.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename gc 08/17/23 06:57:10.065
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:10.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:10.128
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/17/23 06:57:10.227
    STEP: create the rc2 08/17/23 06:57:10.239
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/17/23 06:57:15.342
    STEP: delete the rc simpletest-rc-to-be-deleted 08/17/23 06:57:15.843
    STEP: wait for the rc to be deleted 08/17/23 06:57:15.851
    Aug 17 06:57:20.865: INFO: 75 pods remaining
    Aug 17 06:57:20.865: INFO: 75 pods has nil DeletionTimestamp
    Aug 17 06:57:20.865: INFO: 
    STEP: Gathering metrics 08/17/23 06:57:25.868
    Aug 17 06:57:26.260: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
    Aug 17 06:57:26.273: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 12.720071ms
    Aug 17 06:57:26.273: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
    Aug 17 06:57:26.273: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
    Aug 17 06:57:26.331: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 17 06:57:26.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-2228s" in namespace "gc-6815"
    Aug 17 06:57:26.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-2crmw" in namespace "gc-6815"
    Aug 17 06:57:26.349: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l6qh" in namespace "gc-6815"
    Aug 17 06:57:26.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nvlh" in namespace "gc-6815"
    Aug 17 06:57:26.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tfsz" in namespace "gc-6815"
    Aug 17 06:57:26.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-464jp" in namespace "gc-6815"
    Aug 17 06:57:26.487: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p7vm" in namespace "gc-6815"
    Aug 17 06:57:26.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-55lrf" in namespace "gc-6815"
    Aug 17 06:57:26.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-56d74" in namespace "gc-6815"
    Aug 17 06:57:26.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-58dv9" in namespace "gc-6815"
    Aug 17 06:57:26.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qfqf" in namespace "gc-6815"
    Aug 17 06:57:26.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qp77" in namespace "gc-6815"
    Aug 17 06:57:26.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-664g2" in namespace "gc-6815"
    Aug 17 06:57:26.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-79glz" in namespace "gc-6815"
    Aug 17 06:57:26.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xmq9" in namespace "gc-6815"
    Aug 17 06:57:26.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-856bm" in namespace "gc-6815"
    Aug 17 06:57:26.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b79j" in namespace "gc-6815"
    Aug 17 06:57:26.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jwt2" in namespace "gc-6815"
    Aug 17 06:57:26.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l7th" in namespace "gc-6815"
    Aug 17 06:57:26.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lvj4" in namespace "gc-6815"
    Aug 17 06:57:26.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-976nl" in namespace "gc-6815"
    Aug 17 06:57:26.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f54g" in namespace "gc-6815"
    Aug 17 06:57:26.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nfnr" in namespace "gc-6815"
    Aug 17 06:57:26.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk46h" in namespace "gc-6815"
    Aug 17 06:57:26.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn4pj" in namespace "gc-6815"
    Aug 17 06:57:27.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz5qg" in namespace "gc-6815"
    Aug 17 06:57:27.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjl87" in namespace "gc-6815"
    Aug 17 06:57:27.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnftm" in namespace "gc-6815"
    Aug 17 06:57:27.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct6fm" in namespace "gc-6815"
    Aug 17 06:57:27.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5jjn" in namespace "gc-6815"
    Aug 17 06:57:27.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcmbc" in namespace "gc-6815"
    Aug 17 06:57:27.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg5g4" in namespace "gc-6815"
    Aug 17 06:57:27.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlnlr" in namespace "gc-6815"
    Aug 17 06:57:27.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmm2c" in namespace "gc-6815"
    Aug 17 06:57:27.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-dt27k" in namespace "gc-6815"
    Aug 17 06:57:27.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxbjh" in namespace "gc-6815"
    Aug 17 06:57:27.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxgm9" in namespace "gc-6815"
    Aug 17 06:57:27.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9rvh" in namespace "gc-6815"
    Aug 17 06:57:27.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffbxh" in namespace "gc-6815"
    Aug 17 06:57:27.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl4pk" in namespace "gc-6815"
    Aug 17 06:57:27.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-flsjq" in namespace "gc-6815"
    Aug 17 06:57:27.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-glx9k" in namespace "gc-6815"
    Aug 17 06:57:27.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-grn9w" in namespace "gc-6815"
    Aug 17 06:57:27.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4wks" in namespace "gc-6815"
    Aug 17 06:57:27.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8lfg" in namespace "gc-6815"
    Aug 17 06:57:27.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8tgt" in namespace "gc-6815"
    Aug 17 06:57:27.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-jczc5" in namespace "gc-6815"
    Aug 17 06:57:27.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-jlnpt" in namespace "gc-6815"
    Aug 17 06:57:27.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvgnv" in namespace "gc-6815"
    Aug 17 06:57:27.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2l26" in namespace "gc-6815"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:57:27.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6815" for this suite. 08/17/23 06:57:27.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:57:27.739
Aug 17 06:57:27.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 06:57:27.741
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:27.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:27.768
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 17 06:57:27.770: INFO: Creating simple deployment test-new-deployment
Aug 17 06:57:27.932: INFO: deployment "test-new-deployment" doesn't have the required revision set
Aug 17 06:57:29.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 08/17/23 06:57:31.943
STEP: updating a scale subresource 08/17/23 06:57:31.945
STEP: verifying the deployment Spec.Replicas was modified 08/17/23 06:57:31.948
STEP: Patch a scale subresource 08/17/23 06:57:31.95
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 06:57:31.961: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-198  e674c1b9-b192-4707-a321-b4a3f4e978fa 27000687 3 2023-08-17 06:57:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-17 06:57:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00384e848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 06:57:30 +0000 UTC,LastTransitionTime:2023-08-17 06:57:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-17 06:57:30 +0000 UTC,LastTransitionTime:2023-08-17 06:57:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 06:57:31.964: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-198  20fed6be-d4bc-4d6b-b646-dc3c720a026e 27000692 2 2023-08-17 06:57:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e674c1b9-b192-4707-a321-b4a3f4e978fa 0xc00384ec87 0xc00384ec88}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e674c1b9-b192-4707-a321-b4a3f4e978fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00384ed18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 06:57:31.967: INFO: Pod "test-new-deployment-7f5969cbc7-5rppn" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-5rppn test-new-deployment-7f5969cbc7- deployment-198  9fb7b7a3-27aa-4fee-bf2f-c70e02f07d3f 27000691 0 2023-08-17 06:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 20fed6be-d4bc-4d6b-b646-dc3c720a026e 0xc00384f107 0xc00384f108}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20fed6be-d4bc-4d6b-b646-dc3c720a026e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m26f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m26f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 06:57:31.967: INFO: Pod "test-new-deployment-7f5969cbc7-h78pj" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-h78pj test-new-deployment-7f5969cbc7- deployment-198  58749eb6-86ba-437a-90b2-e0299fd272bc 27000676 0 2023-08-17 06:57:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2bad541c268357b84c404c38960106e1030ba165ffeda74165f96e1c014578de cni.projectcalico.org/podIP:172.32.238.87/32 cni.projectcalico.org/podIPs:172.32.238.87/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 20fed6be-d4bc-4d6b-b646-dc3c720a026e 0xc00384f297 0xc00384f298}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20fed6be-d4bc-4d6b-b646-dc3c720a026e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-444r6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-444r6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.87,StartTime:2023-08-17 06:57:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6c629cad2da69cfd236e572f2f75a22f341058b6ea44dca0c93856494142e808,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 06:57:31.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-198" for this suite. 08/17/23 06:57:31.97
------------------------------
â€¢ [4.236 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:57:27.739
    Aug 17 06:57:27.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 06:57:27.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:27.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:27.768
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 17 06:57:27.770: INFO: Creating simple deployment test-new-deployment
    Aug 17 06:57:27.932: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Aug 17 06:57:29.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 6, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 08/17/23 06:57:31.943
    STEP: updating a scale subresource 08/17/23 06:57:31.945
    STEP: verifying the deployment Spec.Replicas was modified 08/17/23 06:57:31.948
    STEP: Patch a scale subresource 08/17/23 06:57:31.95
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 06:57:31.961: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-198  e674c1b9-b192-4707-a321-b4a3f4e978fa 27000687 3 2023-08-17 06:57:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-17 06:57:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00384e848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 06:57:30 +0000 UTC,LastTransitionTime:2023-08-17 06:57:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-17 06:57:30 +0000 UTC,LastTransitionTime:2023-08-17 06:57:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 06:57:31.964: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-198  20fed6be-d4bc-4d6b-b646-dc3c720a026e 27000692 2 2023-08-17 06:57:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e674c1b9-b192-4707-a321-b4a3f4e978fa 0xc00384ec87 0xc00384ec88}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e674c1b9-b192-4707-a321-b4a3f4e978fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00384ed18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 06:57:31.967: INFO: Pod "test-new-deployment-7f5969cbc7-5rppn" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-5rppn test-new-deployment-7f5969cbc7- deployment-198  9fb7b7a3-27aa-4fee-bf2f-c70e02f07d3f 27000691 0 2023-08-17 06:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 20fed6be-d4bc-4d6b-b646-dc3c720a026e 0xc00384f107 0xc00384f108}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20fed6be-d4bc-4d6b-b646-dc3c720a026e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m26f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m26f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 06:57:31.967: INFO: Pod "test-new-deployment-7f5969cbc7-h78pj" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-h78pj test-new-deployment-7f5969cbc7- deployment-198  58749eb6-86ba-437a-90b2-e0299fd272bc 27000676 0 2023-08-17 06:57:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2bad541c268357b84c404c38960106e1030ba165ffeda74165f96e1c014578de cni.projectcalico.org/podIP:172.32.238.87/32 cni.projectcalico.org/podIPs:172.32.238.87/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 20fed6be-d4bc-4d6b-b646-dc3c720a026e 0xc00384f297 0xc00384f298}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20fed6be-d4bc-4d6b-b646-dc3c720a026e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-444r6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-444r6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.87,StartTime:2023-08-17 06:57:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6c629cad2da69cfd236e572f2f75a22f341058b6ea44dca0c93856494142e808,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:57:31.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-198" for this suite. 08/17/23 06:57:31.97
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:57:31.976
Aug 17 06:57:31.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replication-controller 08/17/23 06:57:31.977
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:31.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:31.993
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/17/23 06:57:32.041
STEP: waiting for RC to be added 08/17/23 06:57:32.055
STEP: waiting for available Replicas 08/17/23 06:57:32.056
STEP: patching ReplicationController 08/17/23 06:57:33.592
STEP: waiting for RC to be modified 08/17/23 06:57:33.6
STEP: patching ReplicationController status 08/17/23 06:57:33.6
STEP: waiting for RC to be modified 08/17/23 06:57:33.605
STEP: waiting for available Replicas 08/17/23 06:57:33.605
STEP: fetching ReplicationController status 08/17/23 06:57:33.609
STEP: patching ReplicationController scale 08/17/23 06:57:33.612
STEP: waiting for RC to be modified 08/17/23 06:57:33.622
STEP: waiting for ReplicationController's scale to be the max amount 08/17/23 06:57:33.622
STEP: fetching ReplicationController; ensuring that it's patched 08/17/23 06:57:35.036
STEP: updating ReplicationController status 08/17/23 06:57:35.037
STEP: waiting for RC to be modified 08/17/23 06:57:35.041
STEP: listing all ReplicationControllers 08/17/23 06:57:35.041
STEP: checking that ReplicationController has expected values 08/17/23 06:57:35.043
STEP: deleting ReplicationControllers by collection 08/17/23 06:57:35.044
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/17/23 06:57:35.047
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 06:57:35.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8263" for this suite. 08/17/23 06:57:35.126
------------------------------
â€¢ [3.158 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:57:31.976
    Aug 17 06:57:31.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replication-controller 08/17/23 06:57:31.977
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:31.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:31.993
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/17/23 06:57:32.041
    STEP: waiting for RC to be added 08/17/23 06:57:32.055
    STEP: waiting for available Replicas 08/17/23 06:57:32.056
    STEP: patching ReplicationController 08/17/23 06:57:33.592
    STEP: waiting for RC to be modified 08/17/23 06:57:33.6
    STEP: patching ReplicationController status 08/17/23 06:57:33.6
    STEP: waiting for RC to be modified 08/17/23 06:57:33.605
    STEP: waiting for available Replicas 08/17/23 06:57:33.605
    STEP: fetching ReplicationController status 08/17/23 06:57:33.609
    STEP: patching ReplicationController scale 08/17/23 06:57:33.612
    STEP: waiting for RC to be modified 08/17/23 06:57:33.622
    STEP: waiting for ReplicationController's scale to be the max amount 08/17/23 06:57:33.622
    STEP: fetching ReplicationController; ensuring that it's patched 08/17/23 06:57:35.036
    STEP: updating ReplicationController status 08/17/23 06:57:35.037
    STEP: waiting for RC to be modified 08/17/23 06:57:35.041
    STEP: listing all ReplicationControllers 08/17/23 06:57:35.041
    STEP: checking that ReplicationController has expected values 08/17/23 06:57:35.043
    STEP: deleting ReplicationControllers by collection 08/17/23 06:57:35.044
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/17/23 06:57:35.047
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:57:35.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8263" for this suite. 08/17/23 06:57:35.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:57:35.136
Aug 17 06:57:35.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 06:57:35.136
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:35.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:35.151
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/17/23 06:57:35.169
STEP: waiting for Deployment to be created 08/17/23 06:57:35.176
STEP: waiting for all Replicas to be Ready 08/17/23 06:57:35.177
Aug 17 06:57:35.178: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:35.178: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:35.182: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:35.182: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:35.194: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:35.194: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:35.211: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:35.211: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 06:57:36.069: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 17 06:57:36.069: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 17 06:57:36.606: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/17/23 06:57:36.606
W0817 06:57:36.616381      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 17 06:57:36.617: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/17/23 06:57:36.617
Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.626: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.626: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.641: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.641: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:36.654: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:36.654: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:36.660: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:36.660: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:39.623: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:39.623: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:39.639: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
STEP: listing Deployments 08/17/23 06:57:39.639
Aug 17 06:57:39.644: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/17/23 06:57:39.644
Aug 17 06:57:39.653: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/17/23 06:57:39.653
Aug 17 06:57:39.656: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:39.659: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:39.668: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:39.683: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:39.692: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:39.696: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:40.627: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:40.636: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:40.642: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:40.654: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 06:57:42.075: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/17/23 06:57:42.089
STEP: fetching the DeploymentStatus 08/17/23 06:57:42.094
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 3
STEP: deleting the Deployment 08/17/23 06:57:42.097
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.102: INFO: observed event type MODIFIED
Aug 17 06:57:42.103: INFO: observed event type MODIFIED
Aug 17 06:57:42.103: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 06:57:42.105: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 17 06:57:42.107: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-7021  f77af248-c95e-427b-a1b4-7e6a0a283877 27001575 2 2023-08-17 06:57:39 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6ee4ec6b-1aec-4771-9e3e-54254931b21f 0xc004833c27 0xc004833c28}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ee4ec6b-1aec-4771-9e3e-54254931b21f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004833cb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 17 06:57:42.111: INFO: pod: "test-deployment-7b7876f9d6-6t72d":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-6t72d test-deployment-7b7876f9d6- deployment-7021  65375b64-0342-4d2b-8d9f-a39519336903 27001516 0 2023-08-17 06:57:39 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:d22c22e1fbdeb229ac7bdad83d7ebb2e20687cd942b14d8b4bb0ef203de2e97b cni.projectcalico.org/podIP:172.32.238.67/32 cni.projectcalico.org/podIPs:172.32.238.67/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f77af248-c95e-427b-a1b4-7e6a0a283877 0xc004c98177 0xc004c98178}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f77af248-c95e-427b-a1b4-7e6a0a283877\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnh5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnh5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.67,StartTime:2023-08-17 06:57:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4426e5fe9d9e5ea0fe007ae35f50ebbea1283d9686e98d4cad9e6b73610a0704,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 17 06:57:42.111: INFO: pod: "test-deployment-7b7876f9d6-tzr6q":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-tzr6q test-deployment-7b7876f9d6- deployment-7021  f5ab5d6f-9d50-4964-991f-ee65117600e9 27001574 0 2023-08-17 06:57:40 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:8a4b64c1cf766d0093016a99edd0de992181dc49d3912f67657910e2a2c6fb9c cni.projectcalico.org/podIP:172.32.123.173/32 cni.projectcalico.org/podIPs:172.32.123.173/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f77af248-c95e-427b-a1b4-7e6a0a283877 0xc004c98397 0xc004c98398}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f77af248-c95e-427b-a1b4-7e6a0a283877\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67cfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67cfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.173,StartTime:2023-08-17 06:57:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a701ee9372c6fe1401ccb838b518624205970a61f2977ab86aea1157738b1066,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 17 06:57:42.112: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-7021  73ce5162-3c57-4d8c-9817-554d2f606e63 27001582 4 2023-08-17 06:57:36 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6ee4ec6b-1aec-4771-9e3e-54254931b21f 0xc004833d17 0xc004833d18}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ee4ec6b-1aec-4771-9e3e-54254931b21f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004833da0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 17 06:57:42.114: INFO: pod: "test-deployment-7df74c55ff-5zgk6":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-5zgk6 test-deployment-7df74c55ff- deployment-7021  11a4fa4c-bdf4-46b2-81f1-7a15dd252b41 27001579 0 2023-08-17 06:57:36 +0000 UTC 2023-08-17 06:57:43 +0000 UTC 0xc004c99758 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:55b7d5a132b79d246eebdd0ac86422e3014fcd069145a8d3ac222c668a97ea56 cni.projectcalico.org/podIP:172.32.238.85/32 cni.projectcalico.org/podIPs:172.32.238.85/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 73ce5162-3c57-4d8c-9817-554d2f606e63 0xc004c997a7 0xc004c997a8}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73ce5162-3c57-4d8c-9817-554d2f606e63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb2fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb2fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.85,StartTime:2023-08-17 06:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:10.60.200.175:5000/pause:3.9,ImageID:10.60.200.175:5000/pause@sha256:761efa452e56c074e1d46ccb46ae9655f50876fb68f65b67498609c67b5cc245,ContainerID:containerd://f84e5af0b35d1e225edfef0d3376cfc028ae9eb250d8c39e95dd2cf43f62f106,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 17 06:57:42.114: INFO: pod: "test-deployment-7df74c55ff-cg2hj":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-cg2hj test-deployment-7df74c55ff- deployment-7021  f29b35e8-4013-4b1d-863e-847d3ae42cce 27001522 0 2023-08-17 06:57:39 +0000 UTC 2023-08-17 06:57:41 +0000 UTC 0xc004c99998 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:04595ae3ab0b9a280bb53e3d1a482ee2ae5230878bb6296ac3256ac93150fce9 cni.projectcalico.org/podIP:172.32.123.170/32 cni.projectcalico.org/podIPs:172.32.123.170/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 73ce5162-3c57-4d8c-9817-554d2f606e63 0xc004c999c7 0xc004c999c8}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73ce5162-3c57-4d8c-9817-554d2f606e63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt8ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt8ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:,StartTime:2023-08-17 06:57:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 17 06:57:42.114: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-7021  a28e9104-0cfe-42fc-ae27-5963f195d9e6 27001425 3 2023-08-17 06:57:35 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6ee4ec6b-1aec-4771-9e3e-54254931b21f 0xc004833e07 0xc004833e08}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ee4ec6b-1aec-4771-9e3e-54254931b21f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004833e90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 06:57:42.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7021" for this suite. 08/17/23 06:57:42.119
------------------------------
â€¢ [SLOW TEST] [6.988 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:57:35.136
    Aug 17 06:57:35.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 06:57:35.136
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:35.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:35.151
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/17/23 06:57:35.169
    STEP: waiting for Deployment to be created 08/17/23 06:57:35.176
    STEP: waiting for all Replicas to be Ready 08/17/23 06:57:35.177
    Aug 17 06:57:35.178: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:35.178: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:35.182: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:35.182: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:35.194: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:35.194: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:35.211: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:35.211: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 06:57:36.069: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 17 06:57:36.069: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 17 06:57:36.606: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/17/23 06:57:36.606
    W0817 06:57:36.616381      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 17 06:57:36.617: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/17/23 06:57:36.617
    Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.618: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 0
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.619: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.626: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.626: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.641: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.641: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:36.654: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:36.654: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:36.660: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:36.660: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:39.623: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:39.623: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:39.639: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    STEP: listing Deployments 08/17/23 06:57:39.639
    Aug 17 06:57:39.644: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/17/23 06:57:39.644
    Aug 17 06:57:39.653: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/17/23 06:57:39.653
    Aug 17 06:57:39.656: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:39.659: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:39.668: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:39.683: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:39.692: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:39.696: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:40.627: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:40.636: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:40.642: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:40.654: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 06:57:42.075: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/17/23 06:57:42.089
    STEP: fetching the DeploymentStatus 08/17/23 06:57:42.094
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 1
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 2
    Aug 17 06:57:42.097: INFO: observed Deployment test-deployment in namespace deployment-7021 with ReadyReplicas 3
    STEP: deleting the Deployment 08/17/23 06:57:42.097
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.102: INFO: observed event type MODIFIED
    Aug 17 06:57:42.103: INFO: observed event type MODIFIED
    Aug 17 06:57:42.103: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 06:57:42.105: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 17 06:57:42.107: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-7021  f77af248-c95e-427b-a1b4-7e6a0a283877 27001575 2 2023-08-17 06:57:39 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6ee4ec6b-1aec-4771-9e3e-54254931b21f 0xc004833c27 0xc004833c28}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ee4ec6b-1aec-4771-9e3e-54254931b21f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004833cb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Aug 17 06:57:42.111: INFO: pod: "test-deployment-7b7876f9d6-6t72d":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-6t72d test-deployment-7b7876f9d6- deployment-7021  65375b64-0342-4d2b-8d9f-a39519336903 27001516 0 2023-08-17 06:57:39 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:d22c22e1fbdeb229ac7bdad83d7ebb2e20687cd942b14d8b4bb0ef203de2e97b cni.projectcalico.org/podIP:172.32.238.67/32 cni.projectcalico.org/podIPs:172.32.238.67/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f77af248-c95e-427b-a1b4-7e6a0a283877 0xc004c98177 0xc004c98178}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f77af248-c95e-427b-a1b4-7e6a0a283877\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnh5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnh5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.67,StartTime:2023-08-17 06:57:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4426e5fe9d9e5ea0fe007ae35f50ebbea1283d9686e98d4cad9e6b73610a0704,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 17 06:57:42.111: INFO: pod: "test-deployment-7b7876f9d6-tzr6q":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-tzr6q test-deployment-7b7876f9d6- deployment-7021  f5ab5d6f-9d50-4964-991f-ee65117600e9 27001574 0 2023-08-17 06:57:40 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:8a4b64c1cf766d0093016a99edd0de992181dc49d3912f67657910e2a2c6fb9c cni.projectcalico.org/podIP:172.32.123.173/32 cni.projectcalico.org/podIPs:172.32.123.173/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f77af248-c95e-427b-a1b4-7e6a0a283877 0xc004c98397 0xc004c98398}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f77af248-c95e-427b-a1b4-7e6a0a283877\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67cfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67cfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.173,StartTime:2023-08-17 06:57:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a701ee9372c6fe1401ccb838b518624205970a61f2977ab86aea1157738b1066,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 17 06:57:42.112: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-7021  73ce5162-3c57-4d8c-9817-554d2f606e63 27001582 4 2023-08-17 06:57:36 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6ee4ec6b-1aec-4771-9e3e-54254931b21f 0xc004833d17 0xc004833d18}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ee4ec6b-1aec-4771-9e3e-54254931b21f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004833da0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 17 06:57:42.114: INFO: pod: "test-deployment-7df74c55ff-5zgk6":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-5zgk6 test-deployment-7df74c55ff- deployment-7021  11a4fa4c-bdf4-46b2-81f1-7a15dd252b41 27001579 0 2023-08-17 06:57:36 +0000 UTC 2023-08-17 06:57:43 +0000 UTC 0xc004c99758 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:55b7d5a132b79d246eebdd0ac86422e3014fcd069145a8d3ac222c668a97ea56 cni.projectcalico.org/podIP:172.32.238.85/32 cni.projectcalico.org/podIPs:172.32.238.85/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 73ce5162-3c57-4d8c-9817-554d2f606e63 0xc004c997a7 0xc004c997a8}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73ce5162-3c57-4d8c-9817-554d2f606e63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:57:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb2fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb2fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.85,StartTime:2023-08-17 06:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:57:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:10.60.200.175:5000/pause:3.9,ImageID:10.60.200.175:5000/pause@sha256:761efa452e56c074e1d46ccb46ae9655f50876fb68f65b67498609c67b5cc245,ContainerID:containerd://f84e5af0b35d1e225edfef0d3376cfc028ae9eb250d8c39e95dd2cf43f62f106,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 17 06:57:42.114: INFO: pod: "test-deployment-7df74c55ff-cg2hj":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-cg2hj test-deployment-7df74c55ff- deployment-7021  f29b35e8-4013-4b1d-863e-847d3ae42cce 27001522 0 2023-08-17 06:57:39 +0000 UTC 2023-08-17 06:57:41 +0000 UTC 0xc004c99998 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:04595ae3ab0b9a280bb53e3d1a482ee2ae5230878bb6296ac3256ac93150fce9 cni.projectcalico.org/podIP:172.32.123.170/32 cni.projectcalico.org/podIPs:172.32.123.170/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 73ce5162-3c57-4d8c-9817-554d2f606e63 0xc004c999c7 0xc004c999c8}] [] [{kube-controller-manager Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73ce5162-3c57-4d8c-9817-554d2f606e63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 06:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt8ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt8ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:57:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:,StartTime:2023-08-17 06:57:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 17 06:57:42.114: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-7021  a28e9104-0cfe-42fc-ae27-5963f195d9e6 27001425 3 2023-08-17 06:57:35 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6ee4ec6b-1aec-4771-9e3e-54254931b21f 0xc004833e07 0xc004833e08}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ee4ec6b-1aec-4771-9e3e-54254931b21f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:57:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004833e90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:57:42.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7021" for this suite. 08/17/23 06:57:42.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:57:42.124
Aug 17 06:57:42.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 06:57:42.125
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:42.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:42.14
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-9abae302-569e-4b55-a54c-8c6b203e88fc 08/17/23 06:57:42.171
STEP: Creating a pod to test consume secrets 08/17/23 06:57:42.175
Aug 17 06:57:42.181: INFO: Waiting up to 5m0s for pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773" in namespace "secrets-499" to be "Succeeded or Failed"
Aug 17 06:57:42.204: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Pending", Reason="", readiness=false. Elapsed: 22.992491ms
Aug 17 06:57:44.207: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026106714s
Aug 17 06:57:46.207: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Running", Reason="", readiness=false. Elapsed: 4.026658434s
Aug 17 06:57:48.206: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025467387s
STEP: Saw pod success 08/17/23 06:57:48.206
Aug 17 06:57:48.206: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773" satisfied condition "Succeeded or Failed"
Aug 17 06:57:48.208: INFO: Trying to get logs from node yst-node2 pod pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 06:57:48.217
Aug 17 06:57:48.224: INFO: Waiting for pod pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773 to disappear
Aug 17 06:57:48.225: INFO: Pod pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 06:57:48.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-499" for this suite. 08/17/23 06:57:48.228
STEP: Destroying namespace "secret-namespace-9282" for this suite. 08/17/23 06:57:48.231
------------------------------
â€¢ [SLOW TEST] [6.110 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:57:42.124
    Aug 17 06:57:42.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 06:57:42.125
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:42.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:42.14
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-9abae302-569e-4b55-a54c-8c6b203e88fc 08/17/23 06:57:42.171
    STEP: Creating a pod to test consume secrets 08/17/23 06:57:42.175
    Aug 17 06:57:42.181: INFO: Waiting up to 5m0s for pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773" in namespace "secrets-499" to be "Succeeded or Failed"
    Aug 17 06:57:42.204: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Pending", Reason="", readiness=false. Elapsed: 22.992491ms
    Aug 17 06:57:44.207: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026106714s
    Aug 17 06:57:46.207: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Running", Reason="", readiness=false. Elapsed: 4.026658434s
    Aug 17 06:57:48.206: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025467387s
    STEP: Saw pod success 08/17/23 06:57:48.206
    Aug 17 06:57:48.206: INFO: Pod "pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773" satisfied condition "Succeeded or Failed"
    Aug 17 06:57:48.208: INFO: Trying to get logs from node yst-node2 pod pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 06:57:48.217
    Aug 17 06:57:48.224: INFO: Waiting for pod pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773 to disappear
    Aug 17 06:57:48.225: INFO: Pod pod-secrets-3c12d196-9c7b-49bd-9974-d1ee9ca7b773 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:57:48.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-499" for this suite. 08/17/23 06:57:48.228
    STEP: Destroying namespace "secret-namespace-9282" for this suite. 08/17/23 06:57:48.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:57:48.235
Aug 17 06:57:48.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 06:57:48.236
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:48.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:48.249
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/17/23 06:57:48.252
Aug 17 06:57:48.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 17 06:57:48.337: INFO: stderr: ""
Aug 17 06:57:48.338: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/17/23 06:57:48.338
Aug 17 06:57:48.338: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 17 06:57:48.338: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6568" to be "running and ready, or succeeded"
Aug 17 06:57:48.340: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026783ms
Aug 17 06:57:48.340: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'yst-node2' to be 'Running' but was 'Pending'
Aug 17 06:57:50.343: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005128076s
Aug 17 06:57:50.343: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 17 06:57:50.343: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/17/23 06:57:50.343
Aug 17 06:57:50.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator'
Aug 17 06:57:50.427: INFO: stderr: ""
Aug 17 06:57:50.427: INFO: stdout: "I0817 06:57:49.098257       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/bjn 598\nI0817 06:57:49.298456       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/qvn 312\nI0817 06:57:49.498809       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/tcw 415\nI0817 06:57:49.699204       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/pvfm 439\nI0817 06:57:49.898389       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/qqh 232\nI0817 06:57:50.098742       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/pfjr 370\nI0817 06:57:50.299191       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2j4 207\n"
STEP: limiting log lines 08/17/23 06:57:50.427
Aug 17 06:57:50.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --tail=1'
Aug 17 06:57:50.510: INFO: stderr: ""
Aug 17 06:57:50.510: INFO: stdout: "I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
Aug 17 06:57:50.510: INFO: got output "I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
STEP: limiting log bytes 08/17/23 06:57:50.51
Aug 17 06:57:50.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --limit-bytes=1'
Aug 17 06:57:50.588: INFO: stderr: ""
Aug 17 06:57:50.588: INFO: stdout: "I"
Aug 17 06:57:50.588: INFO: got output "I"
STEP: exposing timestamps 08/17/23 06:57:50.588
Aug 17 06:57:50.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 17 06:57:50.665: INFO: stderr: ""
Aug 17 06:57:50.665: INFO: stdout: "2023-08-17T15:57:50.498820094+09:00 I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
Aug 17 06:57:50.665: INFO: got output "2023-08-17T15:57:50.498820094+09:00 I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
STEP: restricting to a time range 08/17/23 06:57:50.665
Aug 17 06:57:53.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --since=1s'
Aug 17 06:57:53.258: INFO: stderr: ""
Aug 17 06:57:53.258: INFO: stdout: "I0817 06:57:52.298703       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/42n 273\nI0817 06:57:52.499099       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vmws 406\nI0817 06:57:52.698386       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/g64 592\nI0817 06:57:52.898749       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/6tr 213\nI0817 06:57:53.099114       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dc7z 433\n"
Aug 17 06:57:53.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --since=24h'
Aug 17 06:57:53.350: INFO: stderr: ""
Aug 17 06:57:53.350: INFO: stdout: "I0817 06:57:49.098257       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/bjn 598\nI0817 06:57:49.298456       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/qvn 312\nI0817 06:57:49.498809       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/tcw 415\nI0817 06:57:49.699204       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/pvfm 439\nI0817 06:57:49.898389       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/qqh 232\nI0817 06:57:50.098742       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/pfjr 370\nI0817 06:57:50.299191       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2j4 207\nI0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\nI0817 06:57:50.699020       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/lqzt 537\nI0817 06:57:50.898312       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/lh6 292\nI0817 06:57:51.098588       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/mljr 339\nI0817 06:57:51.298956       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/96vx 395\nI0817 06:57:51.499322       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/v9r 378\nI0817 06:57:51.698721       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/h28w 435\nI0817 06:57:51.899082       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/wws 253\nI0817 06:57:52.098371       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/c5h6 435\nI0817 06:57:52.298703       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/42n 273\nI0817 06:57:52.499099       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vmws 406\nI0817 06:57:52.698386       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/g64 592\nI0817 06:57:52.898749       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/6tr 213\nI0817 06:57:53.099114       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dc7z 433\nI0817 06:57:53.298358       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/7brr 492\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 17 06:57:53.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 delete pod logs-generator'
Aug 17 06:57:54.669: INFO: stderr: ""
Aug 17 06:57:54.669: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 06:57:54.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6568" for this suite. 08/17/23 06:57:54.672
------------------------------
â€¢ [SLOW TEST] [6.439 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:57:48.235
    Aug 17 06:57:48.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 06:57:48.236
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:48.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:48.249
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/17/23 06:57:48.252
    Aug 17 06:57:48.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 17 06:57:48.337: INFO: stderr: ""
    Aug 17 06:57:48.338: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/17/23 06:57:48.338
    Aug 17 06:57:48.338: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 17 06:57:48.338: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6568" to be "running and ready, or succeeded"
    Aug 17 06:57:48.340: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026783ms
    Aug 17 06:57:48.340: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'yst-node2' to be 'Running' but was 'Pending'
    Aug 17 06:57:50.343: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005128076s
    Aug 17 06:57:50.343: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 17 06:57:50.343: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/17/23 06:57:50.343
    Aug 17 06:57:50.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator'
    Aug 17 06:57:50.427: INFO: stderr: ""
    Aug 17 06:57:50.427: INFO: stdout: "I0817 06:57:49.098257       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/bjn 598\nI0817 06:57:49.298456       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/qvn 312\nI0817 06:57:49.498809       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/tcw 415\nI0817 06:57:49.699204       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/pvfm 439\nI0817 06:57:49.898389       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/qqh 232\nI0817 06:57:50.098742       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/pfjr 370\nI0817 06:57:50.299191       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2j4 207\n"
    STEP: limiting log lines 08/17/23 06:57:50.427
    Aug 17 06:57:50.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --tail=1'
    Aug 17 06:57:50.510: INFO: stderr: ""
    Aug 17 06:57:50.510: INFO: stdout: "I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
    Aug 17 06:57:50.510: INFO: got output "I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
    STEP: limiting log bytes 08/17/23 06:57:50.51
    Aug 17 06:57:50.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --limit-bytes=1'
    Aug 17 06:57:50.588: INFO: stderr: ""
    Aug 17 06:57:50.588: INFO: stdout: "I"
    Aug 17 06:57:50.588: INFO: got output "I"
    STEP: exposing timestamps 08/17/23 06:57:50.588
    Aug 17 06:57:50.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 17 06:57:50.665: INFO: stderr: ""
    Aug 17 06:57:50.665: INFO: stdout: "2023-08-17T15:57:50.498820094+09:00 I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
    Aug 17 06:57:50.665: INFO: got output "2023-08-17T15:57:50.498820094+09:00 I0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\n"
    STEP: restricting to a time range 08/17/23 06:57:50.665
    Aug 17 06:57:53.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --since=1s'
    Aug 17 06:57:53.258: INFO: stderr: ""
    Aug 17 06:57:53.258: INFO: stdout: "I0817 06:57:52.298703       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/42n 273\nI0817 06:57:52.499099       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vmws 406\nI0817 06:57:52.698386       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/g64 592\nI0817 06:57:52.898749       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/6tr 213\nI0817 06:57:53.099114       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dc7z 433\n"
    Aug 17 06:57:53.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 logs logs-generator logs-generator --since=24h'
    Aug 17 06:57:53.350: INFO: stderr: ""
    Aug 17 06:57:53.350: INFO: stdout: "I0817 06:57:49.098257       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/bjn 598\nI0817 06:57:49.298456       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/qvn 312\nI0817 06:57:49.498809       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/tcw 415\nI0817 06:57:49.699204       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/pvfm 439\nI0817 06:57:49.898389       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/qqh 232\nI0817 06:57:50.098742       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/pfjr 370\nI0817 06:57:50.299191       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2j4 207\nI0817 06:57:50.498703       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/2q6 368\nI0817 06:57:50.699020       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/lqzt 537\nI0817 06:57:50.898312       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/lh6 292\nI0817 06:57:51.098588       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/mljr 339\nI0817 06:57:51.298956       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/96vx 395\nI0817 06:57:51.499322       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/v9r 378\nI0817 06:57:51.698721       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/h28w 435\nI0817 06:57:51.899082       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/wws 253\nI0817 06:57:52.098371       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/c5h6 435\nI0817 06:57:52.298703       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/42n 273\nI0817 06:57:52.499099       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vmws 406\nI0817 06:57:52.698386       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/g64 592\nI0817 06:57:52.898749       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/6tr 213\nI0817 06:57:53.099114       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dc7z 433\nI0817 06:57:53.298358       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/7brr 492\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 17 06:57:53.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6568 delete pod logs-generator'
    Aug 17 06:57:54.669: INFO: stderr: ""
    Aug 17 06:57:54.669: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:57:54.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6568" for this suite. 08/17/23 06:57:54.672
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:57:54.675
Aug 17 06:57:54.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 06:57:54.675
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:54.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:54.693
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-3785 08/17/23 06:57:54.695
STEP: creating service affinity-clusterip in namespace services-3785 08/17/23 06:57:54.695
STEP: creating replication controller affinity-clusterip in namespace services-3785 08/17/23 06:57:54.709
I0817 06:57:54.713598      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3785, replica count: 3
I0817 06:57:57.764824      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 06:57:57.768: INFO: Creating new exec pod
Aug 17 06:57:57.771: INFO: Waiting up to 5m0s for pod "execpod-affinity5vfpw" in namespace "services-3785" to be "running"
Aug 17 06:57:57.773: INFO: Pod "execpod-affinity5vfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284057ms
Aug 17 06:57:59.776: INFO: Pod "execpod-affinity5vfpw": Phase="Running", Reason="", readiness=true. Elapsed: 2.004414321s
Aug 17 06:57:59.776: INFO: Pod "execpod-affinity5vfpw" satisfied condition "running"
Aug 17 06:58:00.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-3785 exec execpod-affinity5vfpw -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 17 06:58:00.913: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 17 06:58:00.913: INFO: stdout: ""
Aug 17 06:58:00.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-3785 exec execpod-affinity5vfpw -- /bin/sh -x -c nc -v -z -w 2 10.102.118.134 80'
Aug 17 06:58:01.037: INFO: stderr: "+ nc -v -z -w 2 10.102.118.134 80\nConnection to 10.102.118.134 80 port [tcp/http] succeeded!\n"
Aug 17 06:58:01.037: INFO: stdout: ""
Aug 17 06:58:01.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-3785 exec execpod-affinity5vfpw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.118.134:80/ ; done'
Aug 17 06:58:01.224: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n"
Aug 17 06:58:01.225: INFO: stdout: "\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p"
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
Aug 17 06:58:01.225: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3785, will wait for the garbage collector to delete the pods 08/17/23 06:58:01.232
Aug 17 06:58:01.290: INFO: Deleting ReplicationController affinity-clusterip took: 3.384655ms
Aug 17 06:58:01.391: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.080811ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 06:58:03.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3785" for this suite. 08/17/23 06:58:03.207
------------------------------
â€¢ [SLOW TEST] [8.535 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:57:54.675
    Aug 17 06:57:54.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 06:57:54.675
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:57:54.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:57:54.693
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-3785 08/17/23 06:57:54.695
    STEP: creating service affinity-clusterip in namespace services-3785 08/17/23 06:57:54.695
    STEP: creating replication controller affinity-clusterip in namespace services-3785 08/17/23 06:57:54.709
    I0817 06:57:54.713598      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3785, replica count: 3
    I0817 06:57:57.764824      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 06:57:57.768: INFO: Creating new exec pod
    Aug 17 06:57:57.771: INFO: Waiting up to 5m0s for pod "execpod-affinity5vfpw" in namespace "services-3785" to be "running"
    Aug 17 06:57:57.773: INFO: Pod "execpod-affinity5vfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284057ms
    Aug 17 06:57:59.776: INFO: Pod "execpod-affinity5vfpw": Phase="Running", Reason="", readiness=true. Elapsed: 2.004414321s
    Aug 17 06:57:59.776: INFO: Pod "execpod-affinity5vfpw" satisfied condition "running"
    Aug 17 06:58:00.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-3785 exec execpod-affinity5vfpw -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 17 06:58:00.913: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 17 06:58:00.913: INFO: stdout: ""
    Aug 17 06:58:00.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-3785 exec execpod-affinity5vfpw -- /bin/sh -x -c nc -v -z -w 2 10.102.118.134 80'
    Aug 17 06:58:01.037: INFO: stderr: "+ nc -v -z -w 2 10.102.118.134 80\nConnection to 10.102.118.134 80 port [tcp/http] succeeded!\n"
    Aug 17 06:58:01.037: INFO: stdout: ""
    Aug 17 06:58:01.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-3785 exec execpod-affinity5vfpw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.118.134:80/ ; done'
    Aug 17 06:58:01.224: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.118.134:80/\n"
    Aug 17 06:58:01.225: INFO: stdout: "\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p\naffinity-clusterip-z425p"
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Received response from host: affinity-clusterip-z425p
    Aug 17 06:58:01.225: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3785, will wait for the garbage collector to delete the pods 08/17/23 06:58:01.232
    Aug 17 06:58:01.290: INFO: Deleting ReplicationController affinity-clusterip took: 3.384655ms
    Aug 17 06:58:01.391: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.080811ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:58:03.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3785" for this suite. 08/17/23 06:58:03.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:58:03.21
Aug 17 06:58:03.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename job 08/17/23 06:58:03.211
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:03.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:03.223
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/17/23 06:58:03.226
STEP: Ensuring job reaches completions 08/17/23 06:58:03.253
STEP: Ensuring pods with index for job exist 08/17/23 06:58:11.256
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 06:58:11.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6408" for this suite. 08/17/23 06:58:11.261
------------------------------
â€¢ [SLOW TEST] [8.053 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:58:03.21
    Aug 17 06:58:03.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename job 08/17/23 06:58:03.211
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:03.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:03.223
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/17/23 06:58:03.226
    STEP: Ensuring job reaches completions 08/17/23 06:58:03.253
    STEP: Ensuring pods with index for job exist 08/17/23 06:58:11.256
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:58:11.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6408" for this suite. 08/17/23 06:58:11.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:58:11.264
Aug 17 06:58:11.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replicaset 08/17/23 06:58:11.265
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:11.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:11.281
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/17/23 06:58:11.285
Aug 17 06:58:11.294: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2817" to be "running and ready"
Aug 17 06:58:11.299: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.807115ms
Aug 17 06:58:11.299: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 06:58:13.303: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008746067s
Aug 17 06:58:13.303: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 17 06:58:13.303: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/17/23 06:58:13.305
STEP: Then the orphan pod is adopted 08/17/23 06:58:13.31
STEP: When the matched label of one of its pods change 08/17/23 06:58:14.315
Aug 17 06:58:14.317: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/17/23 06:58:14.323
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 06:58:15.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2817" for this suite. 08/17/23 06:58:15.331
------------------------------
â€¢ [4.071 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:58:11.264
    Aug 17 06:58:11.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replicaset 08/17/23 06:58:11.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:11.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:11.281
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/17/23 06:58:11.285
    Aug 17 06:58:11.294: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2817" to be "running and ready"
    Aug 17 06:58:11.299: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.807115ms
    Aug 17 06:58:11.299: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 06:58:13.303: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008746067s
    Aug 17 06:58:13.303: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 17 06:58:13.303: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/17/23 06:58:13.305
    STEP: Then the orphan pod is adopted 08/17/23 06:58:13.31
    STEP: When the matched label of one of its pods change 08/17/23 06:58:14.315
    Aug 17 06:58:14.317: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/17/23 06:58:14.323
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:58:15.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2817" for this suite. 08/17/23 06:58:15.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:58:15.336
Aug 17 06:58:15.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 06:58:15.337
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:15.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:15.348
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 06:58:15.351
Aug 17 06:58:15.362: INFO: Waiting up to 5m0s for pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc" in namespace "emptydir-6466" to be "Succeeded or Failed"
Aug 17 06:58:15.365: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000997ms
Aug 17 06:58:17.368: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006049362s
Aug 17 06:58:19.368: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006218917s
STEP: Saw pod success 08/17/23 06:58:19.368
Aug 17 06:58:19.369: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc" satisfied condition "Succeeded or Failed"
Aug 17 06:58:19.370: INFO: Trying to get logs from node yst-node2 pod pod-1f2241a2-84f3-402e-b4d3-cee609393cdc container test-container: <nil>
STEP: delete the pod 08/17/23 06:58:19.373
Aug 17 06:58:19.378: INFO: Waiting for pod pod-1f2241a2-84f3-402e-b4d3-cee609393cdc to disappear
Aug 17 06:58:19.380: INFO: Pod pod-1f2241a2-84f3-402e-b4d3-cee609393cdc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 06:58:19.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6466" for this suite. 08/17/23 06:58:19.382
------------------------------
â€¢ [4.048 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:58:15.336
    Aug 17 06:58:15.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 06:58:15.337
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:15.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:15.348
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 06:58:15.351
    Aug 17 06:58:15.362: INFO: Waiting up to 5m0s for pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc" in namespace "emptydir-6466" to be "Succeeded or Failed"
    Aug 17 06:58:15.365: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000997ms
    Aug 17 06:58:17.368: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006049362s
    Aug 17 06:58:19.368: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006218917s
    STEP: Saw pod success 08/17/23 06:58:19.368
    Aug 17 06:58:19.369: INFO: Pod "pod-1f2241a2-84f3-402e-b4d3-cee609393cdc" satisfied condition "Succeeded or Failed"
    Aug 17 06:58:19.370: INFO: Trying to get logs from node yst-node2 pod pod-1f2241a2-84f3-402e-b4d3-cee609393cdc container test-container: <nil>
    STEP: delete the pod 08/17/23 06:58:19.373
    Aug 17 06:58:19.378: INFO: Waiting for pod pod-1f2241a2-84f3-402e-b4d3-cee609393cdc to disappear
    Aug 17 06:58:19.380: INFO: Pod pod-1f2241a2-84f3-402e-b4d3-cee609393cdc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:58:19.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6466" for this suite. 08/17/23 06:58:19.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:58:19.385
Aug 17 06:58:19.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename subpath 08/17/23 06:58:19.386
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:19.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:19.413
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 06:58:19.417
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-9qpk 08/17/23 06:58:19.447
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 06:58:19.447
Aug 17 06:58:19.462: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9qpk" in namespace "subpath-9552" to be "Succeeded or Failed"
Aug 17 06:58:19.469: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.7993ms
Aug 17 06:58:21.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.00915121s
Aug 17 06:58:23.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 4.009448942s
Aug 17 06:58:25.502: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 6.03962863s
Aug 17 06:58:27.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 8.009428675s
Aug 17 06:58:29.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 10.009288635s
Aug 17 06:58:31.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 12.010016892s
Aug 17 06:58:33.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 14.010190756s
Aug 17 06:58:35.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 16.009501915s
Aug 17 06:58:37.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 18.010338486s
Aug 17 06:58:39.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 20.0092084s
Aug 17 06:58:41.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=false. Elapsed: 22.00962379s
Aug 17 06:58:43.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009483778s
STEP: Saw pod success 08/17/23 06:58:43.472
Aug 17 06:58:43.472: INFO: Pod "pod-subpath-test-configmap-9qpk" satisfied condition "Succeeded or Failed"
Aug 17 06:58:43.474: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-configmap-9qpk container test-container-subpath-configmap-9qpk: <nil>
STEP: delete the pod 08/17/23 06:58:43.477
Aug 17 06:58:43.483: INFO: Waiting for pod pod-subpath-test-configmap-9qpk to disappear
Aug 17 06:58:43.485: INFO: Pod pod-subpath-test-configmap-9qpk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9qpk 08/17/23 06:58:43.485
Aug 17 06:58:43.485: INFO: Deleting pod "pod-subpath-test-configmap-9qpk" in namespace "subpath-9552"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 06:58:43.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9552" for this suite. 08/17/23 06:58:43.489
------------------------------
â€¢ [SLOW TEST] [24.108 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:58:19.385
    Aug 17 06:58:19.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename subpath 08/17/23 06:58:19.386
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:19.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:19.413
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 06:58:19.417
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-9qpk 08/17/23 06:58:19.447
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 06:58:19.447
    Aug 17 06:58:19.462: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9qpk" in namespace "subpath-9552" to be "Succeeded or Failed"
    Aug 17 06:58:19.469: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.7993ms
    Aug 17 06:58:21.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.00915121s
    Aug 17 06:58:23.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 4.009448942s
    Aug 17 06:58:25.502: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 6.03962863s
    Aug 17 06:58:27.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 8.009428675s
    Aug 17 06:58:29.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 10.009288635s
    Aug 17 06:58:31.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 12.010016892s
    Aug 17 06:58:33.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 14.010190756s
    Aug 17 06:58:35.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 16.009501915s
    Aug 17 06:58:37.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 18.010338486s
    Aug 17 06:58:39.471: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=true. Elapsed: 20.0092084s
    Aug 17 06:58:41.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Running", Reason="", readiness=false. Elapsed: 22.00962379s
    Aug 17 06:58:43.472: INFO: Pod "pod-subpath-test-configmap-9qpk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009483778s
    STEP: Saw pod success 08/17/23 06:58:43.472
    Aug 17 06:58:43.472: INFO: Pod "pod-subpath-test-configmap-9qpk" satisfied condition "Succeeded or Failed"
    Aug 17 06:58:43.474: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-configmap-9qpk container test-container-subpath-configmap-9qpk: <nil>
    STEP: delete the pod 08/17/23 06:58:43.477
    Aug 17 06:58:43.483: INFO: Waiting for pod pod-subpath-test-configmap-9qpk to disappear
    Aug 17 06:58:43.485: INFO: Pod pod-subpath-test-configmap-9qpk no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-9qpk 08/17/23 06:58:43.485
    Aug 17 06:58:43.485: INFO: Deleting pod "pod-subpath-test-configmap-9qpk" in namespace "subpath-9552"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:58:43.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9552" for this suite. 08/17/23 06:58:43.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:58:43.494
Aug 17 06:58:43.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename subpath 08/17/23 06:58:43.495
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:43.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:43.505
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 06:58:43.507
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-p276 08/17/23 06:58:43.535
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 06:58:43.535
Aug 17 06:58:43.541: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-p276" in namespace "subpath-8921" to be "Succeeded or Failed"
Aug 17 06:58:43.542: INFO: Pod "pod-subpath-test-secret-p276": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455685ms
Aug 17 06:58:45.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 2.004687005s
Aug 17 06:58:47.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 4.004851581s
Aug 17 06:58:49.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 6.004418739s
Aug 17 06:58:51.546: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 8.005258043s
Aug 17 06:58:53.546: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 10.005315614s
Aug 17 06:58:55.546: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 12.005242477s
Aug 17 06:58:57.544: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 14.003828213s
Aug 17 06:58:59.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 16.004749548s
Aug 17 06:59:01.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 18.004754696s
Aug 17 06:59:03.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 20.004667341s
Aug 17 06:59:05.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=false. Elapsed: 22.004335354s
Aug 17 06:59:07.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004252541s
STEP: Saw pod success 08/17/23 06:59:07.545
Aug 17 06:59:07.545: INFO: Pod "pod-subpath-test-secret-p276" satisfied condition "Succeeded or Failed"
Aug 17 06:59:07.547: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-secret-p276 container test-container-subpath-secret-p276: <nil>
STEP: delete the pod 08/17/23 06:59:07.551
Aug 17 06:59:07.557: INFO: Waiting for pod pod-subpath-test-secret-p276 to disappear
Aug 17 06:59:07.560: INFO: Pod pod-subpath-test-secret-p276 no longer exists
STEP: Deleting pod pod-subpath-test-secret-p276 08/17/23 06:59:07.56
Aug 17 06:59:07.560: INFO: Deleting pod "pod-subpath-test-secret-p276" in namespace "subpath-8921"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 06:59:07.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8921" for this suite. 08/17/23 06:59:07.564
------------------------------
â€¢ [SLOW TEST] [24.074 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:58:43.494
    Aug 17 06:58:43.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename subpath 08/17/23 06:58:43.495
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:58:43.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:58:43.505
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 06:58:43.507
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-p276 08/17/23 06:58:43.535
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 06:58:43.535
    Aug 17 06:58:43.541: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-p276" in namespace "subpath-8921" to be "Succeeded or Failed"
    Aug 17 06:58:43.542: INFO: Pod "pod-subpath-test-secret-p276": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455685ms
    Aug 17 06:58:45.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 2.004687005s
    Aug 17 06:58:47.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 4.004851581s
    Aug 17 06:58:49.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 6.004418739s
    Aug 17 06:58:51.546: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 8.005258043s
    Aug 17 06:58:53.546: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 10.005315614s
    Aug 17 06:58:55.546: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 12.005242477s
    Aug 17 06:58:57.544: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 14.003828213s
    Aug 17 06:58:59.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 16.004749548s
    Aug 17 06:59:01.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 18.004754696s
    Aug 17 06:59:03.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=true. Elapsed: 20.004667341s
    Aug 17 06:59:05.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Running", Reason="", readiness=false. Elapsed: 22.004335354s
    Aug 17 06:59:07.545: INFO: Pod "pod-subpath-test-secret-p276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004252541s
    STEP: Saw pod success 08/17/23 06:59:07.545
    Aug 17 06:59:07.545: INFO: Pod "pod-subpath-test-secret-p276" satisfied condition "Succeeded or Failed"
    Aug 17 06:59:07.547: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-secret-p276 container test-container-subpath-secret-p276: <nil>
    STEP: delete the pod 08/17/23 06:59:07.551
    Aug 17 06:59:07.557: INFO: Waiting for pod pod-subpath-test-secret-p276 to disappear
    Aug 17 06:59:07.560: INFO: Pod pod-subpath-test-secret-p276 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-p276 08/17/23 06:59:07.56
    Aug 17 06:59:07.560: INFO: Deleting pod "pod-subpath-test-secret-p276" in namespace "subpath-8921"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:59:07.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8921" for this suite. 08/17/23 06:59:07.564
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:59:07.569
Aug 17 06:59:07.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 06:59:07.569
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:07.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:07.579
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 06:59:07.608
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 06:59:07.969
STEP: Deploying the webhook pod 08/17/23 06:59:07.974
STEP: Wait for the deployment to be ready 08/17/23 06:59:07.982
Aug 17 06:59:07.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 06:59:09.996
STEP: Verifying the service has paired with the endpoint 08/17/23 06:59:10.004
Aug 17 06:59:11.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 06:59:11.007
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 06:59:11.018
STEP: Creating a dummy validating-webhook-configuration object 08/17/23 06:59:11.028
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/17/23 06:59:11.034
STEP: Creating a dummy mutating-webhook-configuration object 08/17/23 06:59:11.037
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/17/23 06:59:11.042
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 06:59:11.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2721" for this suite. 08/17/23 06:59:11.078
STEP: Destroying namespace "webhook-2721-markers" for this suite. 08/17/23 06:59:11.082
------------------------------
â€¢ [3.520 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:59:07.569
    Aug 17 06:59:07.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 06:59:07.569
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:07.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:07.579
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 06:59:07.608
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 06:59:07.969
    STEP: Deploying the webhook pod 08/17/23 06:59:07.974
    STEP: Wait for the deployment to be ready 08/17/23 06:59:07.982
    Aug 17 06:59:07.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 06:59:09.996
    STEP: Verifying the service has paired with the endpoint 08/17/23 06:59:10.004
    Aug 17 06:59:11.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 06:59:11.007
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 06:59:11.018
    STEP: Creating a dummy validating-webhook-configuration object 08/17/23 06:59:11.028
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/17/23 06:59:11.034
    STEP: Creating a dummy mutating-webhook-configuration object 08/17/23 06:59:11.037
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/17/23 06:59:11.042
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:59:11.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2721" for this suite. 08/17/23 06:59:11.078
    STEP: Destroying namespace "webhook-2721-markers" for this suite. 08/17/23 06:59:11.082
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:59:11.088
Aug 17 06:59:11.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 06:59:11.089
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:11.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:11.11
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/17/23 06:59:11.118
STEP: submitting the pod to kubernetes 08/17/23 06:59:11.118
Aug 17 06:59:11.125: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" in namespace "pods-7746" to be "running and ready"
Aug 17 06:59:11.130: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914821ms
Aug 17 06:59:11.130: INFO: The phase of Pod pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda is Pending, waiting for it to be Running (with Ready = true)
Aug 17 06:59:13.134: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.008204461s
Aug 17 06:59:13.134: INFO: The phase of Pod pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda is Running (Ready = true)
Aug 17 06:59:13.134: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/17/23 06:59:13.136
STEP: updating the pod 08/17/23 06:59:13.138
Aug 17 06:59:13.648: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda"
Aug 17 06:59:13.648: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" in namespace "pods-7746" to be "terminated with reason DeadlineExceeded"
Aug 17 06:59:13.651: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.748085ms
Aug 17 06:59:15.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.005270976s
Aug 17 06:59:17.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=false. Elapsed: 4.005183446s
Aug 17 06:59:19.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005041533s
Aug 17 06:59:19.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 06:59:19.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7746" for this suite. 08/17/23 06:59:19.656
------------------------------
â€¢ [SLOW TEST] [8.570 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:59:11.088
    Aug 17 06:59:11.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 06:59:11.089
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:11.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:11.11
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/17/23 06:59:11.118
    STEP: submitting the pod to kubernetes 08/17/23 06:59:11.118
    Aug 17 06:59:11.125: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" in namespace "pods-7746" to be "running and ready"
    Aug 17 06:59:11.130: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914821ms
    Aug 17 06:59:11.130: INFO: The phase of Pod pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 06:59:13.134: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.008204461s
    Aug 17 06:59:13.134: INFO: The phase of Pod pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda is Running (Ready = true)
    Aug 17 06:59:13.134: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/17/23 06:59:13.136
    STEP: updating the pod 08/17/23 06:59:13.138
    Aug 17 06:59:13.648: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda"
    Aug 17 06:59:13.648: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" in namespace "pods-7746" to be "terminated with reason DeadlineExceeded"
    Aug 17 06:59:13.651: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.748085ms
    Aug 17 06:59:15.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.005270976s
    Aug 17 06:59:17.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Running", Reason="", readiness=false. Elapsed: 4.005183446s
    Aug 17 06:59:19.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005041533s
    Aug 17 06:59:19.653: INFO: Pod "pod-update-activedeadlineseconds-e7c4bcea-dced-41e5-9369-b16004ac6eda" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:59:19.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7746" for this suite. 08/17/23 06:59:19.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:59:19.661
Aug 17 06:59:19.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 06:59:19.662
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:19.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:19.681
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/17/23 06:59:19.684
Aug 17 06:59:19.684: INFO: namespace kubectl-4293
Aug 17 06:59:19.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 create -f -'
Aug 17 06:59:20.997: INFO: stderr: ""
Aug 17 06:59:20.997: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/17/23 06:59:20.997
Aug 17 06:59:22.000: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 06:59:22.000: INFO: Found 1 / 1
Aug 17 06:59:22.000: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 17 06:59:22.002: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 06:59:22.002: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 06:59:22.002: INFO: wait on agnhost-primary startup in kubectl-4293 
Aug 17 06:59:22.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 logs agnhost-primary-qswp9 agnhost-primary'
Aug 17 06:59:22.089: INFO: stderr: ""
Aug 17 06:59:22.089: INFO: stdout: "Paused\n"
STEP: exposing RC 08/17/23 06:59:22.089
Aug 17 06:59:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 17 06:59:22.186: INFO: stderr: ""
Aug 17 06:59:22.186: INFO: stdout: "service/rm2 exposed\n"
Aug 17 06:59:22.188: INFO: Service rm2 in namespace kubectl-4293 found.
STEP: exposing service 08/17/23 06:59:24.192
Aug 17 06:59:24.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 17 06:59:24.287: INFO: stderr: ""
Aug 17 06:59:24.287: INFO: stdout: "service/rm3 exposed\n"
Aug 17 06:59:24.290: INFO: Service rm3 in namespace kubectl-4293 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 06:59:26.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4293" for this suite. 08/17/23 06:59:26.297
------------------------------
â€¢ [SLOW TEST] [6.640 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:59:19.661
    Aug 17 06:59:19.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 06:59:19.662
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:19.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:19.681
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/17/23 06:59:19.684
    Aug 17 06:59:19.684: INFO: namespace kubectl-4293
    Aug 17 06:59:19.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 create -f -'
    Aug 17 06:59:20.997: INFO: stderr: ""
    Aug 17 06:59:20.997: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/17/23 06:59:20.997
    Aug 17 06:59:22.000: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 06:59:22.000: INFO: Found 1 / 1
    Aug 17 06:59:22.000: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 17 06:59:22.002: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 06:59:22.002: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 17 06:59:22.002: INFO: wait on agnhost-primary startup in kubectl-4293 
    Aug 17 06:59:22.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 logs agnhost-primary-qswp9 agnhost-primary'
    Aug 17 06:59:22.089: INFO: stderr: ""
    Aug 17 06:59:22.089: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/17/23 06:59:22.089
    Aug 17 06:59:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 17 06:59:22.186: INFO: stderr: ""
    Aug 17 06:59:22.186: INFO: stdout: "service/rm2 exposed\n"
    Aug 17 06:59:22.188: INFO: Service rm2 in namespace kubectl-4293 found.
    STEP: exposing service 08/17/23 06:59:24.192
    Aug 17 06:59:24.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4293 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 17 06:59:24.287: INFO: stderr: ""
    Aug 17 06:59:24.287: INFO: stdout: "service/rm3 exposed\n"
    Aug 17 06:59:24.290: INFO: Service rm3 in namespace kubectl-4293 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:59:26.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4293" for this suite. 08/17/23 06:59:26.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:59:26.301
Aug 17 06:59:26.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 06:59:26.302
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:26.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:26.313
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2942 08/17/23 06:59:26.315
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2942 08/17/23 06:59:26.321
Aug 17 06:59:26.331: INFO: Found 0 stateful pods, waiting for 1
Aug 17 06:59:36.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/17/23 06:59:36.338
STEP: updating a scale subresource 08/17/23 06:59:36.339
STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 06:59:36.342
STEP: Patch a scale subresource 08/17/23 06:59:36.344
STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 06:59:36.349
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 06:59:36.351: INFO: Deleting all statefulset in ns statefulset-2942
Aug 17 06:59:36.353: INFO: Scaling statefulset ss to 0
Aug 17 06:59:46.364: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 06:59:46.365: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 06:59:46.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2942" for this suite. 08/17/23 06:59:46.378
------------------------------
â€¢ [SLOW TEST] [20.081 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:59:26.301
    Aug 17 06:59:26.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 06:59:26.302
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:26.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:26.313
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2942 08/17/23 06:59:26.315
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2942 08/17/23 06:59:26.321
    Aug 17 06:59:26.331: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 06:59:36.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/17/23 06:59:36.338
    STEP: updating a scale subresource 08/17/23 06:59:36.339
    STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 06:59:36.342
    STEP: Patch a scale subresource 08/17/23 06:59:36.344
    STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 06:59:36.349
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 06:59:36.351: INFO: Deleting all statefulset in ns statefulset-2942
    Aug 17 06:59:36.353: INFO: Scaling statefulset ss to 0
    Aug 17 06:59:46.364: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 06:59:46.365: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:59:46.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2942" for this suite. 08/17/23 06:59:46.378
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:59:46.382
Aug 17 06:59:46.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 06:59:46.383
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:46.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:46.407
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 17 06:59:46.410: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 17 06:59:46.477: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 06:59:51.479: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 06:59:51.48
Aug 17 06:59:51.480: INFO: Creating deployment "test-rolling-update-deployment"
Aug 17 06:59:51.483: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 17 06:59:51.487: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 17 06:59:53.492: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 17 06:59:53.493: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 06:59:53.498: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2752  2156ada7-8ac4-44d3-b2d6-07b86eac6000 27004697 1 2023-08-17 06:59:51 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-17 06:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083806f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 06:59:51 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-17 06:59:53 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 06:59:53.499: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2752  69d73694-dfb0-445f-8707-343d998b2c0a 27004687 1 2023-08-17 06:59:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2156ada7-8ac4-44d3-b2d6-07b86eac6000 0xc00806fa27 0xc00806fa28}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2156ada7-8ac4-44d3-b2d6-07b86eac6000\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00806fad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 06:59:53.499: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 17 06:59:53.499: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2752  45c38c34-0fff-4e47-a247-ccb423c997f2 27004696 2 2023-08-17 06:59:46 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2156ada7-8ac4-44d3-b2d6-07b86eac6000 0xc00806f8f7 0xc00806f8f8}] [] [{e2e.test Update apps/v1 2023-08-17 06:59:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2156ada7-8ac4-44d3-b2d6-07b86eac6000\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00806f9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 06:59:53.501: INFO: Pod "test-rolling-update-deployment-7549d9f46d-5k5kf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-5k5kf test-rolling-update-deployment-7549d9f46d- deployment-2752  314e51fe-9aab-4543-b7ca-09772803d801 27004686 0 2023-08-17 06:59:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:fa64d590fe281343ed71081aa309306dc78af5f26963775d85985cab22dfc7a6 cni.projectcalico.org/podIP:172.32.238.110/32 cni.projectcalico.org/podIPs:172.32.238.110/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 69d73694-dfb0-445f-8707-343d998b2c0a 0xc00806ff37 0xc00806ff38}] [] [{kube-controller-manager Update v1 2023-08-17 06:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d73694-dfb0-445f-8707-343d998b2c0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57tnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57tnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.110,StartTime:2023-08-17 06:59:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:59:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://4c515a3918c99630d1c2c13deed835af39ec3a8c11d626cb5d34f29b4792e01f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 06:59:53.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2752" for this suite. 08/17/23 06:59:53.503
------------------------------
â€¢ [SLOW TEST] [7.124 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:59:46.382
    Aug 17 06:59:46.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 06:59:46.383
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:46.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:46.407
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 17 06:59:46.410: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 17 06:59:46.477: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 06:59:51.479: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 06:59:51.48
    Aug 17 06:59:51.480: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 17 06:59:51.483: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 17 06:59:51.487: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 17 06:59:53.492: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 17 06:59:53.493: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 06:59:53.498: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2752  2156ada7-8ac4-44d3-b2d6-07b86eac6000 27004697 1 2023-08-17 06:59:51 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-17 06:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083806f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 06:59:51 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-17 06:59:53 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 06:59:53.499: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2752  69d73694-dfb0-445f-8707-343d998b2c0a 27004687 1 2023-08-17 06:59:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2156ada7-8ac4-44d3-b2d6-07b86eac6000 0xc00806fa27 0xc00806fa28}] [] [{kube-controller-manager Update apps/v1 2023-08-17 06:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2156ada7-8ac4-44d3-b2d6-07b86eac6000\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00806fad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 06:59:53.499: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 17 06:59:53.499: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2752  45c38c34-0fff-4e47-a247-ccb423c997f2 27004696 2 2023-08-17 06:59:46 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2156ada7-8ac4-44d3-b2d6-07b86eac6000 0xc00806f8f7 0xc00806f8f8}] [] [{e2e.test Update apps/v1 2023-08-17 06:59:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2156ada7-8ac4-44d3-b2d6-07b86eac6000\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00806f9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 06:59:53.501: INFO: Pod "test-rolling-update-deployment-7549d9f46d-5k5kf" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-5k5kf test-rolling-update-deployment-7549d9f46d- deployment-2752  314e51fe-9aab-4543-b7ca-09772803d801 27004686 0 2023-08-17 06:59:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:fa64d590fe281343ed71081aa309306dc78af5f26963775d85985cab22dfc7a6 cni.projectcalico.org/podIP:172.32.238.110/32 cni.projectcalico.org/podIPs:172.32.238.110/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 69d73694-dfb0-445f-8707-343d998b2c0a 0xc00806ff37 0xc00806ff38}] [] [{kube-controller-manager Update v1 2023-08-17 06:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d73694-dfb0-445f-8707-343d998b2c0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 06:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 06:59:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57tnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57tnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 06:59:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.110,StartTime:2023-08-17 06:59:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 06:59:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://4c515a3918c99630d1c2c13deed835af39ec3a8c11d626cb5d34f29b4792e01f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 06:59:53.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2752" for this suite. 08/17/23 06:59:53.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 06:59:53.507
Aug 17 06:59:53.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename cronjob 08/17/23 06:59:53.508
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:53.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:53.573
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/17/23 06:59:53.577
STEP: Ensuring a job is scheduled 08/17/23 06:59:53.582
STEP: Ensuring exactly one is scheduled 08/17/23 07:00:01.585
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 07:00:01.587
STEP: Ensuring the job is replaced with a new one 08/17/23 07:00:01.589
STEP: Removing cronjob 08/17/23 07:01:01.591
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 07:01:01.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7373" for this suite. 08/17/23 07:01:01.637
------------------------------
â€¢ [SLOW TEST] [68.134 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 06:59:53.507
    Aug 17 06:59:53.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename cronjob 08/17/23 06:59:53.508
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 06:59:53.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 06:59:53.573
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/17/23 06:59:53.577
    STEP: Ensuring a job is scheduled 08/17/23 06:59:53.582
    STEP: Ensuring exactly one is scheduled 08/17/23 07:00:01.585
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 07:00:01.587
    STEP: Ensuring the job is replaced with a new one 08/17/23 07:00:01.589
    STEP: Removing cronjob 08/17/23 07:01:01.591
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:01:01.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7373" for this suite. 08/17/23 07:01:01.637
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:01:01.641
Aug 17 07:01:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename job 08/17/23 07:01:01.642
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:01.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:01.655
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/17/23 07:01:01.657
STEP: Ensuring job reaches completions 08/17/23 07:01:01.663
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 07:01:13.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1378" for this suite. 08/17/23 07:01:13.671
------------------------------
â€¢ [SLOW TEST] [12.033 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:01:01.641
    Aug 17 07:01:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename job 08/17/23 07:01:01.642
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:01.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:01.655
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/17/23 07:01:01.657
    STEP: Ensuring job reaches completions 08/17/23 07:01:01.663
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:01:13.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1378" for this suite. 08/17/23 07:01:13.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:01:13.675
Aug 17 07:01:13.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:01:13.676
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:13.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:13.687
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:01:13.722
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:01:14.166
STEP: Deploying the webhook pod 08/17/23 07:01:14.171
STEP: Wait for the deployment to be ready 08/17/23 07:01:14.179
Aug 17 07:01:14.184: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:01:16.194
STEP: Verifying the service has paired with the endpoint 08/17/23 07:01:16.202
Aug 17 07:01:17.202: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/17/23 07:01:17.205
STEP: create a configmap that should be updated by the webhook 08/17/23 07:01:17.216
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:01:17.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6231" for this suite. 08/17/23 07:01:17.257
STEP: Destroying namespace "webhook-6231-markers" for this suite. 08/17/23 07:01:17.261
------------------------------
â€¢ [3.593 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:01:13.675
    Aug 17 07:01:13.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:01:13.676
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:13.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:13.687
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:01:13.722
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:01:14.166
    STEP: Deploying the webhook pod 08/17/23 07:01:14.171
    STEP: Wait for the deployment to be ready 08/17/23 07:01:14.179
    Aug 17 07:01:14.184: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:01:16.194
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:01:16.202
    Aug 17 07:01:17.202: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/17/23 07:01:17.205
    STEP: create a configmap that should be updated by the webhook 08/17/23 07:01:17.216
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:01:17.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6231" for this suite. 08/17/23 07:01:17.257
    STEP: Destroying namespace "webhook-6231-markers" for this suite. 08/17/23 07:01:17.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:01:17.272
Aug 17 07:01:17.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename discovery 08/17/23 07:01:17.273
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:17.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:17.289
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/17/23 07:01:17.292
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 17 07:01:17.752: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 17 07:01:17.753: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 17 07:01:17.753: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 17 07:01:17.753: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 17 07:01:17.753: INFO: Checking APIGroup: apps
Aug 17 07:01:17.753: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 17 07:01:17.753: INFO: Versions found [{apps/v1 v1}]
Aug 17 07:01:17.753: INFO: apps/v1 matches apps/v1
Aug 17 07:01:17.753: INFO: Checking APIGroup: events.k8s.io
Aug 17 07:01:17.754: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 17 07:01:17.754: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 17 07:01:17.754: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 17 07:01:17.754: INFO: Checking APIGroup: authentication.k8s.io
Aug 17 07:01:17.755: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 17 07:01:17.755: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 17 07:01:17.755: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 17 07:01:17.755: INFO: Checking APIGroup: authorization.k8s.io
Aug 17 07:01:17.756: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 17 07:01:17.756: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 17 07:01:17.756: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 17 07:01:17.756: INFO: Checking APIGroup: autoscaling
Aug 17 07:01:17.756: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 17 07:01:17.756: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 17 07:01:17.756: INFO: autoscaling/v2 matches autoscaling/v2
Aug 17 07:01:17.756: INFO: Checking APIGroup: batch
Aug 17 07:01:17.757: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 17 07:01:17.757: INFO: Versions found [{batch/v1 v1}]
Aug 17 07:01:17.757: INFO: batch/v1 matches batch/v1
Aug 17 07:01:17.757: INFO: Checking APIGroup: certificates.k8s.io
Aug 17 07:01:17.758: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 17 07:01:17.758: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 17 07:01:17.758: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 17 07:01:17.758: INFO: Checking APIGroup: networking.k8s.io
Aug 17 07:01:17.758: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 17 07:01:17.758: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 17 07:01:17.758: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 17 07:01:17.758: INFO: Checking APIGroup: policy
Aug 17 07:01:17.759: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 17 07:01:17.759: INFO: Versions found [{policy/v1 v1}]
Aug 17 07:01:17.759: INFO: policy/v1 matches policy/v1
Aug 17 07:01:17.759: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 17 07:01:17.759: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 17 07:01:17.759: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 17 07:01:17.759: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 17 07:01:17.759: INFO: Checking APIGroup: storage.k8s.io
Aug 17 07:01:17.760: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 17 07:01:17.760: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 17 07:01:17.760: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 17 07:01:17.760: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 17 07:01:17.761: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 17 07:01:17.761: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 17 07:01:17.761: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 17 07:01:17.761: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 17 07:01:17.761: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 17 07:01:17.761: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 17 07:01:17.761: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 17 07:01:17.761: INFO: Checking APIGroup: scheduling.k8s.io
Aug 17 07:01:17.762: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 17 07:01:17.762: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 17 07:01:17.762: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 17 07:01:17.762: INFO: Checking APIGroup: coordination.k8s.io
Aug 17 07:01:17.762: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 17 07:01:17.762: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 17 07:01:17.762: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 17 07:01:17.762: INFO: Checking APIGroup: node.k8s.io
Aug 17 07:01:17.763: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 17 07:01:17.763: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 17 07:01:17.763: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 17 07:01:17.763: INFO: Checking APIGroup: discovery.k8s.io
Aug 17 07:01:17.764: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 17 07:01:17.764: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 17 07:01:17.764: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 17 07:01:17.764: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 17 07:01:17.764: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 17 07:01:17.764: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 17 07:01:17.764: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 17 07:01:17.764: INFO: Checking APIGroup: crd.projectcalico.org
Aug 17 07:01:17.766: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 17 07:01:17.766: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 17 07:01:17.766: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 17 07:01:17.766: INFO: Checking APIGroup: monitoring.accordions.co.kr
Aug 17 07:01:17.766: INFO: PreferredVersion.GroupVersion: monitoring.accordions.co.kr/v1
Aug 17 07:01:17.766: INFO: Versions found [{monitoring.accordions.co.kr/v1 v1} {monitoring.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.766: INFO: monitoring.accordions.co.kr/v1 matches monitoring.accordions.co.kr/v1
Aug 17 07:01:17.766: INFO: Checking APIGroup: monitoring.coreos.com
Aug 17 07:01:17.767: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 17 07:01:17.767: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 17 07:01:17.767: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 17 07:01:17.767: INFO: Checking APIGroup: argoproj.io
Aug 17 07:01:17.767: INFO: PreferredVersion.GroupVersion: argoproj.io/v1alpha1
Aug 17 07:01:17.767: INFO: Versions found [{argoproj.io/v1alpha1 v1alpha1}]
Aug 17 07:01:17.767: INFO: argoproj.io/v1alpha1 matches argoproj.io/v1alpha1
Aug 17 07:01:17.767: INFO: Checking APIGroup: extensions.istio.io
Aug 17 07:01:17.768: INFO: PreferredVersion.GroupVersion: extensions.istio.io/v1alpha1
Aug 17 07:01:17.768: INFO: Versions found [{extensions.istio.io/v1alpha1 v1alpha1}]
Aug 17 07:01:17.768: INFO: extensions.istio.io/v1alpha1 matches extensions.istio.io/v1alpha1
Aug 17 07:01:17.768: INFO: Checking APIGroup: telemetry.istio.io
Aug 17 07:01:17.769: INFO: PreferredVersion.GroupVersion: telemetry.istio.io/v1alpha1
Aug 17 07:01:17.769: INFO: Versions found [{telemetry.istio.io/v1alpha1 v1alpha1}]
Aug 17 07:01:17.769: INFO: telemetry.istio.io/v1alpha1 matches telemetry.istio.io/v1alpha1
Aug 17 07:01:17.769: INFO: Checking APIGroup: networking.istio.io
Aug 17 07:01:17.769: INFO: PreferredVersion.GroupVersion: networking.istio.io/v1beta1
Aug 17 07:01:17.769: INFO: Versions found [{networking.istio.io/v1beta1 v1beta1} {networking.istio.io/v1alpha3 v1alpha3}]
Aug 17 07:01:17.769: INFO: networking.istio.io/v1beta1 matches networking.istio.io/v1beta1
Aug 17 07:01:17.769: INFO: Checking APIGroup: alert.accordions.co.kr
Aug 17 07:01:17.770: INFO: PreferredVersion.GroupVersion: alert.accordions.co.kr/v1beta1
Aug 17 07:01:17.770: INFO: Versions found [{alert.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.770: INFO: alert.accordions.co.kr/v1beta1 matches alert.accordions.co.kr/v1beta1
Aug 17 07:01:17.770: INFO: Checking APIGroup: auth.accordions.co.kr
Aug 17 07:01:17.770: INFO: PreferredVersion.GroupVersion: auth.accordions.co.kr/v1beta1
Aug 17 07:01:17.770: INFO: Versions found [{auth.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.770: INFO: auth.accordions.co.kr/v1beta1 matches auth.accordions.co.kr/v1beta1
Aug 17 07:01:17.770: INFO: Checking APIGroup: autoscaling.alibabacloud.com
Aug 17 07:01:17.771: INFO: PreferredVersion.GroupVersion: autoscaling.alibabacloud.com/v1beta1
Aug 17 07:01:17.771: INFO: Versions found [{autoscaling.alibabacloud.com/v1beta1 v1beta1}]
Aug 17 07:01:17.771: INFO: autoscaling.alibabacloud.com/v1beta1 matches autoscaling.alibabacloud.com/v1beta1
Aug 17 07:01:17.771: INFO: Checking APIGroup: cicd.accordions.co.kr
Aug 17 07:01:17.772: INFO: PreferredVersion.GroupVersion: cicd.accordions.co.kr/v1beta1
Aug 17 07:01:17.772: INFO: Versions found [{cicd.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.772: INFO: cicd.accordions.co.kr/v1beta1 matches cicd.accordions.co.kr/v1beta1
Aug 17 07:01:17.772: INFO: Checking APIGroup: cluster.accordions.co.kr
Aug 17 07:01:17.772: INFO: PreferredVersion.GroupVersion: cluster.accordions.co.kr/v1beta1
Aug 17 07:01:17.772: INFO: Versions found [{cluster.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.772: INFO: cluster.accordions.co.kr/v1beta1 matches cluster.accordions.co.kr/v1beta1
Aug 17 07:01:17.772: INFO: Checking APIGroup: event.accordions.co.kr
Aug 17 07:01:17.773: INFO: PreferredVersion.GroupVersion: event.accordions.co.kr/v1beta1
Aug 17 07:01:17.773: INFO: Versions found [{event.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.773: INFO: event.accordions.co.kr/v1beta1 matches event.accordions.co.kr/v1beta1
Aug 17 07:01:17.773: INFO: Checking APIGroup: helm.accordions.co.kr
Aug 17 07:01:17.773: INFO: PreferredVersion.GroupVersion: helm.accordions.co.kr/v1beta1
Aug 17 07:01:17.773: INFO: Versions found [{helm.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.773: INFO: helm.accordions.co.kr/v1beta1 matches helm.accordions.co.kr/v1beta1
Aug 17 07:01:17.773: INFO: Checking APIGroup: log.accordions.co.kr
Aug 17 07:01:17.774: INFO: PreferredVersion.GroupVersion: log.accordions.co.kr/v1beta1
Aug 17 07:01:17.774: INFO: Versions found [{log.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.774: INFO: log.accordions.co.kr/v1beta1 matches log.accordions.co.kr/v1beta1
Aug 17 07:01:17.774: INFO: Checking APIGroup: management.accordions.co.kr
Aug 17 07:01:17.774: INFO: PreferredVersion.GroupVersion: management.accordions.co.kr/v1beta1
Aug 17 07:01:17.774: INFO: Versions found [{management.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.774: INFO: management.accordions.co.kr/v1beta1 matches management.accordions.co.kr/v1beta1
Aug 17 07:01:17.774: INFO: Checking APIGroup: security.istio.io
Aug 17 07:01:17.775: INFO: PreferredVersion.GroupVersion: security.istio.io/v1beta1
Aug 17 07:01:17.775: INFO: Versions found [{security.istio.io/v1beta1 v1beta1}]
Aug 17 07:01:17.775: INFO: security.istio.io/v1beta1 matches security.istio.io/v1beta1
Aug 17 07:01:17.775: INFO: Checking APIGroup: activator.accordions.co.kr
Aug 17 07:01:17.775: INFO: PreferredVersion.GroupVersion: activator.accordions.co.kr/v1beta1
Aug 17 07:01:17.775: INFO: Versions found [{activator.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.775: INFO: activator.accordions.co.kr/v1beta1 matches activator.accordions.co.kr/v1beta1
Aug 17 07:01:17.775: INFO: Checking APIGroup: custom.metrics.k8s.io
Aug 17 07:01:17.776: INFO: PreferredVersion.GroupVersion: custom.metrics.k8s.io/v1beta1
Aug 17 07:01:17.776: INFO: Versions found [{custom.metrics.k8s.io/v1beta1 v1beta1}]
Aug 17 07:01:17.776: INFO: custom.metrics.k8s.io/v1beta1 matches custom.metrics.k8s.io/v1beta1
Aug 17 07:01:17.776: INFO: Checking APIGroup: keycloak.accordions.co.kr
Aug 17 07:01:17.776: INFO: PreferredVersion.GroupVersion: keycloak.accordions.co.kr/v1beta1
Aug 17 07:01:17.776: INFO: Versions found [{keycloak.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.776: INFO: keycloak.accordions.co.kr/v1beta1 matches keycloak.accordions.co.kr/v1beta1
Aug 17 07:01:17.776: INFO: Checking APIGroup: metrics.k8s.io
Aug 17 07:01:17.777: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 17 07:01:17.777: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 17 07:01:17.777: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
Aug 17 07:01:17.777: INFO: Checking APIGroup: token.accordions.co.kr
Aug 17 07:01:17.778: INFO: PreferredVersion.GroupVersion: token.accordions.co.kr/v1beta1
Aug 17 07:01:17.778: INFO: Versions found [{token.accordions.co.kr/v1beta1 v1beta1}]
Aug 17 07:01:17.778: INFO: token.accordions.co.kr/v1beta1 matches token.accordions.co.kr/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 17 07:01:17.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-738" for this suite. 08/17/23 07:01:17.781
------------------------------
â€¢ [0.513 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:01:17.272
    Aug 17 07:01:17.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename discovery 08/17/23 07:01:17.273
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:17.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:17.289
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/17/23 07:01:17.292
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 17 07:01:17.752: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 17 07:01:17.753: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 17 07:01:17.753: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 17 07:01:17.753: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 17 07:01:17.753: INFO: Checking APIGroup: apps
    Aug 17 07:01:17.753: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 17 07:01:17.753: INFO: Versions found [{apps/v1 v1}]
    Aug 17 07:01:17.753: INFO: apps/v1 matches apps/v1
    Aug 17 07:01:17.753: INFO: Checking APIGroup: events.k8s.io
    Aug 17 07:01:17.754: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 17 07:01:17.754: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 17 07:01:17.754: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 17 07:01:17.754: INFO: Checking APIGroup: authentication.k8s.io
    Aug 17 07:01:17.755: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 17 07:01:17.755: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 17 07:01:17.755: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 17 07:01:17.755: INFO: Checking APIGroup: authorization.k8s.io
    Aug 17 07:01:17.756: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 17 07:01:17.756: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 17 07:01:17.756: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 17 07:01:17.756: INFO: Checking APIGroup: autoscaling
    Aug 17 07:01:17.756: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 17 07:01:17.756: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 17 07:01:17.756: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 17 07:01:17.756: INFO: Checking APIGroup: batch
    Aug 17 07:01:17.757: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 17 07:01:17.757: INFO: Versions found [{batch/v1 v1}]
    Aug 17 07:01:17.757: INFO: batch/v1 matches batch/v1
    Aug 17 07:01:17.757: INFO: Checking APIGroup: certificates.k8s.io
    Aug 17 07:01:17.758: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 17 07:01:17.758: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 17 07:01:17.758: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 17 07:01:17.758: INFO: Checking APIGroup: networking.k8s.io
    Aug 17 07:01:17.758: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 17 07:01:17.758: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 17 07:01:17.758: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 17 07:01:17.758: INFO: Checking APIGroup: policy
    Aug 17 07:01:17.759: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 17 07:01:17.759: INFO: Versions found [{policy/v1 v1}]
    Aug 17 07:01:17.759: INFO: policy/v1 matches policy/v1
    Aug 17 07:01:17.759: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 17 07:01:17.759: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 17 07:01:17.759: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 17 07:01:17.759: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 17 07:01:17.759: INFO: Checking APIGroup: storage.k8s.io
    Aug 17 07:01:17.760: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 17 07:01:17.760: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 17 07:01:17.760: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 17 07:01:17.760: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 17 07:01:17.761: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 17 07:01:17.761: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 17 07:01:17.761: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 17 07:01:17.761: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 17 07:01:17.761: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 17 07:01:17.761: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 17 07:01:17.761: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 17 07:01:17.761: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 17 07:01:17.762: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 17 07:01:17.762: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 17 07:01:17.762: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 17 07:01:17.762: INFO: Checking APIGroup: coordination.k8s.io
    Aug 17 07:01:17.762: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 17 07:01:17.762: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 17 07:01:17.762: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 17 07:01:17.762: INFO: Checking APIGroup: node.k8s.io
    Aug 17 07:01:17.763: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 17 07:01:17.763: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 17 07:01:17.763: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 17 07:01:17.763: INFO: Checking APIGroup: discovery.k8s.io
    Aug 17 07:01:17.764: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 17 07:01:17.764: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 17 07:01:17.764: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 17 07:01:17.764: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 17 07:01:17.764: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 17 07:01:17.764: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 17 07:01:17.764: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 17 07:01:17.764: INFO: Checking APIGroup: crd.projectcalico.org
    Aug 17 07:01:17.766: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Aug 17 07:01:17.766: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Aug 17 07:01:17.766: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Aug 17 07:01:17.766: INFO: Checking APIGroup: monitoring.accordions.co.kr
    Aug 17 07:01:17.766: INFO: PreferredVersion.GroupVersion: monitoring.accordions.co.kr/v1
    Aug 17 07:01:17.766: INFO: Versions found [{monitoring.accordions.co.kr/v1 v1} {monitoring.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.766: INFO: monitoring.accordions.co.kr/v1 matches monitoring.accordions.co.kr/v1
    Aug 17 07:01:17.766: INFO: Checking APIGroup: monitoring.coreos.com
    Aug 17 07:01:17.767: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Aug 17 07:01:17.767: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Aug 17 07:01:17.767: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Aug 17 07:01:17.767: INFO: Checking APIGroup: argoproj.io
    Aug 17 07:01:17.767: INFO: PreferredVersion.GroupVersion: argoproj.io/v1alpha1
    Aug 17 07:01:17.767: INFO: Versions found [{argoproj.io/v1alpha1 v1alpha1}]
    Aug 17 07:01:17.767: INFO: argoproj.io/v1alpha1 matches argoproj.io/v1alpha1
    Aug 17 07:01:17.767: INFO: Checking APIGroup: extensions.istio.io
    Aug 17 07:01:17.768: INFO: PreferredVersion.GroupVersion: extensions.istio.io/v1alpha1
    Aug 17 07:01:17.768: INFO: Versions found [{extensions.istio.io/v1alpha1 v1alpha1}]
    Aug 17 07:01:17.768: INFO: extensions.istio.io/v1alpha1 matches extensions.istio.io/v1alpha1
    Aug 17 07:01:17.768: INFO: Checking APIGroup: telemetry.istio.io
    Aug 17 07:01:17.769: INFO: PreferredVersion.GroupVersion: telemetry.istio.io/v1alpha1
    Aug 17 07:01:17.769: INFO: Versions found [{telemetry.istio.io/v1alpha1 v1alpha1}]
    Aug 17 07:01:17.769: INFO: telemetry.istio.io/v1alpha1 matches telemetry.istio.io/v1alpha1
    Aug 17 07:01:17.769: INFO: Checking APIGroup: networking.istio.io
    Aug 17 07:01:17.769: INFO: PreferredVersion.GroupVersion: networking.istio.io/v1beta1
    Aug 17 07:01:17.769: INFO: Versions found [{networking.istio.io/v1beta1 v1beta1} {networking.istio.io/v1alpha3 v1alpha3}]
    Aug 17 07:01:17.769: INFO: networking.istio.io/v1beta1 matches networking.istio.io/v1beta1
    Aug 17 07:01:17.769: INFO: Checking APIGroup: alert.accordions.co.kr
    Aug 17 07:01:17.770: INFO: PreferredVersion.GroupVersion: alert.accordions.co.kr/v1beta1
    Aug 17 07:01:17.770: INFO: Versions found [{alert.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.770: INFO: alert.accordions.co.kr/v1beta1 matches alert.accordions.co.kr/v1beta1
    Aug 17 07:01:17.770: INFO: Checking APIGroup: auth.accordions.co.kr
    Aug 17 07:01:17.770: INFO: PreferredVersion.GroupVersion: auth.accordions.co.kr/v1beta1
    Aug 17 07:01:17.770: INFO: Versions found [{auth.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.770: INFO: auth.accordions.co.kr/v1beta1 matches auth.accordions.co.kr/v1beta1
    Aug 17 07:01:17.770: INFO: Checking APIGroup: autoscaling.alibabacloud.com
    Aug 17 07:01:17.771: INFO: PreferredVersion.GroupVersion: autoscaling.alibabacloud.com/v1beta1
    Aug 17 07:01:17.771: INFO: Versions found [{autoscaling.alibabacloud.com/v1beta1 v1beta1}]
    Aug 17 07:01:17.771: INFO: autoscaling.alibabacloud.com/v1beta1 matches autoscaling.alibabacloud.com/v1beta1
    Aug 17 07:01:17.771: INFO: Checking APIGroup: cicd.accordions.co.kr
    Aug 17 07:01:17.772: INFO: PreferredVersion.GroupVersion: cicd.accordions.co.kr/v1beta1
    Aug 17 07:01:17.772: INFO: Versions found [{cicd.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.772: INFO: cicd.accordions.co.kr/v1beta1 matches cicd.accordions.co.kr/v1beta1
    Aug 17 07:01:17.772: INFO: Checking APIGroup: cluster.accordions.co.kr
    Aug 17 07:01:17.772: INFO: PreferredVersion.GroupVersion: cluster.accordions.co.kr/v1beta1
    Aug 17 07:01:17.772: INFO: Versions found [{cluster.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.772: INFO: cluster.accordions.co.kr/v1beta1 matches cluster.accordions.co.kr/v1beta1
    Aug 17 07:01:17.772: INFO: Checking APIGroup: event.accordions.co.kr
    Aug 17 07:01:17.773: INFO: PreferredVersion.GroupVersion: event.accordions.co.kr/v1beta1
    Aug 17 07:01:17.773: INFO: Versions found [{event.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.773: INFO: event.accordions.co.kr/v1beta1 matches event.accordions.co.kr/v1beta1
    Aug 17 07:01:17.773: INFO: Checking APIGroup: helm.accordions.co.kr
    Aug 17 07:01:17.773: INFO: PreferredVersion.GroupVersion: helm.accordions.co.kr/v1beta1
    Aug 17 07:01:17.773: INFO: Versions found [{helm.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.773: INFO: helm.accordions.co.kr/v1beta1 matches helm.accordions.co.kr/v1beta1
    Aug 17 07:01:17.773: INFO: Checking APIGroup: log.accordions.co.kr
    Aug 17 07:01:17.774: INFO: PreferredVersion.GroupVersion: log.accordions.co.kr/v1beta1
    Aug 17 07:01:17.774: INFO: Versions found [{log.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.774: INFO: log.accordions.co.kr/v1beta1 matches log.accordions.co.kr/v1beta1
    Aug 17 07:01:17.774: INFO: Checking APIGroup: management.accordions.co.kr
    Aug 17 07:01:17.774: INFO: PreferredVersion.GroupVersion: management.accordions.co.kr/v1beta1
    Aug 17 07:01:17.774: INFO: Versions found [{management.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.774: INFO: management.accordions.co.kr/v1beta1 matches management.accordions.co.kr/v1beta1
    Aug 17 07:01:17.774: INFO: Checking APIGroup: security.istio.io
    Aug 17 07:01:17.775: INFO: PreferredVersion.GroupVersion: security.istio.io/v1beta1
    Aug 17 07:01:17.775: INFO: Versions found [{security.istio.io/v1beta1 v1beta1}]
    Aug 17 07:01:17.775: INFO: security.istio.io/v1beta1 matches security.istio.io/v1beta1
    Aug 17 07:01:17.775: INFO: Checking APIGroup: activator.accordions.co.kr
    Aug 17 07:01:17.775: INFO: PreferredVersion.GroupVersion: activator.accordions.co.kr/v1beta1
    Aug 17 07:01:17.775: INFO: Versions found [{activator.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.775: INFO: activator.accordions.co.kr/v1beta1 matches activator.accordions.co.kr/v1beta1
    Aug 17 07:01:17.775: INFO: Checking APIGroup: custom.metrics.k8s.io
    Aug 17 07:01:17.776: INFO: PreferredVersion.GroupVersion: custom.metrics.k8s.io/v1beta1
    Aug 17 07:01:17.776: INFO: Versions found [{custom.metrics.k8s.io/v1beta1 v1beta1}]
    Aug 17 07:01:17.776: INFO: custom.metrics.k8s.io/v1beta1 matches custom.metrics.k8s.io/v1beta1
    Aug 17 07:01:17.776: INFO: Checking APIGroup: keycloak.accordions.co.kr
    Aug 17 07:01:17.776: INFO: PreferredVersion.GroupVersion: keycloak.accordions.co.kr/v1beta1
    Aug 17 07:01:17.776: INFO: Versions found [{keycloak.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.776: INFO: keycloak.accordions.co.kr/v1beta1 matches keycloak.accordions.co.kr/v1beta1
    Aug 17 07:01:17.776: INFO: Checking APIGroup: metrics.k8s.io
    Aug 17 07:01:17.777: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Aug 17 07:01:17.777: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Aug 17 07:01:17.777: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    Aug 17 07:01:17.777: INFO: Checking APIGroup: token.accordions.co.kr
    Aug 17 07:01:17.778: INFO: PreferredVersion.GroupVersion: token.accordions.co.kr/v1beta1
    Aug 17 07:01:17.778: INFO: Versions found [{token.accordions.co.kr/v1beta1 v1beta1}]
    Aug 17 07:01:17.778: INFO: token.accordions.co.kr/v1beta1 matches token.accordions.co.kr/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:01:17.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-738" for this suite. 08/17/23 07:01:17.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:01:17.785
Aug 17 07:01:17.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename init-container 08/17/23 07:01:17.786
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:17.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:17.801
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/17/23 07:01:17.803
Aug 17 07:01:17.803: INFO: PodSpec: initContainers in spec.initContainers
Aug 17 07:02:04.390: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-13da873a-5df7-4a24-b487-9a51fbc3f669", GenerateName:"", Namespace:"init-container-2330", SelfLink:"", UID:"020fc064-0843-41c3-9e2f-88dd769d9483", ResourceVersion:"27006516", Generation:0, CreationTimestamp:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"803704822"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"1778b4528b8e739a4c51c92916adc1003339ce5efd903cdad7bc1b68ea024ddf", "cni.projectcalico.org/podIP":"172.32.238.80/32", "cni.projectcalico.org/podIPs":"172.32.238.80/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001298a38), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 7, 1, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001298a68), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 7, 2, 4, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001298ab0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-xk267", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00316c260), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xk267", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xk267", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xk267", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c827b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"yst-node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000bb50a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c82b80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c82ba0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c82ba8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c82bac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0010ffcc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.60.200.177", PodIP:"172.32.238.80", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.32.238.80"}}, StartTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000bb5180)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000bb51f0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://b2d51ffb84ec844b9dfa59c698ec61091155263bfb85ef2c646026ea8020e387", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00316c2e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00316c2c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc000c82e34)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:04.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2330" for this suite. 08/17/23 07:02:04.394
------------------------------
â€¢ [SLOW TEST] [46.614 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:01:17.785
    Aug 17 07:01:17.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename init-container 08/17/23 07:01:17.786
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:01:17.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:01:17.801
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/17/23 07:01:17.803
    Aug 17 07:01:17.803: INFO: PodSpec: initContainers in spec.initContainers
    Aug 17 07:02:04.390: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-13da873a-5df7-4a24-b487-9a51fbc3f669", GenerateName:"", Namespace:"init-container-2330", SelfLink:"", UID:"020fc064-0843-41c3-9e2f-88dd769d9483", ResourceVersion:"27006516", Generation:0, CreationTimestamp:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"803704822"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"1778b4528b8e739a4c51c92916adc1003339ce5efd903cdad7bc1b68ea024ddf", "cni.projectcalico.org/podIP":"172.32.238.80/32", "cni.projectcalico.org/podIPs":"172.32.238.80/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001298a38), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 7, 1, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001298a68), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 7, 2, 4, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001298ab0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-xk267", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00316c260), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xk267", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xk267", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xk267", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c827b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"yst-node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000bb50a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c82b80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c82ba0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c82ba8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c82bac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0010ffcc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.60.200.177", PodIP:"172.32.238.80", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.32.238.80"}}, StartTime:time.Date(2023, time.August, 17, 7, 1, 17, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000bb5180)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000bb51f0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://b2d51ffb84ec844b9dfa59c698ec61091155263bfb85ef2c646026ea8020e387", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00316c2e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00316c2c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc000c82e34)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:04.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2330" for this suite. 08/17/23 07:02:04.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:04.4
Aug 17 07:02:04.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pod-network-test 08/17/23 07:02:04.401
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:04.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:04.412
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5216 08/17/23 07:02:04.417
STEP: creating a selector 08/17/23 07:02:04.417
STEP: Creating the service pods in kubernetes 08/17/23 07:02:04.417
Aug 17 07:02:04.417: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 07:02:04.449: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5216" to be "running and ready"
Aug 17 07:02:04.451: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.670503ms
Aug 17 07:02:04.451: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:02:06.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005132293s
Aug 17 07:02:06.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:08.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005966238s
Aug 17 07:02:08.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:10.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004564355s
Aug 17 07:02:10.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:12.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005042464s
Aug 17 07:02:12.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:14.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00482259s
Aug 17 07:02:14.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:16.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005893269s
Aug 17 07:02:16.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:18.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005475004s
Aug 17 07:02:18.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:20.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005217158s
Aug 17 07:02:20.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:22.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004702733s
Aug 17 07:02:22.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:24.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004879252s
Aug 17 07:02:24.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:02:26.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004588391s
Aug 17 07:02:26.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 07:02:26.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 07:02:26.456: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5216" to be "running and ready"
Aug 17 07:02:26.457: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.47418ms
Aug 17 07:02:26.458: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 07:02:26.458: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 17 07:02:26.459: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5216" to be "running and ready"
Aug 17 07:02:26.461: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.716011ms
Aug 17 07:02:26.461: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 17 07:02:26.461: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 07:02:26.462
Aug 17 07:02:26.469: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5216" to be "running"
Aug 17 07:02:26.472: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643796ms
Aug 17 07:02:28.475: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006121269s
Aug 17 07:02:28.476: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 07:02:28.477: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5216" to be "running"
Aug 17 07:02:28.479: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.475489ms
Aug 17 07:02:28.479: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 17 07:02:28.480: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 17 07:02:28.480: INFO: Going to poll 172.32.3.41 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 17 07:02:28.481: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.32.3.41 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5216 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:02:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:02:28.482: INFO: ExecWithOptions: Clientset creation
Aug 17 07:02:28.482: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5216/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.32.3.41+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 07:02:29.533: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 17 07:02:29.533: INFO: Going to poll 172.32.123.175 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 17 07:02:29.535: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.32.123.175 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5216 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:02:29.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:02:29.535: INFO: ExecWithOptions: Clientset creation
Aug 17 07:02:29.535: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5216/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.32.123.175+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 07:02:30.586: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 17 07:02:30.586: INFO: Going to poll 172.32.238.86 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 17 07:02:30.589: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.32.238.86 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5216 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:02:30.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:02:30.589: INFO: ExecWithOptions: Clientset creation
Aug 17 07:02:30.589: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5216/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.32.238.86+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 07:02:31.641: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:31.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5216" for this suite. 08/17/23 07:02:31.645
------------------------------
â€¢ [SLOW TEST] [27.249 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:04.4
    Aug 17 07:02:04.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 07:02:04.401
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:04.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:04.412
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5216 08/17/23 07:02:04.417
    STEP: creating a selector 08/17/23 07:02:04.417
    STEP: Creating the service pods in kubernetes 08/17/23 07:02:04.417
    Aug 17 07:02:04.417: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 07:02:04.449: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5216" to be "running and ready"
    Aug 17 07:02:04.451: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.670503ms
    Aug 17 07:02:04.451: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:02:06.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005132293s
    Aug 17 07:02:06.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:08.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005966238s
    Aug 17 07:02:08.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:10.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004564355s
    Aug 17 07:02:10.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:12.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005042464s
    Aug 17 07:02:12.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:14.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00482259s
    Aug 17 07:02:14.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:16.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005893269s
    Aug 17 07:02:16.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:18.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005475004s
    Aug 17 07:02:18.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:20.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005217158s
    Aug 17 07:02:20.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:22.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004702733s
    Aug 17 07:02:22.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:24.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004879252s
    Aug 17 07:02:24.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:02:26.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004588391s
    Aug 17 07:02:26.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 07:02:26.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 07:02:26.456: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5216" to be "running and ready"
    Aug 17 07:02:26.457: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.47418ms
    Aug 17 07:02:26.458: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 07:02:26.458: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 17 07:02:26.459: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5216" to be "running and ready"
    Aug 17 07:02:26.461: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.716011ms
    Aug 17 07:02:26.461: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 17 07:02:26.461: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 07:02:26.462
    Aug 17 07:02:26.469: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5216" to be "running"
    Aug 17 07:02:26.472: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643796ms
    Aug 17 07:02:28.475: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006121269s
    Aug 17 07:02:28.476: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 07:02:28.477: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5216" to be "running"
    Aug 17 07:02:28.479: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.475489ms
    Aug 17 07:02:28.479: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 17 07:02:28.480: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 17 07:02:28.480: INFO: Going to poll 172.32.3.41 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 17 07:02:28.481: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.32.3.41 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5216 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:02:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:02:28.482: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:02:28.482: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5216/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.32.3.41+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 07:02:29.533: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 17 07:02:29.533: INFO: Going to poll 172.32.123.175 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 17 07:02:29.535: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.32.123.175 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5216 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:02:29.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:02:29.535: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:02:29.535: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5216/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.32.123.175+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 07:02:30.586: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 17 07:02:30.586: INFO: Going to poll 172.32.238.86 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 17 07:02:30.589: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.32.238.86 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5216 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:02:30.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:02:30.589: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:02:30.589: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5216/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.32.238.86+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 07:02:31.641: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:31.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5216" for this suite. 08/17/23 07:02:31.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:31.649
Aug 17 07:02:31.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:02:31.649
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:31.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:31.662
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 17 07:02:31.681: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0" in namespace "kubelet-test-577" to be "running and ready"
Aug 17 07:02:31.684: INFO: Pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556943ms
Aug 17 07:02:31.684: INFO: The phase of Pod busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:02:33.687: INFO: Pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.005750599s
Aug 17 07:02:33.687: INFO: The phase of Pod busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 is Running (Ready = true)
Aug 17 07:02:33.687: INFO: Pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:33.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-577" for this suite. 08/17/23 07:02:33.702
------------------------------
â€¢ [2.056 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:31.649
    Aug 17 07:02:31.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:02:31.649
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:31.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:31.662
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 17 07:02:31.681: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0" in namespace "kubelet-test-577" to be "running and ready"
    Aug 17 07:02:31.684: INFO: Pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556943ms
    Aug 17 07:02:31.684: INFO: The phase of Pod busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:02:33.687: INFO: Pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.005750599s
    Aug 17 07:02:33.687: INFO: The phase of Pod busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 is Running (Ready = true)
    Aug 17 07:02:33.687: INFO: Pod "busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:33.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-577" for this suite. 08/17/23 07:02:33.702
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:33.705
Aug 17 07:02:33.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:02:33.706
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:33.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:33.724
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-f3ddc76c-1c0c-4833-a28a-bd7554e9eb7c 08/17/23 07:02:33.736
STEP: Creating the pod 08/17/23 07:02:33.742
Aug 17 07:02:33.748: INFO: Waiting up to 5m0s for pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4" in namespace "configmap-81" to be "running and ready"
Aug 17 07:02:33.750: INFO: Pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489209ms
Aug 17 07:02:33.750: INFO: The phase of Pod pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:02:35.753: INFO: Pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004739448s
Aug 17 07:02:35.753: INFO: The phase of Pod pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4 is Running (Ready = true)
Aug 17 07:02:35.753: INFO: Pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-f3ddc76c-1c0c-4833-a28a-bd7554e9eb7c 08/17/23 07:02:35.76
STEP: waiting to observe update in volume 08/17/23 07:02:35.763
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:39.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-81" for this suite. 08/17/23 07:02:39.78
------------------------------
â€¢ [SLOW TEST] [6.079 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:33.705
    Aug 17 07:02:33.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:02:33.706
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:33.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:33.724
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-f3ddc76c-1c0c-4833-a28a-bd7554e9eb7c 08/17/23 07:02:33.736
    STEP: Creating the pod 08/17/23 07:02:33.742
    Aug 17 07:02:33.748: INFO: Waiting up to 5m0s for pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4" in namespace "configmap-81" to be "running and ready"
    Aug 17 07:02:33.750: INFO: Pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489209ms
    Aug 17 07:02:33.750: INFO: The phase of Pod pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:02:35.753: INFO: Pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004739448s
    Aug 17 07:02:35.753: INFO: The phase of Pod pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4 is Running (Ready = true)
    Aug 17 07:02:35.753: INFO: Pod "pod-configmaps-017d6878-9b8e-48f1-a84d-3d3dc8cd42f4" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-f3ddc76c-1c0c-4833-a28a-bd7554e9eb7c 08/17/23 07:02:35.76
    STEP: waiting to observe update in volume 08/17/23 07:02:35.763
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:39.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-81" for this suite. 08/17/23 07:02:39.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:39.785
Aug 17 07:02:39.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-webhook 08/17/23 07:02:39.785
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:39.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:39.798
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/17/23 07:02:39.802
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 07:02:40.274
STEP: Deploying the custom resource conversion webhook pod 08/17/23 07:02:40.28
STEP: Wait for the deployment to be ready 08/17/23 07:02:40.287
Aug 17 07:02:40.290: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:02:42.297
STEP: Verifying the service has paired with the endpoint 08/17/23 07:02:42.305
Aug 17 07:02:43.306: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 17 07:02:43.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Creating a v1 custom resource 08/17/23 07:02:45.877
STEP: Create a v2 custom resource 08/17/23 07:02:45.886
STEP: List CRs in v1 08/17/23 07:02:45.889
STEP: List CRs in v2 08/17/23 07:02:45.927
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:46.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1208" for this suite. 08/17/23 07:02:46.488
------------------------------
â€¢ [SLOW TEST] [6.707 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:39.785
    Aug 17 07:02:39.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-webhook 08/17/23 07:02:39.785
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:39.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:39.798
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/17/23 07:02:39.802
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 07:02:40.274
    STEP: Deploying the custom resource conversion webhook pod 08/17/23 07:02:40.28
    STEP: Wait for the deployment to be ready 08/17/23 07:02:40.287
    Aug 17 07:02:40.290: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:02:42.297
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:02:42.305
    Aug 17 07:02:43.306: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 17 07:02:43.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Creating a v1 custom resource 08/17/23 07:02:45.877
    STEP: Create a v2 custom resource 08/17/23 07:02:45.886
    STEP: List CRs in v1 08/17/23 07:02:45.889
    STEP: List CRs in v2 08/17/23 07:02:45.927
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:46.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1208" for this suite. 08/17/23 07:02:46.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:46.492
Aug 17 07:02:46.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-pred 08/17/23 07:02:46.493
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:46.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:46.513
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 07:02:46.517: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 07:02:46.531: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 07:02:46.536: INFO: 
Logging pods the apiserver thinks is on node yst-master before test
Aug 17 07:02:46.549: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container app ready: true, restart count 1
Aug 17 07:02:46.549: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container chartmuseum ready: true, restart count 2
Aug 17 07:02:46.549: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container cluster-server ready: true, restart count 7
Aug 17 07:02:46.549: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container proxy ready: true, restart count 8
Aug 17 07:02:46.549: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container keycloak ready: true, restart count 2
Aug 17 07:02:46.549: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container postgres ready: true, restart count 2
Aug 17 07:02:46.549: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container keycloak-observer ready: true, restart count 6
Aug 17 07:02:46.549: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container manual-webserver ready: true, restart count 1
Aug 17 07:02:46.549: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Aug 17 07:02:46.549: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Aug 17 07:02:46.549: INFO: 	Container kube-state-metrics ready: true, restart count 1
Aug 17 07:02:46.549: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:02:46.549: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:02:46.549: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
Aug 17 07:02:46.549: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container apiserver ready: true, restart count 2
Aug 17 07:02:46.549: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container manager ready: true, restart count 4
Aug 17 07:02:46.549: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:02:46.549: INFO: 	Container etcd-date ready: false, restart count 0
Aug 17 07:02:46.549: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:02:46.549: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container core ready: true, restart count 4
Aug 17 07:02:46.549: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container database ready: true, restart count 2
Aug 17 07:02:46.549: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container jobservice ready: true, restart count 11
Aug 17 07:02:46.549: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container nginx ready: true, restart count 9
Aug 17 07:02:46.549: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container notary-signer ready: true, restart count 6
Aug 17 07:02:46.549: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kiali ready: true, restart count 1
Aug 17 07:02:46.549: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kube-event-exporter ready: true, restart count 2
Aug 17 07:02:46.549: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container minio ready: true, restart count 2
Aug 17 07:02:46.549: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container index-clear ready: false, restart count 0
Aug 17 07:02:46.549: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:02:46.549: INFO: 	Container pgdata-backup-node ready: false, restart count 0
Aug 17 07:02:46.549: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container manager ready: true, restart count 5
Aug 17 07:02:46.549: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:02:46.549: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:02:46.549: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container backup-check ready: false, restart count 1
Aug 17 07:02:46.549: INFO: 	Container backup-date ready: false, restart count 0
Aug 17 07:02:46.549: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Aug 17 07:02:46.549: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:02:46.549: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container coredns ready: true, restart count 1
Aug 17 07:02:46.549: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container etcd ready: true, restart count 2
Aug 17 07:02:46.549: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kube-apiserver ready: true, restart count 2
Aug 17 07:02:46.549: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kube-controller-manager ready: true, restart count 1
Aug 17 07:02:46.549: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:02:46.549: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container kube-scheduler ready: true, restart count 1
Aug 17 07:02:46.549: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container metrics-server ready: true, restart count 3
Aug 17 07:02:46.549: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:02:46.549: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:02:46.549: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container acc-tomcat ready: true, restart count 1
Aug 17 07:02:46.549: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.549: INFO: 	Container exporter ready: true, restart count 2
Aug 17 07:02:46.549: INFO: 	Container server ready: true, restart count 2
Aug 17 07:02:46.549: INFO: 
Logging pods the apiserver thinks is on node yst-node1 before test
Aug 17 07:02:46.563: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container webserver ready: true, restart count 1
Aug 17 07:02:46.563: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container gateway ready: true, restart count 7
Aug 17 07:02:46.563: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container helm-server ready: true, restart count 7
Aug 17 07:02:46.563: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container app ready: true, restart count 2
Aug 17 07:02:46.563: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:02:46.563: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:02:46.563: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:02:46.563: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:02:46.563: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container server ready: true, restart count 5
Aug 17 07:02:46.563: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container alertmanager ready: true, restart count 3
Aug 17 07:02:46.563: INFO: 	Container config-reloader ready: true, restart count 2
Aug 17 07:02:46.563: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container auth-server ready: true, restart count 5
Aug 17 07:02:46.563: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container blackbox-exporter ready: true, restart count 1
Aug 17 07:02:46.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:02:46.563: INFO: 	Container module-configmap-reloader ready: true, restart count 1
Aug 17 07:02:46.563: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:02:46.563: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:02:46.563: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container cronhpa-controller ready: true, restart count 1
Aug 17 07:02:46.563: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container default-http-backend ready: true, restart count 1
Aug 17 07:02:46.563: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:02:46.563: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container chartmuseum ready: true, restart count 1
Aug 17 07:02:46.563: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container notary-server ready: true, restart count 7
Aug 17 07:02:46.563: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container portal ready: true, restart count 2
Aug 17 07:02:46.563: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container redis ready: true, restart count 2
Aug 17 07:02:46.563: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container registry ready: true, restart count 2
Aug 17 07:02:46.563: INFO: 	Container registryctl ready: true, restart count 2
Aug 17 07:02:46.563: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container trivy ready: true, restart count 2
Aug 17 07:02:46.563: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container server ready: true, restart count 7
Aug 17 07:02:46.563: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container logstash ready: true, restart count 2
Aug 17 07:02:46.563: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container agent ready: true, restart count 2
Aug 17 07:02:46.563: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container prometheus-adapter ready: true, restart count 3
Aug 17 07:02:46.563: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:02:46.563: INFO: 	Container prometheus-operator ready: true, restart count 1
Aug 17 07:02:46.563: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:02:46.563: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container thanos-store ready: true, restart count 16
Aug 17 07:02:46.563: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container timescaledb ready: true, restart count 2
Aug 17 07:02:46.563: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:02:46.563: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container workflow-controller ready: true, restart count 7
Aug 17 07:02:46.563: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container apiserver ready: true, restart count 1
Aug 17 07:02:46.563: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:02:46.563: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.563: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:02:46.563: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.564: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:02:46.564: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:02:46.564: INFO: 
Logging pods the apiserver thinks is on node yst-node2 before test
Aug 17 07:02:46.574: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:02:46.574: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:02:46.574: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:02:46.574: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container opensearch ready: true, restart count 1
Aug 17 07:02:46.574: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container config-reloader ready: true, restart count 1
Aug 17 07:02:46.574: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 07:02:46.574: INFO: 	Container thanos-sidecar ready: true, restart count 1
Aug 17 07:02:46.574: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:02:46.574: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:02:46.574: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container coredns ready: true, restart count 1
Aug 17 07:02:46.574: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:02:46.574: INFO: busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 from kubelet-test-577 started at 2023-08-17 07:02:31 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 ready: true, restart count 0
Aug 17 07:02:46.574: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 07:02:46.574: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container e2e ready: true, restart count 0
Aug 17 07:02:46.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:02:46.574: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:02:46.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:02:46.574: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 07:02:46.574
Aug 17 07:02:46.579: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-262" to be "running"
Aug 17 07:02:46.581: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06846ms
Aug 17 07:02:48.585: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005480854s
Aug 17 07:02:48.585: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 07:02:48.587
STEP: Trying to apply a random label on the found node. 08/17/23 07:02:48.591
STEP: verifying the node has the label kubernetes.io/e2e-685ab3cf-d4d3-4d4d-890f-a59f2d303461 42 08/17/23 07:02:48.599
STEP: Trying to relaunch the pod, now with labels. 08/17/23 07:02:48.601
Aug 17 07:02:48.605: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-262" to be "not pending"
Aug 17 07:02:48.609: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.233857ms
Aug 17 07:02:50.611: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.006288691s
Aug 17 07:02:50.611: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-685ab3cf-d4d3-4d4d-890f-a59f2d303461 off the node yst-node2 08/17/23 07:02:50.613
STEP: verifying the node doesn't have the label kubernetes.io/e2e-685ab3cf-d4d3-4d4d-890f-a59f2d303461 08/17/23 07:02:50.62
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:50.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-262" for this suite. 08/17/23 07:02:50.625
------------------------------
â€¢ [4.138 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:46.492
    Aug 17 07:02:46.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-pred 08/17/23 07:02:46.493
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:46.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:46.513
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 07:02:46.517: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 07:02:46.531: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 07:02:46.536: INFO: 
    Logging pods the apiserver thinks is on node yst-master before test
    Aug 17 07:02:46.549: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container app ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container chartmuseum ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container cluster-server ready: true, restart count 7
    Aug 17 07:02:46.549: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container proxy ready: true, restart count 8
    Aug 17 07:02:46.549: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container keycloak ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container postgres ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container keycloak-observer ready: true, restart count 6
    Aug 17 07:02:46.549: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container manual-webserver ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: 	Container kube-state-metrics ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
    Aug 17 07:02:46.549: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container apiserver ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container manager ready: true, restart count 4
    Aug 17 07:02:46.549: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:02:46.549: INFO: 	Container etcd-date ready: false, restart count 0
    Aug 17 07:02:46.549: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container core ready: true, restart count 4
    Aug 17 07:02:46.549: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container database ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container jobservice ready: true, restart count 11
    Aug 17 07:02:46.549: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container nginx ready: true, restart count 9
    Aug 17 07:02:46.549: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container notary-signer ready: true, restart count 6
    Aug 17 07:02:46.549: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kiali ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kube-event-exporter ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container minio ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container index-clear ready: false, restart count 0
    Aug 17 07:02:46.549: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:02:46.549: INFO: 	Container pgdata-backup-node ready: false, restart count 0
    Aug 17 07:02:46.549: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container manager ready: true, restart count 5
    Aug 17 07:02:46.549: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container backup-check ready: false, restart count 1
    Aug 17 07:02:46.549: INFO: 	Container backup-date ready: false, restart count 0
    Aug 17 07:02:46.549: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container coredns ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container etcd ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kube-apiserver ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container kube-scheduler ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container metrics-server ready: true, restart count 3
    Aug 17 07:02:46.549: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:02:46.549: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:02:46.549: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container acc-tomcat ready: true, restart count 1
    Aug 17 07:02:46.549: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.549: INFO: 	Container exporter ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: 	Container server ready: true, restart count 2
    Aug 17 07:02:46.549: INFO: 
    Logging pods the apiserver thinks is on node yst-node1 before test
    Aug 17 07:02:46.563: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container webserver ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container gateway ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container helm-server ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container app ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container server ready: true, restart count 5
    Aug 17 07:02:46.563: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container alertmanager ready: true, restart count 3
    Aug 17 07:02:46.563: INFO: 	Container config-reloader ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container auth-server ready: true, restart count 5
    Aug 17 07:02:46.563: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container blackbox-exporter ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: 	Container module-configmap-reloader ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container cronhpa-controller ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container default-http-backend ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container chartmuseum ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container notary-server ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container portal ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container redis ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container registry ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: 	Container registryctl ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container trivy ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container server ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container logstash ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container agent ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container prometheus-adapter ready: true, restart count 3
    Aug 17 07:02:46.563: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: 	Container prometheus-operator ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container thanos-store ready: true, restart count 16
    Aug 17 07:02:46.563: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container timescaledb ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container workflow-controller ready: true, restart count 7
    Aug 17 07:02:46.563: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container apiserver ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:02:46.563: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.563: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:02:46.563: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.564: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:02:46.564: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:02:46.564: INFO: 
    Logging pods the apiserver thinks is on node yst-node2 before test
    Aug 17 07:02:46.574: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:02:46.574: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:02:46.574: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:02:46.574: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container opensearch ready: true, restart count 1
    Aug 17 07:02:46.574: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container config-reloader ready: true, restart count 1
    Aug 17 07:02:46.574: INFO: 	Container prometheus ready: true, restart count 1
    Aug 17 07:02:46.574: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Aug 17 07:02:46.574: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:02:46.574: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:02:46.574: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container coredns ready: true, restart count 1
    Aug 17 07:02:46.574: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:02:46.574: INFO: busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 from kubelet-test-577 started at 2023-08-17 07:02:31 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container busybox-scheduling-3c6de1b6-1971-438a-9880-eb07057d87e0 ready: true, restart count 0
    Aug 17 07:02:46.574: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 07:02:46.574: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 07:02:46.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:02:46.574: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:02:46.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:02:46.574: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 07:02:46.574
    Aug 17 07:02:46.579: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-262" to be "running"
    Aug 17 07:02:46.581: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06846ms
    Aug 17 07:02:48.585: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005480854s
    Aug 17 07:02:48.585: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 07:02:48.587
    STEP: Trying to apply a random label on the found node. 08/17/23 07:02:48.591
    STEP: verifying the node has the label kubernetes.io/e2e-685ab3cf-d4d3-4d4d-890f-a59f2d303461 42 08/17/23 07:02:48.599
    STEP: Trying to relaunch the pod, now with labels. 08/17/23 07:02:48.601
    Aug 17 07:02:48.605: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-262" to be "not pending"
    Aug 17 07:02:48.609: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.233857ms
    Aug 17 07:02:50.611: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.006288691s
    Aug 17 07:02:50.611: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-685ab3cf-d4d3-4d4d-890f-a59f2d303461 off the node yst-node2 08/17/23 07:02:50.613
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-685ab3cf-d4d3-4d4d-890f-a59f2d303461 08/17/23 07:02:50.62
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:50.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-262" for this suite. 08/17/23 07:02:50.625
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:50.63
Aug 17 07:02:50.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:02:50.631
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:50.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:50.641
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/17/23 07:02:50.644
Aug 17 07:02:50.644: INFO: Creating e2e-svc-a-nsbtt
Aug 17 07:02:50.669: INFO: Creating e2e-svc-b-w7vj8
Aug 17 07:02:50.680: INFO: Creating e2e-svc-c-xk68r
STEP: deleting service collection 08/17/23 07:02:50.688
Aug 17 07:02:50.708: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:50.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6041" for this suite. 08/17/23 07:02:50.71
------------------------------
â€¢ [0.083 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:50.63
    Aug 17 07:02:50.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:02:50.631
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:50.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:50.641
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/17/23 07:02:50.644
    Aug 17 07:02:50.644: INFO: Creating e2e-svc-a-nsbtt
    Aug 17 07:02:50.669: INFO: Creating e2e-svc-b-w7vj8
    Aug 17 07:02:50.680: INFO: Creating e2e-svc-c-xk68r
    STEP: deleting service collection 08/17/23 07:02:50.688
    Aug 17 07:02:50.708: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:50.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6041" for this suite. 08/17/23 07:02:50.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:50.714
Aug 17 07:02:50.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename watch 08/17/23 07:02:50.715
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:50.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:50.726
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/17/23 07:02:50.729
STEP: modifying the configmap once 08/17/23 07:02:50.736
STEP: modifying the configmap a second time 08/17/23 07:02:50.745
STEP: deleting the configmap 08/17/23 07:02:50.75
STEP: creating a watch on configmaps from the resource version returned by the first update 08/17/23 07:02:50.755
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/17/23 07:02:50.756
Aug 17 07:02:50.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8141  46f91c11-1993-4bc6-a9a0-d2978db7c686 27007677 0 2023-08-17 07:02:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 07:02:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:02:50.757: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8141  46f91c11-1993-4bc6-a9a0-d2978db7c686 27007681 0 2023-08-17 07:02:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 07:02:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8141" for this suite. 08/17/23 07:02:50.761
------------------------------
â€¢ [0.057 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:50.714
    Aug 17 07:02:50.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename watch 08/17/23 07:02:50.715
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:50.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:50.726
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/17/23 07:02:50.729
    STEP: modifying the configmap once 08/17/23 07:02:50.736
    STEP: modifying the configmap a second time 08/17/23 07:02:50.745
    STEP: deleting the configmap 08/17/23 07:02:50.75
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/17/23 07:02:50.755
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/17/23 07:02:50.756
    Aug 17 07:02:50.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8141  46f91c11-1993-4bc6-a9a0-d2978db7c686 27007677 0 2023-08-17 07:02:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 07:02:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:02:50.757: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8141  46f91c11-1993-4bc6-a9a0-d2978db7c686 27007681 0 2023-08-17 07:02:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 07:02:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8141" for this suite. 08/17/23 07:02:50.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:50.772
Aug 17 07:02:50.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 07:02:50.773
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:50.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:50.785
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/17/23 07:02:50.788
Aug 17 07:02:50.803: INFO: Waiting up to 5m0s for pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d" in namespace "emptydir-4956" to be "Succeeded or Failed"
Aug 17 07:02:50.805: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67627ms
Aug 17 07:02:52.809: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006370666s
Aug 17 07:02:54.809: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006893449s
STEP: Saw pod success 08/17/23 07:02:54.809
Aug 17 07:02:54.810: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d" satisfied condition "Succeeded or Failed"
Aug 17 07:02:54.811: INFO: Trying to get logs from node yst-node2 pod pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d container test-container: <nil>
STEP: delete the pod 08/17/23 07:02:54.815
Aug 17 07:02:54.821: INFO: Waiting for pod pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d to disappear
Aug 17 07:02:54.825: INFO: Pod pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:54.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4956" for this suite. 08/17/23 07:02:54.828
------------------------------
â€¢ [4.061 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:50.772
    Aug 17 07:02:50.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 07:02:50.773
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:50.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:50.785
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/17/23 07:02:50.788
    Aug 17 07:02:50.803: INFO: Waiting up to 5m0s for pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d" in namespace "emptydir-4956" to be "Succeeded or Failed"
    Aug 17 07:02:50.805: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67627ms
    Aug 17 07:02:52.809: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006370666s
    Aug 17 07:02:54.809: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006893449s
    STEP: Saw pod success 08/17/23 07:02:54.809
    Aug 17 07:02:54.810: INFO: Pod "pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d" satisfied condition "Succeeded or Failed"
    Aug 17 07:02:54.811: INFO: Trying to get logs from node yst-node2 pod pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d container test-container: <nil>
    STEP: delete the pod 08/17/23 07:02:54.815
    Aug 17 07:02:54.821: INFO: Waiting for pod pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d to disappear
    Aug 17 07:02:54.825: INFO: Pod pod-89b92ad6-a11f-45d0-a2d1-9b5fc8d40c7d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:54.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4956" for this suite. 08/17/23 07:02:54.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:54.838
Aug 17 07:02:54.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:02:54.839
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:54.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:54.852
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-0f7684b2-c1af-4799-b32b-41b67c9ee63a 08/17/23 07:02:54.854
STEP: Creating a pod to test consume configMaps 08/17/23 07:02:54.86
Aug 17 07:02:54.880: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268" in namespace "projected-8974" to be "Succeeded or Failed"
Aug 17 07:02:54.889: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268": Phase="Pending", Reason="", readiness=false. Elapsed: 8.615032ms
Aug 17 07:02:56.892: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011772355s
Aug 17 07:02:58.892: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011816177s
STEP: Saw pod success 08/17/23 07:02:58.892
Aug 17 07:02:58.892: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268" satisfied condition "Succeeded or Failed"
Aug 17 07:02:58.893: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:02:58.896
Aug 17 07:02:58.901: INFO: Waiting for pod pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268 to disappear
Aug 17 07:02:58.903: INFO: Pod pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:02:58.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8974" for this suite. 08/17/23 07:02:58.905
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:54.838
    Aug 17 07:02:54.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:02:54.839
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:54.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:54.852
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-0f7684b2-c1af-4799-b32b-41b67c9ee63a 08/17/23 07:02:54.854
    STEP: Creating a pod to test consume configMaps 08/17/23 07:02:54.86
    Aug 17 07:02:54.880: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268" in namespace "projected-8974" to be "Succeeded or Failed"
    Aug 17 07:02:54.889: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268": Phase="Pending", Reason="", readiness=false. Elapsed: 8.615032ms
    Aug 17 07:02:56.892: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011772355s
    Aug 17 07:02:58.892: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011816177s
    STEP: Saw pod success 08/17/23 07:02:58.892
    Aug 17 07:02:58.892: INFO: Pod "pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268" satisfied condition "Succeeded or Failed"
    Aug 17 07:02:58.893: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:02:58.896
    Aug 17 07:02:58.901: INFO: Waiting for pod pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268 to disappear
    Aug 17 07:02:58.903: INFO: Pod pod-projected-configmaps-11e297fd-bffe-4ba7-aaef-91b5e588d268 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:02:58.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8974" for this suite. 08/17/23 07:02:58.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:02:58.909
Aug 17 07:02:58.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 07:02:58.91
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:58.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:58.931
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 07:02:58.937
Aug 17 07:02:58.946: INFO: Waiting up to 5m0s for pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9" in namespace "emptydir-1808" to be "Succeeded or Failed"
Aug 17 07:02:58.948: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429268ms
Aug 17 07:03:00.951: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00527465s
Aug 17 07:03:02.953: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006878517s
STEP: Saw pod success 08/17/23 07:03:02.953
Aug 17 07:03:02.953: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9" satisfied condition "Succeeded or Failed"
Aug 17 07:03:02.955: INFO: Trying to get logs from node yst-node2 pod pod-0e405336-da7f-4236-9132-f35e3ea004b9 container test-container: <nil>
STEP: delete the pod 08/17/23 07:03:02.958
Aug 17 07:03:02.964: INFO: Waiting for pod pod-0e405336-da7f-4236-9132-f35e3ea004b9 to disappear
Aug 17 07:03:02.965: INFO: Pod pod-0e405336-da7f-4236-9132-f35e3ea004b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:03:02.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1808" for this suite. 08/17/23 07:03:02.968
------------------------------
â€¢ [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:02:58.909
    Aug 17 07:02:58.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 07:02:58.91
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:02:58.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:02:58.931
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 07:02:58.937
    Aug 17 07:02:58.946: INFO: Waiting up to 5m0s for pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9" in namespace "emptydir-1808" to be "Succeeded or Failed"
    Aug 17 07:02:58.948: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429268ms
    Aug 17 07:03:00.951: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00527465s
    Aug 17 07:03:02.953: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006878517s
    STEP: Saw pod success 08/17/23 07:03:02.953
    Aug 17 07:03:02.953: INFO: Pod "pod-0e405336-da7f-4236-9132-f35e3ea004b9" satisfied condition "Succeeded or Failed"
    Aug 17 07:03:02.955: INFO: Trying to get logs from node yst-node2 pod pod-0e405336-da7f-4236-9132-f35e3ea004b9 container test-container: <nil>
    STEP: delete the pod 08/17/23 07:03:02.958
    Aug 17 07:03:02.964: INFO: Waiting for pod pod-0e405336-da7f-4236-9132-f35e3ea004b9 to disappear
    Aug 17 07:03:02.965: INFO: Pod pod-0e405336-da7f-4236-9132-f35e3ea004b9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:03:02.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1808" for this suite. 08/17/23 07:03:02.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:03:02.972
Aug 17 07:03:02.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename cronjob 08/17/23 07:03:02.972
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:03:03.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:03:03.03
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/17/23 07:03:03.032
STEP: Ensuring no jobs are scheduled 08/17/23 07:03:03.046
STEP: Ensuring no job exists by listing jobs explicitly 08/17/23 07:08:03.05
STEP: Removing cronjob 08/17/23 07:08:03.051
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:03.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9663" for this suite. 08/17/23 07:08:03.057
------------------------------
â€¢ [SLOW TEST] [300.094 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:03:02.972
    Aug 17 07:03:02.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename cronjob 08/17/23 07:03:02.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:03:03.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:03:03.03
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/17/23 07:03:03.032
    STEP: Ensuring no jobs are scheduled 08/17/23 07:03:03.046
    STEP: Ensuring no job exists by listing jobs explicitly 08/17/23 07:08:03.05
    STEP: Removing cronjob 08/17/23 07:08:03.051
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:03.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9663" for this suite. 08/17/23 07:08:03.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:03.066
Aug 17 07:08:03.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:08:03.067
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:03.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:03.076
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:08:03.079
Aug 17 07:08:03.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48" in namespace "downward-api-8886" to be "Succeeded or Failed"
Aug 17 07:08:03.095: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.594305ms
Aug 17 07:08:05.099: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00877015s
Aug 17 07:08:07.098: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008451891s
STEP: Saw pod success 08/17/23 07:08:07.098
Aug 17 07:08:07.098: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48" satisfied condition "Succeeded or Failed"
Aug 17 07:08:07.100: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48 container client-container: <nil>
STEP: delete the pod 08/17/23 07:08:07.11
Aug 17 07:08:07.115: INFO: Waiting for pod downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48 to disappear
Aug 17 07:08:07.117: INFO: Pod downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:07.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8886" for this suite. 08/17/23 07:08:07.124
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:03.066
    Aug 17 07:08:03.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:08:03.067
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:03.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:03.076
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:08:03.079
    Aug 17 07:08:03.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48" in namespace "downward-api-8886" to be "Succeeded or Failed"
    Aug 17 07:08:03.095: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.594305ms
    Aug 17 07:08:05.099: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00877015s
    Aug 17 07:08:07.098: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008451891s
    STEP: Saw pod success 08/17/23 07:08:07.098
    Aug 17 07:08:07.098: INFO: Pod "downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48" satisfied condition "Succeeded or Failed"
    Aug 17 07:08:07.100: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:08:07.11
    Aug 17 07:08:07.115: INFO: Waiting for pod downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48 to disappear
    Aug 17 07:08:07.117: INFO: Pod downwardapi-volume-84640ebf-5f33-4589-ac55-b6277a272f48 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:07.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8886" for this suite. 08/17/23 07:08:07.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:07.129
Aug 17 07:08:07.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:08:07.129
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:07.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:07.145
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-409b1c28-53c8-4613-a984-28ab3e878330 08/17/23 07:08:07.147
STEP: Creating a pod to test consume configMaps 08/17/23 07:08:07.159
Aug 17 07:08:07.165: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb" in namespace "projected-9317" to be "Succeeded or Failed"
Aug 17 07:08:07.167: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.952672ms
Aug 17 07:08:09.169: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00464825s
Aug 17 07:08:11.170: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005272924s
STEP: Saw pod success 08/17/23 07:08:11.17
Aug 17 07:08:11.170: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb" satisfied condition "Succeeded or Failed"
Aug 17 07:08:11.172: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:08:11.175
Aug 17 07:08:11.181: INFO: Waiting for pod pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb to disappear
Aug 17 07:08:11.183: INFO: Pod pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:11.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9317" for this suite. 08/17/23 07:08:11.186
------------------------------
â€¢ [4.060 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:07.129
    Aug 17 07:08:07.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:08:07.129
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:07.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:07.145
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-409b1c28-53c8-4613-a984-28ab3e878330 08/17/23 07:08:07.147
    STEP: Creating a pod to test consume configMaps 08/17/23 07:08:07.159
    Aug 17 07:08:07.165: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb" in namespace "projected-9317" to be "Succeeded or Failed"
    Aug 17 07:08:07.167: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.952672ms
    Aug 17 07:08:09.169: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00464825s
    Aug 17 07:08:11.170: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005272924s
    STEP: Saw pod success 08/17/23 07:08:11.17
    Aug 17 07:08:11.170: INFO: Pod "pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb" satisfied condition "Succeeded or Failed"
    Aug 17 07:08:11.172: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:08:11.175
    Aug 17 07:08:11.181: INFO: Waiting for pod pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb to disappear
    Aug 17 07:08:11.183: INFO: Pod pod-projected-configmaps-0b502088-7d74-40e6-baa1-36c5503fb0cb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:11.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9317" for this suite. 08/17/23 07:08:11.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:11.19
Aug 17 07:08:11.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:08:11.191
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:11.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:11.206
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-4d244a88-fc4e-48d5-84f0-5bba69aa13dc 08/17/23 07:08:11.209
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:11.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2662" for this suite. 08/17/23 07:08:11.22
------------------------------
â€¢ [0.035 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:11.19
    Aug 17 07:08:11.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:08:11.191
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:11.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:11.206
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-4d244a88-fc4e-48d5-84f0-5bba69aa13dc 08/17/23 07:08:11.209
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:11.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2662" for this suite. 08/17/23 07:08:11.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:11.225
Aug 17 07:08:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:08:11.226
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:11.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:11.236
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:08:11.335
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:08:11.91
STEP: Deploying the webhook pod 08/17/23 07:08:11.914
STEP: Wait for the deployment to be ready 08/17/23 07:08:11.919
Aug 17 07:08:11.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:08:13.933
STEP: Verifying the service has paired with the endpoint 08/17/23 07:08:13.94
Aug 17 07:08:14.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/17/23 07:08:14.943
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/17/23 07:08:14.944
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 07:08:14.944
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/17/23 07:08:14.944
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/17/23 07:08:14.945
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 07:08:14.945
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 07:08:14.945
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:14.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4963" for this suite. 08/17/23 07:08:14.969
STEP: Destroying namespace "webhook-4963-markers" for this suite. 08/17/23 07:08:14.975
------------------------------
â€¢ [3.755 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:11.225
    Aug 17 07:08:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:08:11.226
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:11.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:11.236
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:08:11.335
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:08:11.91
    STEP: Deploying the webhook pod 08/17/23 07:08:11.914
    STEP: Wait for the deployment to be ready 08/17/23 07:08:11.919
    Aug 17 07:08:11.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:08:13.933
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:08:13.94
    Aug 17 07:08:14.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/17/23 07:08:14.943
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/17/23 07:08:14.944
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 07:08:14.944
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/17/23 07:08:14.944
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/17/23 07:08:14.945
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 07:08:14.945
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 07:08:14.945
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:14.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4963" for this suite. 08/17/23 07:08:14.969
    STEP: Destroying namespace "webhook-4963-markers" for this suite. 08/17/23 07:08:14.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:14.98
Aug 17 07:08:14.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 07:08:14.981
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:14.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:14.994
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/17/23 07:08:14.997
Aug 17 07:08:15.014: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e" in namespace "emptydir-6821" to be "running"
Aug 17 07:08:15.020: INFO: Pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512998ms
Aug 17 07:08:17.023: INFO: Pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e": Phase="Running", Reason="", readiness=false. Elapsed: 2.00929304s
Aug 17 07:08:17.023: INFO: Pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/17/23 07:08:17.023
Aug 17 07:08:17.023: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6821 PodName:pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:08:17.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:08:17.024: INFO: ExecWithOptions: Clientset creation
Aug 17 07:08:17.024: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-6821/pods/pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 17 07:08:17.073: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:17.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6821" for this suite. 08/17/23 07:08:17.076
------------------------------
â€¢ [2.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:14.98
    Aug 17 07:08:14.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 07:08:14.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:14.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:14.994
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/17/23 07:08:14.997
    Aug 17 07:08:15.014: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e" in namespace "emptydir-6821" to be "running"
    Aug 17 07:08:15.020: INFO: Pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512998ms
    Aug 17 07:08:17.023: INFO: Pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e": Phase="Running", Reason="", readiness=false. Elapsed: 2.00929304s
    Aug 17 07:08:17.023: INFO: Pod "pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/17/23 07:08:17.023
    Aug 17 07:08:17.023: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6821 PodName:pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:08:17.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:08:17.024: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:08:17.024: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-6821/pods/pod-sharedvolume-84ee6ba3-6d53-44d9-b209-2bb28277132e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 17 07:08:17.073: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:17.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6821" for this suite. 08/17/23 07:08:17.076
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:17.081
Aug 17 07:08:17.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:08:17.082
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:17.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:17.099
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:08:17.129
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:08:17.459
STEP: Deploying the webhook pod 08/17/23 07:08:17.461
STEP: Wait for the deployment to be ready 08/17/23 07:08:17.47
Aug 17 07:08:17.474: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:08:19.479
STEP: Verifying the service has paired with the endpoint 08/17/23 07:08:19.485
Aug 17 07:08:20.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/17/23 07:08:20.488
STEP: create a pod that should be updated by the webhook 08/17/23 07:08:20.497
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:20.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-768" for this suite. 08/17/23 07:08:20.533
STEP: Destroying namespace "webhook-768-markers" for this suite. 08/17/23 07:08:20.537
------------------------------
â€¢ [3.460 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:17.081
    Aug 17 07:08:17.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:08:17.082
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:17.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:17.099
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:08:17.129
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:08:17.459
    STEP: Deploying the webhook pod 08/17/23 07:08:17.461
    STEP: Wait for the deployment to be ready 08/17/23 07:08:17.47
    Aug 17 07:08:17.474: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:08:19.479
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:08:19.485
    Aug 17 07:08:20.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/17/23 07:08:20.488
    STEP: create a pod that should be updated by the webhook 08/17/23 07:08:20.497
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:20.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-768" for this suite. 08/17/23 07:08:20.533
    STEP: Destroying namespace "webhook-768-markers" for this suite. 08/17/23 07:08:20.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:20.541
Aug 17 07:08:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename daemonsets 08/17/23 07:08:20.543
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:20.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:20.582
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Aug 17 07:08:20.602: INFO: Create a RollingUpdate DaemonSet
Aug 17 07:08:20.606: INFO: Check that daemon pods launch on every node of the cluster
Aug 17 07:08:20.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:08:20.611: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:08:21.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:08:21.616: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:08:22.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:08:22.615: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 17 07:08:22.615: INFO: Update the DaemonSet to trigger a rollout
Aug 17 07:08:22.620: INFO: Updating DaemonSet daemon-set
Aug 17 07:08:25.628: INFO: Roll back the DaemonSet before rollout is complete
Aug 17 07:08:25.633: INFO: Updating DaemonSet daemon-set
Aug 17 07:08:25.633: INFO: Make sure DaemonSet rollback is complete
Aug 17 07:08:25.635: INFO: Wrong image for pod: daemon-set-ptdxf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 17 07:08:25.635: INFO: Pod daemon-set-ptdxf is not available
Aug 17 07:08:30.642: INFO: Pod daemon-set-n42bp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:08:30.647
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1751, will wait for the garbage collector to delete the pods 08/17/23 07:08:30.647
Aug 17 07:08:30.702: INFO: Deleting DaemonSet.extensions daemon-set took: 2.882514ms
Aug 17 07:08:30.803: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.627465ms
Aug 17 07:08:32.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:08:32.406: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 07:08:32.407: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27011881"},"items":null}

Aug 17 07:08:32.408: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27011881"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:32.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1751" for this suite. 08/17/23 07:08:32.417
------------------------------
â€¢ [SLOW TEST] [11.880 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:20.541
    Aug 17 07:08:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename daemonsets 08/17/23 07:08:20.543
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:20.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:20.582
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Aug 17 07:08:20.602: INFO: Create a RollingUpdate DaemonSet
    Aug 17 07:08:20.606: INFO: Check that daemon pods launch on every node of the cluster
    Aug 17 07:08:20.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:08:20.611: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:08:21.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:08:21.616: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:08:22.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:08:22.615: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Aug 17 07:08:22.615: INFO: Update the DaemonSet to trigger a rollout
    Aug 17 07:08:22.620: INFO: Updating DaemonSet daemon-set
    Aug 17 07:08:25.628: INFO: Roll back the DaemonSet before rollout is complete
    Aug 17 07:08:25.633: INFO: Updating DaemonSet daemon-set
    Aug 17 07:08:25.633: INFO: Make sure DaemonSet rollback is complete
    Aug 17 07:08:25.635: INFO: Wrong image for pod: daemon-set-ptdxf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 17 07:08:25.635: INFO: Pod daemon-set-ptdxf is not available
    Aug 17 07:08:30.642: INFO: Pod daemon-set-n42bp is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:08:30.647
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1751, will wait for the garbage collector to delete the pods 08/17/23 07:08:30.647
    Aug 17 07:08:30.702: INFO: Deleting DaemonSet.extensions daemon-set took: 2.882514ms
    Aug 17 07:08:30.803: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.627465ms
    Aug 17 07:08:32.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:08:32.406: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 07:08:32.407: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27011881"},"items":null}

    Aug 17 07:08:32.408: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27011881"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:32.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1751" for this suite. 08/17/23 07:08:32.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:32.423
Aug 17 07:08:32.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 07:08:32.424
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:32.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:32.435
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/17/23 07:08:32.438
STEP: getting 08/17/23 07:08:32.454
STEP: listing 08/17/23 07:08:32.457
STEP: deleting 08/17/23 07:08:32.459
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:32.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9580" for this suite. 08/17/23 07:08:32.468
------------------------------
â€¢ [0.050 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:32.423
    Aug 17 07:08:32.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 07:08:32.424
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:32.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:32.435
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/17/23 07:08:32.438
    STEP: getting 08/17/23 07:08:32.454
    STEP: listing 08/17/23 07:08:32.457
    STEP: deleting 08/17/23 07:08:32.459
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:32.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9580" for this suite. 08/17/23 07:08:32.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:32.473
Aug 17 07:08:32.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename proxy 08/17/23 07:08:32.474
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:32.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:32.491
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 17 07:08:32.496: INFO: Creating pod...
Aug 17 07:08:32.506: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4796" to be "running"
Aug 17 07:08:32.507: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422847ms
Aug 17 07:08:34.509: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003664385s
Aug 17 07:08:34.509: INFO: Pod "agnhost" satisfied condition "running"
Aug 17 07:08:34.509: INFO: Creating service...
Aug 17 07:08:34.516: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=DELETE
Aug 17 07:08:34.520: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 07:08:34.520: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=OPTIONS
Aug 17 07:08:34.523: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 07:08:34.523: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=PATCH
Aug 17 07:08:34.527: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 07:08:34.527: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=POST
Aug 17 07:08:34.528: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 07:08:34.528: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=PUT
Aug 17 07:08:34.530: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 17 07:08:34.530: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 17 07:08:34.532: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 07:08:34.532: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 17 07:08:34.535: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 07:08:34.535: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 17 07:08:34.537: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 07:08:34.537: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=POST
Aug 17 07:08:34.539: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 07:08:34.539: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=PUT
Aug 17 07:08:34.541: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 17 07:08:34.541: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=GET
Aug 17 07:08:34.542: INFO: http.Client request:GET StatusCode:301
Aug 17 07:08:34.542: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=GET
Aug 17 07:08:34.544: INFO: http.Client request:GET StatusCode:301
Aug 17 07:08:34.544: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=HEAD
Aug 17 07:08:34.546: INFO: http.Client request:HEAD StatusCode:301
Aug 17 07:08:34.546: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 17 07:08:34.548: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4796" for this suite. 08/17/23 07:08:34.551
------------------------------
â€¢ [2.083 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:32.473
    Aug 17 07:08:32.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename proxy 08/17/23 07:08:32.474
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:32.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:32.491
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 17 07:08:32.496: INFO: Creating pod...
    Aug 17 07:08:32.506: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4796" to be "running"
    Aug 17 07:08:32.507: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422847ms
    Aug 17 07:08:34.509: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003664385s
    Aug 17 07:08:34.509: INFO: Pod "agnhost" satisfied condition "running"
    Aug 17 07:08:34.509: INFO: Creating service...
    Aug 17 07:08:34.516: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=DELETE
    Aug 17 07:08:34.520: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 07:08:34.520: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=OPTIONS
    Aug 17 07:08:34.523: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 07:08:34.523: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=PATCH
    Aug 17 07:08:34.527: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 07:08:34.527: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=POST
    Aug 17 07:08:34.528: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 07:08:34.528: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=PUT
    Aug 17 07:08:34.530: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 17 07:08:34.530: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 17 07:08:34.532: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 07:08:34.532: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 17 07:08:34.535: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 07:08:34.535: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 17 07:08:34.537: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 07:08:34.537: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=POST
    Aug 17 07:08:34.539: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 07:08:34.539: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 17 07:08:34.541: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 17 07:08:34.541: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=GET
    Aug 17 07:08:34.542: INFO: http.Client request:GET StatusCode:301
    Aug 17 07:08:34.542: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=GET
    Aug 17 07:08:34.544: INFO: http.Client request:GET StatusCode:301
    Aug 17 07:08:34.544: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/pods/agnhost/proxy?method=HEAD
    Aug 17 07:08:34.546: INFO: http.Client request:HEAD StatusCode:301
    Aug 17 07:08:34.546: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4796/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 17 07:08:34.548: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4796" for this suite. 08/17/23 07:08:34.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:34.557
Aug 17 07:08:34.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename csistoragecapacity 08/17/23 07:08:34.557
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:34.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:34.58
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/17/23 07:08:34.584
STEP: getting /apis/storage.k8s.io 08/17/23 07:08:34.587
STEP: getting /apis/storage.k8s.io/v1 08/17/23 07:08:34.589
STEP: creating 08/17/23 07:08:34.59
STEP: watching 08/17/23 07:08:34.604
Aug 17 07:08:34.604: INFO: starting watch
STEP: getting 08/17/23 07:08:34.609
STEP: listing in namespace 08/17/23 07:08:34.612
STEP: listing across namespaces 08/17/23 07:08:34.614
STEP: patching 08/17/23 07:08:34.616
STEP: updating 08/17/23 07:08:34.619
Aug 17 07:08:34.622: INFO: waiting for watch events with expected annotations in namespace
Aug 17 07:08:34.622: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/17/23 07:08:34.623
STEP: deleting a collection 08/17/23 07:08:34.629
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-4748" for this suite. 08/17/23 07:08:34.64
------------------------------
â€¢ [0.089 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:34.557
    Aug 17 07:08:34.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename csistoragecapacity 08/17/23 07:08:34.557
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:34.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:34.58
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/17/23 07:08:34.584
    STEP: getting /apis/storage.k8s.io 08/17/23 07:08:34.587
    STEP: getting /apis/storage.k8s.io/v1 08/17/23 07:08:34.589
    STEP: creating 08/17/23 07:08:34.59
    STEP: watching 08/17/23 07:08:34.604
    Aug 17 07:08:34.604: INFO: starting watch
    STEP: getting 08/17/23 07:08:34.609
    STEP: listing in namespace 08/17/23 07:08:34.612
    STEP: listing across namespaces 08/17/23 07:08:34.614
    STEP: patching 08/17/23 07:08:34.616
    STEP: updating 08/17/23 07:08:34.619
    Aug 17 07:08:34.622: INFO: waiting for watch events with expected annotations in namespace
    Aug 17 07:08:34.622: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/17/23 07:08:34.623
    STEP: deleting a collection 08/17/23 07:08:34.629
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-4748" for this suite. 08/17/23 07:08:34.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:34.646
Aug 17 07:08:34.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 07:08:34.647
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:34.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:34.667
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 07:08:34.669
Aug 17 07:08:34.683: INFO: Waiting up to 5m0s for pod "pod-b52bcdee-b666-4edd-8215-682bf7521305" in namespace "emptydir-2324" to be "Succeeded or Failed"
Aug 17 07:08:34.685: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034289ms
Aug 17 07:08:36.688: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005478795s
Aug 17 07:08:38.689: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006058336s
STEP: Saw pod success 08/17/23 07:08:38.689
Aug 17 07:08:38.689: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305" satisfied condition "Succeeded or Failed"
Aug 17 07:08:38.690: INFO: Trying to get logs from node yst-node2 pod pod-b52bcdee-b666-4edd-8215-682bf7521305 container test-container: <nil>
STEP: delete the pod 08/17/23 07:08:38.694
Aug 17 07:08:38.701: INFO: Waiting for pod pod-b52bcdee-b666-4edd-8215-682bf7521305 to disappear
Aug 17 07:08:38.706: INFO: Pod pod-b52bcdee-b666-4edd-8215-682bf7521305 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:38.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2324" for this suite. 08/17/23 07:08:38.708
------------------------------
â€¢ [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:34.646
    Aug 17 07:08:34.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 07:08:34.647
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:34.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:34.667
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 07:08:34.669
    Aug 17 07:08:34.683: INFO: Waiting up to 5m0s for pod "pod-b52bcdee-b666-4edd-8215-682bf7521305" in namespace "emptydir-2324" to be "Succeeded or Failed"
    Aug 17 07:08:34.685: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034289ms
    Aug 17 07:08:36.688: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005478795s
    Aug 17 07:08:38.689: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006058336s
    STEP: Saw pod success 08/17/23 07:08:38.689
    Aug 17 07:08:38.689: INFO: Pod "pod-b52bcdee-b666-4edd-8215-682bf7521305" satisfied condition "Succeeded or Failed"
    Aug 17 07:08:38.690: INFO: Trying to get logs from node yst-node2 pod pod-b52bcdee-b666-4edd-8215-682bf7521305 container test-container: <nil>
    STEP: delete the pod 08/17/23 07:08:38.694
    Aug 17 07:08:38.701: INFO: Waiting for pod pod-b52bcdee-b666-4edd-8215-682bf7521305 to disappear
    Aug 17 07:08:38.706: INFO: Pod pod-b52bcdee-b666-4edd-8215-682bf7521305 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:38.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2324" for this suite. 08/17/23 07:08:38.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:38.712
Aug 17 07:08:38.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:08:38.713
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:38.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:38.723
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:08:38.75
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:08:39.237
STEP: Deploying the webhook pod 08/17/23 07:08:39.24
STEP: Wait for the deployment to be ready 08/17/23 07:08:39.247
Aug 17 07:08:39.250: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/17/23 07:08:41.257
STEP: Verifying the service has paired with the endpoint 08/17/23 07:08:41.268
Aug 17 07:08:42.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/17/23 07:08:42.272
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/17/23 07:08:42.284
STEP: Creating a configMap that should not be mutated 08/17/23 07:08:42.288
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/17/23 07:08:42.295
STEP: Creating a configMap that should be mutated 08/17/23 07:08:42.299
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:42.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9784" for this suite. 08/17/23 07:08:42.334
STEP: Destroying namespace "webhook-9784-markers" for this suite. 08/17/23 07:08:42.339
------------------------------
â€¢ [3.632 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:38.712
    Aug 17 07:08:38.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:08:38.713
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:38.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:38.723
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:08:38.75
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:08:39.237
    STEP: Deploying the webhook pod 08/17/23 07:08:39.24
    STEP: Wait for the deployment to be ready 08/17/23 07:08:39.247
    Aug 17 07:08:39.250: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/17/23 07:08:41.257
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:08:41.268
    Aug 17 07:08:42.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/17/23 07:08:42.272
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/17/23 07:08:42.284
    STEP: Creating a configMap that should not be mutated 08/17/23 07:08:42.288
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/17/23 07:08:42.295
    STEP: Creating a configMap that should be mutated 08/17/23 07:08:42.299
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:42.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9784" for this suite. 08/17/23 07:08:42.334
    STEP: Destroying namespace "webhook-9784-markers" for this suite. 08/17/23 07:08:42.339
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:42.344
Aug 17 07:08:42.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 07:08:42.345
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:42.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:42.374
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/17/23 07:08:42.376
STEP: setting up watch 08/17/23 07:08:42.376
STEP: submitting the pod to kubernetes 08/17/23 07:08:42.489
STEP: verifying the pod is in kubernetes 08/17/23 07:08:42.516
STEP: verifying pod creation was observed 08/17/23 07:08:42.518
Aug 17 07:08:42.519: INFO: Waiting up to 5m0s for pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e" in namespace "pods-343" to be "running"
Aug 17 07:08:42.536: INFO: Pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.436493ms
Aug 17 07:08:44.540: INFO: Pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021033502s
Aug 17 07:08:44.540: INFO: Pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e" satisfied condition "running"
STEP: deleting the pod gracefully 08/17/23 07:08:44.542
STEP: verifying pod deletion was observed 08/17/23 07:08:44.548
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 07:08:46.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-343" for this suite. 08/17/23 07:08:46.391
------------------------------
â€¢ [4.050 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:42.344
    Aug 17 07:08:42.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 07:08:42.345
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:42.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:42.374
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/17/23 07:08:42.376
    STEP: setting up watch 08/17/23 07:08:42.376
    STEP: submitting the pod to kubernetes 08/17/23 07:08:42.489
    STEP: verifying the pod is in kubernetes 08/17/23 07:08:42.516
    STEP: verifying pod creation was observed 08/17/23 07:08:42.518
    Aug 17 07:08:42.519: INFO: Waiting up to 5m0s for pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e" in namespace "pods-343" to be "running"
    Aug 17 07:08:42.536: INFO: Pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.436493ms
    Aug 17 07:08:44.540: INFO: Pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021033502s
    Aug 17 07:08:44.540: INFO: Pod "pod-submit-remove-73f9e813-ea88-4a77-a448-4f20eb274f4e" satisfied condition "running"
    STEP: deleting the pod gracefully 08/17/23 07:08:44.542
    STEP: verifying pod deletion was observed 08/17/23 07:08:44.548
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:08:46.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-343" for this suite. 08/17/23 07:08:46.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:08:46.396
Aug 17 07:08:46.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 07:08:46.396
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:46.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:46.412
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/17/23 07:08:46.417
STEP: Creating RC which spawns configmap-volume pods 08/17/23 07:08:46.65
Aug 17 07:08:46.754: INFO: Pod name wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/17/23 07:08:46.754
Aug 17 07:08:46.754: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:08:46.799: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 45.293223ms
Aug 17 07:08:48.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047632983s
Aug 17 07:08:50.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048007271s
Aug 17 07:08:52.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048120116s
Aug 17 07:08:54.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048483976s
Aug 17 07:08:56.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047953006s
Aug 17 07:08:58.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047851941s
Aug 17 07:09:00.803: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 14.048800024s
Aug 17 07:09:02.803: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Running", Reason="", readiness=true. Elapsed: 16.049211893s
Aug 17 07:09:02.803: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn" satisfied condition "running"
Aug 17 07:09:02.803: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-9xdrd" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:02.806: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-9xdrd": Phase="Running", Reason="", readiness=true. Elapsed: 3.11211ms
Aug 17 07:09:02.806: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-9xdrd" satisfied condition "running"
Aug 17 07:09:02.806: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-hxst5" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:02.809: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-hxst5": Phase="Running", Reason="", readiness=true. Elapsed: 3.167141ms
Aug 17 07:09:02.809: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-hxst5" satisfied condition "running"
Aug 17 07:09:02.810: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-nkdbd" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:02.812: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-nkdbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.381508ms
Aug 17 07:09:02.812: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-nkdbd" satisfied condition "running"
Aug 17 07:09:02.812: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-vwj8z" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:02.814: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-vwj8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.071039ms
Aug 17 07:09:02.814: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-vwj8z" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346 in namespace emptydir-wrapper-3722, will wait for the garbage collector to delete the pods 08/17/23 07:09:02.814
Aug 17 07:09:02.925: INFO: Deleting ReplicationController wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346 took: 32.584244ms
Aug 17 07:09:03.026: INFO: Terminating ReplicationController wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346 pods took: 100.627377ms
STEP: Creating RC which spawns configmap-volume pods 08/17/23 07:09:05.529
Aug 17 07:09:05.562: INFO: Pod name wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed: Found 0 pods out of 5
Aug 17 07:09:10.710: INFO: Pod name wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/17/23 07:09:10.71
Aug 17 07:09:10.710: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:10.723: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 12.502128ms
Aug 17 07:09:12.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015316306s
Aug 17 07:09:14.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01586558s
Aug 17 07:09:16.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01568104s
Aug 17 07:09:18.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Running", Reason="", readiness=true. Elapsed: 8.015536172s
Aug 17 07:09:18.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw" satisfied condition "running"
Aug 17 07:09:18.726: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:18.728: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243635ms
Aug 17 07:09:20.731: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005427423s
Aug 17 07:09:20.732: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d" satisfied condition "running"
Aug 17 07:09:20.732: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-jl26b" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:20.734: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-jl26b": Phase="Running", Reason="", readiness=true. Elapsed: 2.418375ms
Aug 17 07:09:20.734: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-jl26b" satisfied condition "running"
Aug 17 07:09:20.734: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-m5pn4" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:20.736: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-m5pn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.042279ms
Aug 17 07:09:20.736: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-m5pn4" satisfied condition "running"
Aug 17 07:09:20.736: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-wv5mk" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:20.738: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-wv5mk": Phase="Running", Reason="", readiness=true. Elapsed: 1.796882ms
Aug 17 07:09:20.738: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-wv5mk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed in namespace emptydir-wrapper-3722, will wait for the garbage collector to delete the pods 08/17/23 07:09:20.738
Aug 17 07:09:20.794: INFO: Deleting ReplicationController wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed took: 3.693482ms
Aug 17 07:09:20.895: INFO: Terminating ReplicationController wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed pods took: 100.794329ms
STEP: Creating RC which spawns configmap-volume pods 08/17/23 07:09:24.598
Aug 17 07:09:24.607: INFO: Pod name wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408: Found 0 pods out of 5
Aug 17 07:09:29.615: INFO: Pod name wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/17/23 07:09:29.615
Aug 17 07:09:29.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:29.618: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501039ms
Aug 17 07:09:31.622: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006861673s
Aug 17 07:09:33.621: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006269321s
Aug 17 07:09:35.620: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005218315s
Aug 17 07:09:37.621: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Running", Reason="", readiness=true. Elapsed: 8.006340657s
Aug 17 07:09:37.621: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc" satisfied condition "running"
Aug 17 07:09:37.621: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:37.624: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169169ms
Aug 17 07:09:39.627: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.00593934s
Aug 17 07:09:39.627: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp" satisfied condition "running"
Aug 17 07:09:39.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-7bhl4" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:39.630: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-7bhl4": Phase="Running", Reason="", readiness=true. Elapsed: 2.158709ms
Aug 17 07:09:39.630: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-7bhl4" satisfied condition "running"
Aug 17 07:09:39.630: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-lbkw9" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:39.632: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-lbkw9": Phase="Running", Reason="", readiness=true. Elapsed: 2.501487ms
Aug 17 07:09:39.632: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-lbkw9" satisfied condition "running"
Aug 17 07:09:39.632: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-mcppj" in namespace "emptydir-wrapper-3722" to be "running"
Aug 17 07:09:39.634: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-mcppj": Phase="Running", Reason="", readiness=true. Elapsed: 2.25838ms
Aug 17 07:09:39.634: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-mcppj" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408 in namespace emptydir-wrapper-3722, will wait for the garbage collector to delete the pods 08/17/23 07:09:39.634
Aug 17 07:09:39.693: INFO: Deleting ReplicationController wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408 took: 5.385059ms
Aug 17 07:09:39.794: INFO: Terminating ReplicationController wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408 pods took: 100.899697ms
STEP: Cleaning up the configMaps 08/17/23 07:09:43.695
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:09:43.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3722" for this suite. 08/17/23 07:09:43.829
------------------------------
â€¢ [SLOW TEST] [57.435 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:08:46.396
    Aug 17 07:08:46.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 07:08:46.396
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:08:46.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:08:46.412
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/17/23 07:08:46.417
    STEP: Creating RC which spawns configmap-volume pods 08/17/23 07:08:46.65
    Aug 17 07:08:46.754: INFO: Pod name wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/17/23 07:08:46.754
    Aug 17 07:08:46.754: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:08:46.799: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 45.293223ms
    Aug 17 07:08:48.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047632983s
    Aug 17 07:08:50.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048007271s
    Aug 17 07:08:52.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048120116s
    Aug 17 07:08:54.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048483976s
    Aug 17 07:08:56.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047953006s
    Aug 17 07:08:58.802: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047851941s
    Aug 17 07:09:00.803: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Pending", Reason="", readiness=false. Elapsed: 14.048800024s
    Aug 17 07:09:02.803: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn": Phase="Running", Reason="", readiness=true. Elapsed: 16.049211893s
    Aug 17 07:09:02.803: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-8b8cn" satisfied condition "running"
    Aug 17 07:09:02.803: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-9xdrd" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:02.806: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-9xdrd": Phase="Running", Reason="", readiness=true. Elapsed: 3.11211ms
    Aug 17 07:09:02.806: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-9xdrd" satisfied condition "running"
    Aug 17 07:09:02.806: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-hxst5" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:02.809: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-hxst5": Phase="Running", Reason="", readiness=true. Elapsed: 3.167141ms
    Aug 17 07:09:02.809: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-hxst5" satisfied condition "running"
    Aug 17 07:09:02.810: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-nkdbd" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:02.812: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-nkdbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.381508ms
    Aug 17 07:09:02.812: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-nkdbd" satisfied condition "running"
    Aug 17 07:09:02.812: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-vwj8z" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:02.814: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-vwj8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.071039ms
    Aug 17 07:09:02.814: INFO: Pod "wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346-vwj8z" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346 in namespace emptydir-wrapper-3722, will wait for the garbage collector to delete the pods 08/17/23 07:09:02.814
    Aug 17 07:09:02.925: INFO: Deleting ReplicationController wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346 took: 32.584244ms
    Aug 17 07:09:03.026: INFO: Terminating ReplicationController wrapped-volume-race-049c5919-3ed2-4c77-a994-f78d784cf346 pods took: 100.627377ms
    STEP: Creating RC which spawns configmap-volume pods 08/17/23 07:09:05.529
    Aug 17 07:09:05.562: INFO: Pod name wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed: Found 0 pods out of 5
    Aug 17 07:09:10.710: INFO: Pod name wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/17/23 07:09:10.71
    Aug 17 07:09:10.710: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:10.723: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 12.502128ms
    Aug 17 07:09:12.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015316306s
    Aug 17 07:09:14.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01586558s
    Aug 17 07:09:16.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01568104s
    Aug 17 07:09:18.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw": Phase="Running", Reason="", readiness=true. Elapsed: 8.015536172s
    Aug 17 07:09:18.726: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-bn2tw" satisfied condition "running"
    Aug 17 07:09:18.726: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:18.728: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243635ms
    Aug 17 07:09:20.731: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005427423s
    Aug 17 07:09:20.732: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-fmw4d" satisfied condition "running"
    Aug 17 07:09:20.732: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-jl26b" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:20.734: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-jl26b": Phase="Running", Reason="", readiness=true. Elapsed: 2.418375ms
    Aug 17 07:09:20.734: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-jl26b" satisfied condition "running"
    Aug 17 07:09:20.734: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-m5pn4" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:20.736: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-m5pn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.042279ms
    Aug 17 07:09:20.736: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-m5pn4" satisfied condition "running"
    Aug 17 07:09:20.736: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-wv5mk" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:20.738: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-wv5mk": Phase="Running", Reason="", readiness=true. Elapsed: 1.796882ms
    Aug 17 07:09:20.738: INFO: Pod "wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed-wv5mk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed in namespace emptydir-wrapper-3722, will wait for the garbage collector to delete the pods 08/17/23 07:09:20.738
    Aug 17 07:09:20.794: INFO: Deleting ReplicationController wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed took: 3.693482ms
    Aug 17 07:09:20.895: INFO: Terminating ReplicationController wrapped-volume-race-5a901136-91ba-4eff-a357-424ec79022ed pods took: 100.794329ms
    STEP: Creating RC which spawns configmap-volume pods 08/17/23 07:09:24.598
    Aug 17 07:09:24.607: INFO: Pod name wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408: Found 0 pods out of 5
    Aug 17 07:09:29.615: INFO: Pod name wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/17/23 07:09:29.615
    Aug 17 07:09:29.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:29.618: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501039ms
    Aug 17 07:09:31.622: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006861673s
    Aug 17 07:09:33.621: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006269321s
    Aug 17 07:09:35.620: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005218315s
    Aug 17 07:09:37.621: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc": Phase="Running", Reason="", readiness=true. Elapsed: 8.006340657s
    Aug 17 07:09:37.621: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-564cc" satisfied condition "running"
    Aug 17 07:09:37.621: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:37.624: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169169ms
    Aug 17 07:09:39.627: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.00593934s
    Aug 17 07:09:39.627: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-5rhkp" satisfied condition "running"
    Aug 17 07:09:39.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-7bhl4" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:39.630: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-7bhl4": Phase="Running", Reason="", readiness=true. Elapsed: 2.158709ms
    Aug 17 07:09:39.630: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-7bhl4" satisfied condition "running"
    Aug 17 07:09:39.630: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-lbkw9" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:39.632: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-lbkw9": Phase="Running", Reason="", readiness=true. Elapsed: 2.501487ms
    Aug 17 07:09:39.632: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-lbkw9" satisfied condition "running"
    Aug 17 07:09:39.632: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-mcppj" in namespace "emptydir-wrapper-3722" to be "running"
    Aug 17 07:09:39.634: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-mcppj": Phase="Running", Reason="", readiness=true. Elapsed: 2.25838ms
    Aug 17 07:09:39.634: INFO: Pod "wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408-mcppj" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408 in namespace emptydir-wrapper-3722, will wait for the garbage collector to delete the pods 08/17/23 07:09:39.634
    Aug 17 07:09:39.693: INFO: Deleting ReplicationController wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408 took: 5.385059ms
    Aug 17 07:09:39.794: INFO: Terminating ReplicationController wrapped-volume-race-bde84d19-39bb-4703-822d-4c029f97e408 pods took: 100.899697ms
    STEP: Cleaning up the configMaps 08/17/23 07:09:43.695
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:09:43.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3722" for this suite. 08/17/23 07:09:43.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:09:43.832
Aug 17 07:09:43.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 07:09:43.833
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:43.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:43.851
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 07:09:43.862
Aug 17 07:09:43.872: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2506" to be "running and ready"
Aug 17 07:09:43.884: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.426519ms
Aug 17 07:09:43.884: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:09:45.887: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014443935s
Aug 17 07:09:45.887: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 07:09:45.887: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/17/23 07:09:45.889
Aug 17 07:09:45.892: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2506" to be "running and ready"
Aug 17 07:09:45.893: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596145ms
Aug 17 07:09:45.893: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:09:47.897: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004880863s
Aug 17 07:09:47.897: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 17 07:09:47.897: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/17/23 07:09:47.898
Aug 17 07:09:47.909: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 07:09:47.911: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 07:09:49.912: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 07:09:49.914: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 07:09:51.912: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 07:09:51.915: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/17/23 07:09:51.915
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 07:09:51.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2506" for this suite. 08/17/23 07:09:51.93
------------------------------
â€¢ [SLOW TEST] [8.101 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:09:43.832
    Aug 17 07:09:43.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 07:09:43.833
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:43.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:43.851
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 07:09:43.862
    Aug 17 07:09:43.872: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2506" to be "running and ready"
    Aug 17 07:09:43.884: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.426519ms
    Aug 17 07:09:43.884: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:09:45.887: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014443935s
    Aug 17 07:09:45.887: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 07:09:45.887: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/17/23 07:09:45.889
    Aug 17 07:09:45.892: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2506" to be "running and ready"
    Aug 17 07:09:45.893: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596145ms
    Aug 17 07:09:45.893: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:09:47.897: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004880863s
    Aug 17 07:09:47.897: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 17 07:09:47.897: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/17/23 07:09:47.898
    Aug 17 07:09:47.909: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 17 07:09:47.911: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 17 07:09:49.912: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 17 07:09:49.914: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 17 07:09:51.912: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 17 07:09:51.915: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/17/23 07:09:51.915
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:09:51.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2506" for this suite. 08/17/23 07:09:51.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:09:51.934
Aug 17 07:09:51.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 07:09:51.935
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:51.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:51.946
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 17 07:09:51.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:09:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1246" for this suite. 08/17/23 07:09:55.068
------------------------------
â€¢ [3.263 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:09:51.934
    Aug 17 07:09:51.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 07:09:51.935
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:51.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:51.946
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 17 07:09:51.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:09:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1246" for this suite. 08/17/23 07:09:55.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:09:55.197
Aug 17 07:09:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:09:55.198
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:55.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:55.383
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 17 07:09:55.393: INFO: Got root ca configmap in namespace "svcaccounts-337"
Aug 17 07:09:55.411: INFO: Deleted root ca configmap in namespace "svcaccounts-337"
STEP: waiting for a new root ca configmap created 08/17/23 07:09:55.912
Aug 17 07:09:55.914: INFO: Recreated root ca configmap in namespace "svcaccounts-337"
Aug 17 07:09:55.917: INFO: Updated root ca configmap in namespace "svcaccounts-337"
STEP: waiting for the root ca configmap reconciled 08/17/23 07:09:56.418
Aug 17 07:09:56.420: INFO: Reconciled root ca configmap in namespace "svcaccounts-337"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 07:09:56.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-337" for this suite. 08/17/23 07:09:56.423
------------------------------
â€¢ [1.230 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:09:55.197
    Aug 17 07:09:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:09:55.198
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:55.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:55.383
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 17 07:09:55.393: INFO: Got root ca configmap in namespace "svcaccounts-337"
    Aug 17 07:09:55.411: INFO: Deleted root ca configmap in namespace "svcaccounts-337"
    STEP: waiting for a new root ca configmap created 08/17/23 07:09:55.912
    Aug 17 07:09:55.914: INFO: Recreated root ca configmap in namespace "svcaccounts-337"
    Aug 17 07:09:55.917: INFO: Updated root ca configmap in namespace "svcaccounts-337"
    STEP: waiting for the root ca configmap reconciled 08/17/23 07:09:56.418
    Aug 17 07:09:56.420: INFO: Reconciled root ca configmap in namespace "svcaccounts-337"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:09:56.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-337" for this suite. 08/17/23 07:09:56.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:09:56.428
Aug 17 07:09:56.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:09:56.428
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:56.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:56.45
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-de6a0cb8-1fde-4aaa-a186-3f9c1e952673 08/17/23 07:09:56.457
STEP: Creating a pod to test consume configMaps 08/17/23 07:09:56.463
Aug 17 07:09:56.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99" in namespace "configmap-4374" to be "Succeeded or Failed"
Aug 17 07:09:56.471: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.294074ms
Aug 17 07:09:58.474: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00535996s
Aug 17 07:10:00.473: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004266204s
STEP: Saw pod success 08/17/23 07:10:00.473
Aug 17 07:10:00.473: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99" satisfied condition "Succeeded or Failed"
Aug 17 07:10:00.474: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:10:00.481
Aug 17 07:10:00.490: INFO: Waiting for pod pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99 to disappear
Aug 17 07:10:00.493: INFO: Pod pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:00.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4374" for this suite. 08/17/23 07:10:00.496
------------------------------
â€¢ [4.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:09:56.428
    Aug 17 07:09:56.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:09:56.428
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:09:56.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:09:56.45
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-de6a0cb8-1fde-4aaa-a186-3f9c1e952673 08/17/23 07:09:56.457
    STEP: Creating a pod to test consume configMaps 08/17/23 07:09:56.463
    Aug 17 07:09:56.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99" in namespace "configmap-4374" to be "Succeeded or Failed"
    Aug 17 07:09:56.471: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.294074ms
    Aug 17 07:09:58.474: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00535996s
    Aug 17 07:10:00.473: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004266204s
    STEP: Saw pod success 08/17/23 07:10:00.473
    Aug 17 07:10:00.473: INFO: Pod "pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99" satisfied condition "Succeeded or Failed"
    Aug 17 07:10:00.474: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:10:00.481
    Aug 17 07:10:00.490: INFO: Waiting for pod pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99 to disappear
    Aug 17 07:10:00.493: INFO: Pod pod-configmaps-7b10fa31-43aa-4ad5-ab7e-02af663eee99 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:00.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4374" for this suite. 08/17/23 07:10:00.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:00.499
Aug 17 07:10:00.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 07:10:00.5
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:00.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:00.514
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 17 07:10:00.540: INFO: Waiting up to 5m0s for pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4" in namespace "emptydir-wrapper-3890" to be "running and ready"
Aug 17 07:10:00.542: INFO: Pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011411ms
Aug 17 07:10:00.542: INFO: The phase of Pod pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:10:02.545: INFO: Pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004732666s
Aug 17 07:10:02.545: INFO: The phase of Pod pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4 is Running (Ready = true)
Aug 17 07:10:02.545: INFO: Pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/17/23 07:10:02.547
STEP: Cleaning up the configmap 08/17/23 07:10:02.549
STEP: Cleaning up the pod 08/17/23 07:10:02.557
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:02.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3890" for this suite. 08/17/23 07:10:02.574
------------------------------
â€¢ [2.079 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:00.499
    Aug 17 07:10:00.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 07:10:00.5
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:00.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:00.514
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 17 07:10:00.540: INFO: Waiting up to 5m0s for pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4" in namespace "emptydir-wrapper-3890" to be "running and ready"
    Aug 17 07:10:00.542: INFO: Pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011411ms
    Aug 17 07:10:00.542: INFO: The phase of Pod pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:10:02.545: INFO: Pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004732666s
    Aug 17 07:10:02.545: INFO: The phase of Pod pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4 is Running (Ready = true)
    Aug 17 07:10:02.545: INFO: Pod "pod-secrets-83b35d75-3f61-4b79-bace-5fc8c68cc2c4" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/17/23 07:10:02.547
    STEP: Cleaning up the configmap 08/17/23 07:10:02.549
    STEP: Cleaning up the pod 08/17/23 07:10:02.557
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:02.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3890" for this suite. 08/17/23 07:10:02.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:02.58
Aug 17 07:10:02.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:10:02.581
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:02.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:02.592
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-4238/secret-test-650a67d0-3602-4465-a8e1-bb72a043f144 08/17/23 07:10:02.596
STEP: Creating a pod to test consume secrets 08/17/23 07:10:02.603
Aug 17 07:10:02.610: INFO: Waiting up to 5m0s for pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2" in namespace "secrets-4238" to be "Succeeded or Failed"
Aug 17 07:10:02.613: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.766598ms
Aug 17 07:10:04.617: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006374135s
Aug 17 07:10:06.616: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005349685s
STEP: Saw pod success 08/17/23 07:10:06.616
Aug 17 07:10:06.616: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2" satisfied condition "Succeeded or Failed"
Aug 17 07:10:06.618: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2 container env-test: <nil>
STEP: delete the pod 08/17/23 07:10:06.622
Aug 17 07:10:06.628: INFO: Waiting for pod pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2 to disappear
Aug 17 07:10:06.630: INFO: Pod pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4238" for this suite. 08/17/23 07:10:06.633
------------------------------
â€¢ [4.056 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:02.58
    Aug 17 07:10:02.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:10:02.581
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:02.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:02.592
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-4238/secret-test-650a67d0-3602-4465-a8e1-bb72a043f144 08/17/23 07:10:02.596
    STEP: Creating a pod to test consume secrets 08/17/23 07:10:02.603
    Aug 17 07:10:02.610: INFO: Waiting up to 5m0s for pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2" in namespace "secrets-4238" to be "Succeeded or Failed"
    Aug 17 07:10:02.613: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.766598ms
    Aug 17 07:10:04.617: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006374135s
    Aug 17 07:10:06.616: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005349685s
    STEP: Saw pod success 08/17/23 07:10:06.616
    Aug 17 07:10:06.616: INFO: Pod "pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2" satisfied condition "Succeeded or Failed"
    Aug 17 07:10:06.618: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2 container env-test: <nil>
    STEP: delete the pod 08/17/23 07:10:06.622
    Aug 17 07:10:06.628: INFO: Waiting for pod pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2 to disappear
    Aug 17 07:10:06.630: INFO: Pod pod-configmaps-114edefe-5778-4eb6-b0b5-4a1370fc8ff2 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4238" for this suite. 08/17/23 07:10:06.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:06.639
Aug 17 07:10:06.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replication-controller 08/17/23 07:10:06.64
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:06.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:06.653
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/17/23 07:10:06.655
Aug 17 07:10:06.670: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6433" to be "running and ready"
Aug 17 07:10:06.674: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.473316ms
Aug 17 07:10:06.674: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:10:08.678: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008221946s
Aug 17 07:10:08.678: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 17 07:10:08.678: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/17/23 07:10:08.68
STEP: Then the orphan pod is adopted 08/17/23 07:10:08.685
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:09.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6433" for this suite. 08/17/23 07:10:09.692
------------------------------
â€¢ [3.058 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:06.639
    Aug 17 07:10:06.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replication-controller 08/17/23 07:10:06.64
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:06.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:06.653
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/17/23 07:10:06.655
    Aug 17 07:10:06.670: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6433" to be "running and ready"
    Aug 17 07:10:06.674: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.473316ms
    Aug 17 07:10:06.674: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:10:08.678: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008221946s
    Aug 17 07:10:08.678: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 17 07:10:08.678: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/17/23 07:10:08.68
    STEP: Then the orphan pod is adopted 08/17/23 07:10:08.685
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:09.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6433" for this suite. 08/17/23 07:10:09.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:09.698
Aug 17 07:10:09.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:10:09.698
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:09.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:09.726
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-2862d375-28d6-468b-b215-ebea678a8323 08/17/23 07:10:09.728
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:09.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-143" for this suite. 08/17/23 07:10:09.765
------------------------------
â€¢ [0.074 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:09.698
    Aug 17 07:10:09.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:10:09.698
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:09.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:09.726
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-2862d375-28d6-468b-b215-ebea678a8323 08/17/23 07:10:09.728
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:09.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-143" for this suite. 08/17/23 07:10:09.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:09.772
Aug 17 07:10:09.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename cronjob 08/17/23 07:10:09.774
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:09.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:09.785
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/17/23 07:10:09.787
STEP: creating 08/17/23 07:10:09.787
STEP: getting 08/17/23 07:10:09.801
STEP: listing 08/17/23 07:10:09.807
STEP: watching 08/17/23 07:10:09.809
Aug 17 07:10:09.809: INFO: starting watch
STEP: cluster-wide listing 08/17/23 07:10:09.81
STEP: cluster-wide watching 08/17/23 07:10:09.812
Aug 17 07:10:09.812: INFO: starting watch
STEP: patching 08/17/23 07:10:09.814
STEP: updating 08/17/23 07:10:09.819
Aug 17 07:10:09.829: INFO: waiting for watch events with expected annotations
Aug 17 07:10:09.829: INFO: saw patched and updated annotations
STEP: patching /status 08/17/23 07:10:09.83
STEP: updating /status 08/17/23 07:10:09.834
STEP: get /status 08/17/23 07:10:09.838
STEP: deleting 08/17/23 07:10:09.841
STEP: deleting a collection 08/17/23 07:10:09.85
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:09.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7346" for this suite. 08/17/23 07:10:09.859
------------------------------
â€¢ [0.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:09.772
    Aug 17 07:10:09.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename cronjob 08/17/23 07:10:09.774
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:09.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:09.785
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/17/23 07:10:09.787
    STEP: creating 08/17/23 07:10:09.787
    STEP: getting 08/17/23 07:10:09.801
    STEP: listing 08/17/23 07:10:09.807
    STEP: watching 08/17/23 07:10:09.809
    Aug 17 07:10:09.809: INFO: starting watch
    STEP: cluster-wide listing 08/17/23 07:10:09.81
    STEP: cluster-wide watching 08/17/23 07:10:09.812
    Aug 17 07:10:09.812: INFO: starting watch
    STEP: patching 08/17/23 07:10:09.814
    STEP: updating 08/17/23 07:10:09.819
    Aug 17 07:10:09.829: INFO: waiting for watch events with expected annotations
    Aug 17 07:10:09.829: INFO: saw patched and updated annotations
    STEP: patching /status 08/17/23 07:10:09.83
    STEP: updating /status 08/17/23 07:10:09.834
    STEP: get /status 08/17/23 07:10:09.838
    STEP: deleting 08/17/23 07:10:09.841
    STEP: deleting a collection 08/17/23 07:10:09.85
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:09.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7346" for this suite. 08/17/23 07:10:09.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:09.864
Aug 17 07:10:09.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:10:09.865
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:09.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:09.886
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 17 07:10:09.907: INFO: Waiting up to 5m0s for pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90" in namespace "svcaccounts-4307" to be "running"
Aug 17 07:10:09.910: INFO: Pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327469ms
Aug 17 07:10:11.912: INFO: Pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90": Phase="Running", Reason="", readiness=true. Elapsed: 2.004161022s
Aug 17 07:10:11.912: INFO: Pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90" satisfied condition "running"
STEP: reading a file in the container 08/17/23 07:10:11.912
Aug 17 07:10:11.912: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4307 pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/17/23 07:10:12.051
Aug 17 07:10:12.052: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4307 pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/17/23 07:10:12.189
Aug 17 07:10:12.189: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4307 pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 17 07:10:12.337: INFO: Got root ca configmap in namespace "svcaccounts-4307"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:12.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4307" for this suite. 08/17/23 07:10:12.343
------------------------------
â€¢ [2.482 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:09.864
    Aug 17 07:10:09.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:10:09.865
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:09.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:09.886
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 17 07:10:09.907: INFO: Waiting up to 5m0s for pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90" in namespace "svcaccounts-4307" to be "running"
    Aug 17 07:10:09.910: INFO: Pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327469ms
    Aug 17 07:10:11.912: INFO: Pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90": Phase="Running", Reason="", readiness=true. Elapsed: 2.004161022s
    Aug 17 07:10:11.912: INFO: Pod "pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90" satisfied condition "running"
    STEP: reading a file in the container 08/17/23 07:10:11.912
    Aug 17 07:10:11.912: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4307 pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/17/23 07:10:12.051
    Aug 17 07:10:12.052: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4307 pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/17/23 07:10:12.189
    Aug 17 07:10:12.189: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4307 pod-service-account-8b473b60-7cd4-4dcc-940a-52bef3d93d90 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 17 07:10:12.337: INFO: Got root ca configmap in namespace "svcaccounts-4307"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:12.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4307" for this suite. 08/17/23 07:10:12.343
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:12.347
Aug 17 07:10:12.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:10:12.347
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:12.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:12.371
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/17/23 07:10:12.374
Aug 17 07:10:12.384: INFO: Waiting up to 5m0s for pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f" in namespace "downward-api-5496" to be "Succeeded or Failed"
Aug 17 07:10:12.387: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.634252ms
Aug 17 07:10:14.389: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005173236s
Aug 17 07:10:16.390: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00600393s
STEP: Saw pod success 08/17/23 07:10:16.39
Aug 17 07:10:16.390: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f" satisfied condition "Succeeded or Failed"
Aug 17 07:10:16.392: INFO: Trying to get logs from node yst-node2 pod downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f container dapi-container: <nil>
STEP: delete the pod 08/17/23 07:10:16.397
Aug 17 07:10:16.402: INFO: Waiting for pod downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f to disappear
Aug 17 07:10:16.405: INFO: Pod downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:16.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5496" for this suite. 08/17/23 07:10:16.407
------------------------------
â€¢ [4.064 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:12.347
    Aug 17 07:10:12.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:10:12.347
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:12.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:12.371
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/17/23 07:10:12.374
    Aug 17 07:10:12.384: INFO: Waiting up to 5m0s for pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f" in namespace "downward-api-5496" to be "Succeeded or Failed"
    Aug 17 07:10:12.387: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.634252ms
    Aug 17 07:10:14.389: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005173236s
    Aug 17 07:10:16.390: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00600393s
    STEP: Saw pod success 08/17/23 07:10:16.39
    Aug 17 07:10:16.390: INFO: Pod "downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f" satisfied condition "Succeeded or Failed"
    Aug 17 07:10:16.392: INFO: Trying to get logs from node yst-node2 pod downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f container dapi-container: <nil>
    STEP: delete the pod 08/17/23 07:10:16.397
    Aug 17 07:10:16.402: INFO: Waiting for pod downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f to disappear
    Aug 17 07:10:16.405: INFO: Pod downward-api-d2716bae-d257-45c9-a186-0f747fcc9b7f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:16.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5496" for this suite. 08/17/23 07:10:16.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:16.411
Aug 17 07:10:16.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:10:16.412
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:16.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:16.431
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-fc43fa39-67b7-489c-ba26-17332c5f2a1e 08/17/23 07:10:16.443
STEP: Creating configMap with name cm-test-opt-upd-d0364cfe-c73e-4d53-83c9-09149ae70672 08/17/23 07:10:16.449
STEP: Creating the pod 08/17/23 07:10:16.453
Aug 17 07:10:16.458: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228" in namespace "configmap-5910" to be "running and ready"
Aug 17 07:10:16.460: INFO: Pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228": Phase="Pending", Reason="", readiness=false. Elapsed: 1.972327ms
Aug 17 07:10:16.460: INFO: The phase of Pod pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:10:18.463: INFO: Pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228": Phase="Running", Reason="", readiness=true. Elapsed: 2.004852999s
Aug 17 07:10:18.463: INFO: The phase of Pod pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228 is Running (Ready = true)
Aug 17 07:10:18.463: INFO: Pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-fc43fa39-67b7-489c-ba26-17332c5f2a1e 08/17/23 07:10:18.474
STEP: Updating configmap cm-test-opt-upd-d0364cfe-c73e-4d53-83c9-09149ae70672 08/17/23 07:10:18.478
STEP: Creating configMap with name cm-test-opt-create-b319454b-b7f6-42f7-bc79-6179a55124d2 08/17/23 07:10:18.481
STEP: waiting to observe update in volume 08/17/23 07:10:18.484
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:20.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5910" for this suite. 08/17/23 07:10:20.767
------------------------------
â€¢ [4.372 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:16.411
    Aug 17 07:10:16.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:10:16.412
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:16.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:16.431
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-fc43fa39-67b7-489c-ba26-17332c5f2a1e 08/17/23 07:10:16.443
    STEP: Creating configMap with name cm-test-opt-upd-d0364cfe-c73e-4d53-83c9-09149ae70672 08/17/23 07:10:16.449
    STEP: Creating the pod 08/17/23 07:10:16.453
    Aug 17 07:10:16.458: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228" in namespace "configmap-5910" to be "running and ready"
    Aug 17 07:10:16.460: INFO: Pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228": Phase="Pending", Reason="", readiness=false. Elapsed: 1.972327ms
    Aug 17 07:10:16.460: INFO: The phase of Pod pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:10:18.463: INFO: Pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228": Phase="Running", Reason="", readiness=true. Elapsed: 2.004852999s
    Aug 17 07:10:18.463: INFO: The phase of Pod pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228 is Running (Ready = true)
    Aug 17 07:10:18.463: INFO: Pod "pod-configmaps-6e290544-95c6-4f2f-87b6-8c702c5cd228" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-fc43fa39-67b7-489c-ba26-17332c5f2a1e 08/17/23 07:10:18.474
    STEP: Updating configmap cm-test-opt-upd-d0364cfe-c73e-4d53-83c9-09149ae70672 08/17/23 07:10:18.478
    STEP: Creating configMap with name cm-test-opt-create-b319454b-b7f6-42f7-bc79-6179a55124d2 08/17/23 07:10:18.481
    STEP: waiting to observe update in volume 08/17/23 07:10:18.484
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:20.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5910" for this suite. 08/17/23 07:10:20.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:20.783
Aug 17 07:10:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename namespaces 08/17/23 07:10:20.784
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:21.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:21.125
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-6nc8t" 08/17/23 07:10:21.13
Aug 17 07:10:21.158: INFO: Namespace "e2e-ns-6nc8t-7415" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-6nc8t-7415" 08/17/23 07:10:21.158
Aug 17 07:10:21.178: INFO: Namespace "e2e-ns-6nc8t-7415" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-6nc8t-7415" 08/17/23 07:10:21.178
Aug 17 07:10:21.184: INFO: Namespace "e2e-ns-6nc8t-7415" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:21.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6734" for this suite. 08/17/23 07:10:21.188
STEP: Destroying namespace "e2e-ns-6nc8t-7415" for this suite. 08/17/23 07:10:21.195
------------------------------
â€¢ [0.416 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:20.783
    Aug 17 07:10:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename namespaces 08/17/23 07:10:20.784
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:21.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:21.125
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-6nc8t" 08/17/23 07:10:21.13
    Aug 17 07:10:21.158: INFO: Namespace "e2e-ns-6nc8t-7415" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-6nc8t-7415" 08/17/23 07:10:21.158
    Aug 17 07:10:21.178: INFO: Namespace "e2e-ns-6nc8t-7415" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-6nc8t-7415" 08/17/23 07:10:21.178
    Aug 17 07:10:21.184: INFO: Namespace "e2e-ns-6nc8t-7415" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:21.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6734" for this suite. 08/17/23 07:10:21.188
    STEP: Destroying namespace "e2e-ns-6nc8t-7415" for this suite. 08/17/23 07:10:21.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:21.2
Aug 17 07:10:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:10:21.203
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:21.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:21.219
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/17/23 07:10:21.222
Aug 17 07:10:21.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:10:24.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:33.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6626" for this suite. 08/17/23 07:10:33.559
------------------------------
â€¢ [SLOW TEST] [12.364 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:21.2
    Aug 17 07:10:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:10:21.203
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:21.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:21.219
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/17/23 07:10:21.222
    Aug 17 07:10:21.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:10:24.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:33.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6626" for this suite. 08/17/23 07:10:33.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:33.565
Aug 17 07:10:33.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:10:33.566
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:33.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:33.582
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/17/23 07:10:33.589
Aug 17 07:10:33.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 create -f -'
Aug 17 07:10:34.619: INFO: stderr: ""
Aug 17 07:10:34.619: INFO: stdout: "pod/pause created\n"
Aug 17 07:10:34.619: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 17 07:10:34.619: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6609" to be "running and ready"
Aug 17 07:10:34.621: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110476ms
Aug 17 07:10:34.621: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'yst-node2' to be 'Running' but was 'Pending'
Aug 17 07:10:36.624: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004258646s
Aug 17 07:10:36.624: INFO: Pod "pause" satisfied condition "running and ready"
Aug 17 07:10:36.624: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/17/23 07:10:36.624
Aug 17 07:10:36.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 label pods pause testing-label=testing-label-value'
Aug 17 07:10:36.715: INFO: stderr: ""
Aug 17 07:10:36.715: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/17/23 07:10:36.715
Aug 17 07:10:36.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get pod pause -L testing-label'
Aug 17 07:10:36.794: INFO: stderr: ""
Aug 17 07:10:36.794: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/17/23 07:10:36.794
Aug 17 07:10:36.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 label pods pause testing-label-'
Aug 17 07:10:36.904: INFO: stderr: ""
Aug 17 07:10:36.904: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/17/23 07:10:36.904
Aug 17 07:10:36.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get pod pause -L testing-label'
Aug 17 07:10:36.982: INFO: stderr: ""
Aug 17 07:10:36.982: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/17/23 07:10:36.982
Aug 17 07:10:36.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 delete --grace-period=0 --force -f -'
Aug 17 07:10:37.062: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:10:37.062: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 17 07:10:37.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get rc,svc -l name=pause --no-headers'
Aug 17 07:10:37.149: INFO: stderr: "No resources found in kubectl-6609 namespace.\n"
Aug 17 07:10:37.149: INFO: stdout: ""
Aug 17 07:10:37.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 07:10:37.224: INFO: stderr: ""
Aug 17 07:10:37.224: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:37.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6609" for this suite. 08/17/23 07:10:37.227
------------------------------
â€¢ [3.665 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:33.565
    Aug 17 07:10:33.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:10:33.566
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:33.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:33.582
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/17/23 07:10:33.589
    Aug 17 07:10:33.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 create -f -'
    Aug 17 07:10:34.619: INFO: stderr: ""
    Aug 17 07:10:34.619: INFO: stdout: "pod/pause created\n"
    Aug 17 07:10:34.619: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 17 07:10:34.619: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6609" to be "running and ready"
    Aug 17 07:10:34.621: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110476ms
    Aug 17 07:10:34.621: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'yst-node2' to be 'Running' but was 'Pending'
    Aug 17 07:10:36.624: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004258646s
    Aug 17 07:10:36.624: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 17 07:10:36.624: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/17/23 07:10:36.624
    Aug 17 07:10:36.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 label pods pause testing-label=testing-label-value'
    Aug 17 07:10:36.715: INFO: stderr: ""
    Aug 17 07:10:36.715: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/17/23 07:10:36.715
    Aug 17 07:10:36.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get pod pause -L testing-label'
    Aug 17 07:10:36.794: INFO: stderr: ""
    Aug 17 07:10:36.794: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/17/23 07:10:36.794
    Aug 17 07:10:36.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 label pods pause testing-label-'
    Aug 17 07:10:36.904: INFO: stderr: ""
    Aug 17 07:10:36.904: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/17/23 07:10:36.904
    Aug 17 07:10:36.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get pod pause -L testing-label'
    Aug 17 07:10:36.982: INFO: stderr: ""
    Aug 17 07:10:36.982: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/17/23 07:10:36.982
    Aug 17 07:10:36.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 delete --grace-period=0 --force -f -'
    Aug 17 07:10:37.062: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:10:37.062: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 17 07:10:37.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get rc,svc -l name=pause --no-headers'
    Aug 17 07:10:37.149: INFO: stderr: "No resources found in kubectl-6609 namespace.\n"
    Aug 17 07:10:37.149: INFO: stdout: ""
    Aug 17 07:10:37.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-6609 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 17 07:10:37.224: INFO: stderr: ""
    Aug 17 07:10:37.224: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:37.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6609" for this suite. 08/17/23 07:10:37.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:37.231
Aug 17 07:10:37.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename daemonsets 08/17/23 07:10:37.232
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:37.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:37.246
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Aug 17 07:10:37.273: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/17/23 07:10:37.276
Aug 17 07:10:37.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:37.279: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/17/23 07:10:37.279
Aug 17 07:10:37.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:37.293: INFO: Node yst-node2 is running 0 daemon pod, expected 1
Aug 17 07:10:38.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:38.295: INFO: Node yst-node2 is running 0 daemon pod, expected 1
Aug 17 07:10:39.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 07:10:39.294: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/17/23 07:10:39.296
Aug 17 07:10:39.311: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 07:10:39.311: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 17 07:10:40.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:40.314: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/17/23 07:10:40.314
Aug 17 07:10:40.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:40.324: INFO: Node yst-node2 is running 0 daemon pod, expected 1
Aug 17 07:10:41.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:41.327: INFO: Node yst-node2 is running 0 daemon pod, expected 1
Aug 17 07:10:42.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:42.327: INFO: Node yst-node2 is running 0 daemon pod, expected 1
Aug 17 07:10:43.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 07:10:43.327: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:10:43.33
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1996, will wait for the garbage collector to delete the pods 08/17/23 07:10:43.33
Aug 17 07:10:43.385: INFO: Deleting DaemonSet.extensions daemon-set took: 3.148278ms
Aug 17 07:10:43.486: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.043713ms
Aug 17 07:10:45.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:45.789: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 07:10:45.790: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27016648"},"items":null}

Aug 17 07:10:45.792: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27016650"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:45.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1996" for this suite. 08/17/23 07:10:45.808
------------------------------
â€¢ [SLOW TEST] [8.581 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:37.231
    Aug 17 07:10:37.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename daemonsets 08/17/23 07:10:37.232
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:37.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:37.246
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Aug 17 07:10:37.273: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/17/23 07:10:37.276
    Aug 17 07:10:37.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:37.279: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/17/23 07:10:37.279
    Aug 17 07:10:37.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:37.293: INFO: Node yst-node2 is running 0 daemon pod, expected 1
    Aug 17 07:10:38.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:38.295: INFO: Node yst-node2 is running 0 daemon pod, expected 1
    Aug 17 07:10:39.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 07:10:39.294: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/17/23 07:10:39.296
    Aug 17 07:10:39.311: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 07:10:39.311: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 17 07:10:40.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:40.314: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/17/23 07:10:40.314
    Aug 17 07:10:40.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:40.324: INFO: Node yst-node2 is running 0 daemon pod, expected 1
    Aug 17 07:10:41.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:41.327: INFO: Node yst-node2 is running 0 daemon pod, expected 1
    Aug 17 07:10:42.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:42.327: INFO: Node yst-node2 is running 0 daemon pod, expected 1
    Aug 17 07:10:43.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 07:10:43.327: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:10:43.33
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1996, will wait for the garbage collector to delete the pods 08/17/23 07:10:43.33
    Aug 17 07:10:43.385: INFO: Deleting DaemonSet.extensions daemon-set took: 3.148278ms
    Aug 17 07:10:43.486: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.043713ms
    Aug 17 07:10:45.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:45.789: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 07:10:45.790: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27016648"},"items":null}

    Aug 17 07:10:45.792: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27016650"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:45.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1996" for this suite. 08/17/23 07:10:45.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:45.812
Aug 17 07:10:45.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename daemonsets 08/17/23 07:10:45.813
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:45.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:45.826
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 08/17/23 07:10:45.889
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:10:45.892
Aug 17 07:10:45.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:10:45.895: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:10:46.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 07:10:46.901: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:10:47.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:10:47.900: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 08/17/23 07:10:47.902
STEP: DeleteCollection of the DaemonSets 08/17/23 07:10:47.904
STEP: Verify that ReplicaSets have been deleted 08/17/23 07:10:47.907
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Aug 17 07:10:47.912: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27016761"},"items":null}

Aug 17 07:10:47.921: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27016764"},"items":[{"metadata":{"name":"daemon-set-dv467","generateName":"daemon-set-","namespace":"daemonsets-8223","uid":"540daec8-d657-461e-ac17-c6eee5aaf639","resourceVersion":"27016764","creationTimestamp":"2023-08-17T07:10:45Z","deletionTimestamp":"2023-08-17T07:11:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4f548466cf30c42fe01bca59c1698975bf2cf2c9816f738c9f50f4961548e9e6","cni.projectcalico.org/podIP":"172.32.123.191/32","cni.projectcalico.org/podIPs":"172.32.123.191/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"51722993-576a-410a-a8dc-6be6f6500efc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51722993-576a-410a-a8dc-6be6f6500efc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-24q48","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-24q48","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"yst-node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["yst-node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"}],"hostIP":"10.60.200.176","podIP":"172.32.123.191","podIPs":[{"ip":"172.32.123.191"}],"startTime":"2023-08-17T07:10:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T07:10:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a5becfb6018e48fbea6beaa522c0edc7b9268a90836d1cf6f545701e34b3a989","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hcfgz","generateName":"daemon-set-","namespace":"daemonsets-8223","uid":"de5f0e8a-f440-470c-8c61-899965fb48f8","resourceVersion":"27016763","creationTimestamp":"2023-08-17T07:10:45Z","deletionTimestamp":"2023-08-17T07:11:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"da51afc176056de28d2fbdb28a0525648fdbf9bb2af13d8c16aa8c30e79c1d5a","cni.projectcalico.org/podIP":"172.32.3.52/32","cni.projectcalico.org/podIPs":"172.32.3.52/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"51722993-576a-410a-a8dc-6be6f6500efc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51722993-576a-410a-a8dc-6be6f6500efc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wmdjv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wmdjv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"yst-master","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["yst-master"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"}],"hostIP":"10.60.200.175","podIP":"172.32.3.52","podIPs":[{"ip":"172.32.3.52"}],"startTime":"2023-08-17T07:10:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T07:10:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0790fa6b171676e208ba9ce71868d7f5e99a5c8beade11f63efc9409ce871325","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jgsvs","generateName":"daemon-set-","namespace":"daemonsets-8223","uid":"99dd134a-ec13-4072-a672-ef368cceb426","resourceVersion":"27016762","creationTimestamp":"2023-08-17T07:10:45Z","deletionTimestamp":"2023-08-17T07:11:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c629bc19accd17910245851fda1bff9a5b0be21b7b7795be315f64d07dec11a4","cni.projectcalico.org/podIP":"172.32.238.91/32","cni.projectcalico.org/podIPs":"172.32.238.91/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"51722993-576a-410a-a8dc-6be6f6500efc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51722993-576a-410a-a8dc-6be6f6500efc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8m6q2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8m6q2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"yst-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["yst-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"}],"hostIP":"10.60.200.177","podIP":"172.32.238.91","podIPs":[{"ip":"172.32.238.91"}],"startTime":"2023-08-17T07:10:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T07:10:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1d4fd55fbcfdeb25789b06072e06908fd567ea56211b24f782c25d20ab5ee371","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8223" for this suite. 08/17/23 07:10:47.933
------------------------------
â€¢ [2.124 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:45.812
    Aug 17 07:10:45.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename daemonsets 08/17/23 07:10:45.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:45.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:45.826
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 08/17/23 07:10:45.889
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:10:45.892
    Aug 17 07:10:45.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:10:45.895: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:10:46.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 07:10:46.901: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:10:47.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:10:47.900: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 08/17/23 07:10:47.902
    STEP: DeleteCollection of the DaemonSets 08/17/23 07:10:47.904
    STEP: Verify that ReplicaSets have been deleted 08/17/23 07:10:47.907
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Aug 17 07:10:47.912: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27016761"},"items":null}

    Aug 17 07:10:47.921: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27016764"},"items":[{"metadata":{"name":"daemon-set-dv467","generateName":"daemon-set-","namespace":"daemonsets-8223","uid":"540daec8-d657-461e-ac17-c6eee5aaf639","resourceVersion":"27016764","creationTimestamp":"2023-08-17T07:10:45Z","deletionTimestamp":"2023-08-17T07:11:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4f548466cf30c42fe01bca59c1698975bf2cf2c9816f738c9f50f4961548e9e6","cni.projectcalico.org/podIP":"172.32.123.191/32","cni.projectcalico.org/podIPs":"172.32.123.191/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"51722993-576a-410a-a8dc-6be6f6500efc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51722993-576a-410a-a8dc-6be6f6500efc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-24q48","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-24q48","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"yst-node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["yst-node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"}],"hostIP":"10.60.200.176","podIP":"172.32.123.191","podIPs":[{"ip":"172.32.123.191"}],"startTime":"2023-08-17T07:10:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T07:10:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a5becfb6018e48fbea6beaa522c0edc7b9268a90836d1cf6f545701e34b3a989","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hcfgz","generateName":"daemon-set-","namespace":"daemonsets-8223","uid":"de5f0e8a-f440-470c-8c61-899965fb48f8","resourceVersion":"27016763","creationTimestamp":"2023-08-17T07:10:45Z","deletionTimestamp":"2023-08-17T07:11:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"da51afc176056de28d2fbdb28a0525648fdbf9bb2af13d8c16aa8c30e79c1d5a","cni.projectcalico.org/podIP":"172.32.3.52/32","cni.projectcalico.org/podIPs":"172.32.3.52/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"51722993-576a-410a-a8dc-6be6f6500efc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51722993-576a-410a-a8dc-6be6f6500efc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wmdjv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wmdjv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"yst-master","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["yst-master"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"}],"hostIP":"10.60.200.175","podIP":"172.32.3.52","podIPs":[{"ip":"172.32.3.52"}],"startTime":"2023-08-17T07:10:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T07:10:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0790fa6b171676e208ba9ce71868d7f5e99a5c8beade11f63efc9409ce871325","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jgsvs","generateName":"daemon-set-","namespace":"daemonsets-8223","uid":"99dd134a-ec13-4072-a672-ef368cceb426","resourceVersion":"27016762","creationTimestamp":"2023-08-17T07:10:45Z","deletionTimestamp":"2023-08-17T07:11:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c629bc19accd17910245851fda1bff9a5b0be21b7b7795be315f64d07dec11a4","cni.projectcalico.org/podIP":"172.32.238.91/32","cni.projectcalico.org/podIPs":"172.32.238.91/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"51722993-576a-410a-a8dc-6be6f6500efc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51722993-576a-410a-a8dc-6be6f6500efc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T07:10:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8m6q2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8m6q2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"yst-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["yst-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T07:10:45Z"}],"hostIP":"10.60.200.177","podIP":"172.32.238.91","podIPs":[{"ip":"172.32.238.91"}],"startTime":"2023-08-17T07:10:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T07:10:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1d4fd55fbcfdeb25789b06072e06908fd567ea56211b24f782c25d20ab5ee371","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8223" for this suite. 08/17/23 07:10:47.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:47.937
Aug 17 07:10:47.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:10:47.938
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:47.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:47.955
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:10:47.957
Aug 17 07:10:47.976: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82" in namespace "downward-api-4367" to be "Succeeded or Failed"
Aug 17 07:10:47.979: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726069ms
Aug 17 07:10:49.982: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005865836s
Aug 17 07:10:51.981: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005492167s
STEP: Saw pod success 08/17/23 07:10:51.981
Aug 17 07:10:51.981: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82" satisfied condition "Succeeded or Failed"
Aug 17 07:10:51.983: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82 container client-container: <nil>
STEP: delete the pod 08/17/23 07:10:51.986
Aug 17 07:10:51.992: INFO: Waiting for pod downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82 to disappear
Aug 17 07:10:52.050: INFO: Pod downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:52.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4367" for this suite. 08/17/23 07:10:52.052
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:47.937
    Aug 17 07:10:47.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:10:47.938
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:47.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:47.955
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:10:47.957
    Aug 17 07:10:47.976: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82" in namespace "downward-api-4367" to be "Succeeded or Failed"
    Aug 17 07:10:47.979: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726069ms
    Aug 17 07:10:49.982: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005865836s
    Aug 17 07:10:51.981: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005492167s
    STEP: Saw pod success 08/17/23 07:10:51.981
    Aug 17 07:10:51.981: INFO: Pod "downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82" satisfied condition "Succeeded or Failed"
    Aug 17 07:10:51.983: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:10:51.986
    Aug 17 07:10:51.992: INFO: Waiting for pod downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82 to disappear
    Aug 17 07:10:52.050: INFO: Pod downwardapi-volume-e48842e8-6e21-4fb2-adef-468471b3cb82 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:52.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4367" for this suite. 08/17/23 07:10:52.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:52.056
Aug 17 07:10:52.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-pred 08/17/23 07:10:52.057
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:52.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:52.069
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 07:10:52.072: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 07:10:52.084: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 07:10:52.088: INFO: 
Logging pods the apiserver thinks is on node yst-master before test
Aug 17 07:10:52.101: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container app ready: true, restart count 1
Aug 17 07:10:52.101: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container chartmuseum ready: true, restart count 2
Aug 17 07:10:52.101: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container cluster-server ready: true, restart count 7
Aug 17 07:10:52.101: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container proxy ready: true, restart count 8
Aug 17 07:10:52.101: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container keycloak ready: true, restart count 2
Aug 17 07:10:52.101: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container postgres ready: true, restart count 2
Aug 17 07:10:52.101: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container keycloak-observer ready: true, restart count 6
Aug 17 07:10:52.101: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container manual-webserver ready: true, restart count 1
Aug 17 07:10:52.101: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Aug 17 07:10:52.101: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Aug 17 07:10:52.101: INFO: 	Container kube-state-metrics ready: true, restart count 1
Aug 17 07:10:52.101: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:10:52.101: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:10:52.101: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
Aug 17 07:10:52.101: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container apiserver ready: true, restart count 2
Aug 17 07:10:52.101: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container manager ready: true, restart count 4
Aug 17 07:10:52.101: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:10:52.101: INFO: 	Container etcd-date ready: false, restart count 0
Aug 17 07:10:52.101: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:10:52.101: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container core ready: true, restart count 4
Aug 17 07:10:52.101: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container database ready: true, restart count 2
Aug 17 07:10:52.101: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container jobservice ready: true, restart count 11
Aug 17 07:10:52.101: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container nginx ready: true, restart count 9
Aug 17 07:10:52.101: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container notary-signer ready: true, restart count 6
Aug 17 07:10:52.101: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.101: INFO: 	Container kiali ready: true, restart count 1
Aug 17 07:10:52.101: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container kube-event-exporter ready: true, restart count 2
Aug 17 07:10:52.102: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container minio ready: true, restart count 2
Aug 17 07:10:52.102: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container index-clear ready: false, restart count 0
Aug 17 07:10:52.102: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:10:52.102: INFO: 	Container pgdata-backup-node ready: false, restart count 0
Aug 17 07:10:52.102: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container manager ready: true, restart count 5
Aug 17 07:10:52.102: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:10:52.102: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:10:52.102: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container backup-check ready: false, restart count 1
Aug 17 07:10:52.102: INFO: 	Container backup-date ready: false, restart count 0
Aug 17 07:10:52.102: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Aug 17 07:10:52.102: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:10:52.102: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container coredns ready: true, restart count 1
Aug 17 07:10:52.102: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container etcd ready: true, restart count 2
Aug 17 07:10:52.102: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container kube-apiserver ready: true, restart count 2
Aug 17 07:10:52.102: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container kube-controller-manager ready: true, restart count 1
Aug 17 07:10:52.102: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:10:52.102: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container kube-scheduler ready: true, restart count 1
Aug 17 07:10:52.102: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container metrics-server ready: true, restart count 3
Aug 17 07:10:52.102: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:10:52.102: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:10:52.102: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container acc-tomcat ready: true, restart count 1
Aug 17 07:10:52.102: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.102: INFO: 	Container exporter ready: true, restart count 2
Aug 17 07:10:52.102: INFO: 	Container server ready: true, restart count 2
Aug 17 07:10:52.102: INFO: 
Logging pods the apiserver thinks is on node yst-node1 before test
Aug 17 07:10:52.113: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container webserver ready: true, restart count 1
Aug 17 07:10:52.113: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container gateway ready: true, restart count 7
Aug 17 07:10:52.113: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container helm-server ready: true, restart count 7
Aug 17 07:10:52.113: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container app ready: true, restart count 2
Aug 17 07:10:52.113: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:10:52.113: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:10:52.113: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:10:52.113: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:10:52.113: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container server ready: true, restart count 5
Aug 17 07:10:52.113: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container alertmanager ready: true, restart count 3
Aug 17 07:10:52.113: INFO: 	Container config-reloader ready: true, restart count 2
Aug 17 07:10:52.113: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container auth-server ready: true, restart count 5
Aug 17 07:10:52.113: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container blackbox-exporter ready: true, restart count 1
Aug 17 07:10:52.113: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:10:52.113: INFO: 	Container module-configmap-reloader ready: true, restart count 1
Aug 17 07:10:52.113: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:10:52.113: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:10:52.113: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container cronhpa-controller ready: true, restart count 1
Aug 17 07:10:52.113: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container default-http-backend ready: true, restart count 1
Aug 17 07:10:52.113: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:10:52.113: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container chartmuseum ready: true, restart count 1
Aug 17 07:10:52.113: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container notary-server ready: true, restart count 7
Aug 17 07:10:52.113: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container portal ready: true, restart count 2
Aug 17 07:10:52.113: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container redis ready: true, restart count 2
Aug 17 07:10:52.113: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container registry ready: true, restart count 2
Aug 17 07:10:52.113: INFO: 	Container registryctl ready: true, restart count 2
Aug 17 07:10:52.113: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container trivy ready: true, restart count 2
Aug 17 07:10:52.113: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container server ready: true, restart count 7
Aug 17 07:10:52.113: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container logstash ready: true, restart count 2
Aug 17 07:10:52.113: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container agent ready: true, restart count 2
Aug 17 07:10:52.113: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container prometheus-adapter ready: true, restart count 3
Aug 17 07:10:52.113: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:10:52.113: INFO: 	Container prometheus-operator ready: true, restart count 1
Aug 17 07:10:52.113: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:10:52.113: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container thanos-store ready: true, restart count 16
Aug 17 07:10:52.113: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container timescaledb ready: true, restart count 2
Aug 17 07:10:52.113: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:10:52.113: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container workflow-controller ready: true, restart count 7
Aug 17 07:10:52.113: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container apiserver ready: true, restart count 1
Aug 17 07:10:52.113: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:10:52.113: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:10:52.113: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.113: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:10:52.113: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:10:52.113: INFO: 
Logging pods the apiserver thinks is on node yst-node2 before test
Aug 17 07:10:52.128: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:10:52.128: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:10:52.128: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:10:52.128: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container opensearch ready: true, restart count 1
Aug 17 07:10:52.128: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container config-reloader ready: true, restart count 1
Aug 17 07:10:52.128: INFO: 	Container prometheus ready: true, restart count 1
Aug 17 07:10:52.128: INFO: 	Container thanos-sidecar ready: true, restart count 1
Aug 17 07:10:52.128: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:10:52.128: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:10:52.128: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container coredns ready: true, restart count 1
Aug 17 07:10:52.128: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:10:52.128: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 07:10:52.128: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.128: INFO: 	Container e2e ready: true, restart count 0
Aug 17 07:10:52.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:10:52.129: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:10:52.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:10:52.129: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node yst-master 08/17/23 07:10:52.153
STEP: verifying the node has the label node yst-node1 08/17/23 07:10:52.162
STEP: verifying the node has the label node yst-node2 08/17/23 07:10:52.172
Aug 17 07:10:52.193: INFO: Pod alert-apiserver-67bffd5d79-4jkdw requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod chartmuseum-chartmuseum-fd9b485cc-98wvk requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod cluster-server-9775c76cc-9g6hh requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod console-7fb6dfb4b6-gfzpf requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod gateway-6f8686745f-7sthl requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod gateway-proxy-v8vpk requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod helm-server-5776765f78-ggckc requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod keycloak-64b4b5569c-x5j9n requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod keycloak-db-5cc8f7f7d5-x6xbq requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod keycloak-observer-86b5cc8d77-qs2lj requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod manual-webserver-66754cc96-5rf5n requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod monitoring-apiserver-db555b567-mbxz5 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod thanos-querier-74bcdf4595-4d8sj requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod acc-kube-state-metrics-86c68f6799-bpbfb requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod acc-node-exporter-4nxg2 requesting resource cpu=112m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod acc-node-exporter-6gbgm requesting resource cpu=112m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod acc-node-exporter-n94lf requesting resource cpu=112m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod accordion-data-provisioner-868fbcb9c9-94xx4 requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod accordion-ingress-controller-56bfbf9c76-hz58x requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod alert-server-57f98d8ff9-vkk9v requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod auth-server-f67b484c7-n8c9m requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod blackbox-exporter-f48cd58c9-w6p8v requesting resource cpu=30m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod cicd-apiserver-59d9f5977b-qj6tl requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod cicd-manager-56f888c97f-wpbd7 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod cicd-template-sync-manager-957d5cff4-6mqz6 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod cicd-trigger-manager-7f78f48b64-9t4dx requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod cronhpa-controller-66bcb7c54c-z5mwq requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod default-http-backend-586b89f667-v7mjh requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod filebeat-filebeat-g4crt requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod filebeat-filebeat-ttr4s requesting resource cpu=0m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod filebeat-filebeat-wt7kd requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod harbor-chartmuseum-777994dc68-nk8fn requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod harbor-core-c69dc8568-dxzjg requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod harbor-jobservice-66b66b4b9-f22t7 requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod harbor-nginx-7bb6fcf984-xgzmf requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod harbor-notary-server-799b956658-k4cjz requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod harbor-notary-signer-cbbf7c95d-nlmpk requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod harbor-portal-77c89f9f8d-jvtfl requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod harbor-registry-64856bdc84-pq8r2 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod harbor-trivy-0 requesting resource cpu=200m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod kiali-6d6fd7b96c-4qtgj requesting resource cpu=10m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod kube-event-exporter-57bb5b5d55-gnbnd requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod log-server-78fd7c75c8-76s7s requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod logstash-logstash-0 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod member-agent-64c8644464-8mqkv requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod minio-0 requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod opensearch-cluster-master-0 requesting resource cpu=1000m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod prometheus-adapter-dcc8655bc-697t4 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod prometheus-operator-779799477d-hg559 requesting resource cpu=100m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod prometheus-prometheus-operator-prometheus-0 requesting resource cpu=100m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod registry-manager-75ff67db55-mzct2 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod scouter-manager-6bd5945b66-zmwv2 requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod thanos-compactor-0 requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod thanos-store-0 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod tsdb-timescaledb-0 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod user-ingress-controller-knj75 requesting resource cpu=0m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod user-ingress-controller-ltkfd requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod user-ingress-controller-r7wrt requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod workflow-controller-746b98c5c-5t6dl requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod xlog-apiserver-b78f4959d-wcfnn requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod calico-kube-controllers-59f6c5b776-ld46r requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod calico-node-hnssl requesting resource cpu=250m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod calico-node-kh6f4 requesting resource cpu=250m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod calico-node-rvq9v requesting resource cpu=250m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod coredns-69754fcccf-bx54l requesting resource cpu=100m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod coredns-69754fcccf-r7xb2 requesting resource cpu=100m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod etcd-yst-master requesting resource cpu=100m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod kube-apiserver-yst-master requesting resource cpu=250m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod kube-controller-manager-yst-master requesting resource cpu=200m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod kube-proxy-mtcmj requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod kube-proxy-vl6rl requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod kube-proxy-znvf7 requesting resource cpu=0m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod kube-scheduler-yst-master requesting resource cpu=100m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod metrics-server-d88c6d6d7-6n64h requesting resource cpu=100m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod sonobuoy requesting resource cpu=0m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod sonobuoy-e2e-job-ab8d33ed8e20458b requesting resource cpu=0m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn requesting resource cpu=0m on Node yst-node2
Aug 17 07:10:52.193: INFO: Pod sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 requesting resource cpu=0m on Node yst-node1
Aug 17 07:10:52.193: INFO: Pod acc-tomcat-ddb885997-2w79z requesting resource cpu=0m on Node yst-master
Aug 17 07:10:52.193: INFO: Pod scouter-server-64f47bc954-r47gx requesting resource cpu=0m on Node yst-master
STEP: Starting Pods to consume most of the cluster CPU. 08/17/23 07:10:52.193
Aug 17 07:10:52.193: INFO: Creating a pod which consumes cpu=4744m on Node yst-master
Aug 17 07:10:52.198: INFO: Creating a pod which consumes cpu=5045m on Node yst-node1
Aug 17 07:10:52.202: INFO: Creating a pod which consumes cpu=4506m on Node yst-node2
Aug 17 07:10:52.206: INFO: Waiting up to 5m0s for pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef" in namespace "sched-pred-1757" to be "running"
Aug 17 07:10:52.209: INFO: Pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.696921ms
Aug 17 07:10:54.212: INFO: Pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.00593618s
Aug 17 07:10:54.212: INFO: Pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef" satisfied condition "running"
Aug 17 07:10:54.212: INFO: Waiting up to 5m0s for pod "filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e" in namespace "sched-pred-1757" to be "running"
Aug 17 07:10:54.213: INFO: Pod "filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e": Phase="Running", Reason="", readiness=true. Elapsed: 1.592764ms
Aug 17 07:10:54.213: INFO: Pod "filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e" satisfied condition "running"
Aug 17 07:10:54.213: INFO: Waiting up to 5m0s for pod "filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160" in namespace "sched-pred-1757" to be "running"
Aug 17 07:10:54.215: INFO: Pod "filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160": Phase="Running", Reason="", readiness=true. Elapsed: 1.395799ms
Aug 17 07:10:54.215: INFO: Pod "filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/17/23 07:10:54.215
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a186e7389a7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1757/filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160 to yst-node2] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a189223d5ef], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a18935b1452], Reason = [Created], Message = [Created container filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a189ab31101], Reason = [Started], Message = [Started container filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a186db87356], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1757/filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e to yst-node1] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a189308525a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a1894698bf1], Reason = [Created], Message = [Created container filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a189b7d5e85], Reason = [Started], Message = [Started container filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a186d6a934c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1757/filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef to yst-master] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a1893d746f4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a1895137c9e], Reason = [Created], Message = [Created container filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a189d8aff9a], Reason = [Started], Message = [Started container filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef] 08/17/23 07:10:54.217
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177c1a18e5c9688e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 08/17/23 07:10:54.224
STEP: removing the label node off the node yst-master 08/17/23 07:10:55.223
STEP: verifying the node doesn't have the label node 08/17/23 07:10:55.234
STEP: removing the label node off the node yst-node1 08/17/23 07:10:55.238
STEP: verifying the node doesn't have the label node 08/17/23 07:10:55.245
STEP: removing the label node off the node yst-node2 08/17/23 07:10:55.247
STEP: verifying the node doesn't have the label node 08/17/23 07:10:55.255
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:10:55.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1757" for this suite. 08/17/23 07:10:55.26
------------------------------
â€¢ [3.207 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:52.056
    Aug 17 07:10:52.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-pred 08/17/23 07:10:52.057
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:52.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:52.069
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 07:10:52.072: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 07:10:52.084: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 07:10:52.088: INFO: 
    Logging pods the apiserver thinks is on node yst-master before test
    Aug 17 07:10:52.101: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container app ready: true, restart count 1
    Aug 17 07:10:52.101: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container chartmuseum ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container cluster-server ready: true, restart count 7
    Aug 17 07:10:52.101: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container proxy ready: true, restart count 8
    Aug 17 07:10:52.101: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container keycloak ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container postgres ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container keycloak-observer ready: true, restart count 6
    Aug 17 07:10:52.101: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container manual-webserver ready: true, restart count 1
    Aug 17 07:10:52.101: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
    Aug 17 07:10:52.101: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
    Aug 17 07:10:52.101: INFO: 	Container kube-state-metrics ready: true, restart count 1
    Aug 17 07:10:52.101: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
    Aug 17 07:10:52.101: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container apiserver ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container manager ready: true, restart count 4
    Aug 17 07:10:52.101: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:10:52.101: INFO: 	Container etcd-date ready: false, restart count 0
    Aug 17 07:10:52.101: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container core ready: true, restart count 4
    Aug 17 07:10:52.101: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container database ready: true, restart count 2
    Aug 17 07:10:52.101: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container jobservice ready: true, restart count 11
    Aug 17 07:10:52.101: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container nginx ready: true, restart count 9
    Aug 17 07:10:52.101: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container notary-signer ready: true, restart count 6
    Aug 17 07:10:52.101: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.101: INFO: 	Container kiali ready: true, restart count 1
    Aug 17 07:10:52.101: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container kube-event-exporter ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container minio ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container index-clear ready: false, restart count 0
    Aug 17 07:10:52.102: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:10:52.102: INFO: 	Container pgdata-backup-node ready: false, restart count 0
    Aug 17 07:10:52.102: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container manager ready: true, restart count 5
    Aug 17 07:10:52.102: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container backup-check ready: false, restart count 1
    Aug 17 07:10:52.102: INFO: 	Container backup-date ready: false, restart count 0
    Aug 17 07:10:52.102: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container coredns ready: true, restart count 1
    Aug 17 07:10:52.102: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container etcd ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container kube-apiserver ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Aug 17 07:10:52.102: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:10:52.102: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container kube-scheduler ready: true, restart count 1
    Aug 17 07:10:52.102: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container metrics-server ready: true, restart count 3
    Aug 17 07:10:52.102: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:10:52.102: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:10:52.102: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container acc-tomcat ready: true, restart count 1
    Aug 17 07:10:52.102: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.102: INFO: 	Container exporter ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: 	Container server ready: true, restart count 2
    Aug 17 07:10:52.102: INFO: 
    Logging pods the apiserver thinks is on node yst-node1 before test
    Aug 17 07:10:52.113: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container webserver ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container gateway ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container helm-server ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container app ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container server ready: true, restart count 5
    Aug 17 07:10:52.113: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container alertmanager ready: true, restart count 3
    Aug 17 07:10:52.113: INFO: 	Container config-reloader ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container auth-server ready: true, restart count 5
    Aug 17 07:10:52.113: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container blackbox-exporter ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: 	Container module-configmap-reloader ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container cronhpa-controller ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container default-http-backend ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container chartmuseum ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container notary-server ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container portal ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container redis ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container registry ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: 	Container registryctl ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container trivy ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container server ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container logstash ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container agent ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container prometheus-adapter ready: true, restart count 3
    Aug 17 07:10:52.113: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: 	Container prometheus-operator ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container thanos-store ready: true, restart count 16
    Aug 17 07:10:52.113: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container timescaledb ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container workflow-controller ready: true, restart count 7
    Aug 17 07:10:52.113: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container apiserver ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:10:52.113: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:10:52.113: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.113: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:10:52.113: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:10:52.113: INFO: 
    Logging pods the apiserver thinks is on node yst-node2 before test
    Aug 17 07:10:52.128: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:10:52.128: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:10:52.128: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:10:52.128: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container opensearch ready: true, restart count 1
    Aug 17 07:10:52.128: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container config-reloader ready: true, restart count 1
    Aug 17 07:10:52.128: INFO: 	Container prometheus ready: true, restart count 1
    Aug 17 07:10:52.128: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Aug 17 07:10:52.128: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:10:52.128: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:10:52.128: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container coredns ready: true, restart count 1
    Aug 17 07:10:52.128: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:10:52.128: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 07:10:52.128: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.128: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 07:10:52.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:10:52.129: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:10:52.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:10:52.129: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node yst-master 08/17/23 07:10:52.153
    STEP: verifying the node has the label node yst-node1 08/17/23 07:10:52.162
    STEP: verifying the node has the label node yst-node2 08/17/23 07:10:52.172
    Aug 17 07:10:52.193: INFO: Pod alert-apiserver-67bffd5d79-4jkdw requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod chartmuseum-chartmuseum-fd9b485cc-98wvk requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod cluster-server-9775c76cc-9g6hh requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod console-7fb6dfb4b6-gfzpf requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod gateway-6f8686745f-7sthl requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod gateway-proxy-v8vpk requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod helm-server-5776765f78-ggckc requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod keycloak-64b4b5569c-x5j9n requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod keycloak-db-5cc8f7f7d5-x6xbq requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod keycloak-observer-86b5cc8d77-qs2lj requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod manual-webserver-66754cc96-5rf5n requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod monitoring-apiserver-db555b567-mbxz5 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod thanos-querier-74bcdf4595-4d8sj requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod acc-kube-state-metrics-86c68f6799-bpbfb requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod acc-node-exporter-4nxg2 requesting resource cpu=112m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod acc-node-exporter-6gbgm requesting resource cpu=112m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod acc-node-exporter-n94lf requesting resource cpu=112m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod accordion-data-provisioner-868fbcb9c9-94xx4 requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod accordion-ingress-controller-56bfbf9c76-hz58x requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod alert-server-57f98d8ff9-vkk9v requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod auth-server-f67b484c7-n8c9m requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod blackbox-exporter-f48cd58c9-w6p8v requesting resource cpu=30m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod cicd-apiserver-59d9f5977b-qj6tl requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod cicd-manager-56f888c97f-wpbd7 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod cicd-template-sync-manager-957d5cff4-6mqz6 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod cicd-trigger-manager-7f78f48b64-9t4dx requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod cronhpa-controller-66bcb7c54c-z5mwq requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod default-http-backend-586b89f667-v7mjh requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod filebeat-filebeat-g4crt requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod filebeat-filebeat-ttr4s requesting resource cpu=0m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod filebeat-filebeat-wt7kd requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod harbor-chartmuseum-777994dc68-nk8fn requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod harbor-core-c69dc8568-dxzjg requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod harbor-jobservice-66b66b4b9-f22t7 requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod harbor-nginx-7bb6fcf984-xgzmf requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod harbor-notary-server-799b956658-k4cjz requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod harbor-notary-signer-cbbf7c95d-nlmpk requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod harbor-portal-77c89f9f8d-jvtfl requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod harbor-registry-64856bdc84-pq8r2 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod harbor-trivy-0 requesting resource cpu=200m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod kiali-6d6fd7b96c-4qtgj requesting resource cpu=10m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod kube-event-exporter-57bb5b5d55-gnbnd requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod log-server-78fd7c75c8-76s7s requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod logstash-logstash-0 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod member-agent-64c8644464-8mqkv requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod minio-0 requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod opensearch-cluster-master-0 requesting resource cpu=1000m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod prometheus-adapter-dcc8655bc-697t4 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod prometheus-operator-779799477d-hg559 requesting resource cpu=100m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod prometheus-prometheus-operator-prometheus-0 requesting resource cpu=100m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod registry-manager-75ff67db55-mzct2 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod scouter-manager-6bd5945b66-zmwv2 requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod thanos-compactor-0 requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod thanos-store-0 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod tsdb-timescaledb-0 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod user-ingress-controller-knj75 requesting resource cpu=0m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod user-ingress-controller-ltkfd requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod user-ingress-controller-r7wrt requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod workflow-controller-746b98c5c-5t6dl requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod xlog-apiserver-b78f4959d-wcfnn requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod calico-kube-controllers-59f6c5b776-ld46r requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod calico-node-hnssl requesting resource cpu=250m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod calico-node-kh6f4 requesting resource cpu=250m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod calico-node-rvq9v requesting resource cpu=250m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod coredns-69754fcccf-bx54l requesting resource cpu=100m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod coredns-69754fcccf-r7xb2 requesting resource cpu=100m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod etcd-yst-master requesting resource cpu=100m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod kube-apiserver-yst-master requesting resource cpu=250m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod kube-controller-manager-yst-master requesting resource cpu=200m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod kube-proxy-mtcmj requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod kube-proxy-vl6rl requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod kube-proxy-znvf7 requesting resource cpu=0m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod kube-scheduler-yst-master requesting resource cpu=100m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod metrics-server-d88c6d6d7-6n64h requesting resource cpu=100m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod sonobuoy requesting resource cpu=0m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod sonobuoy-e2e-job-ab8d33ed8e20458b requesting resource cpu=0m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn requesting resource cpu=0m on Node yst-node2
    Aug 17 07:10:52.193: INFO: Pod sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 requesting resource cpu=0m on Node yst-node1
    Aug 17 07:10:52.193: INFO: Pod acc-tomcat-ddb885997-2w79z requesting resource cpu=0m on Node yst-master
    Aug 17 07:10:52.193: INFO: Pod scouter-server-64f47bc954-r47gx requesting resource cpu=0m on Node yst-master
    STEP: Starting Pods to consume most of the cluster CPU. 08/17/23 07:10:52.193
    Aug 17 07:10:52.193: INFO: Creating a pod which consumes cpu=4744m on Node yst-master
    Aug 17 07:10:52.198: INFO: Creating a pod which consumes cpu=5045m on Node yst-node1
    Aug 17 07:10:52.202: INFO: Creating a pod which consumes cpu=4506m on Node yst-node2
    Aug 17 07:10:52.206: INFO: Waiting up to 5m0s for pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef" in namespace "sched-pred-1757" to be "running"
    Aug 17 07:10:52.209: INFO: Pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.696921ms
    Aug 17 07:10:54.212: INFO: Pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.00593618s
    Aug 17 07:10:54.212: INFO: Pod "filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef" satisfied condition "running"
    Aug 17 07:10:54.212: INFO: Waiting up to 5m0s for pod "filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e" in namespace "sched-pred-1757" to be "running"
    Aug 17 07:10:54.213: INFO: Pod "filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e": Phase="Running", Reason="", readiness=true. Elapsed: 1.592764ms
    Aug 17 07:10:54.213: INFO: Pod "filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e" satisfied condition "running"
    Aug 17 07:10:54.213: INFO: Waiting up to 5m0s for pod "filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160" in namespace "sched-pred-1757" to be "running"
    Aug 17 07:10:54.215: INFO: Pod "filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160": Phase="Running", Reason="", readiness=true. Elapsed: 1.395799ms
    Aug 17 07:10:54.215: INFO: Pod "filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/17/23 07:10:54.215
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a186e7389a7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1757/filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160 to yst-node2] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a189223d5ef], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a18935b1452], Reason = [Created], Message = [Created container filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160.177c1a189ab31101], Reason = [Started], Message = [Started container filler-pod-15b1175f-38c6-45f0-83c4-57209a9f5160] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a186db87356], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1757/filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e to yst-node1] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a189308525a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a1894698bf1], Reason = [Created], Message = [Created container filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e.177c1a189b7d5e85], Reason = [Started], Message = [Started container filler-pod-16cb2be8-747c-422f-8616-cdd5fc06af0e] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a186d6a934c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1757/filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef to yst-master] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a1893d746f4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a1895137c9e], Reason = [Created], Message = [Created container filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef.177c1a189d8aff9a], Reason = [Started], Message = [Started container filler-pod-425b7dbd-d57e-435b-96c8-f1b39497b4ef] 08/17/23 07:10:54.217
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177c1a18e5c9688e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 08/17/23 07:10:54.224
    STEP: removing the label node off the node yst-master 08/17/23 07:10:55.223
    STEP: verifying the node doesn't have the label node 08/17/23 07:10:55.234
    STEP: removing the label node off the node yst-node1 08/17/23 07:10:55.238
    STEP: verifying the node doesn't have the label node 08/17/23 07:10:55.245
    STEP: removing the label node off the node yst-node2 08/17/23 07:10:55.247
    STEP: verifying the node doesn't have the label node 08/17/23 07:10:55.255
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:10:55.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1757" for this suite. 08/17/23 07:10:55.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:10:55.264
Aug 17 07:10:55.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 07:10:55.265
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:55.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:55.275
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 in namespace container-probe-6656 08/17/23 07:10:55.278
Aug 17 07:10:55.301: INFO: Waiting up to 5m0s for pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0" in namespace "container-probe-6656" to be "not pending"
Aug 17 07:10:55.314: INFO: Pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.472336ms
Aug 17 07:10:57.316: INFO: Pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.014395493s
Aug 17 07:10:57.316: INFO: Pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0" satisfied condition "not pending"
Aug 17 07:10:57.316: INFO: Started pod liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 in namespace container-probe-6656
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:10:57.316
Aug 17 07:10:57.317: INFO: Initial restart count of pod liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is 0
Aug 17 07:11:17.354: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 1 (20.036878538s elapsed)
Aug 17 07:11:37.387: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 2 (40.069971945s elapsed)
Aug 17 07:11:57.428: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 3 (1m0.11035288s elapsed)
Aug 17 07:12:17.454: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 4 (1m20.136880404s elapsed)
Aug 17 07:13:19.565: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 5 (2m22.24819176s elapsed)
STEP: deleting the pod 08/17/23 07:13:19.565
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 07:13:19.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6656" for this suite. 08/17/23 07:13:19.575
------------------------------
â€¢ [SLOW TEST] [144.315 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:10:55.264
    Aug 17 07:10:55.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 07:10:55.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:10:55.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:10:55.275
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 in namespace container-probe-6656 08/17/23 07:10:55.278
    Aug 17 07:10:55.301: INFO: Waiting up to 5m0s for pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0" in namespace "container-probe-6656" to be "not pending"
    Aug 17 07:10:55.314: INFO: Pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.472336ms
    Aug 17 07:10:57.316: INFO: Pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.014395493s
    Aug 17 07:10:57.316: INFO: Pod "liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0" satisfied condition "not pending"
    Aug 17 07:10:57.316: INFO: Started pod liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 in namespace container-probe-6656
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:10:57.316
    Aug 17 07:10:57.317: INFO: Initial restart count of pod liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is 0
    Aug 17 07:11:17.354: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 1 (20.036878538s elapsed)
    Aug 17 07:11:37.387: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 2 (40.069971945s elapsed)
    Aug 17 07:11:57.428: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 3 (1m0.11035288s elapsed)
    Aug 17 07:12:17.454: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 4 (1m20.136880404s elapsed)
    Aug 17 07:13:19.565: INFO: Restart count of pod container-probe-6656/liveness-2e6cd8a8-d1dd-470f-8f8a-909e083fa8b0 is now 5 (2m22.24819176s elapsed)
    STEP: deleting the pod 08/17/23 07:13:19.565
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:13:19.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6656" for this suite. 08/17/23 07:13:19.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:13:19.58
Aug 17 07:13:19.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 07:13:19.581
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:19.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:19.598
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-79 08/17/23 07:13:19.603
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/17/23 07:13:19.616
STEP: Creating pod with conflicting port in namespace statefulset-79 08/17/23 07:13:19.629
STEP: Waiting until pod test-pod will start running in namespace statefulset-79 08/17/23 07:13:19.651
Aug 17 07:13:19.651: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-79" to be "running"
Aug 17 07:13:19.655: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70354ms
Aug 17 07:13:21.658: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007080876s
Aug 17 07:13:21.658: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-79 08/17/23 07:13:21.658
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-79 08/17/23 07:13:21.662
Aug 17 07:13:21.669: INFO: Observed stateful pod in namespace: statefulset-79, name: ss-0, uid: 76c087c8-3b22-47fe-8518-2a549363189c, status phase: Pending. Waiting for statefulset controller to delete.
Aug 17 07:13:21.677: INFO: Observed stateful pod in namespace: statefulset-79, name: ss-0, uid: 76c087c8-3b22-47fe-8518-2a549363189c, status phase: Failed. Waiting for statefulset controller to delete.
Aug 17 07:13:21.694: INFO: Observed stateful pod in namespace: statefulset-79, name: ss-0, uid: 76c087c8-3b22-47fe-8518-2a549363189c, status phase: Failed. Waiting for statefulset controller to delete.
Aug 17 07:13:21.696: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-79
STEP: Removing pod with conflicting port in namespace statefulset-79 08/17/23 07:13:21.696
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-79 and will be in running state 08/17/23 07:13:21.704
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 07:13:23.710: INFO: Deleting all statefulset in ns statefulset-79
Aug 17 07:13:23.711: INFO: Scaling statefulset ss to 0
Aug 17 07:13:33.726: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 07:13:33.727: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:13:33.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-79" for this suite. 08/17/23 07:13:33.74
------------------------------
â€¢ [SLOW TEST] [14.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:13:19.58
    Aug 17 07:13:19.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 07:13:19.581
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:19.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:19.598
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-79 08/17/23 07:13:19.603
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/17/23 07:13:19.616
    STEP: Creating pod with conflicting port in namespace statefulset-79 08/17/23 07:13:19.629
    STEP: Waiting until pod test-pod will start running in namespace statefulset-79 08/17/23 07:13:19.651
    Aug 17 07:13:19.651: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-79" to be "running"
    Aug 17 07:13:19.655: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70354ms
    Aug 17 07:13:21.658: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007080876s
    Aug 17 07:13:21.658: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-79 08/17/23 07:13:21.658
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-79 08/17/23 07:13:21.662
    Aug 17 07:13:21.669: INFO: Observed stateful pod in namespace: statefulset-79, name: ss-0, uid: 76c087c8-3b22-47fe-8518-2a549363189c, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 17 07:13:21.677: INFO: Observed stateful pod in namespace: statefulset-79, name: ss-0, uid: 76c087c8-3b22-47fe-8518-2a549363189c, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 17 07:13:21.694: INFO: Observed stateful pod in namespace: statefulset-79, name: ss-0, uid: 76c087c8-3b22-47fe-8518-2a549363189c, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 17 07:13:21.696: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-79
    STEP: Removing pod with conflicting port in namespace statefulset-79 08/17/23 07:13:21.696
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-79 and will be in running state 08/17/23 07:13:21.704
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 07:13:23.710: INFO: Deleting all statefulset in ns statefulset-79
    Aug 17 07:13:23.711: INFO: Scaling statefulset ss to 0
    Aug 17 07:13:33.726: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 07:13:33.727: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:13:33.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-79" for this suite. 08/17/23 07:13:33.74
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:13:33.743
Aug 17 07:13:33.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:13:33.744
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:33.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:33.759
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9800 08/17/23 07:13:33.761
STEP: creating replication controller nodeport-test in namespace services-9800 08/17/23 07:13:33.777
I0817 07:13:33.781413      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9800, replica count: 2
I0817 07:13:36.832501      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 07:13:36.832: INFO: Creating new exec pod
Aug 17 07:13:36.836: INFO: Waiting up to 5m0s for pod "execpodj77lt" in namespace "services-9800" to be "running"
Aug 17 07:13:36.838: INFO: Pod "execpodj77lt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044321ms
Aug 17 07:13:38.842: INFO: Pod "execpodj77lt": Phase="Running", Reason="", readiness=true. Elapsed: 2.006722576s
Aug 17 07:13:38.842: INFO: Pod "execpodj77lt" satisfied condition "running"
Aug 17 07:13:39.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 17 07:13:39.972: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 17 07:13:39.972: INFO: stdout: ""
Aug 17 07:13:39.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 10.96.11.205 80'
Aug 17 07:13:40.092: INFO: stderr: "+ nc -v -z -w 2 10.96.11.205 80\nConnection to 10.96.11.205 80 port [tcp/http] succeeded!\n"
Aug 17 07:13:40.092: INFO: stdout: ""
Aug 17 07:13:40.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.177 30960'
Aug 17 07:13:40.223: INFO: stderr: "+ nc -v -z -w 2 10.60.200.177 30960\nConnection to 10.60.200.177 30960 port [tcp/*] succeeded!\n"
Aug 17 07:13:40.223: INFO: stdout: ""
Aug 17 07:13:40.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.175 30960'
Aug 17 07:13:40.352: INFO: stderr: "+ nc -v -z -w 2 10.60.200.175 30960\nConnection to 10.60.200.175 30960 port [tcp/*] succeeded!\n"
Aug 17 07:13:40.352: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:13:40.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9800" for this suite. 08/17/23 07:13:40.355
------------------------------
â€¢ [SLOW TEST] [6.616 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:13:33.743
    Aug 17 07:13:33.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:13:33.744
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:33.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:33.759
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9800 08/17/23 07:13:33.761
    STEP: creating replication controller nodeport-test in namespace services-9800 08/17/23 07:13:33.777
    I0817 07:13:33.781413      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9800, replica count: 2
    I0817 07:13:36.832501      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 07:13:36.832: INFO: Creating new exec pod
    Aug 17 07:13:36.836: INFO: Waiting up to 5m0s for pod "execpodj77lt" in namespace "services-9800" to be "running"
    Aug 17 07:13:36.838: INFO: Pod "execpodj77lt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044321ms
    Aug 17 07:13:38.842: INFO: Pod "execpodj77lt": Phase="Running", Reason="", readiness=true. Elapsed: 2.006722576s
    Aug 17 07:13:38.842: INFO: Pod "execpodj77lt" satisfied condition "running"
    Aug 17 07:13:39.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 17 07:13:39.972: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 17 07:13:39.972: INFO: stdout: ""
    Aug 17 07:13:39.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 10.96.11.205 80'
    Aug 17 07:13:40.092: INFO: stderr: "+ nc -v -z -w 2 10.96.11.205 80\nConnection to 10.96.11.205 80 port [tcp/http] succeeded!\n"
    Aug 17 07:13:40.092: INFO: stdout: ""
    Aug 17 07:13:40.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.177 30960'
    Aug 17 07:13:40.223: INFO: stderr: "+ nc -v -z -w 2 10.60.200.177 30960\nConnection to 10.60.200.177 30960 port [tcp/*] succeeded!\n"
    Aug 17 07:13:40.223: INFO: stdout: ""
    Aug 17 07:13:40.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9800 exec execpodj77lt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.175 30960'
    Aug 17 07:13:40.352: INFO: stderr: "+ nc -v -z -w 2 10.60.200.175 30960\nConnection to 10.60.200.175 30960 port [tcp/*] succeeded!\n"
    Aug 17 07:13:40.352: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:13:40.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9800" for this suite. 08/17/23 07:13:40.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:13:40.36
Aug 17 07:13:40.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:13:40.361
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:40.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:40.373
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4716 08/17/23 07:13:40.385
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 07:13:40.424
STEP: creating service externalsvc in namespace services-4716 08/17/23 07:13:40.424
STEP: creating replication controller externalsvc in namespace services-4716 08/17/23 07:13:40.477
I0817 07:13:40.483037      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4716, replica count: 2
I0817 07:13:43.533475      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/17/23 07:13:43.535
Aug 17 07:13:43.545: INFO: Creating new exec pod
Aug 17 07:13:43.549: INFO: Waiting up to 5m0s for pod "execpods5c9p" in namespace "services-4716" to be "running"
Aug 17 07:13:43.551: INFO: Pod "execpods5c9p": Phase="Pending", Reason="", readiness=false. Elapsed: 1.830267ms
Aug 17 07:13:45.553: INFO: Pod "execpods5c9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.004377436s
Aug 17 07:13:45.553: INFO: Pod "execpods5c9p" satisfied condition "running"
Aug 17 07:13:45.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4716 exec execpods5c9p -- /bin/sh -x -c nslookup clusterip-service.services-4716.svc.cluster.local'
Aug 17 07:13:45.703: INFO: stderr: "+ nslookup clusterip-service.services-4716.svc.cluster.local\n"
Aug 17 07:13:45.703: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4716.svc.cluster.local\tcanonical name = externalsvc.services-4716.svc.cluster.local.\nName:\texternalsvc.services-4716.svc.cluster.local\nAddress: 10.106.116.66\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4716, will wait for the garbage collector to delete the pods 08/17/23 07:13:45.703
Aug 17 07:13:45.757: INFO: Deleting ReplicationController externalsvc took: 2.489094ms
Aug 17 07:13:45.858: INFO: Terminating ReplicationController externalsvc pods took: 100.447841ms
Aug 17 07:13:47.602: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:13:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4716" for this suite. 08/17/23 07:13:47.61
------------------------------
â€¢ [SLOW TEST] [7.254 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:13:40.36
    Aug 17 07:13:40.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:13:40.361
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:40.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:40.373
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4716 08/17/23 07:13:40.385
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 07:13:40.424
    STEP: creating service externalsvc in namespace services-4716 08/17/23 07:13:40.424
    STEP: creating replication controller externalsvc in namespace services-4716 08/17/23 07:13:40.477
    I0817 07:13:40.483037      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4716, replica count: 2
    I0817 07:13:43.533475      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/17/23 07:13:43.535
    Aug 17 07:13:43.545: INFO: Creating new exec pod
    Aug 17 07:13:43.549: INFO: Waiting up to 5m0s for pod "execpods5c9p" in namespace "services-4716" to be "running"
    Aug 17 07:13:43.551: INFO: Pod "execpods5c9p": Phase="Pending", Reason="", readiness=false. Elapsed: 1.830267ms
    Aug 17 07:13:45.553: INFO: Pod "execpods5c9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.004377436s
    Aug 17 07:13:45.553: INFO: Pod "execpods5c9p" satisfied condition "running"
    Aug 17 07:13:45.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4716 exec execpods5c9p -- /bin/sh -x -c nslookup clusterip-service.services-4716.svc.cluster.local'
    Aug 17 07:13:45.703: INFO: stderr: "+ nslookup clusterip-service.services-4716.svc.cluster.local\n"
    Aug 17 07:13:45.703: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4716.svc.cluster.local\tcanonical name = externalsvc.services-4716.svc.cluster.local.\nName:\texternalsvc.services-4716.svc.cluster.local\nAddress: 10.106.116.66\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4716, will wait for the garbage collector to delete the pods 08/17/23 07:13:45.703
    Aug 17 07:13:45.757: INFO: Deleting ReplicationController externalsvc took: 2.489094ms
    Aug 17 07:13:45.858: INFO: Terminating ReplicationController externalsvc pods took: 100.447841ms
    Aug 17 07:13:47.602: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:13:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4716" for this suite. 08/17/23 07:13:47.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:13:47.614
Aug 17 07:13:47.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename events 08/17/23 07:13:47.615
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:47.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:47.647
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/17/23 07:13:47.65
STEP: listing events in all namespaces 08/17/23 07:13:47.657
STEP: listing events in test namespace 08/17/23 07:13:47.672
STEP: listing events with field selection filtering on source 08/17/23 07:13:47.674
STEP: listing events with field selection filtering on reportingController 08/17/23 07:13:47.676
STEP: getting the test event 08/17/23 07:13:47.678
STEP: patching the test event 08/17/23 07:13:47.681
STEP: getting the test event 08/17/23 07:13:47.687
STEP: updating the test event 08/17/23 07:13:47.689
STEP: getting the test event 08/17/23 07:13:47.692
STEP: deleting the test event 08/17/23 07:13:47.693
STEP: listing events in all namespaces 08/17/23 07:13:47.696
STEP: listing events in test namespace 08/17/23 07:13:47.709
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 17 07:13:47.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2540" for this suite. 08/17/23 07:13:47.712
------------------------------
â€¢ [0.100 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:13:47.614
    Aug 17 07:13:47.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename events 08/17/23 07:13:47.615
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:47.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:47.647
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/17/23 07:13:47.65
    STEP: listing events in all namespaces 08/17/23 07:13:47.657
    STEP: listing events in test namespace 08/17/23 07:13:47.672
    STEP: listing events with field selection filtering on source 08/17/23 07:13:47.674
    STEP: listing events with field selection filtering on reportingController 08/17/23 07:13:47.676
    STEP: getting the test event 08/17/23 07:13:47.678
    STEP: patching the test event 08/17/23 07:13:47.681
    STEP: getting the test event 08/17/23 07:13:47.687
    STEP: updating the test event 08/17/23 07:13:47.689
    STEP: getting the test event 08/17/23 07:13:47.692
    STEP: deleting the test event 08/17/23 07:13:47.693
    STEP: listing events in all namespaces 08/17/23 07:13:47.696
    STEP: listing events in test namespace 08/17/23 07:13:47.709
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:13:47.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2540" for this suite. 08/17/23 07:13:47.712
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:13:47.715
Aug 17 07:13:47.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename security-context-test 08/17/23 07:13:47.715
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:47.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:47.723
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 17 07:13:47.739: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19" in namespace "security-context-test-6725" to be "Succeeded or Failed"
Aug 17 07:13:47.743: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097426ms
Aug 17 07:13:49.746: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007037398s
Aug 17 07:13:51.746: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006812134s
Aug 17 07:13:51.746: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19" satisfied condition "Succeeded or Failed"
Aug 17 07:13:51.758: INFO: Got logs for pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 07:13:51.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6725" for this suite. 08/17/23 07:13:51.761
------------------------------
â€¢ [4.049 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:13:47.715
    Aug 17 07:13:47.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename security-context-test 08/17/23 07:13:47.715
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:47.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:47.723
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 17 07:13:47.739: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19" in namespace "security-context-test-6725" to be "Succeeded or Failed"
    Aug 17 07:13:47.743: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097426ms
    Aug 17 07:13:49.746: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007037398s
    Aug 17 07:13:51.746: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006812134s
    Aug 17 07:13:51.746: INFO: Pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19" satisfied condition "Succeeded or Failed"
    Aug 17 07:13:51.758: INFO: Got logs for pod "busybox-privileged-false-b1eff001-6bbc-4acd-bfa9-38450f991e19": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:13:51.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6725" for this suite. 08/17/23 07:13:51.761
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:13:51.764
Aug 17 07:13:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename taint-single-pod 08/17/23 07:13:51.765
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:51.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:51.779
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 17 07:13:51.781: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 07:14:51.841: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 17 07:14:51.842: INFO: Starting informer...
STEP: Starting pod... 08/17/23 07:14:51.842
Aug 17 07:14:52.051: INFO: Pod is running on yst-node2. Tainting Node
STEP: Trying to apply a taint on the Node 08/17/23 07:14:52.051
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:14:52.062
STEP: Waiting short time to make sure Pod is queued for deletion 08/17/23 07:14:52.065
Aug 17 07:14:52.065: INFO: Pod wasn't evicted. Proceeding
Aug 17 07:14:52.065: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:14:52.101
STEP: Waiting some time to make sure that toleration time passed. 08/17/23 07:14:52.105
Aug 17 07:16:07.105: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:07.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-1295" for this suite. 08/17/23 07:16:07.109
------------------------------
â€¢ [SLOW TEST] [135.348 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:13:51.764
    Aug 17 07:13:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename taint-single-pod 08/17/23 07:13:51.765
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:13:51.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:13:51.779
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 17 07:13:51.781: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 07:14:51.841: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 17 07:14:51.842: INFO: Starting informer...
    STEP: Starting pod... 08/17/23 07:14:51.842
    Aug 17 07:14:52.051: INFO: Pod is running on yst-node2. Tainting Node
    STEP: Trying to apply a taint on the Node 08/17/23 07:14:52.051
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:14:52.062
    STEP: Waiting short time to make sure Pod is queued for deletion 08/17/23 07:14:52.065
    Aug 17 07:14:52.065: INFO: Pod wasn't evicted. Proceeding
    Aug 17 07:14:52.065: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:14:52.101
    STEP: Waiting some time to make sure that toleration time passed. 08/17/23 07:14:52.105
    Aug 17 07:16:07.105: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:07.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-1295" for this suite. 08/17/23 07:16:07.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:07.112
Aug 17 07:16:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:16:07.113
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:07.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:07.123
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:16:07.157
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:16:07.474
STEP: Deploying the webhook pod 08/17/23 07:16:07.478
STEP: Wait for the deployment to be ready 08/17/23 07:16:07.483
Aug 17 07:16:07.486: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:16:09.493
STEP: Verifying the service has paired with the endpoint 08/17/23 07:16:09.5
Aug 17 07:16:10.501: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 07:16:10.503
STEP: create a pod that should be denied by the webhook 08/17/23 07:16:10.513
STEP: create a pod that causes the webhook to hang 08/17/23 07:16:10.522
STEP: create a configmap that should be denied by the webhook 08/17/23 07:16:20.527
STEP: create a configmap that should be admitted by the webhook 08/17/23 07:16:20.547
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 07:16:20.552
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 07:16:20.557
STEP: create a namespace that bypass the webhook 08/17/23 07:16:20.56
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/17/23 07:16:20.563
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:20.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4743" for this suite. 08/17/23 07:16:20.624
STEP: Destroying namespace "webhook-4743-markers" for this suite. 08/17/23 07:16:20.628
------------------------------
â€¢ [SLOW TEST] [13.520 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:07.112
    Aug 17 07:16:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:16:07.113
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:07.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:07.123
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:16:07.157
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:16:07.474
    STEP: Deploying the webhook pod 08/17/23 07:16:07.478
    STEP: Wait for the deployment to be ready 08/17/23 07:16:07.483
    Aug 17 07:16:07.486: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:16:09.493
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:16:09.5
    Aug 17 07:16:10.501: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 07:16:10.503
    STEP: create a pod that should be denied by the webhook 08/17/23 07:16:10.513
    STEP: create a pod that causes the webhook to hang 08/17/23 07:16:10.522
    STEP: create a configmap that should be denied by the webhook 08/17/23 07:16:20.527
    STEP: create a configmap that should be admitted by the webhook 08/17/23 07:16:20.547
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 07:16:20.552
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 07:16:20.557
    STEP: create a namespace that bypass the webhook 08/17/23 07:16:20.56
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/17/23 07:16:20.563
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:20.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4743" for this suite. 08/17/23 07:16:20.624
    STEP: Destroying namespace "webhook-4743-markers" for this suite. 08/17/23 07:16:20.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:20.633
Aug 17 07:16:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename gc 08/17/23 07:16:20.634
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:20.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:20.649
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/17/23 07:16:20.652
STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 07:16:20.659
STEP: delete the deployment 08/17/23 07:16:20.667
STEP: wait for all rs to be garbage collected 08/17/23 07:16:20.679
STEP: expected 0 pods, got 2 pods 08/17/23 07:16:20.723
STEP: Gathering metrics 08/17/23 07:16:21.228
Aug 17 07:16:21.243: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
Aug 17 07:16:21.245: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.87677ms
Aug 17 07:16:21.245: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
Aug 17 07:16:21.245: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
Aug 17 07:16:21.307: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:21.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7506" for this suite. 08/17/23 07:16:21.311
------------------------------
â€¢ [0.681 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:20.633
    Aug 17 07:16:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename gc 08/17/23 07:16:20.634
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:20.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:20.649
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/17/23 07:16:20.652
    STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 07:16:20.659
    STEP: delete the deployment 08/17/23 07:16:20.667
    STEP: wait for all rs to be garbage collected 08/17/23 07:16:20.679
    STEP: expected 0 pods, got 2 pods 08/17/23 07:16:20.723
    STEP: Gathering metrics 08/17/23 07:16:21.228
    Aug 17 07:16:21.243: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
    Aug 17 07:16:21.245: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.87677ms
    Aug 17 07:16:21.245: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
    Aug 17 07:16:21.245: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
    Aug 17 07:16:21.307: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:21.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7506" for this suite. 08/17/23 07:16:21.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:21.315
Aug 17 07:16:21.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:16:21.316
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:21.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:21.326
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:16:21.329
Aug 17 07:16:21.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1" in namespace "downward-api-142" to be "Succeeded or Failed"
Aug 17 07:16:21.344: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973804ms
Aug 17 07:16:23.348: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006632625s
Aug 17 07:16:25.349: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007177313s
STEP: Saw pod success 08/17/23 07:16:25.349
Aug 17 07:16:25.349: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1" satisfied condition "Succeeded or Failed"
Aug 17 07:16:25.351: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1 container client-container: <nil>
STEP: delete the pod 08/17/23 07:16:25.361
Aug 17 07:16:25.367: INFO: Waiting for pod downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1 to disappear
Aug 17 07:16:25.369: INFO: Pod downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:25.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-142" for this suite. 08/17/23 07:16:25.371
------------------------------
â€¢ [4.059 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:21.315
    Aug 17 07:16:21.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:16:21.316
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:21.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:21.326
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:16:21.329
    Aug 17 07:16:21.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1" in namespace "downward-api-142" to be "Succeeded or Failed"
    Aug 17 07:16:21.344: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973804ms
    Aug 17 07:16:23.348: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006632625s
    Aug 17 07:16:25.349: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007177313s
    STEP: Saw pod success 08/17/23 07:16:25.349
    Aug 17 07:16:25.349: INFO: Pod "downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1" satisfied condition "Succeeded or Failed"
    Aug 17 07:16:25.351: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:16:25.361
    Aug 17 07:16:25.367: INFO: Waiting for pod downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1 to disappear
    Aug 17 07:16:25.369: INFO: Pod downwardapi-volume-db275c62-8f93-41b2-ae56-8cbeea2112c1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:25.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-142" for this suite. 08/17/23 07:16:25.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:25.375
Aug 17 07:16:25.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:16:25.376
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:25.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:25.386
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-153eae27-11d6-43a5-8e47-e22aa7621a2f 08/17/23 07:16:25.389
STEP: Creating a pod to test consume configMaps 08/17/23 07:16:25.407
Aug 17 07:16:25.420: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7" in namespace "configmap-2400" to be "Succeeded or Failed"
Aug 17 07:16:25.422: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.984398ms
Aug 17 07:16:27.425: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004807408s
Aug 17 07:16:29.424: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004241067s
STEP: Saw pod success 08/17/23 07:16:29.424
Aug 17 07:16:29.424: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7" satisfied condition "Succeeded or Failed"
Aug 17 07:16:29.426: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:16:29.429
Aug 17 07:16:29.435: INFO: Waiting for pod pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7 to disappear
Aug 17 07:16:29.437: INFO: Pod pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:29.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2400" for this suite. 08/17/23 07:16:29.439
------------------------------
â€¢ [4.067 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:25.375
    Aug 17 07:16:25.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:16:25.376
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:25.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:25.386
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-153eae27-11d6-43a5-8e47-e22aa7621a2f 08/17/23 07:16:25.389
    STEP: Creating a pod to test consume configMaps 08/17/23 07:16:25.407
    Aug 17 07:16:25.420: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7" in namespace "configmap-2400" to be "Succeeded or Failed"
    Aug 17 07:16:25.422: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.984398ms
    Aug 17 07:16:27.425: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004807408s
    Aug 17 07:16:29.424: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004241067s
    STEP: Saw pod success 08/17/23 07:16:29.424
    Aug 17 07:16:29.424: INFO: Pod "pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7" satisfied condition "Succeeded or Failed"
    Aug 17 07:16:29.426: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:16:29.429
    Aug 17 07:16:29.435: INFO: Waiting for pod pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7 to disappear
    Aug 17 07:16:29.437: INFO: Pod pod-configmaps-0d74dc99-f907-488c-9620-1f39133632d7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:29.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2400" for this suite. 08/17/23 07:16:29.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:29.443
Aug 17 07:16:29.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:16:29.443
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:29.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:29.51
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-f00cb84a-3a3b-4923-b86b-22cf41315994 08/17/23 07:16:29.512
STEP: Creating a pod to test consume configMaps 08/17/23 07:16:29.519
Aug 17 07:16:29.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524" in namespace "configmap-8736" to be "Succeeded or Failed"
Aug 17 07:16:29.531: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524": Phase="Pending", Reason="", readiness=false. Elapsed: 1.623956ms
Aug 17 07:16:31.534: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004744052s
Aug 17 07:16:33.534: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004669958s
STEP: Saw pod success 08/17/23 07:16:33.534
Aug 17 07:16:33.534: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524" satisfied condition "Succeeded or Failed"
Aug 17 07:16:33.536: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:16:33.539
Aug 17 07:16:33.545: INFO: Waiting for pod pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524 to disappear
Aug 17 07:16:33.546: INFO: Pod pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8736" for this suite. 08/17/23 07:16:33.548
------------------------------
â€¢ [4.108 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:29.443
    Aug 17 07:16:29.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:16:29.443
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:29.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:29.51
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-f00cb84a-3a3b-4923-b86b-22cf41315994 08/17/23 07:16:29.512
    STEP: Creating a pod to test consume configMaps 08/17/23 07:16:29.519
    Aug 17 07:16:29.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524" in namespace "configmap-8736" to be "Succeeded or Failed"
    Aug 17 07:16:29.531: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524": Phase="Pending", Reason="", readiness=false. Elapsed: 1.623956ms
    Aug 17 07:16:31.534: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004744052s
    Aug 17 07:16:33.534: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004669958s
    STEP: Saw pod success 08/17/23 07:16:33.534
    Aug 17 07:16:33.534: INFO: Pod "pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524" satisfied condition "Succeeded or Failed"
    Aug 17 07:16:33.536: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:16:33.539
    Aug 17 07:16:33.545: INFO: Waiting for pod pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524 to disappear
    Aug 17 07:16:33.546: INFO: Pod pod-configmaps-cae9459f-d494-4b21-a2f7-4a4da317d524 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8736" for this suite. 08/17/23 07:16:33.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:33.552
Aug 17 07:16:33.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:16:33.553
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:33.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:33.564
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:16:33.566
Aug 17 07:16:33.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399" in namespace "projected-2789" to be "Succeeded or Failed"
Aug 17 07:16:33.590: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.571829ms
Aug 17 07:16:35.592: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005274367s
Aug 17 07:16:37.593: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005780529s
STEP: Saw pod success 08/17/23 07:16:37.593
Aug 17 07:16:37.593: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399" satisfied condition "Succeeded or Failed"
Aug 17 07:16:37.595: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399 container client-container: <nil>
STEP: delete the pod 08/17/23 07:16:37.598
Aug 17 07:16:37.603: INFO: Waiting for pod downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399 to disappear
Aug 17 07:16:37.604: INFO: Pod downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:37.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2789" for this suite. 08/17/23 07:16:37.607
------------------------------
â€¢ [4.057 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:33.552
    Aug 17 07:16:33.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:16:33.553
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:33.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:33.564
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:16:33.566
    Aug 17 07:16:33.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399" in namespace "projected-2789" to be "Succeeded or Failed"
    Aug 17 07:16:33.590: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.571829ms
    Aug 17 07:16:35.592: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005274367s
    Aug 17 07:16:37.593: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005780529s
    STEP: Saw pod success 08/17/23 07:16:37.593
    Aug 17 07:16:37.593: INFO: Pod "downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399" satisfied condition "Succeeded or Failed"
    Aug 17 07:16:37.595: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:16:37.598
    Aug 17 07:16:37.603: INFO: Waiting for pod downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399 to disappear
    Aug 17 07:16:37.604: INFO: Pod downwardapi-volume-d606d98b-3cb0-417a-bb8e-6038fd276399 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:37.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2789" for this suite. 08/17/23 07:16:37.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:37.609
Aug 17 07:16:37.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:16:37.61
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:37.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:37.627
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/17/23 07:16:37.654
STEP: watching for the Service to be added 08/17/23 07:16:37.674
Aug 17 07:16:37.675: INFO: Found Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 17 07:16:37.675: INFO: Service test-service-fmjpb created
STEP: Getting /status 08/17/23 07:16:37.675
Aug 17 07:16:37.677: INFO: Service test-service-fmjpb has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/17/23 07:16:37.677
STEP: watching for the Service to be patched 08/17/23 07:16:37.68
Aug 17 07:16:37.681: INFO: observed Service test-service-fmjpb in namespace services-9147 with annotations: map[] & LoadBalancer: {[]}
Aug 17 07:16:37.681: INFO: Found Service test-service-fmjpb in namespace services-9147 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 17 07:16:37.681: INFO: Service test-service-fmjpb has service status patched
STEP: updating the ServiceStatus 08/17/23 07:16:37.681
Aug 17 07:16:37.687: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/17/23 07:16:37.687
Aug 17 07:16:37.688: INFO: Observed Service test-service-fmjpb in namespace services-9147 with annotations: map[] & Conditions: {[]}
Aug 17 07:16:37.688: INFO: Observed event: &Service{ObjectMeta:{test-service-fmjpb  services-9147  b9f3dd70-71c8-4548-aed1-0f52c9ff61be 27021690 0 2023-08-17 07:16:37 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-17 07:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-17 07:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.105.216.216,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.105.216.216],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 17 07:16:37.688: INFO: Found Service test-service-fmjpb in namespace services-9147 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 07:16:37.688: INFO: Service test-service-fmjpb has service status updated
STEP: patching the service 08/17/23 07:16:37.688
STEP: watching for the Service to be patched 08/17/23 07:16:37.698
Aug 17 07:16:37.700: INFO: observed Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true]
Aug 17 07:16:37.700: INFO: observed Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true]
Aug 17 07:16:37.701: INFO: observed Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true]
Aug 17 07:16:37.701: INFO: Found Service test-service-fmjpb in namespace services-9147 with labels: map[test-service:patched test-service-static:true]
Aug 17 07:16:37.701: INFO: Service test-service-fmjpb patched
STEP: deleting the service 08/17/23 07:16:37.701
STEP: watching for the Service to be deleted 08/17/23 07:16:37.711
Aug 17 07:16:37.712: INFO: Observed event: ADDED
Aug 17 07:16:37.712: INFO: Observed event: MODIFIED
Aug 17 07:16:37.712: INFO: Observed event: MODIFIED
Aug 17 07:16:37.712: INFO: Observed event: MODIFIED
Aug 17 07:16:37.712: INFO: Found Service test-service-fmjpb in namespace services-9147 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 17 07:16:37.712: INFO: Service test-service-fmjpb deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:37.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9147" for this suite. 08/17/23 07:16:37.715
------------------------------
â€¢ [0.108 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:37.609
    Aug 17 07:16:37.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:16:37.61
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:37.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:37.627
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/17/23 07:16:37.654
    STEP: watching for the Service to be added 08/17/23 07:16:37.674
    Aug 17 07:16:37.675: INFO: Found Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 17 07:16:37.675: INFO: Service test-service-fmjpb created
    STEP: Getting /status 08/17/23 07:16:37.675
    Aug 17 07:16:37.677: INFO: Service test-service-fmjpb has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/17/23 07:16:37.677
    STEP: watching for the Service to be patched 08/17/23 07:16:37.68
    Aug 17 07:16:37.681: INFO: observed Service test-service-fmjpb in namespace services-9147 with annotations: map[] & LoadBalancer: {[]}
    Aug 17 07:16:37.681: INFO: Found Service test-service-fmjpb in namespace services-9147 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 17 07:16:37.681: INFO: Service test-service-fmjpb has service status patched
    STEP: updating the ServiceStatus 08/17/23 07:16:37.681
    Aug 17 07:16:37.687: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/17/23 07:16:37.687
    Aug 17 07:16:37.688: INFO: Observed Service test-service-fmjpb in namespace services-9147 with annotations: map[] & Conditions: {[]}
    Aug 17 07:16:37.688: INFO: Observed event: &Service{ObjectMeta:{test-service-fmjpb  services-9147  b9f3dd70-71c8-4548-aed1-0f52c9ff61be 27021690 0 2023-08-17 07:16:37 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-17 07:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-17 07:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.105.216.216,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.105.216.216],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 17 07:16:37.688: INFO: Found Service test-service-fmjpb in namespace services-9147 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 07:16:37.688: INFO: Service test-service-fmjpb has service status updated
    STEP: patching the service 08/17/23 07:16:37.688
    STEP: watching for the Service to be patched 08/17/23 07:16:37.698
    Aug 17 07:16:37.700: INFO: observed Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true]
    Aug 17 07:16:37.700: INFO: observed Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true]
    Aug 17 07:16:37.701: INFO: observed Service test-service-fmjpb in namespace services-9147 with labels: map[test-service-static:true]
    Aug 17 07:16:37.701: INFO: Found Service test-service-fmjpb in namespace services-9147 with labels: map[test-service:patched test-service-static:true]
    Aug 17 07:16:37.701: INFO: Service test-service-fmjpb patched
    STEP: deleting the service 08/17/23 07:16:37.701
    STEP: watching for the Service to be deleted 08/17/23 07:16:37.711
    Aug 17 07:16:37.712: INFO: Observed event: ADDED
    Aug 17 07:16:37.712: INFO: Observed event: MODIFIED
    Aug 17 07:16:37.712: INFO: Observed event: MODIFIED
    Aug 17 07:16:37.712: INFO: Observed event: MODIFIED
    Aug 17 07:16:37.712: INFO: Found Service test-service-fmjpb in namespace services-9147 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 17 07:16:37.712: INFO: Service test-service-fmjpb deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:37.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9147" for this suite. 08/17/23 07:16:37.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:37.718
Aug 17 07:16:37.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-pred 08/17/23 07:16:37.719
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:37.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:37.731
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 07:16:37.735: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 07:16:37.745: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 07:16:37.747: INFO: 
Logging pods the apiserver thinks is on node yst-master before test
Aug 17 07:16:37.762: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container app ready: true, restart count 1
Aug 17 07:16:37.762: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container chartmuseum ready: true, restart count 2
Aug 17 07:16:37.762: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container cluster-server ready: true, restart count 7
Aug 17 07:16:37.762: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container proxy ready: true, restart count 8
Aug 17 07:16:37.762: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container keycloak ready: true, restart count 2
Aug 17 07:16:37.762: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container postgres ready: true, restart count 2
Aug 17 07:16:37.762: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container keycloak-observer ready: true, restart count 6
Aug 17 07:16:37.762: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container manual-webserver ready: true, restart count 1
Aug 17 07:16:37.762: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Aug 17 07:16:37.762: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Aug 17 07:16:37.762: INFO: 	Container kube-state-metrics ready: true, restart count 1
Aug 17 07:16:37.762: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:16:37.762: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:16:37.762: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
Aug 17 07:16:37.762: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container apiserver ready: true, restart count 2
Aug 17 07:16:37.762: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container manager ready: true, restart count 4
Aug 17 07:16:37.762: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:16:37.762: INFO: 	Container etcd-date ready: false, restart count 0
Aug 17 07:16:37.762: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:16:37.762: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container core ready: true, restart count 4
Aug 17 07:16:37.762: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container database ready: true, restart count 2
Aug 17 07:16:37.762: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container jobservice ready: true, restart count 11
Aug 17 07:16:37.762: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container nginx ready: true, restart count 9
Aug 17 07:16:37.762: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container notary-signer ready: true, restart count 6
Aug 17 07:16:37.762: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kiali ready: true, restart count 1
Aug 17 07:16:37.762: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kube-event-exporter ready: true, restart count 2
Aug 17 07:16:37.762: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container minio ready: true, restart count 2
Aug 17 07:16:37.762: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container index-clear ready: false, restart count 0
Aug 17 07:16:37.762: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:16:37.762: INFO: 	Container pgdata-backup-node ready: false, restart count 0
Aug 17 07:16:37.762: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container manager ready: true, restart count 5
Aug 17 07:16:37.762: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:16:37.762: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:16:37.762: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container backup-check ready: false, restart count 1
Aug 17 07:16:37.762: INFO: 	Container backup-date ready: false, restart count 0
Aug 17 07:16:37.762: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Aug 17 07:16:37.762: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:16:37.762: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container coredns ready: true, restart count 1
Aug 17 07:16:37.762: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container etcd ready: true, restart count 2
Aug 17 07:16:37.762: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kube-apiserver ready: true, restart count 2
Aug 17 07:16:37.762: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kube-controller-manager ready: true, restart count 1
Aug 17 07:16:37.762: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:16:37.762: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container kube-scheduler ready: true, restart count 1
Aug 17 07:16:37.762: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container metrics-server ready: true, restart count 3
Aug 17 07:16:37.762: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:16:37.762: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:16:37.762: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container acc-tomcat ready: true, restart count 1
Aug 17 07:16:37.762: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.762: INFO: 	Container exporter ready: true, restart count 2
Aug 17 07:16:37.762: INFO: 	Container server ready: true, restart count 2
Aug 17 07:16:37.762: INFO: 
Logging pods the apiserver thinks is on node yst-node1 before test
Aug 17 07:16:37.777: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container webserver ready: true, restart count 1
Aug 17 07:16:37.777: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container gateway ready: true, restart count 7
Aug 17 07:16:37.777: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container helm-server ready: true, restart count 7
Aug 17 07:16:37.777: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container app ready: true, restart count 2
Aug 17 07:16:37.777: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:16:37.777: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:16:37.777: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:16:37.777: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:16:37.777: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container server ready: true, restart count 5
Aug 17 07:16:37.777: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container alertmanager ready: true, restart count 3
Aug 17 07:16:37.777: INFO: 	Container config-reloader ready: true, restart count 2
Aug 17 07:16:37.777: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container auth-server ready: true, restart count 5
Aug 17 07:16:37.777: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container blackbox-exporter ready: true, restart count 1
Aug 17 07:16:37.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:16:37.777: INFO: 	Container module-configmap-reloader ready: true, restart count 1
Aug 17 07:16:37.777: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:16:37.777: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:16:37.777: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container cronhpa-controller ready: true, restart count 1
Aug 17 07:16:37.777: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container default-http-backend ready: true, restart count 1
Aug 17 07:16:37.777: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:16:37.777: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container chartmuseum ready: true, restart count 1
Aug 17 07:16:37.777: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container notary-server ready: true, restart count 7
Aug 17 07:16:37.777: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container portal ready: true, restart count 2
Aug 17 07:16:37.777: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container redis ready: true, restart count 2
Aug 17 07:16:37.777: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container registry ready: true, restart count 2
Aug 17 07:16:37.777: INFO: 	Container registryctl ready: true, restart count 2
Aug 17 07:16:37.777: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container trivy ready: true, restart count 2
Aug 17 07:16:37.777: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container server ready: true, restart count 7
Aug 17 07:16:37.777: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container logstash ready: true, restart count 2
Aug 17 07:16:37.777: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container agent ready: true, restart count 2
Aug 17 07:16:37.777: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container prometheus-adapter ready: true, restart count 3
Aug 17 07:16:37.777: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:16:37.777: INFO: 	Container prometheus-operator ready: true, restart count 1
Aug 17 07:16:37.777: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.777: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:16:37.778: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container thanos-store ready: true, restart count 16
Aug 17 07:16:37.778: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container timescaledb ready: true, restart count 2
Aug 17 07:16:37.778: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:16:37.778: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container workflow-controller ready: true, restart count 7
Aug 17 07:16:37.778: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container apiserver ready: true, restart count 1
Aug 17 07:16:37.778: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:16:37.778: INFO: coredns-69754fcccf-f55mq from kube-system started at 2023-08-17 07:14:52 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container coredns ready: true, restart count 0
Aug 17 07:16:37.778: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:16:37.778: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.778: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:16:37.778: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:16:37.778: INFO: 
Logging pods the apiserver thinks is on node yst-node2 before test
Aug 17 07:16:37.786: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:16:37.786: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:16:37.786: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:16:37.786: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container opensearch ready: true, restart count 1
Aug 17 07:16:37.786: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container config-reloader ready: true, restart count 1
Aug 17 07:16:37.786: INFO: 	Container prometheus ready: false, restart count 1
Aug 17 07:16:37.786: INFO: 	Container thanos-sidecar ready: true, restart count 1
Aug 17 07:16:37.786: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container nginx-ingress-controller ready: false, restart count 2
Aug 17 07:16:37.786: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:16:37.786: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container coredns ready: false, restart count 1
Aug 17 07:16:37.786: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:16:37.786: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 07:16:37.786: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container e2e ready: true, restart count 0
Aug 17 07:16:37.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:16:37.786: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:16:37.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:16:37.786: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/17/23 07:16:37.786
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177c1a68e5f5d949], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 08/17/23 07:16:37.828
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:38.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8585" for this suite. 08/17/23 07:16:38.826
------------------------------
â€¢ [1.113 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:37.718
    Aug 17 07:16:37.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-pred 08/17/23 07:16:37.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:37.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:37.731
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 07:16:37.735: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 07:16:37.745: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 07:16:37.747: INFO: 
    Logging pods the apiserver thinks is on node yst-master before test
    Aug 17 07:16:37.762: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container app ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container chartmuseum ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container cluster-server ready: true, restart count 7
    Aug 17 07:16:37.762: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container proxy ready: true, restart count 8
    Aug 17 07:16:37.762: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container keycloak ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container postgres ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container keycloak-observer ready: true, restart count 6
    Aug 17 07:16:37.762: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container manual-webserver ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: 	Container kube-state-metrics ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
    Aug 17 07:16:37.762: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container apiserver ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container manager ready: true, restart count 4
    Aug 17 07:16:37.762: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:16:37.762: INFO: 	Container etcd-date ready: false, restart count 0
    Aug 17 07:16:37.762: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container core ready: true, restart count 4
    Aug 17 07:16:37.762: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container database ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container jobservice ready: true, restart count 11
    Aug 17 07:16:37.762: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container nginx ready: true, restart count 9
    Aug 17 07:16:37.762: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container notary-signer ready: true, restart count 6
    Aug 17 07:16:37.762: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kiali ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kube-event-exporter ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container minio ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container index-clear ready: false, restart count 0
    Aug 17 07:16:37.762: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:16:37.762: INFO: 	Container pgdata-backup-node ready: false, restart count 0
    Aug 17 07:16:37.762: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container manager ready: true, restart count 5
    Aug 17 07:16:37.762: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container backup-check ready: false, restart count 1
    Aug 17 07:16:37.762: INFO: 	Container backup-date ready: false, restart count 0
    Aug 17 07:16:37.762: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container coredns ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container etcd ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kube-apiserver ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container kube-scheduler ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container metrics-server ready: true, restart count 3
    Aug 17 07:16:37.762: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:16:37.762: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:16:37.762: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container acc-tomcat ready: true, restart count 1
    Aug 17 07:16:37.762: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.762: INFO: 	Container exporter ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: 	Container server ready: true, restart count 2
    Aug 17 07:16:37.762: INFO: 
    Logging pods the apiserver thinks is on node yst-node1 before test
    Aug 17 07:16:37.777: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container webserver ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container gateway ready: true, restart count 7
    Aug 17 07:16:37.777: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container helm-server ready: true, restart count 7
    Aug 17 07:16:37.777: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container app ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container server ready: true, restart count 5
    Aug 17 07:16:37.777: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container alertmanager ready: true, restart count 3
    Aug 17 07:16:37.777: INFO: 	Container config-reloader ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container auth-server ready: true, restart count 5
    Aug 17 07:16:37.777: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container blackbox-exporter ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: 	Container module-configmap-reloader ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:16:37.777: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:16:37.777: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container cronhpa-controller ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container default-http-backend ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container chartmuseum ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container notary-server ready: true, restart count 7
    Aug 17 07:16:37.777: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container portal ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container redis ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container registry ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: 	Container registryctl ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container trivy ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container server ready: true, restart count 7
    Aug 17 07:16:37.777: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container logstash ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container agent ready: true, restart count 2
    Aug 17 07:16:37.777: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container prometheus-adapter ready: true, restart count 3
    Aug 17 07:16:37.777: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: 	Container prometheus-operator ready: true, restart count 1
    Aug 17 07:16:37.777: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.777: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:16:37.778: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container thanos-store ready: true, restart count 16
    Aug 17 07:16:37.778: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container timescaledb ready: true, restart count 2
    Aug 17 07:16:37.778: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:16:37.778: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container workflow-controller ready: true, restart count 7
    Aug 17 07:16:37.778: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container apiserver ready: true, restart count 1
    Aug 17 07:16:37.778: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:16:37.778: INFO: coredns-69754fcccf-f55mq from kube-system started at 2023-08-17 07:14:52 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 07:16:37.778: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:16:37.778: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.778: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:16:37.778: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:16:37.778: INFO: 
    Logging pods the apiserver thinks is on node yst-node2 before test
    Aug 17 07:16:37.786: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:16:37.786: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:16:37.786: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:16:37.786: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container opensearch ready: true, restart count 1
    Aug 17 07:16:37.786: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container config-reloader ready: true, restart count 1
    Aug 17 07:16:37.786: INFO: 	Container prometheus ready: false, restart count 1
    Aug 17 07:16:37.786: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Aug 17 07:16:37.786: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container nginx-ingress-controller ready: false, restart count 2
    Aug 17 07:16:37.786: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:16:37.786: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container coredns ready: false, restart count 1
    Aug 17 07:16:37.786: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:16:37.786: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 07:16:37.786: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 07:16:37.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:16:37.786: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:16:37.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:16:37.786: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/17/23 07:16:37.786
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177c1a68e5f5d949], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 08/17/23 07:16:37.828
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:38.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8585" for this suite. 08/17/23 07:16:38.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:38.831
Aug 17 07:16:38.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:16:38.832
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:38.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:38.845
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:16:38.848
Aug 17 07:16:38.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041" in namespace "downward-api-7715" to be "Succeeded or Failed"
Aug 17 07:16:38.859: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410185ms
Aug 17 07:16:40.861: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005846956s
Aug 17 07:16:42.862: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006680207s
STEP: Saw pod success 08/17/23 07:16:42.862
Aug 17 07:16:42.862: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041" satisfied condition "Succeeded or Failed"
Aug 17 07:16:42.864: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041 container client-container: <nil>
STEP: delete the pod 08/17/23 07:16:42.867
Aug 17 07:16:42.871: INFO: Waiting for pod downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041 to disappear
Aug 17 07:16:42.873: INFO: Pod downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:42.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7715" for this suite. 08/17/23 07:16:42.875
------------------------------
â€¢ [4.046 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:38.831
    Aug 17 07:16:38.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:16:38.832
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:38.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:38.845
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:16:38.848
    Aug 17 07:16:38.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041" in namespace "downward-api-7715" to be "Succeeded or Failed"
    Aug 17 07:16:38.859: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410185ms
    Aug 17 07:16:40.861: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005846956s
    Aug 17 07:16:42.862: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006680207s
    STEP: Saw pod success 08/17/23 07:16:42.862
    Aug 17 07:16:42.862: INFO: Pod "downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041" satisfied condition "Succeeded or Failed"
    Aug 17 07:16:42.864: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:16:42.867
    Aug 17 07:16:42.871: INFO: Waiting for pod downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041 to disappear
    Aug 17 07:16:42.873: INFO: Pod downwardapi-volume-751e4c3b-a353-41dc-a7ee-4da038296041 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:42.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7715" for this suite. 08/17/23 07:16:42.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:42.878
Aug 17 07:16:42.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename daemonsets 08/17/23 07:16:42.879
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:42.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:42.896
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Aug 17 07:16:42.924: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:16:42.941
Aug 17 07:16:42.962: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:16:42.962: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:16:43.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:16:43.967: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:16:44.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:16:44.967: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 08/17/23 07:16:44.973
STEP: Check that daemon pods images are updated. 08/17/23 07:16:44.979
Aug 17 07:16:44.981: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:44.981: INFO: Wrong image for pod: daemon-set-fvfqw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:44.981: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:45.986: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:45.986: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:46.985: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:46.985: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:48.018: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:48.019: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:48.019: INFO: Pod daemon-set-ss92v is not available
Aug 17 07:16:48.986: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:49.986: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 07:16:49.986: INFO: Pod daemon-set-l25sl is not available
Aug 17 07:16:51.986: INFO: Pod daemon-set-nstdn is not available
STEP: Check that daemon pods are still running on every node of the cluster. 08/17/23 07:16:51.989
Aug 17 07:16:51.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 07:16:51.992: INFO: Node yst-node1 is running 0 daemon pod, expected 1
Aug 17 07:16:53.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:16:53.006: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:16:53.013
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-131, will wait for the garbage collector to delete the pods 08/17/23 07:16:53.013
Aug 17 07:16:53.069: INFO: Deleting DaemonSet.extensions daemon-set took: 3.314526ms
Aug 17 07:16:53.169: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.892125ms
Aug 17 07:16:55.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:16:55.172: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 07:16:55.173: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27022350"},"items":null}

Aug 17 07:16:55.175: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27022350"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:55.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-131" for this suite. 08/17/23 07:16:55.184
------------------------------
â€¢ [SLOW TEST] [12.309 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:42.878
    Aug 17 07:16:42.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename daemonsets 08/17/23 07:16:42.879
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:42.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:42.896
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Aug 17 07:16:42.924: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:16:42.941
    Aug 17 07:16:42.962: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:16:42.962: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:16:43.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:16:43.967: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:16:44.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:16:44.967: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 08/17/23 07:16:44.973
    STEP: Check that daemon pods images are updated. 08/17/23 07:16:44.979
    Aug 17 07:16:44.981: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:44.981: INFO: Wrong image for pod: daemon-set-fvfqw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:44.981: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:45.986: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:45.986: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:46.985: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:46.985: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:48.018: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:48.019: INFO: Wrong image for pod: daemon-set-qbfff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:48.019: INFO: Pod daemon-set-ss92v is not available
    Aug 17 07:16:48.986: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:49.986: INFO: Wrong image for pod: daemon-set-9zbj7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 07:16:49.986: INFO: Pod daemon-set-l25sl is not available
    Aug 17 07:16:51.986: INFO: Pod daemon-set-nstdn is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 08/17/23 07:16:51.989
    Aug 17 07:16:51.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 07:16:51.992: INFO: Node yst-node1 is running 0 daemon pod, expected 1
    Aug 17 07:16:53.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:16:53.006: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:16:53.013
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-131, will wait for the garbage collector to delete the pods 08/17/23 07:16:53.013
    Aug 17 07:16:53.069: INFO: Deleting DaemonSet.extensions daemon-set took: 3.314526ms
    Aug 17 07:16:53.169: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.892125ms
    Aug 17 07:16:55.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:16:55.172: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 07:16:55.173: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27022350"},"items":null}

    Aug 17 07:16:55.175: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27022350"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:55.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-131" for this suite. 08/17/23 07:16:55.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:55.187
Aug 17 07:16:55.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:16:55.188
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:55.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:55.198
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/17/23 07:16:55.201
Aug 17 07:16:55.201: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8120 proxy --unix-socket=/tmp/kubectl-proxy-unix2944253026/test'
STEP: retrieving proxy /api/ output 08/17/23 07:16:55.257
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:55.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8120" for this suite. 08/17/23 07:16:55.261
------------------------------
â€¢ [0.076 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:55.187
    Aug 17 07:16:55.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:16:55.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:55.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:55.198
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/17/23 07:16:55.201
    Aug 17 07:16:55.201: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8120 proxy --unix-socket=/tmp/kubectl-proxy-unix2944253026/test'
    STEP: retrieving proxy /api/ output 08/17/23 07:16:55.257
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:55.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8120" for this suite. 08/17/23 07:16:55.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:55.264
Aug 17 07:16:55.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:16:55.265
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:55.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:55.276
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-7b109e7b-d9c1-4de2-8376-c7f5cefccf08 08/17/23 07:16:55.279
STEP: Creating a pod to test consume configMaps 08/17/23 07:16:55.289
Aug 17 07:16:55.299: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad" in namespace "projected-4600" to be "Succeeded or Failed"
Aug 17 07:16:55.305: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.418274ms
Aug 17 07:16:57.307: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007974375s
Aug 17 07:16:59.309: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009984662s
STEP: Saw pod success 08/17/23 07:16:59.309
Aug 17 07:16:59.310: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad" satisfied condition "Succeeded or Failed"
Aug 17 07:16:59.311: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:16:59.315
Aug 17 07:16:59.381: INFO: Waiting for pod pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad to disappear
Aug 17 07:16:59.383: INFO: Pod pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:16:59.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4600" for this suite. 08/17/23 07:16:59.385
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:55.264
    Aug 17 07:16:55.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:16:55.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:55.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:55.276
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-7b109e7b-d9c1-4de2-8376-c7f5cefccf08 08/17/23 07:16:55.279
    STEP: Creating a pod to test consume configMaps 08/17/23 07:16:55.289
    Aug 17 07:16:55.299: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad" in namespace "projected-4600" to be "Succeeded or Failed"
    Aug 17 07:16:55.305: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.418274ms
    Aug 17 07:16:57.307: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007974375s
    Aug 17 07:16:59.309: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009984662s
    STEP: Saw pod success 08/17/23 07:16:59.309
    Aug 17 07:16:59.310: INFO: Pod "pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad" satisfied condition "Succeeded or Failed"
    Aug 17 07:16:59.311: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:16:59.315
    Aug 17 07:16:59.381: INFO: Waiting for pod pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad to disappear
    Aug 17 07:16:59.383: INFO: Pod pod-projected-configmaps-1fa22d58-c418-4981-a937-38f8969a23ad no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:16:59.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4600" for this suite. 08/17/23 07:16:59.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:16:59.388
Aug 17 07:16:59.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-preemption 08/17/23 07:16:59.389
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:59.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:59.409
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 07:16:59.444: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 07:17:59.516: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/17/23 07:17:59.518
Aug 17 07:17:59.535: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 17 07:17:59.538: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 17 07:17:59.550: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 17 07:17:59.554: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 17 07:17:59.566: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 17 07:17:59.572: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/17/23 07:17:59.572
Aug 17 07:17:59.572: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-774" to be "running"
Aug 17 07:17:59.575: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876638ms
Aug 17 07:18:01.578: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006143712s
Aug 17 07:18:01.578: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 17 07:18:01.578: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
Aug 17 07:18:01.580: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.030491ms
Aug 17 07:18:01.580: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:18:01.580: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
Aug 17 07:18:01.582: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.137116ms
Aug 17 07:18:01.583: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:18:01.583: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
Aug 17 07:18:01.584: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.514959ms
Aug 17 07:18:01.584: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:18:01.584: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
Aug 17 07:18:01.586: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.040326ms
Aug 17 07:18:01.586: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:18:01.586: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
Aug 17 07:18:01.588: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.127012ms
Aug 17 07:18:01.588: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/17/23 07:18:01.588
Aug 17 07:18:01.594: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 17 07:18:01.596: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.798413ms
Aug 17 07:18:03.600: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005322589s
Aug 17 07:18:05.607: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012822848s
Aug 17 07:18:07.610: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.015801043s
Aug 17 07:18:07.610: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-774" for this suite. 08/17/23 07:18:07.712
------------------------------
â€¢ [SLOW TEST] [68.328 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:16:59.388
    Aug 17 07:16:59.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 07:16:59.389
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:16:59.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:16:59.409
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 07:16:59.444: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 07:17:59.516: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/17/23 07:17:59.518
    Aug 17 07:17:59.535: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 17 07:17:59.538: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 17 07:17:59.550: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 17 07:17:59.554: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 17 07:17:59.566: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 17 07:17:59.572: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/17/23 07:17:59.572
    Aug 17 07:17:59.572: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-774" to be "running"
    Aug 17 07:17:59.575: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876638ms
    Aug 17 07:18:01.578: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006143712s
    Aug 17 07:18:01.578: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 17 07:18:01.578: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
    Aug 17 07:18:01.580: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.030491ms
    Aug 17 07:18:01.580: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:18:01.580: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
    Aug 17 07:18:01.582: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.137116ms
    Aug 17 07:18:01.583: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:18:01.583: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
    Aug 17 07:18:01.584: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.514959ms
    Aug 17 07:18:01.584: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:18:01.584: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
    Aug 17 07:18:01.586: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.040326ms
    Aug 17 07:18:01.586: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:18:01.586: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-774" to be "running"
    Aug 17 07:18:01.588: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.127012ms
    Aug 17 07:18:01.588: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/17/23 07:18:01.588
    Aug 17 07:18:01.594: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 17 07:18:01.596: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.798413ms
    Aug 17 07:18:03.600: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005322589s
    Aug 17 07:18:05.607: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012822848s
    Aug 17 07:18:07.610: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.015801043s
    Aug 17 07:18:07.610: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-774" for this suite. 08/17/23 07:18:07.712
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:07.716
Aug 17 07:18:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename podtemplate 08/17/23 07:18:07.717
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:07.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:07.734
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:07.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8721" for this suite. 08/17/23 07:18:07.81
------------------------------
â€¢ [0.101 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:07.716
    Aug 17 07:18:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename podtemplate 08/17/23 07:18:07.717
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:07.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:07.734
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:07.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8721" for this suite. 08/17/23 07:18:07.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:07.817
Aug 17 07:18:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replicaset 08/17/23 07:18:07.818
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:07.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:07.835
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 17 07:18:07.839: INFO: Creating ReplicaSet my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4
Aug 17 07:18:07.853: INFO: Pod name my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4: Found 0 pods out of 1
Aug 17 07:18:12.858: INFO: Pod name my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4: Found 1 pods out of 1
Aug 17 07:18:12.858: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4" is running
Aug 17 07:18:12.858: INFO: Waiting up to 5m0s for pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q" in namespace "replicaset-8183" to be "running"
Aug 17 07:18:12.860: INFO: Pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q": Phase="Running", Reason="", readiness=true. Elapsed: 1.843695ms
Aug 17 07:18:12.860: INFO: Pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q" satisfied condition "running"
Aug 17 07:18:12.860: INFO: Pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:07 +0000 UTC Reason: Message:}])
Aug 17 07:18:12.860: INFO: Trying to dial the pod
Aug 17 07:18:17.868: INFO: Controller my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4: Got expected result from replica 1 [my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q]: "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:17.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8183" for this suite. 08/17/23 07:18:17.87
------------------------------
â€¢ [SLOW TEST] [10.056 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:07.817
    Aug 17 07:18:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replicaset 08/17/23 07:18:07.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:07.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:07.835
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 17 07:18:07.839: INFO: Creating ReplicaSet my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4
    Aug 17 07:18:07.853: INFO: Pod name my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4: Found 0 pods out of 1
    Aug 17 07:18:12.858: INFO: Pod name my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4: Found 1 pods out of 1
    Aug 17 07:18:12.858: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4" is running
    Aug 17 07:18:12.858: INFO: Waiting up to 5m0s for pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q" in namespace "replicaset-8183" to be "running"
    Aug 17 07:18:12.860: INFO: Pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q": Phase="Running", Reason="", readiness=true. Elapsed: 1.843695ms
    Aug 17 07:18:12.860: INFO: Pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q" satisfied condition "running"
    Aug 17 07:18:12.860: INFO: Pod "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:18:07 +0000 UTC Reason: Message:}])
    Aug 17 07:18:12.860: INFO: Trying to dial the pod
    Aug 17 07:18:17.868: INFO: Controller my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4: Got expected result from replica 1 [my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q]: "my-hostname-basic-36e5d021-e3b6-4d3a-8999-43e9561214d4-lbc9q", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:17.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8183" for this suite. 08/17/23 07:18:17.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:17.874
Aug 17 07:18:17.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:18:17.875
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:17.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:17.891
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 17 07:18:17.914: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3595 to be scheduled
Aug 17 07:18:17.916: INFO: 1 pods are not scheduled: [runtimeclass-3595/test-runtimeclass-runtimeclass-3595-preconfigured-handler-cw52m(677b1f46-9a11-4bfc-8621-71fd4d24587e)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:19.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3595" for this suite. 08/17/23 07:18:19.925
------------------------------
â€¢ [2.054 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:17.874
    Aug 17 07:18:17.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:18:17.875
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:17.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:17.891
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 17 07:18:17.914: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3595 to be scheduled
    Aug 17 07:18:17.916: INFO: 1 pods are not scheduled: [runtimeclass-3595/test-runtimeclass-runtimeclass-3595-preconfigured-handler-cw52m(677b1f46-9a11-4bfc-8621-71fd4d24587e)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:19.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3595" for this suite. 08/17/23 07:18:19.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:19.932
Aug 17 07:18:19.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename security-context-test 08/17/23 07:18:19.932
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:19.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:19.948
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Aug 17 07:18:19.967: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32" in namespace "security-context-test-319" to be "Succeeded or Failed"
Aug 17 07:18:19.970: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140615ms
Aug 17 07:18:21.972: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004735176s
Aug 17 07:18:23.973: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005349602s
Aug 17 07:18:23.973: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:23.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-319" for this suite. 08/17/23 07:18:23.976
------------------------------
â€¢ [4.048 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:19.932
    Aug 17 07:18:19.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename security-context-test 08/17/23 07:18:19.932
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:19.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:19.948
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Aug 17 07:18:19.967: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32" in namespace "security-context-test-319" to be "Succeeded or Failed"
    Aug 17 07:18:19.970: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140615ms
    Aug 17 07:18:21.972: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004735176s
    Aug 17 07:18:23.973: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005349602s
    Aug 17 07:18:23.973: INFO: Pod "busybox-readonly-false-0a143ea8-ffb6-45b4-82f1-dffc65296a32" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:23.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-319" for this suite. 08/17/23 07:18:23.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:23.981
Aug 17 07:18:23.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 07:18:23.981
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:23.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:23.999
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/17/23 07:18:24.004
Aug 17 07:18:24.013: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4672  37bb8186-ed7d-48ac-8457-dd1bb4e3b562 27023894 0 2023-08-17 07:18:24 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-17 07:18:24 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tk2w9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tk2w9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:18:24.014: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4672" to be "running and ready"
Aug 17 07:18:24.016: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362058ms
Aug 17 07:18:24.016: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:18:26.019: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005701895s
Aug 17 07:18:26.019: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 17 07:18:26.019: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/17/23 07:18:26.019
Aug 17 07:18:26.020: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4672 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:18:26.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:18:26.020: INFO: ExecWithOptions: Clientset creation
Aug 17 07:18:26.020: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4672/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/17/23 07:18:26.085
Aug 17 07:18:26.085: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4672 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:18:26.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:18:26.085: INFO: ExecWithOptions: Clientset creation
Aug 17 07:18:26.086: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4672/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 07:18:26.145: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:26.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4672" for this suite. 08/17/23 07:18:26.157
------------------------------
â€¢ [2.181 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:23.981
    Aug 17 07:18:23.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 07:18:23.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:23.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:23.999
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/17/23 07:18:24.004
    Aug 17 07:18:24.013: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4672  37bb8186-ed7d-48ac-8457-dd1bb4e3b562 27023894 0 2023-08-17 07:18:24 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-17 07:18:24 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tk2w9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tk2w9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:18:24.014: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4672" to be "running and ready"
    Aug 17 07:18:24.016: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362058ms
    Aug 17 07:18:24.016: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:18:26.019: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005701895s
    Aug 17 07:18:26.019: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 17 07:18:26.019: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/17/23 07:18:26.019
    Aug 17 07:18:26.020: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4672 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:18:26.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:18:26.020: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:18:26.020: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4672/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/17/23 07:18:26.085
    Aug 17 07:18:26.085: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4672 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:18:26.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:18:26.085: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:18:26.086: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4672/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 07:18:26.145: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:26.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4672" for this suite. 08/17/23 07:18:26.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:26.163
Aug 17 07:18:26.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:18:26.163
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:26.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:26.174
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/17/23 07:18:26.178
STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:18:26.193
STEP: Creating a ResourceQuota with not best effort scope 08/17/23 07:18:28.196
STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:18:28.199
STEP: Creating a best-effort pod 08/17/23 07:18:30.202
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/17/23 07:18:30.21
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/17/23 07:18:32.213
STEP: Deleting the pod 08/17/23 07:18:34.216
STEP: Ensuring resource quota status released the pod usage 08/17/23 07:18:34.222
STEP: Creating a not best-effort pod 08/17/23 07:18:36.225
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/17/23 07:18:36.231
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/17/23 07:18:38.235
STEP: Deleting the pod 08/17/23 07:18:40.238
STEP: Ensuring resource quota status released the pod usage 08/17/23 07:18:40.244
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:42.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9900" for this suite. 08/17/23 07:18:42.25
------------------------------
â€¢ [SLOW TEST] [16.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:26.163
    Aug 17 07:18:26.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:18:26.163
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:26.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:26.174
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/17/23 07:18:26.178
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:18:26.193
    STEP: Creating a ResourceQuota with not best effort scope 08/17/23 07:18:28.196
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:18:28.199
    STEP: Creating a best-effort pod 08/17/23 07:18:30.202
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/17/23 07:18:30.21
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/17/23 07:18:32.213
    STEP: Deleting the pod 08/17/23 07:18:34.216
    STEP: Ensuring resource quota status released the pod usage 08/17/23 07:18:34.222
    STEP: Creating a not best-effort pod 08/17/23 07:18:36.225
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/17/23 07:18:36.231
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/17/23 07:18:38.235
    STEP: Deleting the pod 08/17/23 07:18:40.238
    STEP: Ensuring resource quota status released the pod usage 08/17/23 07:18:40.244
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:42.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9900" for this suite. 08/17/23 07:18:42.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:42.254
Aug 17 07:18:42.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename containers 08/17/23 07:18:42.255
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:42.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:42.264
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/17/23 07:18:42.266
Aug 17 07:18:42.279: INFO: Waiting up to 5m0s for pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce" in namespace "containers-1711" to be "Succeeded or Failed"
Aug 17 07:18:42.282: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949056ms
Aug 17 07:18:44.286: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00656577s
Aug 17 07:18:46.285: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006321798s
STEP: Saw pod success 08/17/23 07:18:46.285
Aug 17 07:18:46.286: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce" satisfied condition "Succeeded or Failed"
Aug 17 07:18:46.287: INFO: Trying to get logs from node yst-node2 pod client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:18:46.296
Aug 17 07:18:46.302: INFO: Waiting for pod client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce to disappear
Aug 17 07:18:46.303: INFO: Pod client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:46.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1711" for this suite. 08/17/23 07:18:46.307
------------------------------
â€¢ [4.056 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:42.254
    Aug 17 07:18:42.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename containers 08/17/23 07:18:42.255
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:42.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:42.264
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/17/23 07:18:42.266
    Aug 17 07:18:42.279: INFO: Waiting up to 5m0s for pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce" in namespace "containers-1711" to be "Succeeded or Failed"
    Aug 17 07:18:42.282: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949056ms
    Aug 17 07:18:44.286: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00656577s
    Aug 17 07:18:46.285: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006321798s
    STEP: Saw pod success 08/17/23 07:18:46.285
    Aug 17 07:18:46.286: INFO: Pod "client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce" satisfied condition "Succeeded or Failed"
    Aug 17 07:18:46.287: INFO: Trying to get logs from node yst-node2 pod client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:18:46.296
    Aug 17 07:18:46.302: INFO: Waiting for pod client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce to disappear
    Aug 17 07:18:46.303: INFO: Pod client-containers-ff7ff06c-47b3-42ac-b219-b4e6ff6da0ce no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:46.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1711" for this suite. 08/17/23 07:18:46.307
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:46.31
Aug 17 07:18:46.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:18:46.311
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:46.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:46.32
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/17/23 07:18:46.323
STEP: watching for the ServiceAccount to be added 08/17/23 07:18:46.341
STEP: patching the ServiceAccount 08/17/23 07:18:46.342
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/17/23 07:18:46.347
STEP: deleting the ServiceAccount 08/17/23 07:18:46.352
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:46.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-766" for this suite. 08/17/23 07:18:46.365
------------------------------
â€¢ [0.058 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:46.31
    Aug 17 07:18:46.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:18:46.311
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:46.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:46.32
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/17/23 07:18:46.323
    STEP: watching for the ServiceAccount to be added 08/17/23 07:18:46.341
    STEP: patching the ServiceAccount 08/17/23 07:18:46.342
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/17/23 07:18:46.347
    STEP: deleting the ServiceAccount 08/17/23 07:18:46.352
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:46.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-766" for this suite. 08/17/23 07:18:46.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:46.369
Aug 17 07:18:46.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:18:46.37
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:46.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:46.384
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:18:46.409
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:18:46.642
STEP: Deploying the webhook pod 08/17/23 07:18:46.647
STEP: Wait for the deployment to be ready 08/17/23 07:18:46.653
Aug 17 07:18:46.657: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:18:48.663
STEP: Verifying the service has paired with the endpoint 08/17/23 07:18:48.669
Aug 17 07:18:49.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 17 07:18:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/17/23 07:18:50.178
STEP: Creating a custom resource that should be denied by the webhook 08/17/23 07:18:50.208
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/17/23 07:18:52.251
STEP: Updating the custom resource with disallowed data should be denied 08/17/23 07:18:52.255
STEP: Deleting the custom resource should be denied 08/17/23 07:18:52.259
STEP: Remove the offending key and value from the custom resource data 08/17/23 07:18:52.263
STEP: Deleting the updated custom resource should be successful 08/17/23 07:18:52.268
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:18:52.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5512" for this suite. 08/17/23 07:18:52.809
STEP: Destroying namespace "webhook-5512-markers" for this suite. 08/17/23 07:18:52.813
------------------------------
â€¢ [SLOW TEST] [6.448 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:46.369
    Aug 17 07:18:46.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:18:46.37
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:46.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:46.384
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:18:46.409
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:18:46.642
    STEP: Deploying the webhook pod 08/17/23 07:18:46.647
    STEP: Wait for the deployment to be ready 08/17/23 07:18:46.653
    Aug 17 07:18:46.657: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:18:48.663
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:18:48.669
    Aug 17 07:18:49.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 17 07:18:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/17/23 07:18:50.178
    STEP: Creating a custom resource that should be denied by the webhook 08/17/23 07:18:50.208
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/17/23 07:18:52.251
    STEP: Updating the custom resource with disallowed data should be denied 08/17/23 07:18:52.255
    STEP: Deleting the custom resource should be denied 08/17/23 07:18:52.259
    STEP: Remove the offending key and value from the custom resource data 08/17/23 07:18:52.263
    STEP: Deleting the updated custom resource should be successful 08/17/23 07:18:52.268
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:18:52.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5512" for this suite. 08/17/23 07:18:52.809
    STEP: Destroying namespace "webhook-5512-markers" for this suite. 08/17/23 07:18:52.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:18:52.818
Aug 17 07:18:52.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename endpointslice 08/17/23 07:18:52.821
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:52.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:52.834
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 08/17/23 07:18:57.897
STEP: referencing matching pods with named port 08/17/23 07:19:02.902
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/17/23 07:19:07.906
STEP: recreating EndpointSlices after they've been deleted 08/17/23 07:19:12.911
Aug 17 07:19:12.922: INFO: EndpointSlice for Service endpointslice-5446/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 07:19:22.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5446" for this suite. 08/17/23 07:19:22.931
------------------------------
â€¢ [SLOW TEST] [30.116 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:18:52.818
    Aug 17 07:18:52.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename endpointslice 08/17/23 07:18:52.821
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:18:52.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:18:52.834
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 08/17/23 07:18:57.897
    STEP: referencing matching pods with named port 08/17/23 07:19:02.902
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/17/23 07:19:07.906
    STEP: recreating EndpointSlices after they've been deleted 08/17/23 07:19:12.911
    Aug 17 07:19:12.922: INFO: EndpointSlice for Service endpointslice-5446/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:19:22.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5446" for this suite. 08/17/23 07:19:22.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:19:22.935
Aug 17 07:19:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename subpath 08/17/23 07:19:22.936
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:22.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:22.947
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 07:19:22.951
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-vfkw 08/17/23 07:19:22.969
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 07:19:22.969
Aug 17 07:19:22.974: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-vfkw" in namespace "subpath-4833" to be "Succeeded or Failed"
Aug 17 07:19:22.976: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548998ms
Aug 17 07:19:24.980: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005982766s
Aug 17 07:19:26.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 4.004839766s
Aug 17 07:19:28.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 6.005425198s
Aug 17 07:19:30.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 8.004764119s
Aug 17 07:19:32.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 10.005002574s
Aug 17 07:19:34.978: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 12.004427267s
Aug 17 07:19:36.978: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 14.004049147s
Aug 17 07:19:38.978: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 16.004077718s
Aug 17 07:19:40.980: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 18.005623826s
Aug 17 07:19:42.980: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 20.005564402s
Aug 17 07:19:44.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=false. Elapsed: 22.005273212s
Aug 17 07:19:46.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00481571s
STEP: Saw pod success 08/17/23 07:19:46.979
Aug 17 07:19:46.979: INFO: Pod "pod-subpath-test-projected-vfkw" satisfied condition "Succeeded or Failed"
Aug 17 07:19:46.981: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-projected-vfkw container test-container-subpath-projected-vfkw: <nil>
STEP: delete the pod 08/17/23 07:19:46.985
Aug 17 07:19:46.991: INFO: Waiting for pod pod-subpath-test-projected-vfkw to disappear
Aug 17 07:19:46.993: INFO: Pod pod-subpath-test-projected-vfkw no longer exists
STEP: Deleting pod pod-subpath-test-projected-vfkw 08/17/23 07:19:46.993
Aug 17 07:19:46.993: INFO: Deleting pod "pod-subpath-test-projected-vfkw" in namespace "subpath-4833"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 07:19:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4833" for this suite. 08/17/23 07:19:46.997
------------------------------
â€¢ [SLOW TEST] [24.065 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:19:22.935
    Aug 17 07:19:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename subpath 08/17/23 07:19:22.936
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:22.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:22.947
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 07:19:22.951
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-vfkw 08/17/23 07:19:22.969
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 07:19:22.969
    Aug 17 07:19:22.974: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-vfkw" in namespace "subpath-4833" to be "Succeeded or Failed"
    Aug 17 07:19:22.976: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548998ms
    Aug 17 07:19:24.980: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005982766s
    Aug 17 07:19:26.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 4.004839766s
    Aug 17 07:19:28.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 6.005425198s
    Aug 17 07:19:30.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 8.004764119s
    Aug 17 07:19:32.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 10.005002574s
    Aug 17 07:19:34.978: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 12.004427267s
    Aug 17 07:19:36.978: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 14.004049147s
    Aug 17 07:19:38.978: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 16.004077718s
    Aug 17 07:19:40.980: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 18.005623826s
    Aug 17 07:19:42.980: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=true. Elapsed: 20.005564402s
    Aug 17 07:19:44.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Running", Reason="", readiness=false. Elapsed: 22.005273212s
    Aug 17 07:19:46.979: INFO: Pod "pod-subpath-test-projected-vfkw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00481571s
    STEP: Saw pod success 08/17/23 07:19:46.979
    Aug 17 07:19:46.979: INFO: Pod "pod-subpath-test-projected-vfkw" satisfied condition "Succeeded or Failed"
    Aug 17 07:19:46.981: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-projected-vfkw container test-container-subpath-projected-vfkw: <nil>
    STEP: delete the pod 08/17/23 07:19:46.985
    Aug 17 07:19:46.991: INFO: Waiting for pod pod-subpath-test-projected-vfkw to disappear
    Aug 17 07:19:46.993: INFO: Pod pod-subpath-test-projected-vfkw no longer exists
    STEP: Deleting pod pod-subpath-test-projected-vfkw 08/17/23 07:19:46.993
    Aug 17 07:19:46.993: INFO: Deleting pod "pod-subpath-test-projected-vfkw" in namespace "subpath-4833"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:19:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4833" for this suite. 08/17/23 07:19:46.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:19:47
Aug 17 07:19:47.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:19:47.001
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:47.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:47.014
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-5206 08/17/23 07:19:47.017
STEP: creating service affinity-nodeport in namespace services-5206 08/17/23 07:19:47.017
STEP: creating replication controller affinity-nodeport in namespace services-5206 08/17/23 07:19:47.044
I0817 07:19:47.048801      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5206, replica count: 3
I0817 07:19:50.099482      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 07:19:50.105: INFO: Creating new exec pod
Aug 17 07:19:50.108: INFO: Waiting up to 5m0s for pod "execpod-affinityrj4px" in namespace "services-5206" to be "running"
Aug 17 07:19:50.109: INFO: Pod "execpod-affinityrj4px": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596193ms
Aug 17 07:19:52.113: INFO: Pod "execpod-affinityrj4px": Phase="Running", Reason="", readiness=true. Elapsed: 2.005217329s
Aug 17 07:19:52.113: INFO: Pod "execpod-affinityrj4px" satisfied condition "running"
Aug 17 07:19:53.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 17 07:19:53.270: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 17 07:19:53.270: INFO: stdout: ""
Aug 17 07:19:53.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 10.107.221.182 80'
Aug 17 07:19:53.423: INFO: stderr: "+ nc -v -z -w 2 10.107.221.182 80\nConnection to 10.107.221.182 80 port [tcp/http] succeeded!\n"
Aug 17 07:19:53.424: INFO: stdout: ""
Aug 17 07:19:53.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 10.60.200.175 31878'
Aug 17 07:19:53.560: INFO: stderr: "+ nc -v -z -w 2 10.60.200.175 31878\nConnection to 10.60.200.175 31878 port [tcp/*] succeeded!\n"
Aug 17 07:19:53.560: INFO: stdout: ""
Aug 17 07:19:53.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 10.60.200.176 31878'
Aug 17 07:19:53.696: INFO: stderr: "+ nc -v -z -w 2 10.60.200.176 31878\nConnection to 10.60.200.176 31878 port [tcp/*] succeeded!\n"
Aug 17 07:19:53.696: INFO: stdout: ""
Aug 17 07:19:53.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.60.200.175:31878/ ; done'
Aug 17 07:19:53.897: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n"
Aug 17 07:19:53.897: INFO: stdout: "\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv"
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
Aug 17 07:19:53.897: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5206, will wait for the garbage collector to delete the pods 08/17/23 07:19:53.902
Aug 17 07:19:53.962: INFO: Deleting ReplicationController affinity-nodeport took: 6.6008ms
Aug 17 07:19:54.062: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.103385ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:19:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5206" for this suite. 08/17/23 07:19:55.988
------------------------------
â€¢ [SLOW TEST] [8.992 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:19:47
    Aug 17 07:19:47.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:19:47.001
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:47.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:47.014
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-5206 08/17/23 07:19:47.017
    STEP: creating service affinity-nodeport in namespace services-5206 08/17/23 07:19:47.017
    STEP: creating replication controller affinity-nodeport in namespace services-5206 08/17/23 07:19:47.044
    I0817 07:19:47.048801      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5206, replica count: 3
    I0817 07:19:50.099482      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 07:19:50.105: INFO: Creating new exec pod
    Aug 17 07:19:50.108: INFO: Waiting up to 5m0s for pod "execpod-affinityrj4px" in namespace "services-5206" to be "running"
    Aug 17 07:19:50.109: INFO: Pod "execpod-affinityrj4px": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596193ms
    Aug 17 07:19:52.113: INFO: Pod "execpod-affinityrj4px": Phase="Running", Reason="", readiness=true. Elapsed: 2.005217329s
    Aug 17 07:19:52.113: INFO: Pod "execpod-affinityrj4px" satisfied condition "running"
    Aug 17 07:19:53.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 17 07:19:53.270: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 17 07:19:53.270: INFO: stdout: ""
    Aug 17 07:19:53.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 10.107.221.182 80'
    Aug 17 07:19:53.423: INFO: stderr: "+ nc -v -z -w 2 10.107.221.182 80\nConnection to 10.107.221.182 80 port [tcp/http] succeeded!\n"
    Aug 17 07:19:53.424: INFO: stdout: ""
    Aug 17 07:19:53.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 10.60.200.175 31878'
    Aug 17 07:19:53.560: INFO: stderr: "+ nc -v -z -w 2 10.60.200.175 31878\nConnection to 10.60.200.175 31878 port [tcp/*] succeeded!\n"
    Aug 17 07:19:53.560: INFO: stdout: ""
    Aug 17 07:19:53.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c nc -v -z -w 2 10.60.200.176 31878'
    Aug 17 07:19:53.696: INFO: stderr: "+ nc -v -z -w 2 10.60.200.176 31878\nConnection to 10.60.200.176 31878 port [tcp/*] succeeded!\n"
    Aug 17 07:19:53.696: INFO: stdout: ""
    Aug 17 07:19:53.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-5206 exec execpod-affinityrj4px -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.60.200.175:31878/ ; done'
    Aug 17 07:19:53.897: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:31878/\n"
    Aug 17 07:19:53.897: INFO: stdout: "\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv\naffinity-nodeport-x8jwv"
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Received response from host: affinity-nodeport-x8jwv
    Aug 17 07:19:53.897: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-5206, will wait for the garbage collector to delete the pods 08/17/23 07:19:53.902
    Aug 17 07:19:53.962: INFO: Deleting ReplicationController affinity-nodeport took: 6.6008ms
    Aug 17 07:19:54.062: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.103385ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:19:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5206" for this suite. 08/17/23 07:19:55.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:19:55.994
Aug 17 07:19:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:19:55.995
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:56.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:56.022
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 17 07:19:56.063: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1775 to be scheduled
Aug 17 07:19:56.066: INFO: 1 pods are not scheduled: [runtimeclass-1775/test-runtimeclass-runtimeclass-1775-preconfigured-handler-52x9q(1a0f8e5d-435e-4ffc-9497-3ca593c9560c)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 07:19:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1775" for this suite. 08/17/23 07:19:58.087
------------------------------
â€¢ [2.123 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:19:55.994
    Aug 17 07:19:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:19:55.995
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:56.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:56.022
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 17 07:19:56.063: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1775 to be scheduled
    Aug 17 07:19:56.066: INFO: 1 pods are not scheduled: [runtimeclass-1775/test-runtimeclass-runtimeclass-1775-preconfigured-handler-52x9q(1a0f8e5d-435e-4ffc-9497-3ca593c9560c)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:19:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1775" for this suite. 08/17/23 07:19:58.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:19:58.117
Aug 17 07:19:58.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename hostport 08/17/23 07:19:58.118
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:58.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:58.239
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/17/23 07:19:58.35
Aug 17 07:19:58.378: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8440" to be "running and ready"
Aug 17 07:19:58.381: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238443ms
Aug 17 07:19:58.381: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:00.384: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005806692s
Aug 17 07:20:00.384: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 17 07:20:00.384: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.60.200.175 on the node which pod1 resides and expect scheduled 08/17/23 07:20:00.384
Aug 17 07:20:00.388: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8440" to be "running and ready"
Aug 17 07:20:00.390: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066642ms
Aug 17 07:20:00.390: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:02.393: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004811923s
Aug 17 07:20:02.393: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 17 07:20:02.393: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.60.200.175 but use UDP protocol on the node which pod2 resides 08/17/23 07:20:02.393
Aug 17 07:20:02.397: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8440" to be "running and ready"
Aug 17 07:20:02.398: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860363ms
Aug 17 07:20:02.398: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:04.401: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00481897s
Aug 17 07:20:04.401: INFO: The phase of Pod pod3 is Running (Ready = false)
Aug 17 07:20:06.402: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.005747155s
Aug 17 07:20:06.402: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 17 07:20:06.402: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 17 07:20:06.405: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8440" to be "running and ready"
Aug 17 07:20:06.410: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.743484ms
Aug 17 07:20:06.410: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:08.414: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008165082s
Aug 17 07:20:08.414: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 17 07:20:08.414: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/17/23 07:20:08.416
Aug 17 07:20:08.416: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.60.200.175 http://127.0.0.1:54323/hostname] Namespace:hostport-8440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:20:08.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:20:08.417: INFO: ExecWithOptions: Clientset creation
Aug 17 07:20:08.417: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.60.200.175+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.60.200.175, port: 54323 08/17/23 07:20:08.486
Aug 17 07:20:08.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.60.200.175:54323/hostname] Namespace:hostport-8440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:20:08.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:20:08.487: INFO: ExecWithOptions: Clientset creation
Aug 17 07:20:08.487: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.60.200.175%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.60.200.175, port: 54323 UDP 08/17/23 07:20:08.546
Aug 17 07:20:08.546: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.60.200.175 54323] Namespace:hostport-8440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:20:08.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:20:08.547: INFO: ExecWithOptions: Clientset creation
Aug 17 07:20:08.547: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.60.200.175+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:13.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8440" for this suite. 08/17/23 07:20:13.603
------------------------------
â€¢ [SLOW TEST] [15.490 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:19:58.117
    Aug 17 07:19:58.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename hostport 08/17/23 07:19:58.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:19:58.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:19:58.239
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/17/23 07:19:58.35
    Aug 17 07:19:58.378: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8440" to be "running and ready"
    Aug 17 07:19:58.381: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238443ms
    Aug 17 07:19:58.381: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:00.384: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005806692s
    Aug 17 07:20:00.384: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 17 07:20:00.384: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.60.200.175 on the node which pod1 resides and expect scheduled 08/17/23 07:20:00.384
    Aug 17 07:20:00.388: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8440" to be "running and ready"
    Aug 17 07:20:00.390: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066642ms
    Aug 17 07:20:00.390: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:02.393: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004811923s
    Aug 17 07:20:02.393: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 17 07:20:02.393: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.60.200.175 but use UDP protocol on the node which pod2 resides 08/17/23 07:20:02.393
    Aug 17 07:20:02.397: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8440" to be "running and ready"
    Aug 17 07:20:02.398: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860363ms
    Aug 17 07:20:02.398: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:04.401: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00481897s
    Aug 17 07:20:04.401: INFO: The phase of Pod pod3 is Running (Ready = false)
    Aug 17 07:20:06.402: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.005747155s
    Aug 17 07:20:06.402: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 17 07:20:06.402: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 17 07:20:06.405: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8440" to be "running and ready"
    Aug 17 07:20:06.410: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.743484ms
    Aug 17 07:20:06.410: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:08.414: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008165082s
    Aug 17 07:20:08.414: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 17 07:20:08.414: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/17/23 07:20:08.416
    Aug 17 07:20:08.416: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.60.200.175 http://127.0.0.1:54323/hostname] Namespace:hostport-8440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:20:08.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:20:08.417: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:20:08.417: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.60.200.175+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.60.200.175, port: 54323 08/17/23 07:20:08.486
    Aug 17 07:20:08.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.60.200.175:54323/hostname] Namespace:hostport-8440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:20:08.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:20:08.487: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:20:08.487: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.60.200.175%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.60.200.175, port: 54323 UDP 08/17/23 07:20:08.546
    Aug 17 07:20:08.546: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.60.200.175 54323] Namespace:hostport-8440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:20:08.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:20:08.547: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:20:08.547: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.60.200.175+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:13.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8440" for this suite. 08/17/23 07:20:13.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:13.608
Aug 17 07:20:13.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:20:13.609
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:13.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:13.621
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/17/23 07:20:13.624
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/17/23 07:20:13.631
STEP: patching the secret 08/17/23 07:20:13.635
STEP: deleting the secret using a LabelSelector 08/17/23 07:20:13.642
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/17/23 07:20:13.648
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:13.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4476" for this suite. 08/17/23 07:20:13.655
------------------------------
â€¢ [0.050 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:13.608
    Aug 17 07:20:13.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:20:13.609
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:13.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:13.621
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/17/23 07:20:13.624
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/17/23 07:20:13.631
    STEP: patching the secret 08/17/23 07:20:13.635
    STEP: deleting the secret using a LabelSelector 08/17/23 07:20:13.642
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/17/23 07:20:13.648
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:13.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4476" for this suite. 08/17/23 07:20:13.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:13.659
Aug 17 07:20:13.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:20:13.659
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:13.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:13.671
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-773a55fe-b60f-426a-828b-745e24f09fa2 08/17/23 07:20:13.675
STEP: Creating a pod to test consume configMaps 08/17/23 07:20:13.679
Aug 17 07:20:13.696: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107" in namespace "projected-9435" to be "Succeeded or Failed"
Aug 17 07:20:13.700: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.187315ms
Aug 17 07:20:15.704: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007824458s
Aug 17 07:20:17.704: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00787015s
STEP: Saw pod success 08/17/23 07:20:17.704
Aug 17 07:20:17.704: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107" satisfied condition "Succeeded or Failed"
Aug 17 07:20:17.706: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:20:17.709
Aug 17 07:20:18.120: INFO: Waiting for pod pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107 to disappear
Aug 17 07:20:18.284: INFO: Pod pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:18.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9435" for this suite. 08/17/23 07:20:18.288
------------------------------
â€¢ [4.650 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:13.659
    Aug 17 07:20:13.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:20:13.659
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:13.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:13.671
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-773a55fe-b60f-426a-828b-745e24f09fa2 08/17/23 07:20:13.675
    STEP: Creating a pod to test consume configMaps 08/17/23 07:20:13.679
    Aug 17 07:20:13.696: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107" in namespace "projected-9435" to be "Succeeded or Failed"
    Aug 17 07:20:13.700: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.187315ms
    Aug 17 07:20:15.704: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007824458s
    Aug 17 07:20:17.704: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00787015s
    STEP: Saw pod success 08/17/23 07:20:17.704
    Aug 17 07:20:17.704: INFO: Pod "pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107" satisfied condition "Succeeded or Failed"
    Aug 17 07:20:17.706: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:20:17.709
    Aug 17 07:20:18.120: INFO: Waiting for pod pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107 to disappear
    Aug 17 07:20:18.284: INFO: Pod pod-projected-configmaps-18094261-48d4-47a3-8013-b0d3576e8107 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:18.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9435" for this suite. 08/17/23 07:20:18.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:18.309
Aug 17 07:20:18.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:20:18.31
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:18.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:18.384
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/17/23 07:20:18.446
Aug 17 07:20:18.482: INFO: Waiting up to 5m0s for pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138" in namespace "downward-api-7915" to be "Succeeded or Failed"
Aug 17 07:20:18.487: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138": Phase="Pending", Reason="", readiness=false. Elapsed: 4.949686ms
Aug 17 07:20:20.490: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007730548s
Aug 17 07:20:22.490: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007887421s
STEP: Saw pod success 08/17/23 07:20:22.49
Aug 17 07:20:22.490: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138" satisfied condition "Succeeded or Failed"
Aug 17 07:20:22.492: INFO: Trying to get logs from node yst-node2 pod downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138 container dapi-container: <nil>
STEP: delete the pod 08/17/23 07:20:22.496
Aug 17 07:20:22.710: INFO: Waiting for pod downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138 to disappear
Aug 17 07:20:22.734: INFO: Pod downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:22.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7915" for this suite. 08/17/23 07:20:22.746
------------------------------
â€¢ [4.443 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:18.309
    Aug 17 07:20:18.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:20:18.31
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:18.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:18.384
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/17/23 07:20:18.446
    Aug 17 07:20:18.482: INFO: Waiting up to 5m0s for pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138" in namespace "downward-api-7915" to be "Succeeded or Failed"
    Aug 17 07:20:18.487: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138": Phase="Pending", Reason="", readiness=false. Elapsed: 4.949686ms
    Aug 17 07:20:20.490: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007730548s
    Aug 17 07:20:22.490: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007887421s
    STEP: Saw pod success 08/17/23 07:20:22.49
    Aug 17 07:20:22.490: INFO: Pod "downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138" satisfied condition "Succeeded or Failed"
    Aug 17 07:20:22.492: INFO: Trying to get logs from node yst-node2 pod downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 07:20:22.496
    Aug 17 07:20:22.710: INFO: Waiting for pod downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138 to disappear
    Aug 17 07:20:22.734: INFO: Pod downward-api-f7ca1a37-846e-4aeb-a6a1-d9398d11c138 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:22.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7915" for this suite. 08/17/23 07:20:22.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:22.753
Aug 17 07:20:22.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename conformance-tests 08/17/23 07:20:22.754
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:22.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:22.776
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/17/23 07:20:22.779
Aug 17 07:20:22.779: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:22.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-104" for this suite. 08/17/23 07:20:22.804
------------------------------
â€¢ [0.058 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:22.753
    Aug 17 07:20:22.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename conformance-tests 08/17/23 07:20:22.754
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:22.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:22.776
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/17/23 07:20:22.779
    Aug 17 07:20:22.779: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:22.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-104" for this suite. 08/17/23 07:20:22.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:22.812
Aug 17 07:20:22.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 07:20:22.813
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:22.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:22.977
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/17/23 07:20:22.993
Aug 17 07:20:22.993: INFO: Creating simple deployment test-deployment-l68vj
Aug 17 07:20:23.007: INFO: deployment "test-deployment-l68vj" doesn't have the required revision set
STEP: Getting /status 08/17/23 07:20:25.016
Aug 17 07:20:25.019: INFO: Deployment test-deployment-l68vj has Conditions: [{Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/17/23 07:20:25.019
Aug 17 07:20:25.024: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 20, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 20, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 20, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 20, 23, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-l68vj-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/17/23 07:20:25.024
Aug 17 07:20:25.025: INFO: Observed &Deployment event: ADDED
Aug 17 07:20:25.025: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
Aug 17 07:20:25.025: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.025: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
Aug 17 07:20:25.025: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 07:20:25.026: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-l68vj-54bc444df" is progressing.}
Aug 17 07:20:25.026: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
Aug 17 07:20:25.026: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
Aug 17 07:20:25.026: INFO: Found Deployment test-deployment-l68vj in namespace deployment-4848 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 07:20:25.026: INFO: Deployment test-deployment-l68vj has an updated status
STEP: patching the Statefulset Status 08/17/23 07:20:25.026
Aug 17 07:20:25.026: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 17 07:20:25.030: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/17/23 07:20:25.03
Aug 17 07:20:25.031: INFO: Observed &Deployment event: ADDED
Aug 17 07:20:25.031: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
Aug 17 07:20:25.031: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-l68vj-54bc444df" is progressing.}
Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
Aug 17 07:20:25.032: INFO: Found deployment test-deployment-l68vj in namespace deployment-4848 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 17 07:20:25.032: INFO: Deployment test-deployment-l68vj has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 07:20:25.034: INFO: Deployment "test-deployment-l68vj":
&Deployment{ObjectMeta:{test-deployment-l68vj  deployment-4848  04288696-9740-45f9-8974-72732cc26f02 27026788 1 2023-08-17 07:20:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-17 07:20:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:20:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-17 07:20:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a5ec58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 07:20:25.037: INFO: New ReplicaSet "test-deployment-l68vj-54bc444df" of Deployment "test-deployment-l68vj":
&ReplicaSet{ObjectMeta:{test-deployment-l68vj-54bc444df  deployment-4848  8641de46-c29c-475c-8d73-54a9765e80e6 27026783 1 2023-08-17 07:20:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-l68vj 04288696-9740-45f9-8974-72732cc26f02 0xc008420527 0xc008420528}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04288696-9740-45f9-8974-72732cc26f02\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:20:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0084205d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:20:25.039: INFO: Pod "test-deployment-l68vj-54bc444df-sbd6q" is available:
&Pod{ObjectMeta:{test-deployment-l68vj-54bc444df-sbd6q test-deployment-l68vj-54bc444df- deployment-4848  6addccf2-fe20-4c8a-81ae-ebe36309b1f1 27026782 0 2023-08-17 07:20:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:ed57046189d68913c8577b23199b4d7f9a030dc45a190f28038d0a0daf770acb cni.projectcalico.org/podIP:172.32.238.84/32 cni.projectcalico.org/podIPs:172.32.238.84/32] [{apps/v1 ReplicaSet test-deployment-l68vj-54bc444df 8641de46-c29c-475c-8d73-54a9765e80e6 0xc003a5f007 0xc003a5f008}] [] [{calico Update v1 2023-08-17 07:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641de46-c29c-475c-8d73-54a9765e80e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:20:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhvkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhvkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.84,StartTime:2023-08-17 07:20:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:20:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://23030ea34805d6e11573cabcc4a7d1890aec00609497c6f896eeb05637d80ac6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:25.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4848" for this suite. 08/17/23 07:20:25.041
------------------------------
â€¢ [2.233 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:22.812
    Aug 17 07:20:22.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 07:20:22.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:22.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:22.977
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/17/23 07:20:22.993
    Aug 17 07:20:22.993: INFO: Creating simple deployment test-deployment-l68vj
    Aug 17 07:20:23.007: INFO: deployment "test-deployment-l68vj" doesn't have the required revision set
    STEP: Getting /status 08/17/23 07:20:25.016
    Aug 17 07:20:25.019: INFO: Deployment test-deployment-l68vj has Conditions: [{Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/17/23 07:20:25.019
    Aug 17 07:20:25.024: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 20, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 20, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 20, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 20, 23, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-l68vj-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/17/23 07:20:25.024
    Aug 17 07:20:25.025: INFO: Observed &Deployment event: ADDED
    Aug 17 07:20:25.025: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
    Aug 17 07:20:25.025: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.025: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
    Aug 17 07:20:25.025: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 07:20:25.026: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-l68vj-54bc444df" is progressing.}
    Aug 17 07:20:25.026: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
    Aug 17 07:20:25.026: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 07:20:25.026: INFO: Observed Deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
    Aug 17 07:20:25.026: INFO: Found Deployment test-deployment-l68vj in namespace deployment-4848 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 07:20:25.026: INFO: Deployment test-deployment-l68vj has an updated status
    STEP: patching the Statefulset Status 08/17/23 07:20:25.026
    Aug 17 07:20:25.026: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 17 07:20:25.030: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/17/23 07:20:25.03
    Aug 17 07:20:25.031: INFO: Observed &Deployment event: ADDED
    Aug 17 07:20:25.031: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
    Aug 17 07:20:25.031: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l68vj-54bc444df"}
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:23 +0000 UTC 2023-08-17 07:20:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-l68vj-54bc444df" is progressing.}
    Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
    Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 07:20:24 +0000 UTC 2023-08-17 07:20:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l68vj-54bc444df" has successfully progressed.}
    Aug 17 07:20:25.032: INFO: Observed deployment test-deployment-l68vj in namespace deployment-4848 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 07:20:25.032: INFO: Observed &Deployment event: MODIFIED
    Aug 17 07:20:25.032: INFO: Found deployment test-deployment-l68vj in namespace deployment-4848 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 17 07:20:25.032: INFO: Deployment test-deployment-l68vj has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 07:20:25.034: INFO: Deployment "test-deployment-l68vj":
    &Deployment{ObjectMeta:{test-deployment-l68vj  deployment-4848  04288696-9740-45f9-8974-72732cc26f02 27026788 1 2023-08-17 07:20:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-17 07:20:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:20:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-17 07:20:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a5ec58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 07:20:25.037: INFO: New ReplicaSet "test-deployment-l68vj-54bc444df" of Deployment "test-deployment-l68vj":
    &ReplicaSet{ObjectMeta:{test-deployment-l68vj-54bc444df  deployment-4848  8641de46-c29c-475c-8d73-54a9765e80e6 27026783 1 2023-08-17 07:20:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-l68vj 04288696-9740-45f9-8974-72732cc26f02 0xc008420527 0xc008420528}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04288696-9740-45f9-8974-72732cc26f02\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:20:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0084205d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:20:25.039: INFO: Pod "test-deployment-l68vj-54bc444df-sbd6q" is available:
    &Pod{ObjectMeta:{test-deployment-l68vj-54bc444df-sbd6q test-deployment-l68vj-54bc444df- deployment-4848  6addccf2-fe20-4c8a-81ae-ebe36309b1f1 27026782 0 2023-08-17 07:20:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:ed57046189d68913c8577b23199b4d7f9a030dc45a190f28038d0a0daf770acb cni.projectcalico.org/podIP:172.32.238.84/32 cni.projectcalico.org/podIPs:172.32.238.84/32] [{apps/v1 ReplicaSet test-deployment-l68vj-54bc444df 8641de46-c29c-475c-8d73-54a9765e80e6 0xc003a5f007 0xc003a5f008}] [] [{calico Update v1 2023-08-17 07:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8641de46-c29c-475c-8d73-54a9765e80e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:20:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhvkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhvkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:20:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.84,StartTime:2023-08-17 07:20:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:20:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://23030ea34805d6e11573cabcc4a7d1890aec00609497c6f896eeb05637d80ac6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:25.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4848" for this suite. 08/17/23 07:20:25.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:25.045
Aug 17 07:20:25.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pod-network-test 08/17/23 07:20:25.046
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:25.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:25.068
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5328 08/17/23 07:20:25.07
STEP: creating a selector 08/17/23 07:20:25.07
STEP: Creating the service pods in kubernetes 08/17/23 07:20:25.07
Aug 17 07:20:25.070: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 07:20:25.095: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5328" to be "running and ready"
Aug 17 07:20:25.098: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905816ms
Aug 17 07:20:25.098: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:27.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006030639s
Aug 17 07:20:27.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:20:29.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006451249s
Aug 17 07:20:29.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:20:31.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005703619s
Aug 17 07:20:31.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:20:33.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005903291s
Aug 17 07:20:33.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:20:35.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006851929s
Aug 17 07:20:35.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:20:37.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005914412s
Aug 17 07:20:37.101: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 07:20:37.101: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 07:20:37.103: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5328" to be "running and ready"
Aug 17 07:20:37.105: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.920923ms
Aug 17 07:20:37.105: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 07:20:37.105: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 17 07:20:37.107: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5328" to be "running and ready"
Aug 17 07:20:37.109: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.841317ms
Aug 17 07:20:37.109: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 17 07:20:37.109: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 07:20:37.11
Aug 17 07:20:37.118: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5328" to be "running"
Aug 17 07:20:37.120: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01683ms
Aug 17 07:20:39.123: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004938308s
Aug 17 07:20:39.123: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 07:20:39.125: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5328" to be "running"
Aug 17 07:20:39.127: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.841108ms
Aug 17 07:20:39.127: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 17 07:20:39.128: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 17 07:20:39.128: INFO: Going to poll 172.32.3.33 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 17 07:20:39.130: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.32.3.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:20:39.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:20:39.131: INFO: ExecWithOptions: Clientset creation
Aug 17 07:20:39.131: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.32.3.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 07:20:39.183: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 17 07:20:39.183: INFO: Going to poll 172.32.123.134 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 17 07:20:39.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.32.123.134:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:20:39.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:20:39.186: INFO: ExecWithOptions: Clientset creation
Aug 17 07:20:39.186: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.32.123.134%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 07:20:39.239: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 17 07:20:39.239: INFO: Going to poll 172.32.238.76 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 17 07:20:39.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.32.238.76:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:20:39.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:20:39.242: INFO: ExecWithOptions: Clientset creation
Aug 17 07:20:39.242: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.32.238.76%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 07:20:39.305: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:39.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5328" for this suite. 08/17/23 07:20:39.309
------------------------------
â€¢ [SLOW TEST] [14.267 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:25.045
    Aug 17 07:20:25.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 07:20:25.046
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:25.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:25.068
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5328 08/17/23 07:20:25.07
    STEP: creating a selector 08/17/23 07:20:25.07
    STEP: Creating the service pods in kubernetes 08/17/23 07:20:25.07
    Aug 17 07:20:25.070: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 07:20:25.095: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5328" to be "running and ready"
    Aug 17 07:20:25.098: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905816ms
    Aug 17 07:20:25.098: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:27.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006030639s
    Aug 17 07:20:27.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:20:29.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006451249s
    Aug 17 07:20:29.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:20:31.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005703619s
    Aug 17 07:20:31.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:20:33.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005903291s
    Aug 17 07:20:33.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:20:35.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006851929s
    Aug 17 07:20:35.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:20:37.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005914412s
    Aug 17 07:20:37.101: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 07:20:37.101: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 07:20:37.103: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5328" to be "running and ready"
    Aug 17 07:20:37.105: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.920923ms
    Aug 17 07:20:37.105: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 07:20:37.105: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 17 07:20:37.107: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5328" to be "running and ready"
    Aug 17 07:20:37.109: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.841317ms
    Aug 17 07:20:37.109: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 17 07:20:37.109: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 07:20:37.11
    Aug 17 07:20:37.118: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5328" to be "running"
    Aug 17 07:20:37.120: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01683ms
    Aug 17 07:20:39.123: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004938308s
    Aug 17 07:20:39.123: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 07:20:39.125: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5328" to be "running"
    Aug 17 07:20:39.127: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.841108ms
    Aug 17 07:20:39.127: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 17 07:20:39.128: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 17 07:20:39.128: INFO: Going to poll 172.32.3.33 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 17 07:20:39.130: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.32.3.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:20:39.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:20:39.131: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:20:39.131: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.32.3.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 07:20:39.183: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 17 07:20:39.183: INFO: Going to poll 172.32.123.134 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 17 07:20:39.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.32.123.134:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:20:39.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:20:39.186: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:20:39.186: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.32.123.134%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 07:20:39.239: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 17 07:20:39.239: INFO: Going to poll 172.32.238.76 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 17 07:20:39.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.32.238.76:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:20:39.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:20:39.242: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:20:39.242: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.32.238.76%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 07:20:39.305: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:39.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5328" for this suite. 08/17/23 07:20:39.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:39.313
Aug 17 07:20:39.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:20:39.314
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:39.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:39.332
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/17/23 07:20:39.337
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:39.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3314" for this suite. 08/17/23 07:20:39.371
------------------------------
â€¢ [0.062 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:39.313
    Aug 17 07:20:39.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:20:39.314
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:39.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:39.332
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/17/23 07:20:39.337
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:39.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3314" for this suite. 08/17/23 07:20:39.371
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:39.376
Aug 17 07:20:39.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:20:39.377
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:39.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:39.399
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-73b6fa02-ca63-40c3-8411-03be53a03990 08/17/23 07:20:39.404
STEP: Creating a pod to test consume configMaps 08/17/23 07:20:39.411
Aug 17 07:20:39.421: INFO: Waiting up to 5m0s for pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320" in namespace "configmap-6140" to be "Succeeded or Failed"
Aug 17 07:20:39.423: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320": Phase="Pending", Reason="", readiness=false. Elapsed: 1.783827ms
Aug 17 07:20:41.425: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00429898s
Aug 17 07:20:43.428: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006834599s
STEP: Saw pod success 08/17/23 07:20:43.428
Aug 17 07:20:43.428: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320" satisfied condition "Succeeded or Failed"
Aug 17 07:20:43.430: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:20:43.435
Aug 17 07:20:43.442: INFO: Waiting for pod pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320 to disappear
Aug 17 07:20:43.444: INFO: Pod pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6140" for this suite. 08/17/23 07:20:43.446
------------------------------
â€¢ [4.074 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:39.376
    Aug 17 07:20:39.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:20:39.377
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:39.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:39.399
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-73b6fa02-ca63-40c3-8411-03be53a03990 08/17/23 07:20:39.404
    STEP: Creating a pod to test consume configMaps 08/17/23 07:20:39.411
    Aug 17 07:20:39.421: INFO: Waiting up to 5m0s for pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320" in namespace "configmap-6140" to be "Succeeded or Failed"
    Aug 17 07:20:39.423: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320": Phase="Pending", Reason="", readiness=false. Elapsed: 1.783827ms
    Aug 17 07:20:41.425: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00429898s
    Aug 17 07:20:43.428: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006834599s
    STEP: Saw pod success 08/17/23 07:20:43.428
    Aug 17 07:20:43.428: INFO: Pod "pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320" satisfied condition "Succeeded or Failed"
    Aug 17 07:20:43.430: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:20:43.435
    Aug 17 07:20:43.442: INFO: Waiting for pod pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320 to disappear
    Aug 17 07:20:43.444: INFO: Pod pod-configmaps-606c53c0-3634-4d62-9a57-9e1cf681b320 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6140" for this suite. 08/17/23 07:20:43.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:43.451
Aug 17 07:20:43.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:20:43.452
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:43.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:43.469
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-5d759ef2-a686-4f65-b721-42667c4c6b4b 08/17/23 07:20:43.496
STEP: Creating secret with name s-test-opt-upd-8d97ce58-8fc7-4768-98bd-04e629cae4bf 08/17/23 07:20:43.503
STEP: Creating the pod 08/17/23 07:20:43.507
Aug 17 07:20:43.511: INFO: Waiting up to 5m0s for pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f" in namespace "secrets-9347" to be "running and ready"
Aug 17 07:20:43.513: INFO: Pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.96816ms
Aug 17 07:20:43.513: INFO: The phase of Pod pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:45.518: INFO: Pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00632995s
Aug 17 07:20:45.518: INFO: The phase of Pod pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f is Running (Ready = true)
Aug 17 07:20:45.518: INFO: Pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-5d759ef2-a686-4f65-b721-42667c4c6b4b 08/17/23 07:20:45.531
STEP: Updating secret s-test-opt-upd-8d97ce58-8fc7-4768-98bd-04e629cae4bf 08/17/23 07:20:45.535
STEP: Creating secret with name s-test-opt-create-2fd8efa5-56fb-4f7a-93de-540dbea72b30 08/17/23 07:20:45.54
STEP: waiting to observe update in volume 08/17/23 07:20:45.544
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:47.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9347" for this suite. 08/17/23 07:20:47.565
------------------------------
â€¢ [4.117 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:43.451
    Aug 17 07:20:43.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:20:43.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:43.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:43.469
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-5d759ef2-a686-4f65-b721-42667c4c6b4b 08/17/23 07:20:43.496
    STEP: Creating secret with name s-test-opt-upd-8d97ce58-8fc7-4768-98bd-04e629cae4bf 08/17/23 07:20:43.503
    STEP: Creating the pod 08/17/23 07:20:43.507
    Aug 17 07:20:43.511: INFO: Waiting up to 5m0s for pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f" in namespace "secrets-9347" to be "running and ready"
    Aug 17 07:20:43.513: INFO: Pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.96816ms
    Aug 17 07:20:43.513: INFO: The phase of Pod pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:45.518: INFO: Pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00632995s
    Aug 17 07:20:45.518: INFO: The phase of Pod pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f is Running (Ready = true)
    Aug 17 07:20:45.518: INFO: Pod "pod-secrets-f54934e8-f413-4f81-8df5-71524d39530f" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-5d759ef2-a686-4f65-b721-42667c4c6b4b 08/17/23 07:20:45.531
    STEP: Updating secret s-test-opt-upd-8d97ce58-8fc7-4768-98bd-04e629cae4bf 08/17/23 07:20:45.535
    STEP: Creating secret with name s-test-opt-create-2fd8efa5-56fb-4f7a-93de-540dbea72b30 08/17/23 07:20:45.54
    STEP: waiting to observe update in volume 08/17/23 07:20:45.544
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:47.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9347" for this suite. 08/17/23 07:20:47.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:47.569
Aug 17 07:20:47.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:20:47.569
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:47.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:47.582
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-4572 08/17/23 07:20:47.587
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[] 08/17/23 07:20:47.604
Aug 17 07:20:47.611: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4572 08/17/23 07:20:47.611
Aug 17 07:20:47.617: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4572" to be "running and ready"
Aug 17 07:20:47.620: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16732ms
Aug 17 07:20:47.620: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:49.623: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005952828s
Aug 17 07:20:49.623: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 17 07:20:49.623: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[pod1:[100]] 08/17/23 07:20:49.625
Aug 17 07:20:49.631: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4572 08/17/23 07:20:49.631
Aug 17 07:20:49.634: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4572" to be "running and ready"
Aug 17 07:20:49.636: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90241ms
Aug 17 07:20:49.636: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:51.640: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005443924s
Aug 17 07:20:51.640: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 17 07:20:51.640: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[pod1:[100] pod2:[101]] 08/17/23 07:20:51.642
Aug 17 07:20:51.650: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/17/23 07:20:51.65
Aug 17 07:20:51.650: INFO: Creating new exec pod
Aug 17 07:20:51.712: INFO: Waiting up to 5m0s for pod "execpodprcqr" in namespace "services-4572" to be "running"
Aug 17 07:20:51.714: INFO: Pod "execpodprcqr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.763608ms
Aug 17 07:20:53.717: INFO: Pod "execpodprcqr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004605757s
Aug 17 07:20:53.717: INFO: Pod "execpodprcqr" satisfied condition "running"
Aug 17 07:20:54.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 17 07:20:54.848: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 17 07:20:54.848: INFO: stdout: ""
Aug 17 07:20:54.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 10.109.73.139 80'
Aug 17 07:20:54.999: INFO: stderr: "+ nc -v -z -w 2 10.109.73.139 80\nConnection to 10.109.73.139 80 port [tcp/http] succeeded!\n"
Aug 17 07:20:54.999: INFO: stdout: ""
Aug 17 07:20:54.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 17 07:20:55.154: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 17 07:20:55.154: INFO: stdout: ""
Aug 17 07:20:55.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 10.109.73.139 81'
Aug 17 07:20:55.298: INFO: stderr: "+ nc -v -z -w 2 10.109.73.139 81\nConnection to 10.109.73.139 81 port [tcp/*] succeeded!\n"
Aug 17 07:20:55.298: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4572 08/17/23 07:20:55.298
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[pod2:[101]] 08/17/23 07:20:55.306
Aug 17 07:20:56.322: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4572 08/17/23 07:20:56.323
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[] 08/17/23 07:20:56.332
Aug 17 07:20:57.350: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:20:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4572" for this suite. 08/17/23 07:20:57.368
------------------------------
â€¢ [SLOW TEST] [9.804 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:47.569
    Aug 17 07:20:47.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:20:47.569
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:47.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:47.582
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-4572 08/17/23 07:20:47.587
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[] 08/17/23 07:20:47.604
    Aug 17 07:20:47.611: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4572 08/17/23 07:20:47.611
    Aug 17 07:20:47.617: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4572" to be "running and ready"
    Aug 17 07:20:47.620: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16732ms
    Aug 17 07:20:47.620: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:49.623: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005952828s
    Aug 17 07:20:49.623: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 17 07:20:49.623: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[pod1:[100]] 08/17/23 07:20:49.625
    Aug 17 07:20:49.631: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4572 08/17/23 07:20:49.631
    Aug 17 07:20:49.634: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4572" to be "running and ready"
    Aug 17 07:20:49.636: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90241ms
    Aug 17 07:20:49.636: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:51.640: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005443924s
    Aug 17 07:20:51.640: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 17 07:20:51.640: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[pod1:[100] pod2:[101]] 08/17/23 07:20:51.642
    Aug 17 07:20:51.650: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/17/23 07:20:51.65
    Aug 17 07:20:51.650: INFO: Creating new exec pod
    Aug 17 07:20:51.712: INFO: Waiting up to 5m0s for pod "execpodprcqr" in namespace "services-4572" to be "running"
    Aug 17 07:20:51.714: INFO: Pod "execpodprcqr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.763608ms
    Aug 17 07:20:53.717: INFO: Pod "execpodprcqr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004605757s
    Aug 17 07:20:53.717: INFO: Pod "execpodprcqr" satisfied condition "running"
    Aug 17 07:20:54.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 17 07:20:54.848: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 17 07:20:54.848: INFO: stdout: ""
    Aug 17 07:20:54.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 10.109.73.139 80'
    Aug 17 07:20:54.999: INFO: stderr: "+ nc -v -z -w 2 10.109.73.139 80\nConnection to 10.109.73.139 80 port [tcp/http] succeeded!\n"
    Aug 17 07:20:54.999: INFO: stdout: ""
    Aug 17 07:20:54.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 17 07:20:55.154: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 17 07:20:55.154: INFO: stdout: ""
    Aug 17 07:20:55.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-4572 exec execpodprcqr -- /bin/sh -x -c nc -v -z -w 2 10.109.73.139 81'
    Aug 17 07:20:55.298: INFO: stderr: "+ nc -v -z -w 2 10.109.73.139 81\nConnection to 10.109.73.139 81 port [tcp/*] succeeded!\n"
    Aug 17 07:20:55.298: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4572 08/17/23 07:20:55.298
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[pod2:[101]] 08/17/23 07:20:55.306
    Aug 17 07:20:56.322: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4572 08/17/23 07:20:56.323
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4572 to expose endpoints map[] 08/17/23 07:20:56.332
    Aug 17 07:20:57.350: INFO: successfully validated that service multi-endpoint-test in namespace services-4572 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:20:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4572" for this suite. 08/17/23 07:20:57.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:20:57.373
Aug 17 07:20:57.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:20:57.374
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:57.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:57.398
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/17/23 07:20:57.411
Aug 17 07:20:57.452: INFO: Waiting up to 5m0s for pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af" in namespace "downward-api-3664" to be "running and ready"
Aug 17 07:20:57.463: INFO: Pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af": Phase="Pending", Reason="", readiness=false. Elapsed: 11.112723ms
Aug 17 07:20:57.463: INFO: The phase of Pod annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:20:59.466: INFO: Pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af": Phase="Running", Reason="", readiness=true. Elapsed: 2.013994841s
Aug 17 07:20:59.466: INFO: The phase of Pod annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af is Running (Ready = true)
Aug 17 07:20:59.466: INFO: Pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af" satisfied condition "running and ready"
Aug 17 07:20:59.981: INFO: Successfully updated pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:21:01.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3664" for this suite. 08/17/23 07:21:01.994
------------------------------
â€¢ [4.625 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:20:57.373
    Aug 17 07:20:57.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:20:57.374
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:20:57.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:20:57.398
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/17/23 07:20:57.411
    Aug 17 07:20:57.452: INFO: Waiting up to 5m0s for pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af" in namespace "downward-api-3664" to be "running and ready"
    Aug 17 07:20:57.463: INFO: Pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af": Phase="Pending", Reason="", readiness=false. Elapsed: 11.112723ms
    Aug 17 07:20:57.463: INFO: The phase of Pod annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:20:59.466: INFO: Pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af": Phase="Running", Reason="", readiness=true. Elapsed: 2.013994841s
    Aug 17 07:20:59.466: INFO: The phase of Pod annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af is Running (Ready = true)
    Aug 17 07:20:59.466: INFO: Pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af" satisfied condition "running and ready"
    Aug 17 07:20:59.981: INFO: Successfully updated pod "annotationupdate25f2e117-4fc4-4d8c-bfd3-ec83381b44af"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:21:01.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3664" for this suite. 08/17/23 07:21:01.994
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:21:01.999
Aug 17 07:21:01.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 07:21:02
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:02.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:02.019
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/17/23 07:21:02.023
Aug 17 07:21:02.041: INFO: Waiting up to 5m0s for pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7" in namespace "var-expansion-7579" to be "Succeeded or Failed"
Aug 17 07:21:02.044: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652465ms
Aug 17 07:21:04.047: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005343647s
Aug 17 07:21:06.047: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Running", Reason="", readiness=false. Elapsed: 4.006239643s
Aug 17 07:21:08.051: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009609391s
STEP: Saw pod success 08/17/23 07:21:08.051
Aug 17 07:21:08.051: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7" satisfied condition "Succeeded or Failed"
Aug 17 07:21:08.053: INFO: Trying to get logs from node yst-node2 pod var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7 container dapi-container: <nil>
STEP: delete the pod 08/17/23 07:21:08.057
Aug 17 07:21:08.067: INFO: Waiting for pod var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7 to disappear
Aug 17 07:21:08.070: INFO: Pod var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 07:21:08.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7579" for this suite. 08/17/23 07:21:08.073
------------------------------
â€¢ [SLOW TEST] [6.088 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:21:01.999
    Aug 17 07:21:01.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 07:21:02
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:02.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:02.019
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/17/23 07:21:02.023
    Aug 17 07:21:02.041: INFO: Waiting up to 5m0s for pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7" in namespace "var-expansion-7579" to be "Succeeded or Failed"
    Aug 17 07:21:02.044: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652465ms
    Aug 17 07:21:04.047: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005343647s
    Aug 17 07:21:06.047: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Running", Reason="", readiness=false. Elapsed: 4.006239643s
    Aug 17 07:21:08.051: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009609391s
    STEP: Saw pod success 08/17/23 07:21:08.051
    Aug 17 07:21:08.051: INFO: Pod "var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7" satisfied condition "Succeeded or Failed"
    Aug 17 07:21:08.053: INFO: Trying to get logs from node yst-node2 pod var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 07:21:08.057
    Aug 17 07:21:08.067: INFO: Waiting for pod var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7 to disappear
    Aug 17 07:21:08.070: INFO: Pod var-expansion-3d9b3803-3898-457e-8176-4389ac551ff7 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:21:08.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7579" for this suite. 08/17/23 07:21:08.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:21:08.088
Aug 17 07:21:08.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 07:21:08.089
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:08.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:08.214
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 17 07:21:08.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:21:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8440" for this suite. 08/17/23 07:21:09.426
------------------------------
â€¢ [1.444 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:21:08.088
    Aug 17 07:21:08.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 07:21:08.089
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:08.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:08.214
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 17 07:21:08.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:21:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8440" for this suite. 08/17/23 07:21:09.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:21:09.534
Aug 17 07:21:09.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename subpath 08/17/23 07:21:09.535
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:09.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:09.586
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 07:21:09.593
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-42gf 08/17/23 07:21:09.63
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 07:21:09.63
Aug 17 07:21:09.639: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-42gf" in namespace "subpath-7176" to be "Succeeded or Failed"
Aug 17 07:21:09.642: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.710253ms
Aug 17 07:21:11.675: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 2.0360043s
Aug 17 07:21:13.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006767751s
Aug 17 07:21:15.647: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 6.007794888s
Aug 17 07:21:17.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 8.005850405s
Aug 17 07:21:19.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 10.005257457s
Aug 17 07:21:21.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 12.005328163s
Aug 17 07:21:23.647: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 14.0073236s
Aug 17 07:21:25.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 16.006530748s
Aug 17 07:21:27.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 18.006514415s
Aug 17 07:21:29.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 20.006923415s
Aug 17 07:21:31.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=false. Elapsed: 22.005694291s
Aug 17 07:21:33.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005719174s
STEP: Saw pod success 08/17/23 07:21:33.645
Aug 17 07:21:33.645: INFO: Pod "pod-subpath-test-configmap-42gf" satisfied condition "Succeeded or Failed"
Aug 17 07:21:33.647: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-configmap-42gf container test-container-subpath-configmap-42gf: <nil>
STEP: delete the pod 08/17/23 07:21:33.65
Aug 17 07:21:33.655: INFO: Waiting for pod pod-subpath-test-configmap-42gf to disappear
Aug 17 07:21:33.657: INFO: Pod pod-subpath-test-configmap-42gf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-42gf 08/17/23 07:21:33.657
Aug 17 07:21:33.657: INFO: Deleting pod "pod-subpath-test-configmap-42gf" in namespace "subpath-7176"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 07:21:33.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7176" for this suite. 08/17/23 07:21:33.661
------------------------------
â€¢ [SLOW TEST] [24.130 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:21:09.534
    Aug 17 07:21:09.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename subpath 08/17/23 07:21:09.535
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:09.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:09.586
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 07:21:09.593
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-42gf 08/17/23 07:21:09.63
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 07:21:09.63
    Aug 17 07:21:09.639: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-42gf" in namespace "subpath-7176" to be "Succeeded or Failed"
    Aug 17 07:21:09.642: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.710253ms
    Aug 17 07:21:11.675: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 2.0360043s
    Aug 17 07:21:13.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006767751s
    Aug 17 07:21:15.647: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 6.007794888s
    Aug 17 07:21:17.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 8.005850405s
    Aug 17 07:21:19.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 10.005257457s
    Aug 17 07:21:21.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 12.005328163s
    Aug 17 07:21:23.647: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 14.0073236s
    Aug 17 07:21:25.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 16.006530748s
    Aug 17 07:21:27.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 18.006514415s
    Aug 17 07:21:29.646: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=true. Elapsed: 20.006923415s
    Aug 17 07:21:31.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Running", Reason="", readiness=false. Elapsed: 22.005694291s
    Aug 17 07:21:33.645: INFO: Pod "pod-subpath-test-configmap-42gf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005719174s
    STEP: Saw pod success 08/17/23 07:21:33.645
    Aug 17 07:21:33.645: INFO: Pod "pod-subpath-test-configmap-42gf" satisfied condition "Succeeded or Failed"
    Aug 17 07:21:33.647: INFO: Trying to get logs from node yst-node2 pod pod-subpath-test-configmap-42gf container test-container-subpath-configmap-42gf: <nil>
    STEP: delete the pod 08/17/23 07:21:33.65
    Aug 17 07:21:33.655: INFO: Waiting for pod pod-subpath-test-configmap-42gf to disappear
    Aug 17 07:21:33.657: INFO: Pod pod-subpath-test-configmap-42gf no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-42gf 08/17/23 07:21:33.657
    Aug 17 07:21:33.657: INFO: Deleting pod "pod-subpath-test-configmap-42gf" in namespace "subpath-7176"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:21:33.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7176" for this suite. 08/17/23 07:21:33.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:21:33.664
Aug 17 07:21:33.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/17/23 07:21:33.665
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:33.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:33.684
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/17/23 07:21:33.686
STEP: Creating hostNetwork=false pod 08/17/23 07:21:33.686
Aug 17 07:21:33.691: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8198" to be "running and ready"
Aug 17 07:21:33.693: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045954ms
Aug 17 07:21:33.693: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:21:35.696: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00484309s
Aug 17 07:21:35.696: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 17 07:21:35.696: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/17/23 07:21:35.698
Aug 17 07:21:35.701: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8198" to be "running and ready"
Aug 17 07:21:35.703: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.303879ms
Aug 17 07:21:35.703: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:21:37.705: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004015651s
Aug 17 07:21:37.705: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 17 07:21:37.705: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/17/23 07:21:37.707
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/17/23 07:21:37.707
Aug 17 07:21:37.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:37.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:37.708: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:37.708: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 07:21:37.754: INFO: Exec stderr: ""
Aug 17 07:21:37.754: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:37.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:37.754: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:37.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 07:21:37.798: INFO: Exec stderr: ""
Aug 17 07:21:37.798: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:37.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:37.798: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:37.798: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 07:21:37.847: INFO: Exec stderr: ""
Aug 17 07:21:37.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:37.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:37.848: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:37.848: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 07:21:37.897: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/17/23 07:21:37.897
Aug 17 07:21:37.897: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:37.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:37.898: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:37.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 17 07:21:37.944: INFO: Exec stderr: ""
Aug 17 07:21:37.944: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:37.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:37.945: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:37.945: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 17 07:21:37.994: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/17/23 07:21:37.994
Aug 17 07:21:37.994: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:37.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:37.994: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:37.994: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 07:21:38.047: INFO: Exec stderr: ""
Aug 17 07:21:38.047: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:38.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:38.047: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:38.047: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 07:21:38.093: INFO: Exec stderr: ""
Aug 17 07:21:38.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:38.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:38.094: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:38.094: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 07:21:38.140: INFO: Exec stderr: ""
Aug 17 07:21:38.140: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:21:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:38.141: INFO: ExecWithOptions: Clientset creation
Aug 17 07:21:38.141: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 07:21:38.184: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 17 07:21:38.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8198" for this suite. 08/17/23 07:21:38.188
------------------------------
â€¢ [4.528 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:21:33.664
    Aug 17 07:21:33.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/17/23 07:21:33.665
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:33.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:33.684
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/17/23 07:21:33.686
    STEP: Creating hostNetwork=false pod 08/17/23 07:21:33.686
    Aug 17 07:21:33.691: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8198" to be "running and ready"
    Aug 17 07:21:33.693: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045954ms
    Aug 17 07:21:33.693: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:21:35.696: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00484309s
    Aug 17 07:21:35.696: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 17 07:21:35.696: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/17/23 07:21:35.698
    Aug 17 07:21:35.701: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8198" to be "running and ready"
    Aug 17 07:21:35.703: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.303879ms
    Aug 17 07:21:35.703: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:21:37.705: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004015651s
    Aug 17 07:21:37.705: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 17 07:21:37.705: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/17/23 07:21:37.707
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/17/23 07:21:37.707
    Aug 17 07:21:37.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:37.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:37.708: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:37.708: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 07:21:37.754: INFO: Exec stderr: ""
    Aug 17 07:21:37.754: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:37.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:37.754: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:37.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 07:21:37.798: INFO: Exec stderr: ""
    Aug 17 07:21:37.798: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:37.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:37.798: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:37.798: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 07:21:37.847: INFO: Exec stderr: ""
    Aug 17 07:21:37.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:37.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:37.848: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:37.848: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 07:21:37.897: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/17/23 07:21:37.897
    Aug 17 07:21:37.897: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:37.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:37.898: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:37.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 17 07:21:37.944: INFO: Exec stderr: ""
    Aug 17 07:21:37.944: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:37.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:37.945: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:37.945: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 17 07:21:37.994: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/17/23 07:21:37.994
    Aug 17 07:21:37.994: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:37.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:37.994: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:37.994: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 07:21:38.047: INFO: Exec stderr: ""
    Aug 17 07:21:38.047: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:38.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:38.047: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:38.047: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 07:21:38.093: INFO: Exec stderr: ""
    Aug 17 07:21:38.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:38.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:38.094: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:38.094: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 07:21:38.140: INFO: Exec stderr: ""
    Aug 17 07:21:38.140: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8198 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:21:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:38.141: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:21:38.141: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8198/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 07:21:38.184: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:21:38.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8198" for this suite. 08/17/23 07:21:38.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:21:38.192
Aug 17 07:21:38.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:21:38.193
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:38.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:38.207
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/17/23 07:21:38.211
STEP: Creating a ResourceQuota 08/17/23 07:21:43.213
STEP: Ensuring resource quota status is calculated 08/17/23 07:21:43.216
STEP: Creating a Pod that fits quota 08/17/23 07:21:45.218
STEP: Ensuring ResourceQuota status captures the pod usage 08/17/23 07:21:45.23
STEP: Not allowing a pod to be created that exceeds remaining quota 08/17/23 07:21:47.234
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/17/23 07:21:47.236
STEP: Ensuring a pod cannot update its resource requirements 08/17/23 07:21:47.238
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/17/23 07:21:47.241
STEP: Deleting the pod 08/17/23 07:21:49.244
STEP: Ensuring resource quota status released the pod usage 08/17/23 07:21:49.25
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:21:51.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1051" for this suite. 08/17/23 07:21:51.255
------------------------------
â€¢ [SLOW TEST] [13.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:21:38.192
    Aug 17 07:21:38.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:21:38.193
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:38.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:38.207
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/17/23 07:21:38.211
    STEP: Creating a ResourceQuota 08/17/23 07:21:43.213
    STEP: Ensuring resource quota status is calculated 08/17/23 07:21:43.216
    STEP: Creating a Pod that fits quota 08/17/23 07:21:45.218
    STEP: Ensuring ResourceQuota status captures the pod usage 08/17/23 07:21:45.23
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/17/23 07:21:47.234
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/17/23 07:21:47.236
    STEP: Ensuring a pod cannot update its resource requirements 08/17/23 07:21:47.238
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/17/23 07:21:47.241
    STEP: Deleting the pod 08/17/23 07:21:49.244
    STEP: Ensuring resource quota status released the pod usage 08/17/23 07:21:49.25
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:21:51.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1051" for this suite. 08/17/23 07:21:51.255
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:21:51.26
Aug 17 07:21:51.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:21:51.261
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:51.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:51.279
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/17/23 07:21:51.281
Aug 17 07:21:51.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:21:53.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:02.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6306" for this suite. 08/17/23 07:22:02.126
------------------------------
â€¢ [SLOW TEST] [10.869 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:21:51.26
    Aug 17 07:21:51.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:21:51.261
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:21:51.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:21:51.279
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/17/23 07:21:51.281
    Aug 17 07:21:51.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:21:53.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:02.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6306" for this suite. 08/17/23 07:22:02.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:02.13
Aug 17 07:22:02.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:22:02.131
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:02.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:02.194
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/17/23 07:22:02.197
Aug 17 07:22:02.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1562 create -f -'
Aug 17 07:22:03.273: INFO: stderr: ""
Aug 17 07:22:03.273: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/17/23 07:22:03.273
Aug 17 07:22:03.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1562 diff -f -'
Aug 17 07:22:03.583: INFO: rc: 1
Aug 17 07:22:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1562 delete -f -'
Aug 17 07:22:03.663: INFO: stderr: ""
Aug 17 07:22:03.663: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1562" for this suite. 08/17/23 07:22:03.666
------------------------------
â€¢ [1.542 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:02.13
    Aug 17 07:22:02.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:22:02.131
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:02.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:02.194
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/17/23 07:22:02.197
    Aug 17 07:22:02.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1562 create -f -'
    Aug 17 07:22:03.273: INFO: stderr: ""
    Aug 17 07:22:03.273: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/17/23 07:22:03.273
    Aug 17 07:22:03.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1562 diff -f -'
    Aug 17 07:22:03.583: INFO: rc: 1
    Aug 17 07:22:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1562 delete -f -'
    Aug 17 07:22:03.663: INFO: stderr: ""
    Aug 17 07:22:03.663: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1562" for this suite. 08/17/23 07:22:03.666
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:03.672
Aug 17 07:22:03.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename init-container 08/17/23 07:22:03.672
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:03.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:03.681
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/17/23 07:22:03.683
Aug 17 07:22:03.683: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:08.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1284" for this suite. 08/17/23 07:22:08.674
------------------------------
â€¢ [SLOW TEST] [5.006 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:03.672
    Aug 17 07:22:03.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename init-container 08/17/23 07:22:03.672
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:03.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:03.681
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/17/23 07:22:03.683
    Aug 17 07:22:03.683: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:08.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1284" for this suite. 08/17/23 07:22:08.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:08.679
Aug 17 07:22:08.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:22:08.679
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:08.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:08.696
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:12.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-862" for this suite. 08/17/23 07:22:12.722
------------------------------
â€¢ [4.061 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:08.679
    Aug 17 07:22:08.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:22:08.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:08.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:08.696
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:12.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-862" for this suite. 08/17/23 07:22:12.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:12.74
Aug 17 07:22:12.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:22:12.741
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:12.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:12.81
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:22:12.831
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:22:13.52
STEP: Deploying the webhook pod 08/17/23 07:22:13.524
STEP: Wait for the deployment to be ready 08/17/23 07:22:13.53
Aug 17 07:22:13.533: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/17/23 07:22:15.539
STEP: Verifying the service has paired with the endpoint 08/17/23 07:22:15.546
Aug 17 07:22:16.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/17/23 07:22:16.549
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:16.549
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/17/23 07:22:16.558
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/17/23 07:22:17.564
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:17.564
STEP: Having no error when timeout is longer than webhook latency 08/17/23 07:22:18.594
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:18.594
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/17/23 07:22:23.612
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:23.612
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:28.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8457" for this suite. 08/17/23 07:22:28.645
STEP: Destroying namespace "webhook-8457-markers" for this suite. 08/17/23 07:22:28.649
------------------------------
â€¢ [SLOW TEST] [15.914 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:12.74
    Aug 17 07:22:12.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:22:12.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:12.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:12.81
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:22:12.831
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:22:13.52
    STEP: Deploying the webhook pod 08/17/23 07:22:13.524
    STEP: Wait for the deployment to be ready 08/17/23 07:22:13.53
    Aug 17 07:22:13.533: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/17/23 07:22:15.539
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:22:15.546
    Aug 17 07:22:16.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/17/23 07:22:16.549
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:16.549
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/17/23 07:22:16.558
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/17/23 07:22:17.564
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:17.564
    STEP: Having no error when timeout is longer than webhook latency 08/17/23 07:22:18.594
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:18.594
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/17/23 07:22:23.612
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 07:22:23.612
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:28.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8457" for this suite. 08/17/23 07:22:28.645
    STEP: Destroying namespace "webhook-8457-markers" for this suite. 08/17/23 07:22:28.649
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:28.654
Aug 17 07:22:28.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 07:22:28.655
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:28.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:28.67
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/17/23 07:22:28.672
Aug 17 07:22:28.680: INFO: Waiting up to 5m0s for pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8" in namespace "var-expansion-18" to be "Succeeded or Failed"
Aug 17 07:22:28.685: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996434ms
Aug 17 07:22:30.688: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007323994s
Aug 17 07:22:32.690: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009381486s
STEP: Saw pod success 08/17/23 07:22:32.69
Aug 17 07:22:32.690: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8" satisfied condition "Succeeded or Failed"
Aug 17 07:22:32.691: INFO: Trying to get logs from node yst-node2 pod var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8 container dapi-container: <nil>
STEP: delete the pod 08/17/23 07:22:32.695
Aug 17 07:22:32.700: INFO: Waiting for pod var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8 to disappear
Aug 17 07:22:32.701: INFO: Pod var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:32.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-18" for this suite. 08/17/23 07:22:32.704
------------------------------
â€¢ [4.053 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:28.654
    Aug 17 07:22:28.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 07:22:28.655
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:28.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:28.67
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/17/23 07:22:28.672
    Aug 17 07:22:28.680: INFO: Waiting up to 5m0s for pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8" in namespace "var-expansion-18" to be "Succeeded or Failed"
    Aug 17 07:22:28.685: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996434ms
    Aug 17 07:22:30.688: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007323994s
    Aug 17 07:22:32.690: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009381486s
    STEP: Saw pod success 08/17/23 07:22:32.69
    Aug 17 07:22:32.690: INFO: Pod "var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8" satisfied condition "Succeeded or Failed"
    Aug 17 07:22:32.691: INFO: Trying to get logs from node yst-node2 pod var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 07:22:32.695
    Aug 17 07:22:32.700: INFO: Waiting for pod var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8 to disappear
    Aug 17 07:22:32.701: INFO: Pod var-expansion-626e76ef-1663-437e-b06d-9668f259b8d8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:32.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-18" for this suite. 08/17/23 07:22:32.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:32.708
Aug 17 07:22:32.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svc-latency 08/17/23 07:22:32.709
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:32.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:32.719
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 17 07:22:32.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7674 08/17/23 07:22:32.722
I0817 07:22:32.732377      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7674, replica count: 1
I0817 07:22:33.783171      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 07:22:34.783850      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 07:22:34.892: INFO: Created: latency-svc-w4tw7
Aug 17 07:22:34.896: INFO: Got endpoints: latency-svc-w4tw7 [11.956143ms]
Aug 17 07:22:34.906: INFO: Created: latency-svc-t2hm7
Aug 17 07:22:34.910: INFO: Got endpoints: latency-svc-t2hm7 [13.952789ms]
Aug 17 07:22:34.911: INFO: Created: latency-svc-phcvg
Aug 17 07:22:34.913: INFO: Got endpoints: latency-svc-phcvg [16.201693ms]
Aug 17 07:22:34.915: INFO: Created: latency-svc-tqphd
Aug 17 07:22:34.918: INFO: Got endpoints: latency-svc-tqphd [21.700299ms]
Aug 17 07:22:34.922: INFO: Created: latency-svc-7gkb7
Aug 17 07:22:34.930: INFO: Got endpoints: latency-svc-7gkb7 [34.036291ms]
Aug 17 07:22:34.931: INFO: Created: latency-svc-9sf2h
Aug 17 07:22:34.934: INFO: Got endpoints: latency-svc-9sf2h [37.475214ms]
Aug 17 07:22:34.948: INFO: Created: latency-svc-dxpmb
Aug 17 07:22:34.954: INFO: Created: latency-svc-4gp75
Aug 17 07:22:34.954: INFO: Got endpoints: latency-svc-dxpmb [57.426257ms]
Aug 17 07:22:34.957: INFO: Got endpoints: latency-svc-4gp75 [60.454102ms]
Aug 17 07:22:34.959: INFO: Created: latency-svc-fbfd9
Aug 17 07:22:34.963: INFO: Got endpoints: latency-svc-fbfd9 [65.902546ms]
Aug 17 07:22:34.964: INFO: Created: latency-svc-b9pmd
Aug 17 07:22:34.968: INFO: Got endpoints: latency-svc-b9pmd [71.094575ms]
Aug 17 07:22:34.970: INFO: Created: latency-svc-sk2qn
Aug 17 07:22:34.972: INFO: Got endpoints: latency-svc-sk2qn [75.02446ms]
Aug 17 07:22:34.979: INFO: Created: latency-svc-rlksl
Aug 17 07:22:34.980: INFO: Got endpoints: latency-svc-rlksl [82.777275ms]
Aug 17 07:22:34.981: INFO: Created: latency-svc-f8l65
Aug 17 07:22:34.985: INFO: Got endpoints: latency-svc-f8l65 [88.541149ms]
Aug 17 07:22:34.986: INFO: Created: latency-svc-zm6g4
Aug 17 07:22:34.990: INFO: Got endpoints: latency-svc-zm6g4 [92.604856ms]
Aug 17 07:22:34.991: INFO: Created: latency-svc-sv2lv
Aug 17 07:22:34.996: INFO: Got endpoints: latency-svc-sv2lv [99.476514ms]
Aug 17 07:22:34.998: INFO: Created: latency-svc-lbglz
Aug 17 07:22:35.001: INFO: Got endpoints: latency-svc-lbglz [103.930253ms]
Aug 17 07:22:35.005: INFO: Created: latency-svc-fmncw
Aug 17 07:22:35.010: INFO: Got endpoints: latency-svc-fmncw [99.103211ms]
Aug 17 07:22:35.011: INFO: Created: latency-svc-v2xhf
Aug 17 07:22:35.016: INFO: Got endpoints: latency-svc-v2xhf [102.658107ms]
Aug 17 07:22:35.017: INFO: Created: latency-svc-ltfss
Aug 17 07:22:35.019: INFO: Got endpoints: latency-svc-ltfss [100.425792ms]
Aug 17 07:22:35.022: INFO: Created: latency-svc-v9qjx
Aug 17 07:22:35.026: INFO: Got endpoints: latency-svc-v9qjx [95.272147ms]
Aug 17 07:22:35.028: INFO: Created: latency-svc-8xnfc
Aug 17 07:22:35.031: INFO: Got endpoints: latency-svc-8xnfc [96.440347ms]
Aug 17 07:22:35.033: INFO: Created: latency-svc-7f77p
Aug 17 07:22:35.037: INFO: Got endpoints: latency-svc-7f77p [83.295546ms]
Aug 17 07:22:35.040: INFO: Created: latency-svc-br4bd
Aug 17 07:22:35.043: INFO: Got endpoints: latency-svc-br4bd [85.984742ms]
Aug 17 07:22:35.043: INFO: Created: latency-svc-sgvjl
Aug 17 07:22:35.061: INFO: Got endpoints: latency-svc-sgvjl [98.474434ms]
Aug 17 07:22:35.063: INFO: Created: latency-svc-t5p5j
Aug 17 07:22:35.068: INFO: Got endpoints: latency-svc-t5p5j [99.610293ms]
Aug 17 07:22:35.070: INFO: Created: latency-svc-msp94
Aug 17 07:22:35.075: INFO: Got endpoints: latency-svc-msp94 [102.824391ms]
Aug 17 07:22:35.077: INFO: Created: latency-svc-ljfpp
Aug 17 07:22:35.081: INFO: Got endpoints: latency-svc-ljfpp [101.049148ms]
Aug 17 07:22:35.083: INFO: Created: latency-svc-s9qd5
Aug 17 07:22:35.085: INFO: Got endpoints: latency-svc-s9qd5 [99.811216ms]
Aug 17 07:22:35.090: INFO: Created: latency-svc-qv4mg
Aug 17 07:22:35.093: INFO: Got endpoints: latency-svc-qv4mg [103.515906ms]
Aug 17 07:22:35.096: INFO: Created: latency-svc-bmq8s
Aug 17 07:22:35.100: INFO: Got endpoints: latency-svc-bmq8s [103.295471ms]
Aug 17 07:22:35.102: INFO: Created: latency-svc-rnqgz
Aug 17 07:22:35.106: INFO: Got endpoints: latency-svc-rnqgz [105.099447ms]
Aug 17 07:22:35.108: INFO: Created: latency-svc-bwzhw
Aug 17 07:22:35.117: INFO: Got endpoints: latency-svc-bwzhw [107.854493ms]
Aug 17 07:22:35.120: INFO: Created: latency-svc-hfjmg
Aug 17 07:22:35.123: INFO: Got endpoints: latency-svc-hfjmg [107.716162ms]
Aug 17 07:22:35.125: INFO: Created: latency-svc-f2r5f
Aug 17 07:22:35.129: INFO: Got endpoints: latency-svc-f2r5f [110.110233ms]
Aug 17 07:22:35.131: INFO: Created: latency-svc-crf5v
Aug 17 07:22:35.136: INFO: Created: latency-svc-42hrs
Aug 17 07:22:35.141: INFO: Created: latency-svc-n94gm
Aug 17 07:22:35.145: INFO: Created: latency-svc-2w2mh
Aug 17 07:22:35.149: INFO: Got endpoints: latency-svc-crf5v [123.119329ms]
Aug 17 07:22:35.149: INFO: Created: latency-svc-8bvzn
Aug 17 07:22:35.154: INFO: Created: latency-svc-wmq76
Aug 17 07:22:35.159: INFO: Created: latency-svc-wsgmq
Aug 17 07:22:35.178: INFO: Created: latency-svc-sjf8v
Aug 17 07:22:35.182: INFO: Created: latency-svc-z4mzp
Aug 17 07:22:35.187: INFO: Created: latency-svc-lgtgf
Aug 17 07:22:35.192: INFO: Created: latency-svc-tvbtq
Aug 17 07:22:35.196: INFO: Got endpoints: latency-svc-42hrs [165.578149ms]
Aug 17 07:22:35.197: INFO: Created: latency-svc-ht5r5
Aug 17 07:22:35.202: INFO: Created: latency-svc-xjzlq
Aug 17 07:22:35.206: INFO: Created: latency-svc-9zw8q
Aug 17 07:22:35.212: INFO: Created: latency-svc-srfxt
Aug 17 07:22:35.216: INFO: Created: latency-svc-d98dg
Aug 17 07:22:35.220: INFO: Created: latency-svc-94td9
Aug 17 07:22:35.247: INFO: Got endpoints: latency-svc-n94gm [209.910376ms]
Aug 17 07:22:35.255: INFO: Created: latency-svc-7jxvq
Aug 17 07:22:35.298: INFO: Got endpoints: latency-svc-2w2mh [254.502001ms]
Aug 17 07:22:35.304: INFO: Created: latency-svc-tqj6z
Aug 17 07:22:35.347: INFO: Got endpoints: latency-svc-8bvzn [285.783356ms]
Aug 17 07:22:35.356: INFO: Created: latency-svc-fjsjm
Aug 17 07:22:35.400: INFO: Got endpoints: latency-svc-wmq76 [331.961971ms]
Aug 17 07:22:35.411: INFO: Created: latency-svc-q7f98
Aug 17 07:22:35.447: INFO: Got endpoints: latency-svc-wsgmq [372.260803ms]
Aug 17 07:22:35.461: INFO: Created: latency-svc-l9mvm
Aug 17 07:22:35.497: INFO: Got endpoints: latency-svc-sjf8v [416.400529ms]
Aug 17 07:22:35.507: INFO: Created: latency-svc-5phdk
Aug 17 07:22:35.546: INFO: Got endpoints: latency-svc-z4mzp [460.800774ms]
Aug 17 07:22:35.558: INFO: Created: latency-svc-v4tzs
Aug 17 07:22:35.597: INFO: Got endpoints: latency-svc-lgtgf [503.501138ms]
Aug 17 07:22:35.606: INFO: Created: latency-svc-wdxg8
Aug 17 07:22:35.646: INFO: Got endpoints: latency-svc-tvbtq [546.096246ms]
Aug 17 07:22:35.653: INFO: Created: latency-svc-8dc9s
Aug 17 07:22:35.698: INFO: Got endpoints: latency-svc-ht5r5 [592.311322ms]
Aug 17 07:22:35.706: INFO: Created: latency-svc-sfpdb
Aug 17 07:22:35.747: INFO: Got endpoints: latency-svc-xjzlq [629.799412ms]
Aug 17 07:22:35.756: INFO: Created: latency-svc-mkmjv
Aug 17 07:22:35.797: INFO: Got endpoints: latency-svc-9zw8q [673.715861ms]
Aug 17 07:22:35.808: INFO: Created: latency-svc-lbh72
Aug 17 07:22:35.847: INFO: Got endpoints: latency-svc-srfxt [717.931165ms]
Aug 17 07:22:35.855: INFO: Created: latency-svc-6kxlw
Aug 17 07:22:35.898: INFO: Got endpoints: latency-svc-d98dg [748.812342ms]
Aug 17 07:22:35.908: INFO: Created: latency-svc-dhvc9
Aug 17 07:22:35.952: INFO: Got endpoints: latency-svc-94td9 [756.192638ms]
Aug 17 07:22:35.963: INFO: Created: latency-svc-slncf
Aug 17 07:22:35.996: INFO: Got endpoints: latency-svc-7jxvq [748.779177ms]
Aug 17 07:22:36.005: INFO: Created: latency-svc-ws2bg
Aug 17 07:22:36.047: INFO: Got endpoints: latency-svc-tqj6z [748.760907ms]
Aug 17 07:22:36.055: INFO: Created: latency-svc-lc7jf
Aug 17 07:22:36.098: INFO: Got endpoints: latency-svc-fjsjm [750.689958ms]
Aug 17 07:22:36.106: INFO: Created: latency-svc-4gvf6
Aug 17 07:22:36.147: INFO: Got endpoints: latency-svc-q7f98 [746.955874ms]
Aug 17 07:22:36.193: INFO: Created: latency-svc-vl7zn
Aug 17 07:22:36.210: INFO: Got endpoints: latency-svc-l9mvm [763.348233ms]
Aug 17 07:22:36.221: INFO: Created: latency-svc-8x2kd
Aug 17 07:22:36.247: INFO: Got endpoints: latency-svc-5phdk [749.669115ms]
Aug 17 07:22:36.254: INFO: Created: latency-svc-zv7gb
Aug 17 07:22:36.297: INFO: Got endpoints: latency-svc-v4tzs [750.56772ms]
Aug 17 07:22:36.304: INFO: Created: latency-svc-tb92z
Aug 17 07:22:36.347: INFO: Got endpoints: latency-svc-wdxg8 [750.158982ms]
Aug 17 07:22:36.354: INFO: Created: latency-svc-2fv8p
Aug 17 07:22:36.397: INFO: Got endpoints: latency-svc-8dc9s [751.601474ms]
Aug 17 07:22:36.404: INFO: Created: latency-svc-hskfn
Aug 17 07:22:36.447: INFO: Got endpoints: latency-svc-sfpdb [748.818941ms]
Aug 17 07:22:36.454: INFO: Created: latency-svc-vfhj5
Aug 17 07:22:36.498: INFO: Got endpoints: latency-svc-mkmjv [750.849673ms]
Aug 17 07:22:36.541: INFO: Created: latency-svc-nld2f
Aug 17 07:22:36.546: INFO: Got endpoints: latency-svc-lbh72 [748.990725ms]
Aug 17 07:22:36.562: INFO: Created: latency-svc-qxnqb
Aug 17 07:22:36.596: INFO: Got endpoints: latency-svc-6kxlw [749.094297ms]
Aug 17 07:22:36.603: INFO: Created: latency-svc-cnmdb
Aug 17 07:22:36.648: INFO: Got endpoints: latency-svc-dhvc9 [750.103559ms]
Aug 17 07:22:36.654: INFO: Created: latency-svc-ktmjh
Aug 17 07:22:36.697: INFO: Got endpoints: latency-svc-slncf [744.084438ms]
Aug 17 07:22:36.704: INFO: Created: latency-svc-tz4sm
Aug 17 07:22:36.746: INFO: Got endpoints: latency-svc-ws2bg [749.900548ms]
Aug 17 07:22:36.755: INFO: Created: latency-svc-6gj6r
Aug 17 07:22:36.797: INFO: Got endpoints: latency-svc-lc7jf [750.371646ms]
Aug 17 07:22:36.805: INFO: Created: latency-svc-pfxs9
Aug 17 07:22:36.847: INFO: Got endpoints: latency-svc-4gvf6 [749.290518ms]
Aug 17 07:22:36.855: INFO: Created: latency-svc-9kcbh
Aug 17 07:22:36.897: INFO: Got endpoints: latency-svc-vl7zn [750.812236ms]
Aug 17 07:22:36.907: INFO: Created: latency-svc-tmnhg
Aug 17 07:22:36.947: INFO: Got endpoints: latency-svc-8x2kd [736.04621ms]
Aug 17 07:22:36.954: INFO: Created: latency-svc-dpqpp
Aug 17 07:22:36.997: INFO: Got endpoints: latency-svc-zv7gb [750.471565ms]
Aug 17 07:22:37.004: INFO: Created: latency-svc-z9t6b
Aug 17 07:22:37.046: INFO: Got endpoints: latency-svc-tb92z [749.58836ms]
Aug 17 07:22:37.058: INFO: Created: latency-svc-m8rb7
Aug 17 07:22:37.097: INFO: Got endpoints: latency-svc-2fv8p [750.425927ms]
Aug 17 07:22:37.105: INFO: Created: latency-svc-nv5sl
Aug 17 07:22:37.148: INFO: Got endpoints: latency-svc-hskfn [750.086283ms]
Aug 17 07:22:37.156: INFO: Created: latency-svc-htgw2
Aug 17 07:22:37.197: INFO: Got endpoints: latency-svc-vfhj5 [749.683227ms]
Aug 17 07:22:37.205: INFO: Created: latency-svc-9s5fb
Aug 17 07:22:37.247: INFO: Got endpoints: latency-svc-nld2f [749.200996ms]
Aug 17 07:22:37.309: INFO: Got endpoints: latency-svc-qxnqb [763.220321ms]
Aug 17 07:22:37.310: INFO: Created: latency-svc-th5hd
Aug 17 07:22:37.317: INFO: Created: latency-svc-8scz6
Aug 17 07:22:37.347: INFO: Got endpoints: latency-svc-cnmdb [750.794615ms]
Aug 17 07:22:37.354: INFO: Created: latency-svc-v4q9w
Aug 17 07:22:37.397: INFO: Got endpoints: latency-svc-ktmjh [748.838377ms]
Aug 17 07:22:37.403: INFO: Created: latency-svc-pt794
Aug 17 07:22:37.448: INFO: Got endpoints: latency-svc-tz4sm [751.394601ms]
Aug 17 07:22:37.457: INFO: Created: latency-svc-zbjt6
Aug 17 07:22:37.497: INFO: Got endpoints: latency-svc-6gj6r [750.531923ms]
Aug 17 07:22:37.504: INFO: Created: latency-svc-84xvj
Aug 17 07:22:37.547: INFO: Got endpoints: latency-svc-pfxs9 [750.241467ms]
Aug 17 07:22:37.554: INFO: Created: latency-svc-4v4bt
Aug 17 07:22:37.598: INFO: Got endpoints: latency-svc-9kcbh [750.94914ms]
Aug 17 07:22:37.606: INFO: Created: latency-svc-2ngbc
Aug 17 07:22:37.647: INFO: Got endpoints: latency-svc-tmnhg [749.588438ms]
Aug 17 07:22:37.653: INFO: Created: latency-svc-9ppgp
Aug 17 07:22:37.696: INFO: Got endpoints: latency-svc-dpqpp [749.415079ms]
Aug 17 07:22:37.703: INFO: Created: latency-svc-6xnc8
Aug 17 07:22:37.746: INFO: Got endpoints: latency-svc-z9t6b [748.918676ms]
Aug 17 07:22:37.754: INFO: Created: latency-svc-bqc9x
Aug 17 07:22:37.796: INFO: Got endpoints: latency-svc-m8rb7 [749.541696ms]
Aug 17 07:22:37.803: INFO: Created: latency-svc-b6ckf
Aug 17 07:22:37.846: INFO: Got endpoints: latency-svc-nv5sl [749.130837ms]
Aug 17 07:22:37.853: INFO: Created: latency-svc-72jq4
Aug 17 07:22:37.899: INFO: Got endpoints: latency-svc-htgw2 [751.806625ms]
Aug 17 07:22:37.906: INFO: Created: latency-svc-9bd4q
Aug 17 07:22:37.949: INFO: Got endpoints: latency-svc-9s5fb [751.86496ms]
Aug 17 07:22:37.957: INFO: Created: latency-svc-2chcr
Aug 17 07:22:37.996: INFO: Got endpoints: latency-svc-th5hd [748.978096ms]
Aug 17 07:22:38.063: INFO: Got endpoints: latency-svc-8scz6 [753.210538ms]
Aug 17 07:22:38.065: INFO: Created: latency-svc-9lm2f
Aug 17 07:22:38.070: INFO: Created: latency-svc-c7xxl
Aug 17 07:22:38.097: INFO: Got endpoints: latency-svc-v4q9w [749.955841ms]
Aug 17 07:22:38.104: INFO: Created: latency-svc-2xc7d
Aug 17 07:22:38.146: INFO: Got endpoints: latency-svc-pt794 [749.281946ms]
Aug 17 07:22:38.153: INFO: Created: latency-svc-zcqsr
Aug 17 07:22:38.197: INFO: Got endpoints: latency-svc-zbjt6 [749.035406ms]
Aug 17 07:22:38.205: INFO: Created: latency-svc-7xt49
Aug 17 07:22:38.247: INFO: Got endpoints: latency-svc-84xvj [750.541612ms]
Aug 17 07:22:38.258: INFO: Created: latency-svc-5fvcg
Aug 17 07:22:38.297: INFO: Got endpoints: latency-svc-4v4bt [749.735003ms]
Aug 17 07:22:38.305: INFO: Created: latency-svc-ccqwk
Aug 17 07:22:38.348: INFO: Got endpoints: latency-svc-2ngbc [750.005378ms]
Aug 17 07:22:38.355: INFO: Created: latency-svc-k8v4h
Aug 17 07:22:38.397: INFO: Got endpoints: latency-svc-9ppgp [749.395968ms]
Aug 17 07:22:38.404: INFO: Created: latency-svc-65db7
Aug 17 07:22:38.447: INFO: Got endpoints: latency-svc-6xnc8 [750.764531ms]
Aug 17 07:22:38.454: INFO: Created: latency-svc-lvs2v
Aug 17 07:22:38.497: INFO: Got endpoints: latency-svc-bqc9x [750.604645ms]
Aug 17 07:22:38.504: INFO: Created: latency-svc-szvlg
Aug 17 07:22:38.547: INFO: Got endpoints: latency-svc-b6ckf [751.135817ms]
Aug 17 07:22:38.554: INFO: Created: latency-svc-zr2mt
Aug 17 07:22:38.597: INFO: Got endpoints: latency-svc-72jq4 [750.27625ms]
Aug 17 07:22:38.603: INFO: Created: latency-svc-pjj6t
Aug 17 07:22:38.647: INFO: Got endpoints: latency-svc-9bd4q [747.356457ms]
Aug 17 07:22:38.653: INFO: Created: latency-svc-dr8kk
Aug 17 07:22:38.696: INFO: Got endpoints: latency-svc-2chcr [747.280686ms]
Aug 17 07:22:38.703: INFO: Created: latency-svc-9rfwd
Aug 17 07:22:38.747: INFO: Got endpoints: latency-svc-9lm2f [750.445797ms]
Aug 17 07:22:38.753: INFO: Created: latency-svc-j2wg2
Aug 17 07:22:38.797: INFO: Got endpoints: latency-svc-c7xxl [734.742057ms]
Aug 17 07:22:38.804: INFO: Created: latency-svc-st87t
Aug 17 07:22:38.847: INFO: Got endpoints: latency-svc-2xc7d [749.990894ms]
Aug 17 07:22:38.855: INFO: Created: latency-svc-5tjmr
Aug 17 07:22:38.896: INFO: Got endpoints: latency-svc-zcqsr [750.561131ms]
Aug 17 07:22:38.904: INFO: Created: latency-svc-gwqzt
Aug 17 07:22:38.947: INFO: Got endpoints: latency-svc-7xt49 [750.067027ms]
Aug 17 07:22:38.956: INFO: Created: latency-svc-tt8pd
Aug 17 07:22:38.997: INFO: Got endpoints: latency-svc-5fvcg [749.413361ms]
Aug 17 07:22:39.004: INFO: Created: latency-svc-8wm6f
Aug 17 07:22:39.047: INFO: Got endpoints: latency-svc-ccqwk [749.672889ms]
Aug 17 07:22:39.053: INFO: Created: latency-svc-dgnsv
Aug 17 07:22:39.098: INFO: Got endpoints: latency-svc-k8v4h [749.535822ms]
Aug 17 07:22:39.105: INFO: Created: latency-svc-4dhpp
Aug 17 07:22:39.147: INFO: Got endpoints: latency-svc-65db7 [750.508385ms]
Aug 17 07:22:39.154: INFO: Created: latency-svc-t7649
Aug 17 07:22:39.197: INFO: Got endpoints: latency-svc-lvs2v [750.097213ms]
Aug 17 07:22:39.206: INFO: Created: latency-svc-jkrl4
Aug 17 07:22:39.247: INFO: Got endpoints: latency-svc-szvlg [749.577016ms]
Aug 17 07:22:39.254: INFO: Created: latency-svc-lhr8v
Aug 17 07:22:39.299: INFO: Got endpoints: latency-svc-zr2mt [751.657419ms]
Aug 17 07:22:39.306: INFO: Created: latency-svc-ppsg7
Aug 17 07:22:39.348: INFO: Got endpoints: latency-svc-pjj6t [750.881101ms]
Aug 17 07:22:39.357: INFO: Created: latency-svc-m848p
Aug 17 07:22:39.397: INFO: Got endpoints: latency-svc-dr8kk [749.71225ms]
Aug 17 07:22:39.420: INFO: Created: latency-svc-nq587
Aug 17 07:22:39.447: INFO: Got endpoints: latency-svc-9rfwd [751.04118ms]
Aug 17 07:22:39.455: INFO: Created: latency-svc-jg6zf
Aug 17 07:22:39.498: INFO: Got endpoints: latency-svc-j2wg2 [750.960642ms]
Aug 17 07:22:39.507: INFO: Created: latency-svc-4vdr5
Aug 17 07:22:39.564: INFO: Got endpoints: latency-svc-st87t [766.618962ms]
Aug 17 07:22:39.572: INFO: Created: latency-svc-z84r7
Aug 17 07:22:39.596: INFO: Got endpoints: latency-svc-5tjmr [749.339049ms]
Aug 17 07:22:39.603: INFO: Created: latency-svc-6s8lc
Aug 17 07:22:39.647: INFO: Got endpoints: latency-svc-gwqzt [750.224372ms]
Aug 17 07:22:39.655: INFO: Created: latency-svc-rrf9b
Aug 17 07:22:39.697: INFO: Got endpoints: latency-svc-tt8pd [750.265524ms]
Aug 17 07:22:39.705: INFO: Created: latency-svc-2s9f7
Aug 17 07:22:39.747: INFO: Got endpoints: latency-svc-8wm6f [750.594035ms]
Aug 17 07:22:39.755: INFO: Created: latency-svc-xgnsh
Aug 17 07:22:39.796: INFO: Got endpoints: latency-svc-dgnsv [749.468089ms]
Aug 17 07:22:39.806: INFO: Created: latency-svc-24mtt
Aug 17 07:22:39.846: INFO: Got endpoints: latency-svc-4dhpp [748.3181ms]
Aug 17 07:22:39.855: INFO: Created: latency-svc-7vpvl
Aug 17 07:22:39.896: INFO: Got endpoints: latency-svc-t7649 [749.074117ms]
Aug 17 07:22:39.905: INFO: Created: latency-svc-28cmw
Aug 17 07:22:39.946: INFO: Got endpoints: latency-svc-jkrl4 [749.403714ms]
Aug 17 07:22:39.958: INFO: Created: latency-svc-spfkd
Aug 17 07:22:39.998: INFO: Got endpoints: latency-svc-lhr8v [750.876579ms]
Aug 17 07:22:40.007: INFO: Created: latency-svc-hj86l
Aug 17 07:22:40.050: INFO: Got endpoints: latency-svc-ppsg7 [751.157865ms]
Aug 17 07:22:40.058: INFO: Created: latency-svc-xl2dt
Aug 17 07:22:40.097: INFO: Got endpoints: latency-svc-m848p [749.452459ms]
Aug 17 07:22:40.106: INFO: Created: latency-svc-gscdl
Aug 17 07:22:40.147: INFO: Got endpoints: latency-svc-nq587 [750.188278ms]
Aug 17 07:22:40.154: INFO: Created: latency-svc-s4nkf
Aug 17 07:22:40.196: INFO: Got endpoints: latency-svc-jg6zf [748.955653ms]
Aug 17 07:22:40.204: INFO: Created: latency-svc-jxm9l
Aug 17 07:22:40.247: INFO: Got endpoints: latency-svc-4vdr5 [748.885095ms]
Aug 17 07:22:40.254: INFO: Created: latency-svc-nf6kq
Aug 17 07:22:40.315: INFO: Got endpoints: latency-svc-z84r7 [751.002244ms]
Aug 17 07:22:40.323: INFO: Created: latency-svc-xbtwr
Aug 17 07:22:40.347: INFO: Got endpoints: latency-svc-6s8lc [750.566357ms]
Aug 17 07:22:40.355: INFO: Created: latency-svc-hbfzz
Aug 17 07:22:40.396: INFO: Got endpoints: latency-svc-rrf9b [749.537508ms]
Aug 17 07:22:40.404: INFO: Created: latency-svc-7d9d5
Aug 17 07:22:40.448: INFO: Got endpoints: latency-svc-2s9f7 [750.39714ms]
Aug 17 07:22:40.458: INFO: Created: latency-svc-744fk
Aug 17 07:22:40.497: INFO: Got endpoints: latency-svc-xgnsh [749.84884ms]
Aug 17 07:22:40.505: INFO: Created: latency-svc-8z85m
Aug 17 07:22:40.547: INFO: Got endpoints: latency-svc-24mtt [750.788634ms]
Aug 17 07:22:40.555: INFO: Created: latency-svc-4tcvb
Aug 17 07:22:40.596: INFO: Got endpoints: latency-svc-7vpvl [750.450626ms]
Aug 17 07:22:40.604: INFO: Created: latency-svc-gnzcv
Aug 17 07:22:40.647: INFO: Got endpoints: latency-svc-28cmw [751.051417ms]
Aug 17 07:22:40.655: INFO: Created: latency-svc-5vf6z
Aug 17 07:22:40.697: INFO: Got endpoints: latency-svc-spfkd [750.130157ms]
Aug 17 07:22:40.704: INFO: Created: latency-svc-g4t72
Aug 17 07:22:40.747: INFO: Got endpoints: latency-svc-hj86l [749.529919ms]
Aug 17 07:22:40.756: INFO: Created: latency-svc-pn57v
Aug 17 07:22:40.797: INFO: Got endpoints: latency-svc-xl2dt [747.2242ms]
Aug 17 07:22:40.804: INFO: Created: latency-svc-mcf75
Aug 17 07:22:40.846: INFO: Got endpoints: latency-svc-gscdl [748.876193ms]
Aug 17 07:22:40.854: INFO: Created: latency-svc-hn4hf
Aug 17 07:22:40.897: INFO: Got endpoints: latency-svc-s4nkf [750.338053ms]
Aug 17 07:22:40.906: INFO: Created: latency-svc-kcbzr
Aug 17 07:22:40.947: INFO: Got endpoints: latency-svc-jxm9l [751.027657ms]
Aug 17 07:22:40.959: INFO: Created: latency-svc-42cm6
Aug 17 07:22:40.997: INFO: Got endpoints: latency-svc-nf6kq [750.605057ms]
Aug 17 07:22:41.005: INFO: Created: latency-svc-5k44x
Aug 17 07:22:41.047: INFO: Got endpoints: latency-svc-xbtwr [731.537321ms]
Aug 17 07:22:41.079: INFO: Created: latency-svc-hh8ts
Aug 17 07:22:41.111: INFO: Got endpoints: latency-svc-hbfzz [763.901267ms]
Aug 17 07:22:41.135: INFO: Created: latency-svc-jxplq
Aug 17 07:22:41.147: INFO: Got endpoints: latency-svc-7d9d5 [750.770595ms]
Aug 17 07:22:41.155: INFO: Created: latency-svc-s4zgp
Aug 17 07:22:41.197: INFO: Got endpoints: latency-svc-744fk [749.200265ms]
Aug 17 07:22:41.203: INFO: Created: latency-svc-f425b
Aug 17 07:22:41.247: INFO: Got endpoints: latency-svc-8z85m [749.750666ms]
Aug 17 07:22:41.256: INFO: Created: latency-svc-sfcvt
Aug 17 07:22:41.298: INFO: Got endpoints: latency-svc-4tcvb [751.145032ms]
Aug 17 07:22:41.306: INFO: Created: latency-svc-d7z4f
Aug 17 07:22:41.350: INFO: Got endpoints: latency-svc-gnzcv [753.792623ms]
Aug 17 07:22:41.357: INFO: Created: latency-svc-4xxvg
Aug 17 07:22:41.396: INFO: Got endpoints: latency-svc-5vf6z [748.77815ms]
Aug 17 07:22:41.403: INFO: Created: latency-svc-xknnd
Aug 17 07:22:41.447: INFO: Got endpoints: latency-svc-g4t72 [750.127265ms]
Aug 17 07:22:41.455: INFO: Created: latency-svc-585qf
Aug 17 07:22:41.497: INFO: Got endpoints: latency-svc-pn57v [749.905962ms]
Aug 17 07:22:41.506: INFO: Created: latency-svc-v8ttz
Aug 17 07:22:41.548: INFO: Got endpoints: latency-svc-mcf75 [750.331352ms]
Aug 17 07:22:41.558: INFO: Created: latency-svc-99jkn
Aug 17 07:22:41.596: INFO: Got endpoints: latency-svc-hn4hf [750.223141ms]
Aug 17 07:22:41.604: INFO: Created: latency-svc-7x2vv
Aug 17 07:22:41.648: INFO: Got endpoints: latency-svc-kcbzr [750.524556ms]
Aug 17 07:22:41.654: INFO: Created: latency-svc-r9bng
Aug 17 07:22:41.697: INFO: Got endpoints: latency-svc-42cm6 [749.146099ms]
Aug 17 07:22:41.705: INFO: Created: latency-svc-fnx2n
Aug 17 07:22:41.748: INFO: Got endpoints: latency-svc-5k44x [750.737445ms]
Aug 17 07:22:41.756: INFO: Created: latency-svc-9pfrp
Aug 17 07:22:41.797: INFO: Got endpoints: latency-svc-hh8ts [750.128432ms]
Aug 17 07:22:41.805: INFO: Created: latency-svc-jn6hv
Aug 17 07:22:41.847: INFO: Got endpoints: latency-svc-jxplq [736.499919ms]
Aug 17 07:22:41.856: INFO: Created: latency-svc-r8hq8
Aug 17 07:22:41.896: INFO: Got endpoints: latency-svc-s4zgp [749.017397ms]
Aug 17 07:22:41.904: INFO: Created: latency-svc-s2pxz
Aug 17 07:22:41.947: INFO: Got endpoints: latency-svc-f425b [750.21456ms]
Aug 17 07:22:41.957: INFO: Created: latency-svc-6blvk
Aug 17 07:22:41.997: INFO: Got endpoints: latency-svc-sfcvt [749.541249ms]
Aug 17 07:22:42.004: INFO: Created: latency-svc-bjr2n
Aug 17 07:22:42.048: INFO: Got endpoints: latency-svc-d7z4f [749.429348ms]
Aug 17 07:22:42.055: INFO: Created: latency-svc-6frr8
Aug 17 07:22:42.097: INFO: Got endpoints: latency-svc-4xxvg [746.735851ms]
Aug 17 07:22:42.104: INFO: Created: latency-svc-tf4cp
Aug 17 07:22:42.147: INFO: Got endpoints: latency-svc-xknnd [750.644068ms]
Aug 17 07:22:42.154: INFO: Created: latency-svc-jkcp7
Aug 17 07:22:42.197: INFO: Got endpoints: latency-svc-585qf [750.538105ms]
Aug 17 07:22:42.205: INFO: Created: latency-svc-s4xsc
Aug 17 07:22:42.247: INFO: Got endpoints: latency-svc-v8ttz [749.416138ms]
Aug 17 07:22:42.254: INFO: Created: latency-svc-dvl2m
Aug 17 07:22:42.297: INFO: Got endpoints: latency-svc-99jkn [749.219564ms]
Aug 17 07:22:42.304: INFO: Created: latency-svc-h9mxv
Aug 17 07:22:42.347: INFO: Got endpoints: latency-svc-7x2vv [750.498227ms]
Aug 17 07:22:42.354: INFO: Created: latency-svc-mst8v
Aug 17 07:22:42.397: INFO: Got endpoints: latency-svc-r9bng [749.066955ms]
Aug 17 07:22:42.404: INFO: Created: latency-svc-7mm5q
Aug 17 07:22:42.447: INFO: Got endpoints: latency-svc-fnx2n [750.202182ms]
Aug 17 07:22:42.455: INFO: Created: latency-svc-l6xpk
Aug 17 07:22:42.497: INFO: Got endpoints: latency-svc-9pfrp [748.596511ms]
Aug 17 07:22:42.504: INFO: Created: latency-svc-vbjmt
Aug 17 07:22:42.547: INFO: Got endpoints: latency-svc-jn6hv [750.340417ms]
Aug 17 07:22:42.555: INFO: Created: latency-svc-hzpqg
Aug 17 07:22:42.597: INFO: Got endpoints: latency-svc-r8hq8 [749.399432ms]
Aug 17 07:22:42.604: INFO: Created: latency-svc-jq4tz
Aug 17 07:22:42.647: INFO: Got endpoints: latency-svc-s2pxz [750.458692ms]
Aug 17 07:22:42.653: INFO: Created: latency-svc-hg8cf
Aug 17 07:22:42.696: INFO: Got endpoints: latency-svc-6blvk [748.859039ms]
Aug 17 07:22:42.705: INFO: Created: latency-svc-q7t42
Aug 17 07:22:42.747: INFO: Got endpoints: latency-svc-bjr2n [750.447835ms]
Aug 17 07:22:42.797: INFO: Got endpoints: latency-svc-6frr8 [749.33466ms]
Aug 17 07:22:42.846: INFO: Got endpoints: latency-svc-tf4cp [749.157924ms]
Aug 17 07:22:42.898: INFO: Got endpoints: latency-svc-jkcp7 [750.745333ms]
Aug 17 07:22:42.947: INFO: Got endpoints: latency-svc-s4xsc [749.797144ms]
Aug 17 07:22:42.998: INFO: Got endpoints: latency-svc-dvl2m [751.688782ms]
Aug 17 07:22:43.047: INFO: Got endpoints: latency-svc-h9mxv [749.906795ms]
Aug 17 07:22:43.097: INFO: Got endpoints: latency-svc-mst8v [750.26624ms]
Aug 17 07:22:43.147: INFO: Got endpoints: latency-svc-7mm5q [749.942736ms]
Aug 17 07:22:43.197: INFO: Got endpoints: latency-svc-l6xpk [750.394435ms]
Aug 17 07:22:43.247: INFO: Got endpoints: latency-svc-vbjmt [750.108871ms]
Aug 17 07:22:43.298: INFO: Got endpoints: latency-svc-hzpqg [750.719338ms]
Aug 17 07:22:43.358: INFO: Got endpoints: latency-svc-jq4tz [761.598107ms]
Aug 17 07:22:43.396: INFO: Got endpoints: latency-svc-hg8cf [749.723095ms]
Aug 17 07:22:43.447: INFO: Got endpoints: latency-svc-q7t42 [750.354801ms]
Aug 17 07:22:43.447: INFO: Latencies: [13.952789ms 16.201693ms 21.700299ms 34.036291ms 37.475214ms 57.426257ms 60.454102ms 65.902546ms 71.094575ms 75.02446ms 82.777275ms 83.295546ms 85.984742ms 88.541149ms 92.604856ms 95.272147ms 96.440347ms 98.474434ms 99.103211ms 99.476514ms 99.610293ms 99.811216ms 100.425792ms 101.049148ms 102.658107ms 102.824391ms 103.295471ms 103.515906ms 103.930253ms 105.099447ms 107.716162ms 107.854493ms 110.110233ms 123.119329ms 165.578149ms 209.910376ms 254.502001ms 285.783356ms 331.961971ms 372.260803ms 416.400529ms 460.800774ms 503.501138ms 546.096246ms 592.311322ms 629.799412ms 673.715861ms 717.931165ms 731.537321ms 734.742057ms 736.04621ms 736.499919ms 744.084438ms 746.735851ms 746.955874ms 747.2242ms 747.280686ms 747.356457ms 748.3181ms 748.596511ms 748.760907ms 748.77815ms 748.779177ms 748.812342ms 748.818941ms 748.838377ms 748.859039ms 748.876193ms 748.885095ms 748.918676ms 748.955653ms 748.978096ms 748.990725ms 749.017397ms 749.035406ms 749.066955ms 749.074117ms 749.094297ms 749.130837ms 749.146099ms 749.157924ms 749.200265ms 749.200996ms 749.219564ms 749.281946ms 749.290518ms 749.33466ms 749.339049ms 749.395968ms 749.399432ms 749.403714ms 749.413361ms 749.415079ms 749.416138ms 749.429348ms 749.452459ms 749.468089ms 749.529919ms 749.535822ms 749.537508ms 749.541249ms 749.541696ms 749.577016ms 749.58836ms 749.588438ms 749.669115ms 749.672889ms 749.683227ms 749.71225ms 749.723095ms 749.735003ms 749.750666ms 749.797144ms 749.84884ms 749.900548ms 749.905962ms 749.906795ms 749.942736ms 749.955841ms 749.990894ms 750.005378ms 750.067027ms 750.086283ms 750.097213ms 750.103559ms 750.108871ms 750.127265ms 750.128432ms 750.130157ms 750.158982ms 750.188278ms 750.202182ms 750.21456ms 750.223141ms 750.224372ms 750.241467ms 750.265524ms 750.26624ms 750.27625ms 750.331352ms 750.338053ms 750.340417ms 750.354801ms 750.371646ms 750.394435ms 750.39714ms 750.425927ms 750.445797ms 750.447835ms 750.450626ms 750.458692ms 750.471565ms 750.498227ms 750.508385ms 750.524556ms 750.531923ms 750.538105ms 750.541612ms 750.561131ms 750.566357ms 750.56772ms 750.594035ms 750.604645ms 750.605057ms 750.644068ms 750.689958ms 750.719338ms 750.737445ms 750.745333ms 750.764531ms 750.770595ms 750.788634ms 750.794615ms 750.812236ms 750.849673ms 750.876579ms 750.881101ms 750.94914ms 750.960642ms 751.002244ms 751.027657ms 751.04118ms 751.051417ms 751.135817ms 751.145032ms 751.157865ms 751.394601ms 751.601474ms 751.657419ms 751.688782ms 751.806625ms 751.86496ms 753.210538ms 753.792623ms 756.192638ms 761.598107ms 763.220321ms 763.348233ms 763.901267ms 766.618962ms]
Aug 17 07:22:43.447: INFO: 50 %ile: 749.541249ms
Aug 17 07:22:43.447: INFO: 90 %ile: 751.027657ms
Aug 17 07:22:43.447: INFO: 99 %ile: 763.901267ms
Aug 17 07:22:43.447: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:43.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7674" for this suite. 08/17/23 07:22:43.451
------------------------------
â€¢ [SLOW TEST] [10.747 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:32.708
    Aug 17 07:22:32.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svc-latency 08/17/23 07:22:32.709
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:32.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:32.719
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 17 07:22:32.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7674 08/17/23 07:22:32.722
    I0817 07:22:32.732377      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7674, replica count: 1
    I0817 07:22:33.783171      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0817 07:22:34.783850      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 07:22:34.892: INFO: Created: latency-svc-w4tw7
    Aug 17 07:22:34.896: INFO: Got endpoints: latency-svc-w4tw7 [11.956143ms]
    Aug 17 07:22:34.906: INFO: Created: latency-svc-t2hm7
    Aug 17 07:22:34.910: INFO: Got endpoints: latency-svc-t2hm7 [13.952789ms]
    Aug 17 07:22:34.911: INFO: Created: latency-svc-phcvg
    Aug 17 07:22:34.913: INFO: Got endpoints: latency-svc-phcvg [16.201693ms]
    Aug 17 07:22:34.915: INFO: Created: latency-svc-tqphd
    Aug 17 07:22:34.918: INFO: Got endpoints: latency-svc-tqphd [21.700299ms]
    Aug 17 07:22:34.922: INFO: Created: latency-svc-7gkb7
    Aug 17 07:22:34.930: INFO: Got endpoints: latency-svc-7gkb7 [34.036291ms]
    Aug 17 07:22:34.931: INFO: Created: latency-svc-9sf2h
    Aug 17 07:22:34.934: INFO: Got endpoints: latency-svc-9sf2h [37.475214ms]
    Aug 17 07:22:34.948: INFO: Created: latency-svc-dxpmb
    Aug 17 07:22:34.954: INFO: Created: latency-svc-4gp75
    Aug 17 07:22:34.954: INFO: Got endpoints: latency-svc-dxpmb [57.426257ms]
    Aug 17 07:22:34.957: INFO: Got endpoints: latency-svc-4gp75 [60.454102ms]
    Aug 17 07:22:34.959: INFO: Created: latency-svc-fbfd9
    Aug 17 07:22:34.963: INFO: Got endpoints: latency-svc-fbfd9 [65.902546ms]
    Aug 17 07:22:34.964: INFO: Created: latency-svc-b9pmd
    Aug 17 07:22:34.968: INFO: Got endpoints: latency-svc-b9pmd [71.094575ms]
    Aug 17 07:22:34.970: INFO: Created: latency-svc-sk2qn
    Aug 17 07:22:34.972: INFO: Got endpoints: latency-svc-sk2qn [75.02446ms]
    Aug 17 07:22:34.979: INFO: Created: latency-svc-rlksl
    Aug 17 07:22:34.980: INFO: Got endpoints: latency-svc-rlksl [82.777275ms]
    Aug 17 07:22:34.981: INFO: Created: latency-svc-f8l65
    Aug 17 07:22:34.985: INFO: Got endpoints: latency-svc-f8l65 [88.541149ms]
    Aug 17 07:22:34.986: INFO: Created: latency-svc-zm6g4
    Aug 17 07:22:34.990: INFO: Got endpoints: latency-svc-zm6g4 [92.604856ms]
    Aug 17 07:22:34.991: INFO: Created: latency-svc-sv2lv
    Aug 17 07:22:34.996: INFO: Got endpoints: latency-svc-sv2lv [99.476514ms]
    Aug 17 07:22:34.998: INFO: Created: latency-svc-lbglz
    Aug 17 07:22:35.001: INFO: Got endpoints: latency-svc-lbglz [103.930253ms]
    Aug 17 07:22:35.005: INFO: Created: latency-svc-fmncw
    Aug 17 07:22:35.010: INFO: Got endpoints: latency-svc-fmncw [99.103211ms]
    Aug 17 07:22:35.011: INFO: Created: latency-svc-v2xhf
    Aug 17 07:22:35.016: INFO: Got endpoints: latency-svc-v2xhf [102.658107ms]
    Aug 17 07:22:35.017: INFO: Created: latency-svc-ltfss
    Aug 17 07:22:35.019: INFO: Got endpoints: latency-svc-ltfss [100.425792ms]
    Aug 17 07:22:35.022: INFO: Created: latency-svc-v9qjx
    Aug 17 07:22:35.026: INFO: Got endpoints: latency-svc-v9qjx [95.272147ms]
    Aug 17 07:22:35.028: INFO: Created: latency-svc-8xnfc
    Aug 17 07:22:35.031: INFO: Got endpoints: latency-svc-8xnfc [96.440347ms]
    Aug 17 07:22:35.033: INFO: Created: latency-svc-7f77p
    Aug 17 07:22:35.037: INFO: Got endpoints: latency-svc-7f77p [83.295546ms]
    Aug 17 07:22:35.040: INFO: Created: latency-svc-br4bd
    Aug 17 07:22:35.043: INFO: Got endpoints: latency-svc-br4bd [85.984742ms]
    Aug 17 07:22:35.043: INFO: Created: latency-svc-sgvjl
    Aug 17 07:22:35.061: INFO: Got endpoints: latency-svc-sgvjl [98.474434ms]
    Aug 17 07:22:35.063: INFO: Created: latency-svc-t5p5j
    Aug 17 07:22:35.068: INFO: Got endpoints: latency-svc-t5p5j [99.610293ms]
    Aug 17 07:22:35.070: INFO: Created: latency-svc-msp94
    Aug 17 07:22:35.075: INFO: Got endpoints: latency-svc-msp94 [102.824391ms]
    Aug 17 07:22:35.077: INFO: Created: latency-svc-ljfpp
    Aug 17 07:22:35.081: INFO: Got endpoints: latency-svc-ljfpp [101.049148ms]
    Aug 17 07:22:35.083: INFO: Created: latency-svc-s9qd5
    Aug 17 07:22:35.085: INFO: Got endpoints: latency-svc-s9qd5 [99.811216ms]
    Aug 17 07:22:35.090: INFO: Created: latency-svc-qv4mg
    Aug 17 07:22:35.093: INFO: Got endpoints: latency-svc-qv4mg [103.515906ms]
    Aug 17 07:22:35.096: INFO: Created: latency-svc-bmq8s
    Aug 17 07:22:35.100: INFO: Got endpoints: latency-svc-bmq8s [103.295471ms]
    Aug 17 07:22:35.102: INFO: Created: latency-svc-rnqgz
    Aug 17 07:22:35.106: INFO: Got endpoints: latency-svc-rnqgz [105.099447ms]
    Aug 17 07:22:35.108: INFO: Created: latency-svc-bwzhw
    Aug 17 07:22:35.117: INFO: Got endpoints: latency-svc-bwzhw [107.854493ms]
    Aug 17 07:22:35.120: INFO: Created: latency-svc-hfjmg
    Aug 17 07:22:35.123: INFO: Got endpoints: latency-svc-hfjmg [107.716162ms]
    Aug 17 07:22:35.125: INFO: Created: latency-svc-f2r5f
    Aug 17 07:22:35.129: INFO: Got endpoints: latency-svc-f2r5f [110.110233ms]
    Aug 17 07:22:35.131: INFO: Created: latency-svc-crf5v
    Aug 17 07:22:35.136: INFO: Created: latency-svc-42hrs
    Aug 17 07:22:35.141: INFO: Created: latency-svc-n94gm
    Aug 17 07:22:35.145: INFO: Created: latency-svc-2w2mh
    Aug 17 07:22:35.149: INFO: Got endpoints: latency-svc-crf5v [123.119329ms]
    Aug 17 07:22:35.149: INFO: Created: latency-svc-8bvzn
    Aug 17 07:22:35.154: INFO: Created: latency-svc-wmq76
    Aug 17 07:22:35.159: INFO: Created: latency-svc-wsgmq
    Aug 17 07:22:35.178: INFO: Created: latency-svc-sjf8v
    Aug 17 07:22:35.182: INFO: Created: latency-svc-z4mzp
    Aug 17 07:22:35.187: INFO: Created: latency-svc-lgtgf
    Aug 17 07:22:35.192: INFO: Created: latency-svc-tvbtq
    Aug 17 07:22:35.196: INFO: Got endpoints: latency-svc-42hrs [165.578149ms]
    Aug 17 07:22:35.197: INFO: Created: latency-svc-ht5r5
    Aug 17 07:22:35.202: INFO: Created: latency-svc-xjzlq
    Aug 17 07:22:35.206: INFO: Created: latency-svc-9zw8q
    Aug 17 07:22:35.212: INFO: Created: latency-svc-srfxt
    Aug 17 07:22:35.216: INFO: Created: latency-svc-d98dg
    Aug 17 07:22:35.220: INFO: Created: latency-svc-94td9
    Aug 17 07:22:35.247: INFO: Got endpoints: latency-svc-n94gm [209.910376ms]
    Aug 17 07:22:35.255: INFO: Created: latency-svc-7jxvq
    Aug 17 07:22:35.298: INFO: Got endpoints: latency-svc-2w2mh [254.502001ms]
    Aug 17 07:22:35.304: INFO: Created: latency-svc-tqj6z
    Aug 17 07:22:35.347: INFO: Got endpoints: latency-svc-8bvzn [285.783356ms]
    Aug 17 07:22:35.356: INFO: Created: latency-svc-fjsjm
    Aug 17 07:22:35.400: INFO: Got endpoints: latency-svc-wmq76 [331.961971ms]
    Aug 17 07:22:35.411: INFO: Created: latency-svc-q7f98
    Aug 17 07:22:35.447: INFO: Got endpoints: latency-svc-wsgmq [372.260803ms]
    Aug 17 07:22:35.461: INFO: Created: latency-svc-l9mvm
    Aug 17 07:22:35.497: INFO: Got endpoints: latency-svc-sjf8v [416.400529ms]
    Aug 17 07:22:35.507: INFO: Created: latency-svc-5phdk
    Aug 17 07:22:35.546: INFO: Got endpoints: latency-svc-z4mzp [460.800774ms]
    Aug 17 07:22:35.558: INFO: Created: latency-svc-v4tzs
    Aug 17 07:22:35.597: INFO: Got endpoints: latency-svc-lgtgf [503.501138ms]
    Aug 17 07:22:35.606: INFO: Created: latency-svc-wdxg8
    Aug 17 07:22:35.646: INFO: Got endpoints: latency-svc-tvbtq [546.096246ms]
    Aug 17 07:22:35.653: INFO: Created: latency-svc-8dc9s
    Aug 17 07:22:35.698: INFO: Got endpoints: latency-svc-ht5r5 [592.311322ms]
    Aug 17 07:22:35.706: INFO: Created: latency-svc-sfpdb
    Aug 17 07:22:35.747: INFO: Got endpoints: latency-svc-xjzlq [629.799412ms]
    Aug 17 07:22:35.756: INFO: Created: latency-svc-mkmjv
    Aug 17 07:22:35.797: INFO: Got endpoints: latency-svc-9zw8q [673.715861ms]
    Aug 17 07:22:35.808: INFO: Created: latency-svc-lbh72
    Aug 17 07:22:35.847: INFO: Got endpoints: latency-svc-srfxt [717.931165ms]
    Aug 17 07:22:35.855: INFO: Created: latency-svc-6kxlw
    Aug 17 07:22:35.898: INFO: Got endpoints: latency-svc-d98dg [748.812342ms]
    Aug 17 07:22:35.908: INFO: Created: latency-svc-dhvc9
    Aug 17 07:22:35.952: INFO: Got endpoints: latency-svc-94td9 [756.192638ms]
    Aug 17 07:22:35.963: INFO: Created: latency-svc-slncf
    Aug 17 07:22:35.996: INFO: Got endpoints: latency-svc-7jxvq [748.779177ms]
    Aug 17 07:22:36.005: INFO: Created: latency-svc-ws2bg
    Aug 17 07:22:36.047: INFO: Got endpoints: latency-svc-tqj6z [748.760907ms]
    Aug 17 07:22:36.055: INFO: Created: latency-svc-lc7jf
    Aug 17 07:22:36.098: INFO: Got endpoints: latency-svc-fjsjm [750.689958ms]
    Aug 17 07:22:36.106: INFO: Created: latency-svc-4gvf6
    Aug 17 07:22:36.147: INFO: Got endpoints: latency-svc-q7f98 [746.955874ms]
    Aug 17 07:22:36.193: INFO: Created: latency-svc-vl7zn
    Aug 17 07:22:36.210: INFO: Got endpoints: latency-svc-l9mvm [763.348233ms]
    Aug 17 07:22:36.221: INFO: Created: latency-svc-8x2kd
    Aug 17 07:22:36.247: INFO: Got endpoints: latency-svc-5phdk [749.669115ms]
    Aug 17 07:22:36.254: INFO: Created: latency-svc-zv7gb
    Aug 17 07:22:36.297: INFO: Got endpoints: latency-svc-v4tzs [750.56772ms]
    Aug 17 07:22:36.304: INFO: Created: latency-svc-tb92z
    Aug 17 07:22:36.347: INFO: Got endpoints: latency-svc-wdxg8 [750.158982ms]
    Aug 17 07:22:36.354: INFO: Created: latency-svc-2fv8p
    Aug 17 07:22:36.397: INFO: Got endpoints: latency-svc-8dc9s [751.601474ms]
    Aug 17 07:22:36.404: INFO: Created: latency-svc-hskfn
    Aug 17 07:22:36.447: INFO: Got endpoints: latency-svc-sfpdb [748.818941ms]
    Aug 17 07:22:36.454: INFO: Created: latency-svc-vfhj5
    Aug 17 07:22:36.498: INFO: Got endpoints: latency-svc-mkmjv [750.849673ms]
    Aug 17 07:22:36.541: INFO: Created: latency-svc-nld2f
    Aug 17 07:22:36.546: INFO: Got endpoints: latency-svc-lbh72 [748.990725ms]
    Aug 17 07:22:36.562: INFO: Created: latency-svc-qxnqb
    Aug 17 07:22:36.596: INFO: Got endpoints: latency-svc-6kxlw [749.094297ms]
    Aug 17 07:22:36.603: INFO: Created: latency-svc-cnmdb
    Aug 17 07:22:36.648: INFO: Got endpoints: latency-svc-dhvc9 [750.103559ms]
    Aug 17 07:22:36.654: INFO: Created: latency-svc-ktmjh
    Aug 17 07:22:36.697: INFO: Got endpoints: latency-svc-slncf [744.084438ms]
    Aug 17 07:22:36.704: INFO: Created: latency-svc-tz4sm
    Aug 17 07:22:36.746: INFO: Got endpoints: latency-svc-ws2bg [749.900548ms]
    Aug 17 07:22:36.755: INFO: Created: latency-svc-6gj6r
    Aug 17 07:22:36.797: INFO: Got endpoints: latency-svc-lc7jf [750.371646ms]
    Aug 17 07:22:36.805: INFO: Created: latency-svc-pfxs9
    Aug 17 07:22:36.847: INFO: Got endpoints: latency-svc-4gvf6 [749.290518ms]
    Aug 17 07:22:36.855: INFO: Created: latency-svc-9kcbh
    Aug 17 07:22:36.897: INFO: Got endpoints: latency-svc-vl7zn [750.812236ms]
    Aug 17 07:22:36.907: INFO: Created: latency-svc-tmnhg
    Aug 17 07:22:36.947: INFO: Got endpoints: latency-svc-8x2kd [736.04621ms]
    Aug 17 07:22:36.954: INFO: Created: latency-svc-dpqpp
    Aug 17 07:22:36.997: INFO: Got endpoints: latency-svc-zv7gb [750.471565ms]
    Aug 17 07:22:37.004: INFO: Created: latency-svc-z9t6b
    Aug 17 07:22:37.046: INFO: Got endpoints: latency-svc-tb92z [749.58836ms]
    Aug 17 07:22:37.058: INFO: Created: latency-svc-m8rb7
    Aug 17 07:22:37.097: INFO: Got endpoints: latency-svc-2fv8p [750.425927ms]
    Aug 17 07:22:37.105: INFO: Created: latency-svc-nv5sl
    Aug 17 07:22:37.148: INFO: Got endpoints: latency-svc-hskfn [750.086283ms]
    Aug 17 07:22:37.156: INFO: Created: latency-svc-htgw2
    Aug 17 07:22:37.197: INFO: Got endpoints: latency-svc-vfhj5 [749.683227ms]
    Aug 17 07:22:37.205: INFO: Created: latency-svc-9s5fb
    Aug 17 07:22:37.247: INFO: Got endpoints: latency-svc-nld2f [749.200996ms]
    Aug 17 07:22:37.309: INFO: Got endpoints: latency-svc-qxnqb [763.220321ms]
    Aug 17 07:22:37.310: INFO: Created: latency-svc-th5hd
    Aug 17 07:22:37.317: INFO: Created: latency-svc-8scz6
    Aug 17 07:22:37.347: INFO: Got endpoints: latency-svc-cnmdb [750.794615ms]
    Aug 17 07:22:37.354: INFO: Created: latency-svc-v4q9w
    Aug 17 07:22:37.397: INFO: Got endpoints: latency-svc-ktmjh [748.838377ms]
    Aug 17 07:22:37.403: INFO: Created: latency-svc-pt794
    Aug 17 07:22:37.448: INFO: Got endpoints: latency-svc-tz4sm [751.394601ms]
    Aug 17 07:22:37.457: INFO: Created: latency-svc-zbjt6
    Aug 17 07:22:37.497: INFO: Got endpoints: latency-svc-6gj6r [750.531923ms]
    Aug 17 07:22:37.504: INFO: Created: latency-svc-84xvj
    Aug 17 07:22:37.547: INFO: Got endpoints: latency-svc-pfxs9 [750.241467ms]
    Aug 17 07:22:37.554: INFO: Created: latency-svc-4v4bt
    Aug 17 07:22:37.598: INFO: Got endpoints: latency-svc-9kcbh [750.94914ms]
    Aug 17 07:22:37.606: INFO: Created: latency-svc-2ngbc
    Aug 17 07:22:37.647: INFO: Got endpoints: latency-svc-tmnhg [749.588438ms]
    Aug 17 07:22:37.653: INFO: Created: latency-svc-9ppgp
    Aug 17 07:22:37.696: INFO: Got endpoints: latency-svc-dpqpp [749.415079ms]
    Aug 17 07:22:37.703: INFO: Created: latency-svc-6xnc8
    Aug 17 07:22:37.746: INFO: Got endpoints: latency-svc-z9t6b [748.918676ms]
    Aug 17 07:22:37.754: INFO: Created: latency-svc-bqc9x
    Aug 17 07:22:37.796: INFO: Got endpoints: latency-svc-m8rb7 [749.541696ms]
    Aug 17 07:22:37.803: INFO: Created: latency-svc-b6ckf
    Aug 17 07:22:37.846: INFO: Got endpoints: latency-svc-nv5sl [749.130837ms]
    Aug 17 07:22:37.853: INFO: Created: latency-svc-72jq4
    Aug 17 07:22:37.899: INFO: Got endpoints: latency-svc-htgw2 [751.806625ms]
    Aug 17 07:22:37.906: INFO: Created: latency-svc-9bd4q
    Aug 17 07:22:37.949: INFO: Got endpoints: latency-svc-9s5fb [751.86496ms]
    Aug 17 07:22:37.957: INFO: Created: latency-svc-2chcr
    Aug 17 07:22:37.996: INFO: Got endpoints: latency-svc-th5hd [748.978096ms]
    Aug 17 07:22:38.063: INFO: Got endpoints: latency-svc-8scz6 [753.210538ms]
    Aug 17 07:22:38.065: INFO: Created: latency-svc-9lm2f
    Aug 17 07:22:38.070: INFO: Created: latency-svc-c7xxl
    Aug 17 07:22:38.097: INFO: Got endpoints: latency-svc-v4q9w [749.955841ms]
    Aug 17 07:22:38.104: INFO: Created: latency-svc-2xc7d
    Aug 17 07:22:38.146: INFO: Got endpoints: latency-svc-pt794 [749.281946ms]
    Aug 17 07:22:38.153: INFO: Created: latency-svc-zcqsr
    Aug 17 07:22:38.197: INFO: Got endpoints: latency-svc-zbjt6 [749.035406ms]
    Aug 17 07:22:38.205: INFO: Created: latency-svc-7xt49
    Aug 17 07:22:38.247: INFO: Got endpoints: latency-svc-84xvj [750.541612ms]
    Aug 17 07:22:38.258: INFO: Created: latency-svc-5fvcg
    Aug 17 07:22:38.297: INFO: Got endpoints: latency-svc-4v4bt [749.735003ms]
    Aug 17 07:22:38.305: INFO: Created: latency-svc-ccqwk
    Aug 17 07:22:38.348: INFO: Got endpoints: latency-svc-2ngbc [750.005378ms]
    Aug 17 07:22:38.355: INFO: Created: latency-svc-k8v4h
    Aug 17 07:22:38.397: INFO: Got endpoints: latency-svc-9ppgp [749.395968ms]
    Aug 17 07:22:38.404: INFO: Created: latency-svc-65db7
    Aug 17 07:22:38.447: INFO: Got endpoints: latency-svc-6xnc8 [750.764531ms]
    Aug 17 07:22:38.454: INFO: Created: latency-svc-lvs2v
    Aug 17 07:22:38.497: INFO: Got endpoints: latency-svc-bqc9x [750.604645ms]
    Aug 17 07:22:38.504: INFO: Created: latency-svc-szvlg
    Aug 17 07:22:38.547: INFO: Got endpoints: latency-svc-b6ckf [751.135817ms]
    Aug 17 07:22:38.554: INFO: Created: latency-svc-zr2mt
    Aug 17 07:22:38.597: INFO: Got endpoints: latency-svc-72jq4 [750.27625ms]
    Aug 17 07:22:38.603: INFO: Created: latency-svc-pjj6t
    Aug 17 07:22:38.647: INFO: Got endpoints: latency-svc-9bd4q [747.356457ms]
    Aug 17 07:22:38.653: INFO: Created: latency-svc-dr8kk
    Aug 17 07:22:38.696: INFO: Got endpoints: latency-svc-2chcr [747.280686ms]
    Aug 17 07:22:38.703: INFO: Created: latency-svc-9rfwd
    Aug 17 07:22:38.747: INFO: Got endpoints: latency-svc-9lm2f [750.445797ms]
    Aug 17 07:22:38.753: INFO: Created: latency-svc-j2wg2
    Aug 17 07:22:38.797: INFO: Got endpoints: latency-svc-c7xxl [734.742057ms]
    Aug 17 07:22:38.804: INFO: Created: latency-svc-st87t
    Aug 17 07:22:38.847: INFO: Got endpoints: latency-svc-2xc7d [749.990894ms]
    Aug 17 07:22:38.855: INFO: Created: latency-svc-5tjmr
    Aug 17 07:22:38.896: INFO: Got endpoints: latency-svc-zcqsr [750.561131ms]
    Aug 17 07:22:38.904: INFO: Created: latency-svc-gwqzt
    Aug 17 07:22:38.947: INFO: Got endpoints: latency-svc-7xt49 [750.067027ms]
    Aug 17 07:22:38.956: INFO: Created: latency-svc-tt8pd
    Aug 17 07:22:38.997: INFO: Got endpoints: latency-svc-5fvcg [749.413361ms]
    Aug 17 07:22:39.004: INFO: Created: latency-svc-8wm6f
    Aug 17 07:22:39.047: INFO: Got endpoints: latency-svc-ccqwk [749.672889ms]
    Aug 17 07:22:39.053: INFO: Created: latency-svc-dgnsv
    Aug 17 07:22:39.098: INFO: Got endpoints: latency-svc-k8v4h [749.535822ms]
    Aug 17 07:22:39.105: INFO: Created: latency-svc-4dhpp
    Aug 17 07:22:39.147: INFO: Got endpoints: latency-svc-65db7 [750.508385ms]
    Aug 17 07:22:39.154: INFO: Created: latency-svc-t7649
    Aug 17 07:22:39.197: INFO: Got endpoints: latency-svc-lvs2v [750.097213ms]
    Aug 17 07:22:39.206: INFO: Created: latency-svc-jkrl4
    Aug 17 07:22:39.247: INFO: Got endpoints: latency-svc-szvlg [749.577016ms]
    Aug 17 07:22:39.254: INFO: Created: latency-svc-lhr8v
    Aug 17 07:22:39.299: INFO: Got endpoints: latency-svc-zr2mt [751.657419ms]
    Aug 17 07:22:39.306: INFO: Created: latency-svc-ppsg7
    Aug 17 07:22:39.348: INFO: Got endpoints: latency-svc-pjj6t [750.881101ms]
    Aug 17 07:22:39.357: INFO: Created: latency-svc-m848p
    Aug 17 07:22:39.397: INFO: Got endpoints: latency-svc-dr8kk [749.71225ms]
    Aug 17 07:22:39.420: INFO: Created: latency-svc-nq587
    Aug 17 07:22:39.447: INFO: Got endpoints: latency-svc-9rfwd [751.04118ms]
    Aug 17 07:22:39.455: INFO: Created: latency-svc-jg6zf
    Aug 17 07:22:39.498: INFO: Got endpoints: latency-svc-j2wg2 [750.960642ms]
    Aug 17 07:22:39.507: INFO: Created: latency-svc-4vdr5
    Aug 17 07:22:39.564: INFO: Got endpoints: latency-svc-st87t [766.618962ms]
    Aug 17 07:22:39.572: INFO: Created: latency-svc-z84r7
    Aug 17 07:22:39.596: INFO: Got endpoints: latency-svc-5tjmr [749.339049ms]
    Aug 17 07:22:39.603: INFO: Created: latency-svc-6s8lc
    Aug 17 07:22:39.647: INFO: Got endpoints: latency-svc-gwqzt [750.224372ms]
    Aug 17 07:22:39.655: INFO: Created: latency-svc-rrf9b
    Aug 17 07:22:39.697: INFO: Got endpoints: latency-svc-tt8pd [750.265524ms]
    Aug 17 07:22:39.705: INFO: Created: latency-svc-2s9f7
    Aug 17 07:22:39.747: INFO: Got endpoints: latency-svc-8wm6f [750.594035ms]
    Aug 17 07:22:39.755: INFO: Created: latency-svc-xgnsh
    Aug 17 07:22:39.796: INFO: Got endpoints: latency-svc-dgnsv [749.468089ms]
    Aug 17 07:22:39.806: INFO: Created: latency-svc-24mtt
    Aug 17 07:22:39.846: INFO: Got endpoints: latency-svc-4dhpp [748.3181ms]
    Aug 17 07:22:39.855: INFO: Created: latency-svc-7vpvl
    Aug 17 07:22:39.896: INFO: Got endpoints: latency-svc-t7649 [749.074117ms]
    Aug 17 07:22:39.905: INFO: Created: latency-svc-28cmw
    Aug 17 07:22:39.946: INFO: Got endpoints: latency-svc-jkrl4 [749.403714ms]
    Aug 17 07:22:39.958: INFO: Created: latency-svc-spfkd
    Aug 17 07:22:39.998: INFO: Got endpoints: latency-svc-lhr8v [750.876579ms]
    Aug 17 07:22:40.007: INFO: Created: latency-svc-hj86l
    Aug 17 07:22:40.050: INFO: Got endpoints: latency-svc-ppsg7 [751.157865ms]
    Aug 17 07:22:40.058: INFO: Created: latency-svc-xl2dt
    Aug 17 07:22:40.097: INFO: Got endpoints: latency-svc-m848p [749.452459ms]
    Aug 17 07:22:40.106: INFO: Created: latency-svc-gscdl
    Aug 17 07:22:40.147: INFO: Got endpoints: latency-svc-nq587 [750.188278ms]
    Aug 17 07:22:40.154: INFO: Created: latency-svc-s4nkf
    Aug 17 07:22:40.196: INFO: Got endpoints: latency-svc-jg6zf [748.955653ms]
    Aug 17 07:22:40.204: INFO: Created: latency-svc-jxm9l
    Aug 17 07:22:40.247: INFO: Got endpoints: latency-svc-4vdr5 [748.885095ms]
    Aug 17 07:22:40.254: INFO: Created: latency-svc-nf6kq
    Aug 17 07:22:40.315: INFO: Got endpoints: latency-svc-z84r7 [751.002244ms]
    Aug 17 07:22:40.323: INFO: Created: latency-svc-xbtwr
    Aug 17 07:22:40.347: INFO: Got endpoints: latency-svc-6s8lc [750.566357ms]
    Aug 17 07:22:40.355: INFO: Created: latency-svc-hbfzz
    Aug 17 07:22:40.396: INFO: Got endpoints: latency-svc-rrf9b [749.537508ms]
    Aug 17 07:22:40.404: INFO: Created: latency-svc-7d9d5
    Aug 17 07:22:40.448: INFO: Got endpoints: latency-svc-2s9f7 [750.39714ms]
    Aug 17 07:22:40.458: INFO: Created: latency-svc-744fk
    Aug 17 07:22:40.497: INFO: Got endpoints: latency-svc-xgnsh [749.84884ms]
    Aug 17 07:22:40.505: INFO: Created: latency-svc-8z85m
    Aug 17 07:22:40.547: INFO: Got endpoints: latency-svc-24mtt [750.788634ms]
    Aug 17 07:22:40.555: INFO: Created: latency-svc-4tcvb
    Aug 17 07:22:40.596: INFO: Got endpoints: latency-svc-7vpvl [750.450626ms]
    Aug 17 07:22:40.604: INFO: Created: latency-svc-gnzcv
    Aug 17 07:22:40.647: INFO: Got endpoints: latency-svc-28cmw [751.051417ms]
    Aug 17 07:22:40.655: INFO: Created: latency-svc-5vf6z
    Aug 17 07:22:40.697: INFO: Got endpoints: latency-svc-spfkd [750.130157ms]
    Aug 17 07:22:40.704: INFO: Created: latency-svc-g4t72
    Aug 17 07:22:40.747: INFO: Got endpoints: latency-svc-hj86l [749.529919ms]
    Aug 17 07:22:40.756: INFO: Created: latency-svc-pn57v
    Aug 17 07:22:40.797: INFO: Got endpoints: latency-svc-xl2dt [747.2242ms]
    Aug 17 07:22:40.804: INFO: Created: latency-svc-mcf75
    Aug 17 07:22:40.846: INFO: Got endpoints: latency-svc-gscdl [748.876193ms]
    Aug 17 07:22:40.854: INFO: Created: latency-svc-hn4hf
    Aug 17 07:22:40.897: INFO: Got endpoints: latency-svc-s4nkf [750.338053ms]
    Aug 17 07:22:40.906: INFO: Created: latency-svc-kcbzr
    Aug 17 07:22:40.947: INFO: Got endpoints: latency-svc-jxm9l [751.027657ms]
    Aug 17 07:22:40.959: INFO: Created: latency-svc-42cm6
    Aug 17 07:22:40.997: INFO: Got endpoints: latency-svc-nf6kq [750.605057ms]
    Aug 17 07:22:41.005: INFO: Created: latency-svc-5k44x
    Aug 17 07:22:41.047: INFO: Got endpoints: latency-svc-xbtwr [731.537321ms]
    Aug 17 07:22:41.079: INFO: Created: latency-svc-hh8ts
    Aug 17 07:22:41.111: INFO: Got endpoints: latency-svc-hbfzz [763.901267ms]
    Aug 17 07:22:41.135: INFO: Created: latency-svc-jxplq
    Aug 17 07:22:41.147: INFO: Got endpoints: latency-svc-7d9d5 [750.770595ms]
    Aug 17 07:22:41.155: INFO: Created: latency-svc-s4zgp
    Aug 17 07:22:41.197: INFO: Got endpoints: latency-svc-744fk [749.200265ms]
    Aug 17 07:22:41.203: INFO: Created: latency-svc-f425b
    Aug 17 07:22:41.247: INFO: Got endpoints: latency-svc-8z85m [749.750666ms]
    Aug 17 07:22:41.256: INFO: Created: latency-svc-sfcvt
    Aug 17 07:22:41.298: INFO: Got endpoints: latency-svc-4tcvb [751.145032ms]
    Aug 17 07:22:41.306: INFO: Created: latency-svc-d7z4f
    Aug 17 07:22:41.350: INFO: Got endpoints: latency-svc-gnzcv [753.792623ms]
    Aug 17 07:22:41.357: INFO: Created: latency-svc-4xxvg
    Aug 17 07:22:41.396: INFO: Got endpoints: latency-svc-5vf6z [748.77815ms]
    Aug 17 07:22:41.403: INFO: Created: latency-svc-xknnd
    Aug 17 07:22:41.447: INFO: Got endpoints: latency-svc-g4t72 [750.127265ms]
    Aug 17 07:22:41.455: INFO: Created: latency-svc-585qf
    Aug 17 07:22:41.497: INFO: Got endpoints: latency-svc-pn57v [749.905962ms]
    Aug 17 07:22:41.506: INFO: Created: latency-svc-v8ttz
    Aug 17 07:22:41.548: INFO: Got endpoints: latency-svc-mcf75 [750.331352ms]
    Aug 17 07:22:41.558: INFO: Created: latency-svc-99jkn
    Aug 17 07:22:41.596: INFO: Got endpoints: latency-svc-hn4hf [750.223141ms]
    Aug 17 07:22:41.604: INFO: Created: latency-svc-7x2vv
    Aug 17 07:22:41.648: INFO: Got endpoints: latency-svc-kcbzr [750.524556ms]
    Aug 17 07:22:41.654: INFO: Created: latency-svc-r9bng
    Aug 17 07:22:41.697: INFO: Got endpoints: latency-svc-42cm6 [749.146099ms]
    Aug 17 07:22:41.705: INFO: Created: latency-svc-fnx2n
    Aug 17 07:22:41.748: INFO: Got endpoints: latency-svc-5k44x [750.737445ms]
    Aug 17 07:22:41.756: INFO: Created: latency-svc-9pfrp
    Aug 17 07:22:41.797: INFO: Got endpoints: latency-svc-hh8ts [750.128432ms]
    Aug 17 07:22:41.805: INFO: Created: latency-svc-jn6hv
    Aug 17 07:22:41.847: INFO: Got endpoints: latency-svc-jxplq [736.499919ms]
    Aug 17 07:22:41.856: INFO: Created: latency-svc-r8hq8
    Aug 17 07:22:41.896: INFO: Got endpoints: latency-svc-s4zgp [749.017397ms]
    Aug 17 07:22:41.904: INFO: Created: latency-svc-s2pxz
    Aug 17 07:22:41.947: INFO: Got endpoints: latency-svc-f425b [750.21456ms]
    Aug 17 07:22:41.957: INFO: Created: latency-svc-6blvk
    Aug 17 07:22:41.997: INFO: Got endpoints: latency-svc-sfcvt [749.541249ms]
    Aug 17 07:22:42.004: INFO: Created: latency-svc-bjr2n
    Aug 17 07:22:42.048: INFO: Got endpoints: latency-svc-d7z4f [749.429348ms]
    Aug 17 07:22:42.055: INFO: Created: latency-svc-6frr8
    Aug 17 07:22:42.097: INFO: Got endpoints: latency-svc-4xxvg [746.735851ms]
    Aug 17 07:22:42.104: INFO: Created: latency-svc-tf4cp
    Aug 17 07:22:42.147: INFO: Got endpoints: latency-svc-xknnd [750.644068ms]
    Aug 17 07:22:42.154: INFO: Created: latency-svc-jkcp7
    Aug 17 07:22:42.197: INFO: Got endpoints: latency-svc-585qf [750.538105ms]
    Aug 17 07:22:42.205: INFO: Created: latency-svc-s4xsc
    Aug 17 07:22:42.247: INFO: Got endpoints: latency-svc-v8ttz [749.416138ms]
    Aug 17 07:22:42.254: INFO: Created: latency-svc-dvl2m
    Aug 17 07:22:42.297: INFO: Got endpoints: latency-svc-99jkn [749.219564ms]
    Aug 17 07:22:42.304: INFO: Created: latency-svc-h9mxv
    Aug 17 07:22:42.347: INFO: Got endpoints: latency-svc-7x2vv [750.498227ms]
    Aug 17 07:22:42.354: INFO: Created: latency-svc-mst8v
    Aug 17 07:22:42.397: INFO: Got endpoints: latency-svc-r9bng [749.066955ms]
    Aug 17 07:22:42.404: INFO: Created: latency-svc-7mm5q
    Aug 17 07:22:42.447: INFO: Got endpoints: latency-svc-fnx2n [750.202182ms]
    Aug 17 07:22:42.455: INFO: Created: latency-svc-l6xpk
    Aug 17 07:22:42.497: INFO: Got endpoints: latency-svc-9pfrp [748.596511ms]
    Aug 17 07:22:42.504: INFO: Created: latency-svc-vbjmt
    Aug 17 07:22:42.547: INFO: Got endpoints: latency-svc-jn6hv [750.340417ms]
    Aug 17 07:22:42.555: INFO: Created: latency-svc-hzpqg
    Aug 17 07:22:42.597: INFO: Got endpoints: latency-svc-r8hq8 [749.399432ms]
    Aug 17 07:22:42.604: INFO: Created: latency-svc-jq4tz
    Aug 17 07:22:42.647: INFO: Got endpoints: latency-svc-s2pxz [750.458692ms]
    Aug 17 07:22:42.653: INFO: Created: latency-svc-hg8cf
    Aug 17 07:22:42.696: INFO: Got endpoints: latency-svc-6blvk [748.859039ms]
    Aug 17 07:22:42.705: INFO: Created: latency-svc-q7t42
    Aug 17 07:22:42.747: INFO: Got endpoints: latency-svc-bjr2n [750.447835ms]
    Aug 17 07:22:42.797: INFO: Got endpoints: latency-svc-6frr8 [749.33466ms]
    Aug 17 07:22:42.846: INFO: Got endpoints: latency-svc-tf4cp [749.157924ms]
    Aug 17 07:22:42.898: INFO: Got endpoints: latency-svc-jkcp7 [750.745333ms]
    Aug 17 07:22:42.947: INFO: Got endpoints: latency-svc-s4xsc [749.797144ms]
    Aug 17 07:22:42.998: INFO: Got endpoints: latency-svc-dvl2m [751.688782ms]
    Aug 17 07:22:43.047: INFO: Got endpoints: latency-svc-h9mxv [749.906795ms]
    Aug 17 07:22:43.097: INFO: Got endpoints: latency-svc-mst8v [750.26624ms]
    Aug 17 07:22:43.147: INFO: Got endpoints: latency-svc-7mm5q [749.942736ms]
    Aug 17 07:22:43.197: INFO: Got endpoints: latency-svc-l6xpk [750.394435ms]
    Aug 17 07:22:43.247: INFO: Got endpoints: latency-svc-vbjmt [750.108871ms]
    Aug 17 07:22:43.298: INFO: Got endpoints: latency-svc-hzpqg [750.719338ms]
    Aug 17 07:22:43.358: INFO: Got endpoints: latency-svc-jq4tz [761.598107ms]
    Aug 17 07:22:43.396: INFO: Got endpoints: latency-svc-hg8cf [749.723095ms]
    Aug 17 07:22:43.447: INFO: Got endpoints: latency-svc-q7t42 [750.354801ms]
    Aug 17 07:22:43.447: INFO: Latencies: [13.952789ms 16.201693ms 21.700299ms 34.036291ms 37.475214ms 57.426257ms 60.454102ms 65.902546ms 71.094575ms 75.02446ms 82.777275ms 83.295546ms 85.984742ms 88.541149ms 92.604856ms 95.272147ms 96.440347ms 98.474434ms 99.103211ms 99.476514ms 99.610293ms 99.811216ms 100.425792ms 101.049148ms 102.658107ms 102.824391ms 103.295471ms 103.515906ms 103.930253ms 105.099447ms 107.716162ms 107.854493ms 110.110233ms 123.119329ms 165.578149ms 209.910376ms 254.502001ms 285.783356ms 331.961971ms 372.260803ms 416.400529ms 460.800774ms 503.501138ms 546.096246ms 592.311322ms 629.799412ms 673.715861ms 717.931165ms 731.537321ms 734.742057ms 736.04621ms 736.499919ms 744.084438ms 746.735851ms 746.955874ms 747.2242ms 747.280686ms 747.356457ms 748.3181ms 748.596511ms 748.760907ms 748.77815ms 748.779177ms 748.812342ms 748.818941ms 748.838377ms 748.859039ms 748.876193ms 748.885095ms 748.918676ms 748.955653ms 748.978096ms 748.990725ms 749.017397ms 749.035406ms 749.066955ms 749.074117ms 749.094297ms 749.130837ms 749.146099ms 749.157924ms 749.200265ms 749.200996ms 749.219564ms 749.281946ms 749.290518ms 749.33466ms 749.339049ms 749.395968ms 749.399432ms 749.403714ms 749.413361ms 749.415079ms 749.416138ms 749.429348ms 749.452459ms 749.468089ms 749.529919ms 749.535822ms 749.537508ms 749.541249ms 749.541696ms 749.577016ms 749.58836ms 749.588438ms 749.669115ms 749.672889ms 749.683227ms 749.71225ms 749.723095ms 749.735003ms 749.750666ms 749.797144ms 749.84884ms 749.900548ms 749.905962ms 749.906795ms 749.942736ms 749.955841ms 749.990894ms 750.005378ms 750.067027ms 750.086283ms 750.097213ms 750.103559ms 750.108871ms 750.127265ms 750.128432ms 750.130157ms 750.158982ms 750.188278ms 750.202182ms 750.21456ms 750.223141ms 750.224372ms 750.241467ms 750.265524ms 750.26624ms 750.27625ms 750.331352ms 750.338053ms 750.340417ms 750.354801ms 750.371646ms 750.394435ms 750.39714ms 750.425927ms 750.445797ms 750.447835ms 750.450626ms 750.458692ms 750.471565ms 750.498227ms 750.508385ms 750.524556ms 750.531923ms 750.538105ms 750.541612ms 750.561131ms 750.566357ms 750.56772ms 750.594035ms 750.604645ms 750.605057ms 750.644068ms 750.689958ms 750.719338ms 750.737445ms 750.745333ms 750.764531ms 750.770595ms 750.788634ms 750.794615ms 750.812236ms 750.849673ms 750.876579ms 750.881101ms 750.94914ms 750.960642ms 751.002244ms 751.027657ms 751.04118ms 751.051417ms 751.135817ms 751.145032ms 751.157865ms 751.394601ms 751.601474ms 751.657419ms 751.688782ms 751.806625ms 751.86496ms 753.210538ms 753.792623ms 756.192638ms 761.598107ms 763.220321ms 763.348233ms 763.901267ms 766.618962ms]
    Aug 17 07:22:43.447: INFO: 50 %ile: 749.541249ms
    Aug 17 07:22:43.447: INFO: 90 %ile: 751.027657ms
    Aug 17 07:22:43.447: INFO: 99 %ile: 763.901267ms
    Aug 17 07:22:43.447: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:43.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7674" for this suite. 08/17/23 07:22:43.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:43.455
Aug 17 07:22:43.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:22:43.458
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:43.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:43.467
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/17/23 07:22:43.469
STEP: Creating a ResourceQuota 08/17/23 07:22:48.472
STEP: Ensuring resource quota status is calculated 08/17/23 07:22:48.475
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:50.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5751" for this suite. 08/17/23 07:22:50.481
------------------------------
â€¢ [SLOW TEST] [7.035 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:43.455
    Aug 17 07:22:43.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:22:43.458
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:43.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:43.467
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/17/23 07:22:43.469
    STEP: Creating a ResourceQuota 08/17/23 07:22:48.472
    STEP: Ensuring resource quota status is calculated 08/17/23 07:22:48.475
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:50.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5751" for this suite. 08/17/23 07:22:50.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:50.491
Aug 17 07:22:50.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename podtemplate 08/17/23 07:22:50.492
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:50.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:50.505
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/17/23 07:22:50.508
Aug 17 07:22:50.516: INFO: created test-podtemplate-1
Aug 17 07:22:50.520: INFO: created test-podtemplate-2
Aug 17 07:22:50.526: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/17/23 07:22:50.526
STEP: delete collection of pod templates 08/17/23 07:22:50.531
Aug 17 07:22:50.531: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/17/23 07:22:50.551
Aug 17 07:22:50.552: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:50.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6387" for this suite. 08/17/23 07:22:50.56
------------------------------
â€¢ [0.075 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:50.491
    Aug 17 07:22:50.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename podtemplate 08/17/23 07:22:50.492
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:50.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:50.505
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/17/23 07:22:50.508
    Aug 17 07:22:50.516: INFO: created test-podtemplate-1
    Aug 17 07:22:50.520: INFO: created test-podtemplate-2
    Aug 17 07:22:50.526: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/17/23 07:22:50.526
    STEP: delete collection of pod templates 08/17/23 07:22:50.531
    Aug 17 07:22:50.531: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/17/23 07:22:50.551
    Aug 17 07:22:50.552: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:50.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6387" for this suite. 08/17/23 07:22:50.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:50.566
Aug 17 07:22:50.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:22:50.567
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:50.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:50.588
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-a64a7d29-6182-47e6-b55e-cef732dc378e 08/17/23 07:22:50.591
STEP: Creating a pod to test consume secrets 08/17/23 07:22:50.597
Aug 17 07:22:50.608: INFO: Waiting up to 5m0s for pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46" in namespace "secrets-1138" to be "Succeeded or Failed"
Aug 17 07:22:50.610: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46": Phase="Pending", Reason="", readiness=false. Elapsed: 1.72562ms
Aug 17 07:22:52.612: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003938111s
Aug 17 07:22:54.612: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004175192s
STEP: Saw pod success 08/17/23 07:22:54.612
Aug 17 07:22:54.612: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46" satisfied condition "Succeeded or Failed"
Aug 17 07:22:54.614: INFO: Trying to get logs from node yst-node2 pod pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46 container secret-env-test: <nil>
STEP: delete the pod 08/17/23 07:22:54.617
Aug 17 07:22:54.621: INFO: Waiting for pod pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46 to disappear
Aug 17 07:22:54.622: INFO: Pod pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1138" for this suite. 08/17/23 07:22:54.625
------------------------------
â€¢ [4.061 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:50.566
    Aug 17 07:22:50.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:22:50.567
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:50.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:50.588
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-a64a7d29-6182-47e6-b55e-cef732dc378e 08/17/23 07:22:50.591
    STEP: Creating a pod to test consume secrets 08/17/23 07:22:50.597
    Aug 17 07:22:50.608: INFO: Waiting up to 5m0s for pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46" in namespace "secrets-1138" to be "Succeeded or Failed"
    Aug 17 07:22:50.610: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46": Phase="Pending", Reason="", readiness=false. Elapsed: 1.72562ms
    Aug 17 07:22:52.612: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003938111s
    Aug 17 07:22:54.612: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004175192s
    STEP: Saw pod success 08/17/23 07:22:54.612
    Aug 17 07:22:54.612: INFO: Pod "pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46" satisfied condition "Succeeded or Failed"
    Aug 17 07:22:54.614: INFO: Trying to get logs from node yst-node2 pod pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46 container secret-env-test: <nil>
    STEP: delete the pod 08/17/23 07:22:54.617
    Aug 17 07:22:54.621: INFO: Waiting for pod pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46 to disappear
    Aug 17 07:22:54.622: INFO: Pod pod-secrets-ec41363e-d996-45c4-874e-2df942e17b46 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1138" for this suite. 08/17/23 07:22:54.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:54.628
Aug 17 07:22:54.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:22:54.628
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:54.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:54.646
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-b0e2be4e-0569-4b9c-9f14-7fb1862b70cd 08/17/23 07:22:54.647
STEP: Creating a pod to test consume configMaps 08/17/23 07:22:54.666
Aug 17 07:22:54.670: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f" in namespace "projected-8489" to be "Succeeded or Failed"
Aug 17 07:22:54.672: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70248ms
Aug 17 07:22:56.674: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004330969s
Aug 17 07:22:58.675: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005346815s
STEP: Saw pod success 08/17/23 07:22:58.675
Aug 17 07:22:58.676: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f" satisfied condition "Succeeded or Failed"
Aug 17 07:22:58.677: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:22:58.68
Aug 17 07:22:58.686: INFO: Waiting for pod pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f to disappear
Aug 17 07:22:58.687: INFO: Pod pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:58.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8489" for this suite. 08/17/23 07:22:58.689
------------------------------
â€¢ [4.064 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:54.628
    Aug 17 07:22:54.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:22:54.628
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:54.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:54.646
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-b0e2be4e-0569-4b9c-9f14-7fb1862b70cd 08/17/23 07:22:54.647
    STEP: Creating a pod to test consume configMaps 08/17/23 07:22:54.666
    Aug 17 07:22:54.670: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f" in namespace "projected-8489" to be "Succeeded or Failed"
    Aug 17 07:22:54.672: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70248ms
    Aug 17 07:22:56.674: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004330969s
    Aug 17 07:22:58.675: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005346815s
    STEP: Saw pod success 08/17/23 07:22:58.675
    Aug 17 07:22:58.676: INFO: Pod "pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f" satisfied condition "Succeeded or Failed"
    Aug 17 07:22:58.677: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:22:58.68
    Aug 17 07:22:58.686: INFO: Waiting for pod pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f to disappear
    Aug 17 07:22:58.687: INFO: Pod pod-projected-configmaps-a24d259c-b10b-4710-8bef-587bea4d765f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:58.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8489" for this suite. 08/17/23 07:22:58.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:58.692
Aug 17 07:22:58.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:22:58.693
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:58.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:58.704
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-1325-delete-me 08/17/23 07:22:58.72
STEP: Waiting for the RuntimeClass to disappear 08/17/23 07:22:58.737
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 07:22:58.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1325" for this suite. 08/17/23 07:22:58.792
------------------------------
â€¢ [0.104 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:58.692
    Aug 17 07:22:58.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:22:58.693
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:58.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:58.704
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-1325-delete-me 08/17/23 07:22:58.72
    STEP: Waiting for the RuntimeClass to disappear 08/17/23 07:22:58.737
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:22:58.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1325" for this suite. 08/17/23 07:22:58.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:22:58.798
Aug 17 07:22:58.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:22:58.798
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:58.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:58.81
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:22:58.812
Aug 17 07:22:58.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab" in namespace "downward-api-3125" to be "Succeeded or Failed"
Aug 17 07:22:58.831: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.113142ms
Aug 17 07:23:00.834: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006536451s
Aug 17 07:23:02.835: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007256211s
STEP: Saw pod success 08/17/23 07:23:02.835
Aug 17 07:23:02.835: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab" satisfied condition "Succeeded or Failed"
Aug 17 07:23:02.837: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab container client-container: <nil>
STEP: delete the pod 08/17/23 07:23:02.841
Aug 17 07:23:02.846: INFO: Waiting for pod downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab to disappear
Aug 17 07:23:02.848: INFO: Pod downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:23:02.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3125" for this suite. 08/17/23 07:23:02.85
------------------------------
â€¢ [4.056 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:22:58.798
    Aug 17 07:22:58.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:22:58.798
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:22:58.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:22:58.81
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:22:58.812
    Aug 17 07:22:58.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab" in namespace "downward-api-3125" to be "Succeeded or Failed"
    Aug 17 07:22:58.831: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.113142ms
    Aug 17 07:23:00.834: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006536451s
    Aug 17 07:23:02.835: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007256211s
    STEP: Saw pod success 08/17/23 07:23:02.835
    Aug 17 07:23:02.835: INFO: Pod "downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab" satisfied condition "Succeeded or Failed"
    Aug 17 07:23:02.837: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab container client-container: <nil>
    STEP: delete the pod 08/17/23 07:23:02.841
    Aug 17 07:23:02.846: INFO: Waiting for pod downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab to disappear
    Aug 17 07:23:02.848: INFO: Pod downwardapi-volume-0f025de5-c080-4b84-9a98-c5e0bd4318ab no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:23:02.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3125" for this suite. 08/17/23 07:23:02.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:23:02.857
Aug 17 07:23:02.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:23:02.858
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:02.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:02.872
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/17/23 07:23:02.877
STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:23:02.88
STEP: Creating a ResourceQuota with not terminating scope 08/17/23 07:23:04.883
STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:23:04.886
STEP: Creating a long running pod 08/17/23 07:23:06.889
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/17/23 07:23:06.896
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/17/23 07:23:08.899
STEP: Deleting the pod 08/17/23 07:23:10.902
STEP: Ensuring resource quota status released the pod usage 08/17/23 07:23:10.909
STEP: Creating a terminating pod 08/17/23 07:23:12.912
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/17/23 07:23:12.918
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/17/23 07:23:14.92
STEP: Deleting the pod 08/17/23 07:23:16.923
STEP: Ensuring resource quota status released the pod usage 08/17/23 07:23:16.929
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:23:18.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5216" for this suite. 08/17/23 07:23:18.935
------------------------------
â€¢ [SLOW TEST] [16.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:23:02.857
    Aug 17 07:23:02.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:23:02.858
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:02.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:02.872
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/17/23 07:23:02.877
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:23:02.88
    STEP: Creating a ResourceQuota with not terminating scope 08/17/23 07:23:04.883
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 07:23:04.886
    STEP: Creating a long running pod 08/17/23 07:23:06.889
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/17/23 07:23:06.896
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/17/23 07:23:08.899
    STEP: Deleting the pod 08/17/23 07:23:10.902
    STEP: Ensuring resource quota status released the pod usage 08/17/23 07:23:10.909
    STEP: Creating a terminating pod 08/17/23 07:23:12.912
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/17/23 07:23:12.918
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/17/23 07:23:14.92
    STEP: Deleting the pod 08/17/23 07:23:16.923
    STEP: Ensuring resource quota status released the pod usage 08/17/23 07:23:16.929
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:23:18.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5216" for this suite. 08/17/23 07:23:18.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:23:18.939
Aug 17 07:23:18.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:23:18.94
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:18.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:18.949
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:23:18.972
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:23:19.345
STEP: Deploying the webhook pod 08/17/23 07:23:19.349
STEP: Wait for the deployment to be ready 08/17/23 07:23:19.355
Aug 17 07:23:19.359: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:23:21.365
STEP: Verifying the service has paired with the endpoint 08/17/23 07:23:21.372
Aug 17 07:23:22.372: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/17/23 07:23:22.461
STEP: Creating a configMap that should be mutated 08/17/23 07:23:22.472
STEP: Deleting the collection of validation webhooks 08/17/23 07:23:22.49
STEP: Creating a configMap that should not be mutated 08/17/23 07:23:22.505
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:23:22.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7833" for this suite. 08/17/23 07:23:22.544
STEP: Destroying namespace "webhook-7833-markers" for this suite. 08/17/23 07:23:22.547
------------------------------
â€¢ [3.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:23:18.939
    Aug 17 07:23:18.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:23:18.94
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:18.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:18.949
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:23:18.972
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:23:19.345
    STEP: Deploying the webhook pod 08/17/23 07:23:19.349
    STEP: Wait for the deployment to be ready 08/17/23 07:23:19.355
    Aug 17 07:23:19.359: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:23:21.365
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:23:21.372
    Aug 17 07:23:22.372: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/17/23 07:23:22.461
    STEP: Creating a configMap that should be mutated 08/17/23 07:23:22.472
    STEP: Deleting the collection of validation webhooks 08/17/23 07:23:22.49
    STEP: Creating a configMap that should not be mutated 08/17/23 07:23:22.505
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:23:22.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7833" for this suite. 08/17/23 07:23:22.544
    STEP: Destroying namespace "webhook-7833-markers" for this suite. 08/17/23 07:23:22.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:23:22.551
Aug 17 07:23:22.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:23:22.552
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:22.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:22.567
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 07:23:22.569
Aug 17 07:23:22.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 17 07:23:22.654: INFO: stderr: ""
Aug 17 07:23:22.654: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/17/23 07:23:22.655
STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 07:23:27.707
Aug 17 07:23:27.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 get pod e2e-test-httpd-pod -o json'
Aug 17 07:23:27.788: INFO: stderr: ""
Aug 17 07:23:27.788: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a013ef37863c2a9c07967e0e738b7877ccbfba60d6c2d56a8ec664d0ac728c52\",\n            \"cni.projectcalico.org/podIP\": \"172.32.238.78/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.32.238.78/32\"\n        },\n        \"creationTimestamp\": \"2023-08-17T07:23:22Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9924\",\n        \"resourceVersion\": \"27033257\",\n        \"uid\": \"9c29df31-2887-4df8-a892-caff390f26dc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-w4pph\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"yst-node2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-w4pph\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:22Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ffbe9928f6c45fc3e39ed1d9f3e1f70c0997aa905786d058dccb7757cf96fa06\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-17T07:23:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.60.200.177\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.32.238.78\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.32.238.78\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-17T07:23:22Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/17/23 07:23:27.789
Aug 17 07:23:27.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 replace -f -'
Aug 17 07:23:28.095: INFO: stderr: ""
Aug 17 07:23:28.095: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/17/23 07:23:28.095
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 17 07:23:28.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 delete pods e2e-test-httpd-pod'
Aug 17 07:23:30.281: INFO: stderr: ""
Aug 17 07:23:30.281: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:23:30.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9924" for this suite. 08/17/23 07:23:30.284
------------------------------
â€¢ [SLOW TEST] [7.737 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:23:22.551
    Aug 17 07:23:22.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:23:22.552
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:22.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:22.567
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 07:23:22.569
    Aug 17 07:23:22.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 17 07:23:22.654: INFO: stderr: ""
    Aug 17 07:23:22.654: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/17/23 07:23:22.655
    STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 07:23:27.707
    Aug 17 07:23:27.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 get pod e2e-test-httpd-pod -o json'
    Aug 17 07:23:27.788: INFO: stderr: ""
    Aug 17 07:23:27.788: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a013ef37863c2a9c07967e0e738b7877ccbfba60d6c2d56a8ec664d0ac728c52\",\n            \"cni.projectcalico.org/podIP\": \"172.32.238.78/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.32.238.78/32\"\n        },\n        \"creationTimestamp\": \"2023-08-17T07:23:22Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9924\",\n        \"resourceVersion\": \"27033257\",\n        \"uid\": \"9c29df31-2887-4df8-a892-caff390f26dc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-w4pph\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"yst-node2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-w4pph\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T07:23:22Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ffbe9928f6c45fc3e39ed1d9f3e1f70c0997aa905786d058dccb7757cf96fa06\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-17T07:23:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.60.200.177\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.32.238.78\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.32.238.78\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-17T07:23:22Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/17/23 07:23:27.789
    Aug 17 07:23:27.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 replace -f -'
    Aug 17 07:23:28.095: INFO: stderr: ""
    Aug 17 07:23:28.095: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/17/23 07:23:28.095
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 17 07:23:28.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9924 delete pods e2e-test-httpd-pod'
    Aug 17 07:23:30.281: INFO: stderr: ""
    Aug 17 07:23:30.281: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:23:30.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9924" for this suite. 08/17/23 07:23:30.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:23:30.289
Aug 17 07:23:30.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:23:30.29
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:30.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:30.299
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/17/23 07:23:30.302
Aug 17 07:23:30.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 create -f -'
Aug 17 07:23:30.615: INFO: stderr: ""
Aug 17 07:23:30.615: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 07:23:30.615
Aug 17 07:23:30.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 07:23:30.697: INFO: stderr: ""
Aug 17 07:23:30.697: INFO: stdout: "update-demo-nautilus-9x48l update-demo-nautilus-g7nx7 "
Aug 17 07:23:30.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 07:23:30.779: INFO: stderr: ""
Aug 17 07:23:30.779: INFO: stdout: ""
Aug 17 07:23:30.779: INFO: update-demo-nautilus-9x48l is created but not running
Aug 17 07:23:35.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 07:23:35.872: INFO: stderr: ""
Aug 17 07:23:35.872: INFO: stdout: "update-demo-nautilus-9x48l update-demo-nautilus-g7nx7 "
Aug 17 07:23:35.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 07:23:35.957: INFO: stderr: ""
Aug 17 07:23:35.957: INFO: stdout: ""
Aug 17 07:23:35.957: INFO: update-demo-nautilus-9x48l is created but not running
Aug 17 07:23:40.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 07:23:41.051: INFO: stderr: ""
Aug 17 07:23:41.051: INFO: stdout: "update-demo-nautilus-9x48l update-demo-nautilus-g7nx7 "
Aug 17 07:23:41.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 07:23:41.130: INFO: stderr: ""
Aug 17 07:23:41.130: INFO: stdout: "true"
Aug 17 07:23:41.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 07:23:41.219: INFO: stderr: ""
Aug 17 07:23:41.219: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 07:23:41.219: INFO: validating pod update-demo-nautilus-9x48l
Aug 17 07:23:41.222: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 07:23:41.222: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 07:23:41.222: INFO: update-demo-nautilus-9x48l is verified up and running
Aug 17 07:23:41.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-g7nx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 07:23:41.308: INFO: stderr: ""
Aug 17 07:23:41.308: INFO: stdout: "true"
Aug 17 07:23:41.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-g7nx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 07:23:41.388: INFO: stderr: ""
Aug 17 07:23:41.388: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 07:23:41.388: INFO: validating pod update-demo-nautilus-g7nx7
Aug 17 07:23:41.391: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 07:23:41.391: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 07:23:41.391: INFO: update-demo-nautilus-g7nx7 is verified up and running
STEP: using delete to clean up resources 08/17/23 07:23:41.391
Aug 17 07:23:41.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 delete --grace-period=0 --force -f -'
Aug 17 07:23:41.478: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:23:41.478: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 17 07:23:41.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get rc,svc -l name=update-demo --no-headers'
Aug 17 07:23:41.577: INFO: stderr: "No resources found in kubectl-1335 namespace.\n"
Aug 17 07:23:41.578: INFO: stdout: ""
Aug 17 07:23:41.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 07:23:41.666: INFO: stderr: ""
Aug 17 07:23:41.666: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:23:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1335" for this suite. 08/17/23 07:23:41.669
------------------------------
â€¢ [SLOW TEST] [11.627 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:23:30.289
    Aug 17 07:23:30.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:23:30.29
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:30.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:30.299
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/17/23 07:23:30.302
    Aug 17 07:23:30.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 create -f -'
    Aug 17 07:23:30.615: INFO: stderr: ""
    Aug 17 07:23:30.615: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 07:23:30.615
    Aug 17 07:23:30.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 07:23:30.697: INFO: stderr: ""
    Aug 17 07:23:30.697: INFO: stdout: "update-demo-nautilus-9x48l update-demo-nautilus-g7nx7 "
    Aug 17 07:23:30.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 07:23:30.779: INFO: stderr: ""
    Aug 17 07:23:30.779: INFO: stdout: ""
    Aug 17 07:23:30.779: INFO: update-demo-nautilus-9x48l is created but not running
    Aug 17 07:23:35.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 07:23:35.872: INFO: stderr: ""
    Aug 17 07:23:35.872: INFO: stdout: "update-demo-nautilus-9x48l update-demo-nautilus-g7nx7 "
    Aug 17 07:23:35.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 07:23:35.957: INFO: stderr: ""
    Aug 17 07:23:35.957: INFO: stdout: ""
    Aug 17 07:23:35.957: INFO: update-demo-nautilus-9x48l is created but not running
    Aug 17 07:23:40.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 07:23:41.051: INFO: stderr: ""
    Aug 17 07:23:41.051: INFO: stdout: "update-demo-nautilus-9x48l update-demo-nautilus-g7nx7 "
    Aug 17 07:23:41.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 07:23:41.130: INFO: stderr: ""
    Aug 17 07:23:41.130: INFO: stdout: "true"
    Aug 17 07:23:41.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-9x48l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 07:23:41.219: INFO: stderr: ""
    Aug 17 07:23:41.219: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 07:23:41.219: INFO: validating pod update-demo-nautilus-9x48l
    Aug 17 07:23:41.222: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 07:23:41.222: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 07:23:41.222: INFO: update-demo-nautilus-9x48l is verified up and running
    Aug 17 07:23:41.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-g7nx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 07:23:41.308: INFO: stderr: ""
    Aug 17 07:23:41.308: INFO: stdout: "true"
    Aug 17 07:23:41.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods update-demo-nautilus-g7nx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 07:23:41.388: INFO: stderr: ""
    Aug 17 07:23:41.388: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 07:23:41.388: INFO: validating pod update-demo-nautilus-g7nx7
    Aug 17 07:23:41.391: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 07:23:41.391: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 07:23:41.391: INFO: update-demo-nautilus-g7nx7 is verified up and running
    STEP: using delete to clean up resources 08/17/23 07:23:41.391
    Aug 17 07:23:41.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 delete --grace-period=0 --force -f -'
    Aug 17 07:23:41.478: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:23:41.478: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 17 07:23:41.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get rc,svc -l name=update-demo --no-headers'
    Aug 17 07:23:41.577: INFO: stderr: "No resources found in kubectl-1335 namespace.\n"
    Aug 17 07:23:41.578: INFO: stdout: ""
    Aug 17 07:23:41.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1335 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 17 07:23:41.666: INFO: stderr: ""
    Aug 17 07:23:41.666: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:23:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1335" for this suite. 08/17/23 07:23:41.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:23:41.916
Aug 17 07:23:41.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:23:41.917
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:41.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:41.928
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-68d5835b-b550-46a0-a28d-2510c36e30a9 08/17/23 07:23:41.93
STEP: Creating a pod to test consume configMaps 08/17/23 07:23:41.944
Aug 17 07:23:42.132: INFO: Waiting up to 5m0s for pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f" in namespace "configmap-4772" to be "Succeeded or Failed"
Aug 17 07:23:42.137: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293247ms
Aug 17 07:23:44.140: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007721891s
Aug 17 07:23:46.142: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009670497s
STEP: Saw pod success 08/17/23 07:23:46.142
Aug 17 07:23:46.142: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f" satisfied condition "Succeeded or Failed"
Aug 17 07:23:46.152: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f container configmap-volume-test: <nil>
STEP: delete the pod 08/17/23 07:23:46.161
Aug 17 07:23:46.184: INFO: Waiting for pod pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f to disappear
Aug 17 07:23:46.187: INFO: Pod pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:23:46.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4772" for this suite. 08/17/23 07:23:46.193
------------------------------
â€¢ [4.287 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:23:41.916
    Aug 17 07:23:41.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:23:41.917
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:41.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:41.928
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-68d5835b-b550-46a0-a28d-2510c36e30a9 08/17/23 07:23:41.93
    STEP: Creating a pod to test consume configMaps 08/17/23 07:23:41.944
    Aug 17 07:23:42.132: INFO: Waiting up to 5m0s for pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f" in namespace "configmap-4772" to be "Succeeded or Failed"
    Aug 17 07:23:42.137: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293247ms
    Aug 17 07:23:44.140: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007721891s
    Aug 17 07:23:46.142: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009670497s
    STEP: Saw pod success 08/17/23 07:23:46.142
    Aug 17 07:23:46.142: INFO: Pod "pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f" satisfied condition "Succeeded or Failed"
    Aug 17 07:23:46.152: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f container configmap-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:23:46.161
    Aug 17 07:23:46.184: INFO: Waiting for pod pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f to disappear
    Aug 17 07:23:46.187: INFO: Pod pod-configmaps-e323eebb-7d00-4105-b8bb-43b4b666be8f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:23:46.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4772" for this suite. 08/17/23 07:23:46.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:23:46.204
Aug 17 07:23:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename endpointslice 08/17/23 07:23:46.204
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:46.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:46.215
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 07:23:48.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6482" for this suite. 08/17/23 07:23:48.286
------------------------------
â€¢ [2.085 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:23:46.204
    Aug 17 07:23:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename endpointslice 08/17/23 07:23:46.204
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:46.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:46.215
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:23:48.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6482" for this suite. 08/17/23 07:23:48.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:23:48.29
Aug 17 07:23:48.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 07:23:48.291
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:48.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:48.301
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5473 08/17/23 07:23:48.306
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/17/23 07:23:48.322
Aug 17 07:23:48.332: INFO: Found 0 stateful pods, waiting for 3
Aug 17 07:23:58.337: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:23:58.337: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:23:58.337: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 07:23:58.342
Aug 17 07:23:58.357: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/17/23 07:23:58.357
STEP: Not applying an update when the partition is greater than the number of replicas 08/17/23 07:24:08.367
STEP: Performing a canary update 08/17/23 07:24:08.368
Aug 17 07:24:08.383: INFO: Updating stateful set ss2
Aug 17 07:24:08.386: INFO: Waiting for Pod statefulset-5473/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/17/23 07:24:18.392
Aug 17 07:24:18.411: INFO: Found 1 stateful pods, waiting for 3
Aug 17 07:24:28.434: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:24:28.434: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:24:28.434: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 17 07:24:38.414: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:24:38.414: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:24:38.414: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/17/23 07:24:38.419
Aug 17 07:24:38.437: INFO: Updating stateful set ss2
Aug 17 07:24:38.440: INFO: Waiting for Pod statefulset-5473/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 17 07:24:48.459: INFO: Updating stateful set ss2
Aug 17 07:24:48.464: INFO: Waiting for StatefulSet statefulset-5473/ss2 to complete update
Aug 17 07:24:48.464: INFO: Waiting for Pod statefulset-5473/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 17 07:24:58.468: INFO: Waiting for StatefulSet statefulset-5473/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 07:25:08.469: INFO: Deleting all statefulset in ns statefulset-5473
Aug 17 07:25:08.471: INFO: Scaling statefulset ss2 to 0
Aug 17 07:25:18.482: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 07:25:18.484: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:25:18.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5473" for this suite. 08/17/23 07:25:18.497
------------------------------
â€¢ [SLOW TEST] [90.210 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:23:48.29
    Aug 17 07:23:48.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 07:23:48.291
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:23:48.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:23:48.301
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5473 08/17/23 07:23:48.306
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/17/23 07:23:48.322
    Aug 17 07:23:48.332: INFO: Found 0 stateful pods, waiting for 3
    Aug 17 07:23:58.337: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:23:58.337: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:23:58.337: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 07:23:58.342
    Aug 17 07:23:58.357: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/17/23 07:23:58.357
    STEP: Not applying an update when the partition is greater than the number of replicas 08/17/23 07:24:08.367
    STEP: Performing a canary update 08/17/23 07:24:08.368
    Aug 17 07:24:08.383: INFO: Updating stateful set ss2
    Aug 17 07:24:08.386: INFO: Waiting for Pod statefulset-5473/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/17/23 07:24:18.392
    Aug 17 07:24:18.411: INFO: Found 1 stateful pods, waiting for 3
    Aug 17 07:24:28.434: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:24:28.434: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:24:28.434: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Aug 17 07:24:38.414: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:24:38.414: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:24:38.414: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/17/23 07:24:38.419
    Aug 17 07:24:38.437: INFO: Updating stateful set ss2
    Aug 17 07:24:38.440: INFO: Waiting for Pod statefulset-5473/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 17 07:24:48.459: INFO: Updating stateful set ss2
    Aug 17 07:24:48.464: INFO: Waiting for StatefulSet statefulset-5473/ss2 to complete update
    Aug 17 07:24:48.464: INFO: Waiting for Pod statefulset-5473/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 17 07:24:58.468: INFO: Waiting for StatefulSet statefulset-5473/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 07:25:08.469: INFO: Deleting all statefulset in ns statefulset-5473
    Aug 17 07:25:08.471: INFO: Scaling statefulset ss2 to 0
    Aug 17 07:25:18.482: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 07:25:18.484: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:25:18.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5473" for this suite. 08/17/23 07:25:18.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:25:18.5
Aug 17 07:25:18.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 07:25:18.501
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:25:18.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:25:18.511
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9099 08/17/23 07:25:18.514
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/17/23 07:25:18.522
STEP: Creating stateful set ss in namespace statefulset-9099 08/17/23 07:25:18.528
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9099 08/17/23 07:25:18.533
Aug 17 07:25:18.540: INFO: Found 0 stateful pods, waiting for 1
Aug 17 07:25:28.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/17/23 07:25:28.544
Aug 17 07:25:28.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 07:25:28.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 07:25:28.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 07:25:28.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 07:25:28.672: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 17 07:25:38.677: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 07:25:38.677: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 07:25:38.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999816s
Aug 17 07:25:39.688: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998213965s
Aug 17 07:25:40.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995755713s
Aug 17 07:25:41.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.99288938s
Aug 17 07:25:42.696: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.990153315s
Aug 17 07:25:43.698: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987313137s
Aug 17 07:25:44.701: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985215039s
Aug 17 07:25:45.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.983060963s
Aug 17 07:25:46.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.980614343s
Aug 17 07:25:47.708: INFO: Verifying statefulset ss doesn't scale past 1 for another 977.985113ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9099 08/17/23 07:25:48.708
Aug 17 07:25:48.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 07:25:48.840: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 07:25:48.840: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 07:25:48.840: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 07:25:48.842: INFO: Found 1 stateful pods, waiting for 3
Aug 17 07:25:58.846: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:25:58.846: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:25:58.846: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/17/23 07:25:58.846
STEP: Scale down will halt with unhealthy stateful pod 08/17/23 07:25:58.846
Aug 17 07:25:58.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 07:25:58.982: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 07:25:58.982: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 07:25:58.982: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 07:25:58.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 07:25:59.116: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 07:25:59.116: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 07:25:59.116: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 07:25:59.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 07:25:59.252: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 07:25:59.252: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 07:25:59.252: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 07:25:59.252: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 07:25:59.254: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 17 07:26:09.294: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 07:26:09.294: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 07:26:09.294: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 07:26:09.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999732s
Aug 17 07:26:10.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997579481s
Aug 17 07:26:11.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992192862s
Aug 17 07:26:12.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98921145s
Aug 17 07:26:13.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986139052s
Aug 17 07:26:14.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983375834s
Aug 17 07:26:15.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980554895s
Aug 17 07:26:16.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977454115s
Aug 17 07:26:17.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974666746s
Aug 17 07:26:18.330: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.86914ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9099 08/17/23 07:26:19.331
Aug 17 07:26:19.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 07:26:19.453: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 07:26:19.453: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 07:26:19.453: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 07:26:19.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 07:26:19.576: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 07:26:19.576: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 07:26:19.576: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 07:26:19.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 07:26:19.708: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 07:26:19.708: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 07:26:19.708: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 07:26:19.708: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/17/23 07:26:29.721
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 07:26:29.721: INFO: Deleting all statefulset in ns statefulset-9099
Aug 17 07:26:29.723: INFO: Scaling statefulset ss to 0
Aug 17 07:26:29.729: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 07:26:29.730: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:26:29.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9099" for this suite. 08/17/23 07:26:29.744
------------------------------
â€¢ [SLOW TEST] [71.247 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:25:18.5
    Aug 17 07:25:18.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 07:25:18.501
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:25:18.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:25:18.511
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9099 08/17/23 07:25:18.514
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/17/23 07:25:18.522
    STEP: Creating stateful set ss in namespace statefulset-9099 08/17/23 07:25:18.528
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9099 08/17/23 07:25:18.533
    Aug 17 07:25:18.540: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 07:25:28.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/17/23 07:25:28.544
    Aug 17 07:25:28.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 07:25:28.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 07:25:28.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 07:25:28.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 07:25:28.672: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 17 07:25:38.677: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 07:25:38.677: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 07:25:38.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999816s
    Aug 17 07:25:39.688: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998213965s
    Aug 17 07:25:40.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995755713s
    Aug 17 07:25:41.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.99288938s
    Aug 17 07:25:42.696: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.990153315s
    Aug 17 07:25:43.698: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987313137s
    Aug 17 07:25:44.701: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985215039s
    Aug 17 07:25:45.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.983060963s
    Aug 17 07:25:46.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.980614343s
    Aug 17 07:25:47.708: INFO: Verifying statefulset ss doesn't scale past 1 for another 977.985113ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9099 08/17/23 07:25:48.708
    Aug 17 07:25:48.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 07:25:48.840: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 07:25:48.840: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 07:25:48.840: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 07:25:48.842: INFO: Found 1 stateful pods, waiting for 3
    Aug 17 07:25:58.846: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:25:58.846: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:25:58.846: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/17/23 07:25:58.846
    STEP: Scale down will halt with unhealthy stateful pod 08/17/23 07:25:58.846
    Aug 17 07:25:58.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 07:25:58.982: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 07:25:58.982: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 07:25:58.982: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 07:25:58.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 07:25:59.116: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 07:25:59.116: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 07:25:59.116: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 07:25:59.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 07:25:59.252: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 07:25:59.252: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 07:25:59.252: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 07:25:59.252: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 07:25:59.254: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 17 07:26:09.294: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 07:26:09.294: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 07:26:09.294: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 07:26:09.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999732s
    Aug 17 07:26:10.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997579481s
    Aug 17 07:26:11.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992192862s
    Aug 17 07:26:12.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98921145s
    Aug 17 07:26:13.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986139052s
    Aug 17 07:26:14.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983375834s
    Aug 17 07:26:15.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980554895s
    Aug 17 07:26:16.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977454115s
    Aug 17 07:26:17.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974666746s
    Aug 17 07:26:18.330: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.86914ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9099 08/17/23 07:26:19.331
    Aug 17 07:26:19.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 07:26:19.453: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 07:26:19.453: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 07:26:19.453: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 07:26:19.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 07:26:19.576: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 07:26:19.576: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 07:26:19.576: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 07:26:19.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-9099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 07:26:19.708: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 07:26:19.708: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 07:26:19.708: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 07:26:19.708: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/17/23 07:26:29.721
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 07:26:29.721: INFO: Deleting all statefulset in ns statefulset-9099
    Aug 17 07:26:29.723: INFO: Scaling statefulset ss to 0
    Aug 17 07:26:29.729: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 07:26:29.730: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:26:29.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9099" for this suite. 08/17/23 07:26:29.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:26:29.749
Aug 17 07:26:29.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:26:29.749
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:26:29.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:26:29.761
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/17/23 07:26:29.764
STEP: fetching the ConfigMap 08/17/23 07:26:29.773
STEP: patching the ConfigMap 08/17/23 07:26:29.778
STEP: listing all ConfigMaps in all namespaces with a label selector 08/17/23 07:26:29.783
STEP: deleting the ConfigMap by collection with a label selector 08/17/23 07:26:29.797
STEP: listing all ConfigMaps in test namespace 08/17/23 07:26:29.801
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:26:29.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8299" for this suite. 08/17/23 07:26:29.805
------------------------------
â€¢ [0.059 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:26:29.749
    Aug 17 07:26:29.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:26:29.749
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:26:29.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:26:29.761
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/17/23 07:26:29.764
    STEP: fetching the ConfigMap 08/17/23 07:26:29.773
    STEP: patching the ConfigMap 08/17/23 07:26:29.778
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/17/23 07:26:29.783
    STEP: deleting the ConfigMap by collection with a label selector 08/17/23 07:26:29.797
    STEP: listing all ConfigMaps in test namespace 08/17/23 07:26:29.801
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:26:29.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8299" for this suite. 08/17/23 07:26:29.805
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:26:29.809
Aug 17 07:26:29.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename namespaces 08/17/23 07:26:29.809
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:26:29.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:26:29.833
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-9684" 08/17/23 07:26:29.838
Aug 17 07:26:29.886: INFO: Namespace "namespaces-9684" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"cc861678-4c64-437f-95a3-e3501ec4e99d", "kubernetes.io/metadata.name":"namespaces-9684", "namespaces-9684":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:26:29.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9684" for this suite. 08/17/23 07:26:29.901
------------------------------
â€¢ [0.103 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:26:29.809
    Aug 17 07:26:29.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename namespaces 08/17/23 07:26:29.809
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:26:29.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:26:29.833
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-9684" 08/17/23 07:26:29.838
    Aug 17 07:26:29.886: INFO: Namespace "namespaces-9684" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"cc861678-4c64-437f-95a3-e3501ec4e99d", "kubernetes.io/metadata.name":"namespaces-9684", "namespaces-9684":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:26:29.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9684" for this suite. 08/17/23 07:26:29.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:26:29.913
Aug 17 07:26:29.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-watch 08/17/23 07:26:29.914
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:26:29.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:26:29.939
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 17 07:26:29.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Creating first CR  08/17/23 07:26:32.484
Aug 17 07:26:32.486: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:32Z]] name:name1 resourceVersion:27036061 uid:7d5e786c-af5c-49e0-b08e-45cb14e9e2ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/17/23 07:26:42.487
Aug 17 07:26:42.491: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:42Z]] name:name2 resourceVersion:27036274 uid:9ea46b8f-873c-402b-b861-2dadee588711] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/17/23 07:26:52.493
Aug 17 07:26:52.497: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:52Z]] name:name1 resourceVersion:27036350 uid:7d5e786c-af5c-49e0-b08e-45cb14e9e2ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/17/23 07:27:02.497
Aug 17 07:27:02.501: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:27:02Z]] name:name2 resourceVersion:27036426 uid:9ea46b8f-873c-402b-b861-2dadee588711] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/17/23 07:27:12.502
Aug 17 07:27:12.507: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:52Z]] name:name1 resourceVersion:27036502 uid:7d5e786c-af5c-49e0-b08e-45cb14e9e2ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/17/23 07:27:22.508
Aug 17 07:27:22.512: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:27:02Z]] name:name2 resourceVersion:27036575 uid:9ea46b8f-873c-402b-b861-2dadee588711] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:27:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-3582" for this suite. 08/17/23 07:27:33.023
------------------------------
â€¢ [SLOW TEST] [63.114 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:26:29.913
    Aug 17 07:26:29.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-watch 08/17/23 07:26:29.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:26:29.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:26:29.939
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 17 07:26:29.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Creating first CR  08/17/23 07:26:32.484
    Aug 17 07:26:32.486: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:32Z]] name:name1 resourceVersion:27036061 uid:7d5e786c-af5c-49e0-b08e-45cb14e9e2ce] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/17/23 07:26:42.487
    Aug 17 07:26:42.491: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:42Z]] name:name2 resourceVersion:27036274 uid:9ea46b8f-873c-402b-b861-2dadee588711] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/17/23 07:26:52.493
    Aug 17 07:26:52.497: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:52Z]] name:name1 resourceVersion:27036350 uid:7d5e786c-af5c-49e0-b08e-45cb14e9e2ce] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/17/23 07:27:02.497
    Aug 17 07:27:02.501: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:27:02Z]] name:name2 resourceVersion:27036426 uid:9ea46b8f-873c-402b-b861-2dadee588711] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/17/23 07:27:12.502
    Aug 17 07:27:12.507: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:26:52Z]] name:name1 resourceVersion:27036502 uid:7d5e786c-af5c-49e0-b08e-45cb14e9e2ce] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/17/23 07:27:22.508
    Aug 17 07:27:22.512: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T07:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T07:27:02Z]] name:name2 resourceVersion:27036575 uid:9ea46b8f-873c-402b-b861-2dadee588711] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:27:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-3582" for this suite. 08/17/23 07:27:33.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:27:33.028
Aug 17 07:27:33.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:27:33.029
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:33.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:33.042
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:27:33.072
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:27:33.335
STEP: Deploying the webhook pod 08/17/23 07:27:33.338
STEP: Wait for the deployment to be ready 08/17/23 07:27:33.346
Aug 17 07:27:33.350: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:27:35.355
STEP: Verifying the service has paired with the endpoint 08/17/23 07:27:35.363
Aug 17 07:27:36.363: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 17 07:27:36.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5920-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 07:27:36.873
STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 07:27:36.884
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:27:39.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9512" for this suite. 08/17/23 07:27:39.462
STEP: Destroying namespace "webhook-9512-markers" for this suite. 08/17/23 07:27:39.465
------------------------------
â€¢ [SLOW TEST] [6.446 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:27:33.028
    Aug 17 07:27:33.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:27:33.029
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:33.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:33.042
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:27:33.072
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:27:33.335
    STEP: Deploying the webhook pod 08/17/23 07:27:33.338
    STEP: Wait for the deployment to be ready 08/17/23 07:27:33.346
    Aug 17 07:27:33.350: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:27:35.355
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:27:35.363
    Aug 17 07:27:36.363: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 17 07:27:36.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5920-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 07:27:36.873
    STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 07:27:36.884
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:27:39.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9512" for this suite. 08/17/23 07:27:39.462
    STEP: Destroying namespace "webhook-9512-markers" for this suite. 08/17/23 07:27:39.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:27:39.474
Aug 17 07:27:39.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 07:27:39.475
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:39.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:39.497
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 07:27:39.506
Aug 17 07:27:39.512: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1310" to be "running and ready"
Aug 17 07:27:39.514: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.345374ms
Aug 17 07:27:39.514: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:27:41.518: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005669681s
Aug 17 07:27:41.518: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 07:27:41.518: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/17/23 07:27:41.519
Aug 17 07:27:41.522: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1310" to be "running and ready"
Aug 17 07:27:41.525: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095411ms
Aug 17 07:27:41.525: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:27:43.527: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004777066s
Aug 17 07:27:43.527: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 17 07:27:43.527: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/17/23 07:27:43.529
STEP: delete the pod with lifecycle hook 08/17/23 07:27:43.54
Aug 17 07:27:43.544: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 07:27:43.547: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 17 07:27:45.548: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 07:27:45.550: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 17 07:27:47.548: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 07:27:47.550: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 07:27:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1310" for this suite. 08/17/23 07:27:47.553
------------------------------
â€¢ [SLOW TEST] [8.084 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:27:39.474
    Aug 17 07:27:39.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 07:27:39.475
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:39.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:39.497
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 07:27:39.506
    Aug 17 07:27:39.512: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1310" to be "running and ready"
    Aug 17 07:27:39.514: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.345374ms
    Aug 17 07:27:39.514: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:27:41.518: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005669681s
    Aug 17 07:27:41.518: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 07:27:41.518: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/17/23 07:27:41.519
    Aug 17 07:27:41.522: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1310" to be "running and ready"
    Aug 17 07:27:41.525: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095411ms
    Aug 17 07:27:41.525: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:27:43.527: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004777066s
    Aug 17 07:27:43.527: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 17 07:27:43.527: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/17/23 07:27:43.529
    STEP: delete the pod with lifecycle hook 08/17/23 07:27:43.54
    Aug 17 07:27:43.544: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 17 07:27:43.547: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 17 07:27:45.548: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 17 07:27:45.550: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 17 07:27:47.548: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 17 07:27:47.550: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:27:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1310" for this suite. 08/17/23 07:27:47.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:27:47.559
Aug 17 07:27:47.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:27:47.56
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:47.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:47.573
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-5864179f-151b-49eb-8316-091bc7f34b75 08/17/23 07:27:47.575
STEP: Creating a pod to test consume secrets 08/17/23 07:27:47.594
Aug 17 07:27:47.612: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b" in namespace "projected-3472" to be "Succeeded or Failed"
Aug 17 07:27:47.615: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901492ms
Aug 17 07:27:49.617: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005246363s
Aug 17 07:27:51.618: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006004745s
STEP: Saw pod success 08/17/23 07:27:51.618
Aug 17 07:27:51.618: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b" satisfied condition "Succeeded or Failed"
Aug 17 07:27:51.620: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 07:27:51.623
Aug 17 07:27:51.629: INFO: Waiting for pod pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b to disappear
Aug 17 07:27:51.631: INFO: Pod pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 07:27:51.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3472" for this suite. 08/17/23 07:27:51.633
------------------------------
â€¢ [4.077 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:27:47.559
    Aug 17 07:27:47.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:27:47.56
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:47.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:47.573
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-5864179f-151b-49eb-8316-091bc7f34b75 08/17/23 07:27:47.575
    STEP: Creating a pod to test consume secrets 08/17/23 07:27:47.594
    Aug 17 07:27:47.612: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b" in namespace "projected-3472" to be "Succeeded or Failed"
    Aug 17 07:27:47.615: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901492ms
    Aug 17 07:27:49.617: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005246363s
    Aug 17 07:27:51.618: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006004745s
    STEP: Saw pod success 08/17/23 07:27:51.618
    Aug 17 07:27:51.618: INFO: Pod "pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b" satisfied condition "Succeeded or Failed"
    Aug 17 07:27:51.620: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:27:51.623
    Aug 17 07:27:51.629: INFO: Waiting for pod pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b to disappear
    Aug 17 07:27:51.631: INFO: Pod pod-projected-secrets-4e5025d8-c4b7-4683-96eb-55eae47c893b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:27:51.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3472" for this suite. 08/17/23 07:27:51.633
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:27:51.637
Aug 17 07:27:51.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 07:27:51.637
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:51.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:51.655
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9331 08/17/23 07:27:51.66
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Aug 17 07:27:51.728: INFO: Found 0 stateful pods, waiting for 1
Aug 17 07:28:01.732: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/17/23 07:28:01.736
W0817 07:28:01.745620      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 17 07:28:01.749: INFO: Found 1 stateful pods, waiting for 2
Aug 17 07:28:11.753: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:28:11.753: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/17/23 07:28:11.757
STEP: Delete all of the StatefulSets 08/17/23 07:28:11.759
STEP: Verify that StatefulSets have been deleted 08/17/23 07:28:11.763
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 07:28:11.765: INFO: Deleting all statefulset in ns statefulset-9331
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:11.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9331" for this suite. 08/17/23 07:28:11.775
------------------------------
â€¢ [SLOW TEST] [20.141 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:27:51.637
    Aug 17 07:27:51.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 07:27:51.637
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:27:51.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:27:51.655
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9331 08/17/23 07:27:51.66
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Aug 17 07:27:51.728: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 07:28:01.732: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/17/23 07:28:01.736
    W0817 07:28:01.745620      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 17 07:28:01.749: INFO: Found 1 stateful pods, waiting for 2
    Aug 17 07:28:11.753: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:28:11.753: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/17/23 07:28:11.757
    STEP: Delete all of the StatefulSets 08/17/23 07:28:11.759
    STEP: Verify that StatefulSets have been deleted 08/17/23 07:28:11.763
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 07:28:11.765: INFO: Deleting all statefulset in ns statefulset-9331
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:11.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9331" for this suite. 08/17/23 07:28:11.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:11.78
Aug 17 07:28:11.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename job 08/17/23 07:28:11.781
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:11.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:11.835
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/17/23 07:28:11.84
STEP: Ensuring active pods == parallelism 08/17/23 07:28:11.853
STEP: Orphaning one of the Job's Pods 08/17/23 07:28:13.857
Aug 17 07:28:14.367: INFO: Successfully updated pod "adopt-release-4g88z"
STEP: Checking that the Job readopts the Pod 08/17/23 07:28:14.367
Aug 17 07:28:14.367: INFO: Waiting up to 15m0s for pod "adopt-release-4g88z" in namespace "job-985" to be "adopted"
Aug 17 07:28:14.369: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 1.521072ms
Aug 17 07:28:16.372: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004301145s
Aug 17 07:28:16.372: INFO: Pod "adopt-release-4g88z" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/17/23 07:28:16.372
Aug 17 07:28:16.880: INFO: Successfully updated pod "adopt-release-4g88z"
STEP: Checking that the Job releases the Pod 08/17/23 07:28:16.88
Aug 17 07:28:16.880: INFO: Waiting up to 15m0s for pod "adopt-release-4g88z" in namespace "job-985" to be "released"
Aug 17 07:28:16.882: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 1.826249ms
Aug 17 07:28:18.885: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004970175s
Aug 17 07:28:18.885: INFO: Pod "adopt-release-4g88z" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:18.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-985" for this suite. 08/17/23 07:28:18.887
------------------------------
â€¢ [SLOW TEST] [7.111 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:11.78
    Aug 17 07:28:11.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename job 08/17/23 07:28:11.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:11.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:11.835
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/17/23 07:28:11.84
    STEP: Ensuring active pods == parallelism 08/17/23 07:28:11.853
    STEP: Orphaning one of the Job's Pods 08/17/23 07:28:13.857
    Aug 17 07:28:14.367: INFO: Successfully updated pod "adopt-release-4g88z"
    STEP: Checking that the Job readopts the Pod 08/17/23 07:28:14.367
    Aug 17 07:28:14.367: INFO: Waiting up to 15m0s for pod "adopt-release-4g88z" in namespace "job-985" to be "adopted"
    Aug 17 07:28:14.369: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 1.521072ms
    Aug 17 07:28:16.372: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004301145s
    Aug 17 07:28:16.372: INFO: Pod "adopt-release-4g88z" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/17/23 07:28:16.372
    Aug 17 07:28:16.880: INFO: Successfully updated pod "adopt-release-4g88z"
    STEP: Checking that the Job releases the Pod 08/17/23 07:28:16.88
    Aug 17 07:28:16.880: INFO: Waiting up to 15m0s for pod "adopt-release-4g88z" in namespace "job-985" to be "released"
    Aug 17 07:28:16.882: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 1.826249ms
    Aug 17 07:28:18.885: INFO: Pod "adopt-release-4g88z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004970175s
    Aug 17 07:28:18.885: INFO: Pod "adopt-release-4g88z" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:18.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-985" for this suite. 08/17/23 07:28:18.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:18.891
Aug 17 07:28:18.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 07:28:18.892
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:18.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:18.901
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 07:28:18.905
Aug 17 07:28:18.913: INFO: Waiting up to 5m0s for pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c" in namespace "emptydir-9425" to be "Succeeded or Failed"
Aug 17 07:28:18.916: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12319ms
Aug 17 07:28:20.919: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006517919s
Aug 17 07:28:22.919: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006588565s
STEP: Saw pod success 08/17/23 07:28:22.919
Aug 17 07:28:22.919: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c" satisfied condition "Succeeded or Failed"
Aug 17 07:28:22.921: INFO: Trying to get logs from node yst-node2 pod pod-4861f83c-2a31-4525-afaf-974bfadd429c container test-container: <nil>
STEP: delete the pod 08/17/23 07:28:22.925
Aug 17 07:28:22.931: INFO: Waiting for pod pod-4861f83c-2a31-4525-afaf-974bfadd429c to disappear
Aug 17 07:28:22.933: INFO: Pod pod-4861f83c-2a31-4525-afaf-974bfadd429c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:22.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9425" for this suite. 08/17/23 07:28:22.936
------------------------------
â€¢ [4.047 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:18.891
    Aug 17 07:28:18.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 07:28:18.892
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:18.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:18.901
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 07:28:18.905
    Aug 17 07:28:18.913: INFO: Waiting up to 5m0s for pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c" in namespace "emptydir-9425" to be "Succeeded or Failed"
    Aug 17 07:28:18.916: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12319ms
    Aug 17 07:28:20.919: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006517919s
    Aug 17 07:28:22.919: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006588565s
    STEP: Saw pod success 08/17/23 07:28:22.919
    Aug 17 07:28:22.919: INFO: Pod "pod-4861f83c-2a31-4525-afaf-974bfadd429c" satisfied condition "Succeeded or Failed"
    Aug 17 07:28:22.921: INFO: Trying to get logs from node yst-node2 pod pod-4861f83c-2a31-4525-afaf-974bfadd429c container test-container: <nil>
    STEP: delete the pod 08/17/23 07:28:22.925
    Aug 17 07:28:22.931: INFO: Waiting for pod pod-4861f83c-2a31-4525-afaf-974bfadd429c to disappear
    Aug 17 07:28:22.933: INFO: Pod pod-4861f83c-2a31-4525-afaf-974bfadd429c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:22.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9425" for this suite. 08/17/23 07:28:22.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:22.94
Aug 17 07:28:22.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:28:22.941
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:22.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:22.954
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 17 07:28:22.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1360 version'
Aug 17 07:28:23.021: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 17 07:28:23.021: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:49:08Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:23.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1360" for this suite. 08/17/23 07:28:23.024
------------------------------
â€¢ [0.088 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:22.94
    Aug 17 07:28:22.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:28:22.941
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:22.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:22.954
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 17 07:28:22.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-1360 version'
    Aug 17 07:28:23.021: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 17 07:28:23.021: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:49:08Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:23.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1360" for this suite. 08/17/23 07:28:23.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:23.029
Aug 17 07:28:23.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-webhook 08/17/23 07:28:23.03
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:23.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:23.053
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/17/23 07:28:23.056
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 07:28:23.301
STEP: Deploying the custom resource conversion webhook pod 08/17/23 07:28:23.305
STEP: Wait for the deployment to be ready 08/17/23 07:28:23.315
Aug 17 07:28:23.320: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:28:25.326
STEP: Verifying the service has paired with the endpoint 08/17/23 07:28:25.333
Aug 17 07:28:26.333: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 17 07:28:26.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Creating a v1 custom resource 08/17/23 07:28:28.895
STEP: v2 custom resource should be converted 08/17/23 07:28:28.898
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:29.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-5899" for this suite. 08/17/23 07:28:29.493
------------------------------
â€¢ [SLOW TEST] [6.471 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:23.029
    Aug 17 07:28:23.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-webhook 08/17/23 07:28:23.03
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:23.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:23.053
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/17/23 07:28:23.056
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 07:28:23.301
    STEP: Deploying the custom resource conversion webhook pod 08/17/23 07:28:23.305
    STEP: Wait for the deployment to be ready 08/17/23 07:28:23.315
    Aug 17 07:28:23.320: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:28:25.326
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:28:25.333
    Aug 17 07:28:26.333: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 17 07:28:26.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Creating a v1 custom resource 08/17/23 07:28:28.895
    STEP: v2 custom resource should be converted 08/17/23 07:28:28.898
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:29.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-5899" for this suite. 08/17/23 07:28:29.493
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:29.5
Aug 17 07:28:29.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:28:29.501
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:29.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:29.537
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:29.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7885" for this suite. 08/17/23 07:28:29.585
------------------------------
â€¢ [0.089 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:29.5
    Aug 17 07:28:29.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:28:29.501
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:29.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:29.537
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:29.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7885" for this suite. 08/17/23 07:28:29.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:29.59
Aug 17 07:28:29.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 07:28:29.591
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:29.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:29.604
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 07:28:29.611
Aug 17 07:28:29.630: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-178" to be "running and ready"
Aug 17 07:28:29.632: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280176ms
Aug 17 07:28:29.632: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:28:31.634: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004455103s
Aug 17 07:28:31.634: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 07:28:31.634: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/17/23 07:28:31.636
Aug 17 07:28:31.640: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-178" to be "running and ready"
Aug 17 07:28:31.644: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684533ms
Aug 17 07:28:31.644: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:28:33.646: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006377111s
Aug 17 07:28:33.646: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 17 07:28:33.646: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/17/23 07:28:33.648
Aug 17 07:28:33.651: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 07:28:33.653: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 07:28:35.654: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 07:28:35.656: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/17/23 07:28:35.656
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:35.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-178" for this suite. 08/17/23 07:28:35.67
------------------------------
â€¢ [SLOW TEST] [6.083 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:29.59
    Aug 17 07:28:29.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 07:28:29.591
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:29.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:29.604
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 07:28:29.611
    Aug 17 07:28:29.630: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-178" to be "running and ready"
    Aug 17 07:28:29.632: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280176ms
    Aug 17 07:28:29.632: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:28:31.634: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004455103s
    Aug 17 07:28:31.634: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 07:28:31.634: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/17/23 07:28:31.636
    Aug 17 07:28:31.640: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-178" to be "running and ready"
    Aug 17 07:28:31.644: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684533ms
    Aug 17 07:28:31.644: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:28:33.646: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006377111s
    Aug 17 07:28:33.646: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 17 07:28:33.646: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/17/23 07:28:33.648
    Aug 17 07:28:33.651: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 17 07:28:33.653: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 17 07:28:35.654: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 17 07:28:35.656: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/17/23 07:28:35.656
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:35.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-178" for this suite. 08/17/23 07:28:35.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:35.674
Aug 17 07:28:35.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replicaset 08/17/23 07:28:35.674
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:35.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:35.698
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/17/23 07:28:35.706
STEP: Verify that the required pods have come up. 08/17/23 07:28:35.713
Aug 17 07:28:35.716: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 07:28:40.718: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 07:28:40.718
STEP: Getting /status 08/17/23 07:28:40.719
Aug 17 07:28:40.720: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/17/23 07:28:40.72
Aug 17 07:28:40.725: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/17/23 07:28:40.725
Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: ADDED
Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 07:28:40.727: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 07:28:40.727: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/17/23 07:28:40.727
Aug 17 07:28:40.727: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 17 07:28:40.731: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/17/23 07:28:40.731
Aug 17 07:28:40.732: INFO: Observed &ReplicaSet event: ADDED
Aug 17 07:28:40.732: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 07:28:40.732: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 07:28:40.733: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 07:28:40.733: INFO: Observed replicaset test-rs in namespace replicaset-3226 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 07:28:40.733: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 07:28:40.733: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 17 07:28:40.733: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:40.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3226" for this suite. 08/17/23 07:28:40.736
------------------------------
â€¢ [SLOW TEST] [5.066 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:35.674
    Aug 17 07:28:35.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replicaset 08/17/23 07:28:35.674
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:35.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:35.698
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/17/23 07:28:35.706
    STEP: Verify that the required pods have come up. 08/17/23 07:28:35.713
    Aug 17 07:28:35.716: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 07:28:40.718: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 07:28:40.718
    STEP: Getting /status 08/17/23 07:28:40.719
    Aug 17 07:28:40.720: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/17/23 07:28:40.72
    Aug 17 07:28:40.725: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/17/23 07:28:40.725
    Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: ADDED
    Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 07:28:40.727: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 07:28:40.727: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 07:28:40.727: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/17/23 07:28:40.727
    Aug 17 07:28:40.727: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 17 07:28:40.731: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/17/23 07:28:40.731
    Aug 17 07:28:40.732: INFO: Observed &ReplicaSet event: ADDED
    Aug 17 07:28:40.732: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 07:28:40.732: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 07:28:40.733: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 07:28:40.733: INFO: Observed replicaset test-rs in namespace replicaset-3226 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 07:28:40.733: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 07:28:40.733: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 17 07:28:40.733: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:40.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3226" for this suite. 08/17/23 07:28:40.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:40.741
Aug 17 07:28:40.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename job 08/17/23 07:28:40.741
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:40.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:40.764
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/17/23 07:28:40.766
STEP: Ensure pods equal to parallelism count is attached to the job 08/17/23 07:28:40.776
STEP: patching /status 08/17/23 07:28:42.779
STEP: updating /status 08/17/23 07:28:42.783
STEP: get /status 08/17/23 07:28:42.803
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:42.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1750" for this suite. 08/17/23 07:28:42.807
------------------------------
â€¢ [2.071 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:40.741
    Aug 17 07:28:40.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename job 08/17/23 07:28:40.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:40.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:40.764
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/17/23 07:28:40.766
    STEP: Ensure pods equal to parallelism count is attached to the job 08/17/23 07:28:40.776
    STEP: patching /status 08/17/23 07:28:42.779
    STEP: updating /status 08/17/23 07:28:42.783
    STEP: get /status 08/17/23 07:28:42.803
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:42.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1750" for this suite. 08/17/23 07:28:42.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:42.812
Aug 17 07:28:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 07:28:42.813
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:42.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:42.824
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Aug 17 07:28:42.851: INFO: Waiting up to 2m0s for pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" in namespace "var-expansion-7019" to be "container 0 failed with reason CreateContainerConfigError"
Aug 17 07:28:42.854: INFO: Pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477712ms
Aug 17 07:28:44.856: INFO: Pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005074054s
Aug 17 07:28:44.856: INFO: Pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 17 07:28:44.856: INFO: Deleting pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" in namespace "var-expansion-7019"
Aug 17 07:28:44.859: INFO: Wait up to 5m0s for pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:46.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7019" for this suite. 08/17/23 07:28:46.866
------------------------------
â€¢ [4.058 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:42.812
    Aug 17 07:28:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 07:28:42.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:42.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:42.824
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Aug 17 07:28:42.851: INFO: Waiting up to 2m0s for pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" in namespace "var-expansion-7019" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 17 07:28:42.854: INFO: Pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477712ms
    Aug 17 07:28:44.856: INFO: Pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005074054s
    Aug 17 07:28:44.856: INFO: Pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 17 07:28:44.856: INFO: Deleting pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" in namespace "var-expansion-7019"
    Aug 17 07:28:44.859: INFO: Wait up to 5m0s for pod "var-expansion-51fd16ff-8673-4e2e-aff5-5bbdf879f5f1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:46.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7019" for this suite. 08/17/23 07:28:46.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:46.871
Aug 17 07:28:46.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:28:46.871
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:46.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:46.882
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/17/23 07:28:46.885
Aug 17 07:28:46.892: INFO: Waiting up to 5m0s for pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4" in namespace "downward-api-8468" to be "Succeeded or Failed"
Aug 17 07:28:46.904: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.602172ms
Aug 17 07:28:48.907: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014649569s
Aug 17 07:28:50.906: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014245379s
STEP: Saw pod success 08/17/23 07:28:50.906
Aug 17 07:28:50.906: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4" satisfied condition "Succeeded or Failed"
Aug 17 07:28:50.908: INFO: Trying to get logs from node yst-node2 pod downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4 container dapi-container: <nil>
STEP: delete the pod 08/17/23 07:28:50.912
Aug 17 07:28:50.918: INFO: Waiting for pod downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4 to disappear
Aug 17 07:28:50.919: INFO: Pod downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:50.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8468" for this suite. 08/17/23 07:28:50.922
------------------------------
â€¢ [4.054 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:46.871
    Aug 17 07:28:46.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:28:46.871
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:46.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:46.882
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/17/23 07:28:46.885
    Aug 17 07:28:46.892: INFO: Waiting up to 5m0s for pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4" in namespace "downward-api-8468" to be "Succeeded or Failed"
    Aug 17 07:28:46.904: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.602172ms
    Aug 17 07:28:48.907: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014649569s
    Aug 17 07:28:50.906: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014245379s
    STEP: Saw pod success 08/17/23 07:28:50.906
    Aug 17 07:28:50.906: INFO: Pod "downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4" satisfied condition "Succeeded or Failed"
    Aug 17 07:28:50.908: INFO: Trying to get logs from node yst-node2 pod downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 07:28:50.912
    Aug 17 07:28:50.918: INFO: Waiting for pod downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4 to disappear
    Aug 17 07:28:50.919: INFO: Pod downward-api-e1fc4b20-4616-4f0a-be7b-86b157b291e4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:50.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8468" for this suite. 08/17/23 07:28:50.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:50.925
Aug 17 07:28:50.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename namespaces 08/17/23 07:28:50.926
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:50.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:50.941
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/17/23 07:28:50.943
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:51.005
STEP: Creating a service in the namespace 08/17/23 07:28:51.008
STEP: Deleting the namespace 08/17/23 07:28:51.028
STEP: Waiting for the namespace to be removed. 08/17/23 07:28:51.036
STEP: Recreating the namespace 08/17/23 07:28:57.039
STEP: Verifying there is no service in the namespace 08/17/23 07:28:57.046
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:28:57.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1257" for this suite. 08/17/23 07:28:57.067
STEP: Destroying namespace "nsdeletetest-9780" for this suite. 08/17/23 07:28:57.07
Aug 17 07:28:57.072: INFO: Namespace nsdeletetest-9780 was already deleted
STEP: Destroying namespace "nsdeletetest-7611" for this suite. 08/17/23 07:28:57.072
------------------------------
â€¢ [SLOW TEST] [6.150 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:50.925
    Aug 17 07:28:50.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename namespaces 08/17/23 07:28:50.926
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:50.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:50.941
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/17/23 07:28:50.943
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:51.005
    STEP: Creating a service in the namespace 08/17/23 07:28:51.008
    STEP: Deleting the namespace 08/17/23 07:28:51.028
    STEP: Waiting for the namespace to be removed. 08/17/23 07:28:51.036
    STEP: Recreating the namespace 08/17/23 07:28:57.039
    STEP: Verifying there is no service in the namespace 08/17/23 07:28:57.046
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:28:57.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1257" for this suite. 08/17/23 07:28:57.067
    STEP: Destroying namespace "nsdeletetest-9780" for this suite. 08/17/23 07:28:57.07
    Aug 17 07:28:57.072: INFO: Namespace nsdeletetest-9780 was already deleted
    STEP: Destroying namespace "nsdeletetest-7611" for this suite. 08/17/23 07:28:57.072
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:28:57.075
Aug 17 07:28:57.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename job 08/17/23 07:28:57.076
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:57.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:57.086
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/17/23 07:28:57.1
STEP: Patching the Job 08/17/23 07:28:57.107
STEP: Watching for Job to be patched 08/17/23 07:28:57.118
Aug 17 07:28:57.119: INFO: Event ADDED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 17 07:28:57.119: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 17 07:28:57.119: INFO: Event MODIFIED found for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/17/23 07:28:57.119
STEP: Watching for Job to be updated 08/17/23 07:28:57.124
Aug 17 07:28:57.125: INFO: Event MODIFIED found for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 07:28:57.125: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/17/23 07:28:57.125
Aug 17 07:28:57.126: INFO: Job: e2e-wlm25 as labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched]
STEP: Waiting for job to complete 08/17/23 07:28:57.126
STEP: Delete a job collection with a labelselector 08/17/23 07:29:05.129
STEP: Watching for Job to be deleted 08/17/23 07:29:05.132
Aug 17 07:29:05.133: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 07:29:05.133: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 07:29:05.133: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 07:29:05.134: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 07:29:05.134: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 07:29:05.134: INFO: Event DELETED found for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/17/23 07:29:05.134
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2461" for this suite. 08/17/23 07:29:05.14
------------------------------
â€¢ [SLOW TEST] [8.068 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:28:57.075
    Aug 17 07:28:57.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename job 08/17/23 07:28:57.076
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:28:57.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:28:57.086
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/17/23 07:28:57.1
    STEP: Patching the Job 08/17/23 07:28:57.107
    STEP: Watching for Job to be patched 08/17/23 07:28:57.118
    Aug 17 07:28:57.119: INFO: Event ADDED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 17 07:28:57.119: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 17 07:28:57.119: INFO: Event MODIFIED found for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/17/23 07:28:57.119
    STEP: Watching for Job to be updated 08/17/23 07:28:57.124
    Aug 17 07:28:57.125: INFO: Event MODIFIED found for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 07:28:57.125: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/17/23 07:28:57.125
    Aug 17 07:28:57.126: INFO: Job: e2e-wlm25 as labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched]
    STEP: Waiting for job to complete 08/17/23 07:28:57.126
    STEP: Delete a job collection with a labelselector 08/17/23 07:29:05.129
    STEP: Watching for Job to be deleted 08/17/23 07:29:05.132
    Aug 17 07:29:05.133: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 07:29:05.133: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 07:29:05.133: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 07:29:05.134: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 07:29:05.134: INFO: Event MODIFIED observed for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 07:29:05.134: INFO: Event DELETED found for Job e2e-wlm25 in namespace job-2461 with labels: map[e2e-job-label:e2e-wlm25 e2e-wlm25:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/17/23 07:29:05.134
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2461" for this suite. 08/17/23 07:29:05.14
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:05.144
Aug 17 07:29:05.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:29:05.145
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:05.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:05.157
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:29:05.16
Aug 17 07:29:05.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01" in namespace "downward-api-1163" to be "Succeeded or Failed"
Aug 17 07:29:05.176: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416612ms
Aug 17 07:29:07.179: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004368552s
Aug 17 07:29:09.179: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004362119s
STEP: Saw pod success 08/17/23 07:29:09.179
Aug 17 07:29:09.179: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01" satisfied condition "Succeeded or Failed"
Aug 17 07:29:09.180: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01 container client-container: <nil>
STEP: delete the pod 08/17/23 07:29:09.183
Aug 17 07:29:09.188: INFO: Waiting for pod downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01 to disappear
Aug 17 07:29:09.190: INFO: Pod downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:09.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1163" for this suite. 08/17/23 07:29:09.192
------------------------------
â€¢ [4.098 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:05.144
    Aug 17 07:29:05.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:29:05.145
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:05.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:05.157
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:29:05.16
    Aug 17 07:29:05.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01" in namespace "downward-api-1163" to be "Succeeded or Failed"
    Aug 17 07:29:05.176: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416612ms
    Aug 17 07:29:07.179: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004368552s
    Aug 17 07:29:09.179: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004362119s
    STEP: Saw pod success 08/17/23 07:29:09.179
    Aug 17 07:29:09.179: INFO: Pod "downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01" satisfied condition "Succeeded or Failed"
    Aug 17 07:29:09.180: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:29:09.183
    Aug 17 07:29:09.188: INFO: Waiting for pod downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01 to disappear
    Aug 17 07:29:09.190: INFO: Pod downwardapi-volume-353c29f7-c5fc-4716-8f6f-e102689f3a01 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:09.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1163" for this suite. 08/17/23 07:29:09.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:09.244
Aug 17 07:29:09.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 07:29:09.245
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:09.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:09.256
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/17/23 07:29:09.258
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/17/23 07:29:09.258
STEP: creating a pod to probe DNS 08/17/23 07:29:09.259
STEP: submitting the pod to kubernetes 08/17/23 07:29:09.259
Aug 17 07:29:09.274: INFO: Waiting up to 15m0s for pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6" in namespace "dns-6183" to be "running"
Aug 17 07:29:09.277: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430119ms
Aug 17 07:29:11.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00601688s
Aug 17 07:29:13.282: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007969419s
Aug 17 07:29:15.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006248661s
Aug 17 07:29:17.279: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005604225s
Aug 17 07:29:19.279: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005496252s
Aug 17 07:29:21.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Running", Reason="", readiness=true. Elapsed: 12.006526827s
Aug 17 07:29:21.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6" satisfied condition "running"
STEP: retrieving the pod 08/17/23 07:29:21.28
STEP: looking for the results for each expected name from probers 08/17/23 07:29:21.282
Aug 17 07:29:21.289: INFO: DNS probes using dns-6183/dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6 succeeded

STEP: deleting the pod 08/17/23 07:29:21.289
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:21.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6183" for this suite. 08/17/23 07:29:21.298
------------------------------
â€¢ [SLOW TEST] [12.057 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:09.244
    Aug 17 07:29:09.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 07:29:09.245
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:09.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:09.256
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/17/23 07:29:09.258
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/17/23 07:29:09.258
    STEP: creating a pod to probe DNS 08/17/23 07:29:09.259
    STEP: submitting the pod to kubernetes 08/17/23 07:29:09.259
    Aug 17 07:29:09.274: INFO: Waiting up to 15m0s for pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6" in namespace "dns-6183" to be "running"
    Aug 17 07:29:09.277: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430119ms
    Aug 17 07:29:11.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00601688s
    Aug 17 07:29:13.282: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007969419s
    Aug 17 07:29:15.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006248661s
    Aug 17 07:29:17.279: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005604225s
    Aug 17 07:29:19.279: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005496252s
    Aug 17 07:29:21.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6": Phase="Running", Reason="", readiness=true. Elapsed: 12.006526827s
    Aug 17 07:29:21.280: INFO: Pod "dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 07:29:21.28
    STEP: looking for the results for each expected name from probers 08/17/23 07:29:21.282
    Aug 17 07:29:21.289: INFO: DNS probes using dns-6183/dns-test-6ad0171d-f4b6-4a63-a677-31ca142dd3d6 succeeded

    STEP: deleting the pod 08/17/23 07:29:21.289
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:21.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6183" for this suite. 08/17/23 07:29:21.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:21.302
Aug 17 07:29:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:29:21.303
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:21.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:21.321
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/17/23 07:29:21.33
STEP: getting /apis/node.k8s.io 08/17/23 07:29:21.333
STEP: getting /apis/node.k8s.io/v1 08/17/23 07:29:21.334
STEP: creating 08/17/23 07:29:21.335
STEP: watching 08/17/23 07:29:21.345
Aug 17 07:29:21.345: INFO: starting watch
STEP: getting 08/17/23 07:29:21.349
STEP: listing 08/17/23 07:29:21.351
STEP: patching 08/17/23 07:29:21.353
STEP: updating 08/17/23 07:29:21.355
Aug 17 07:29:21.358: INFO: waiting for watch events with expected annotations
STEP: deleting 08/17/23 07:29:21.358
STEP: deleting a collection 08/17/23 07:29:21.366
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:21.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4751" for this suite. 08/17/23 07:29:21.375
------------------------------
â€¢ [0.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:21.302
    Aug 17 07:29:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 07:29:21.303
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:21.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:21.321
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/17/23 07:29:21.33
    STEP: getting /apis/node.k8s.io 08/17/23 07:29:21.333
    STEP: getting /apis/node.k8s.io/v1 08/17/23 07:29:21.334
    STEP: creating 08/17/23 07:29:21.335
    STEP: watching 08/17/23 07:29:21.345
    Aug 17 07:29:21.345: INFO: starting watch
    STEP: getting 08/17/23 07:29:21.349
    STEP: listing 08/17/23 07:29:21.351
    STEP: patching 08/17/23 07:29:21.353
    STEP: updating 08/17/23 07:29:21.355
    Aug 17 07:29:21.358: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/17/23 07:29:21.358
    STEP: deleting a collection 08/17/23 07:29:21.366
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:21.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4751" for this suite. 08/17/23 07:29:21.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:21.38
Aug 17 07:29:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:29:21.381
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:21.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:21.396
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-6f0af2a8-47d8-472f-a4d8-3897b87b7a6c 08/17/23 07:29:21.399
STEP: Creating a pod to test consume configMaps 08/17/23 07:29:21.404
Aug 17 07:29:21.417: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863" in namespace "projected-2722" to be "Succeeded or Failed"
Aug 17 07:29:21.420: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018908ms
Aug 17 07:29:23.422: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005223515s
Aug 17 07:29:25.422: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005457754s
STEP: Saw pod success 08/17/23 07:29:25.422
Aug 17 07:29:25.423: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863" satisfied condition "Succeeded or Failed"
Aug 17 07:29:25.425: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863 container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/17/23 07:29:25.429
Aug 17 07:29:25.434: INFO: Waiting for pod pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863 to disappear
Aug 17 07:29:25.436: INFO: Pod pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:25.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2722" for this suite. 08/17/23 07:29:25.438
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:21.38
    Aug 17 07:29:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:29:21.381
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:21.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:21.396
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-6f0af2a8-47d8-472f-a4d8-3897b87b7a6c 08/17/23 07:29:21.399
    STEP: Creating a pod to test consume configMaps 08/17/23 07:29:21.404
    Aug 17 07:29:21.417: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863" in namespace "projected-2722" to be "Succeeded or Failed"
    Aug 17 07:29:21.420: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018908ms
    Aug 17 07:29:23.422: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005223515s
    Aug 17 07:29:25.422: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005457754s
    STEP: Saw pod success 08/17/23 07:29:25.422
    Aug 17 07:29:25.423: INFO: Pod "pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863" satisfied condition "Succeeded or Failed"
    Aug 17 07:29:25.425: INFO: Trying to get logs from node yst-node2 pod pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:29:25.429
    Aug 17 07:29:25.434: INFO: Waiting for pod pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863 to disappear
    Aug 17 07:29:25.436: INFO: Pod pod-projected-configmaps-1ffea7b7-30e9-473f-b5ed-21d41719b863 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:25.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2722" for this suite. 08/17/23 07:29:25.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:25.441
Aug 17 07:29:25.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename proxy 08/17/23 07:29:25.442
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:25.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:25.451
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/17/23 07:29:25.485
STEP: creating replication controller proxy-service-7wth7 in namespace proxy-6578 08/17/23 07:29:25.485
I0817 07:29:25.489647      21 runners.go:193] Created replication controller with name: proxy-service-7wth7, namespace: proxy-6578, replica count: 1
I0817 07:29:26.541183      21 runners.go:193] proxy-service-7wth7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 07:29:27.542019      21 runners.go:193] proxy-service-7wth7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0817 07:29:28.543200      21 runners.go:193] proxy-service-7wth7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 07:29:28.545: INFO: setup took 3.092024821s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/17/23 07:29:28.545
Aug 17 07:29:28.550: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.007877ms)
Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.088487ms)
Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.691011ms)
Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.389727ms)
Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 5.295533ms)
Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 6.115956ms)
Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.86715ms)
Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.848258ms)
Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 8.553312ms)
Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 8.878974ms)
Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 9.108717ms)
Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 9.088598ms)
Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 8.874715ms)
Aug 17 07:29:28.555: INFO: (0) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 9.478367ms)
Aug 17 07:29:28.555: INFO: (0) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 9.880642ms)
Aug 17 07:29:28.555: INFO: (0) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 10.017799ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.271815ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.681602ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.551676ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.974233ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.035736ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.1661ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.985685ms)
Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.384125ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.090609ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.36959ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.416976ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.282596ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.539752ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.785815ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.817824ms)
Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.719136ms)
Aug 17 07:29:28.565: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.918732ms)
Aug 17 07:29:28.565: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.103545ms)
Aug 17 07:29:28.565: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 5.242457ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.416233ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.854642ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.961533ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 5.134516ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 5.254041ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.539877ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 5.115877ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.422462ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.091366ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.323304ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.403394ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.26325ms)
Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.54666ms)
Aug 17 07:29:28.568: INFO: (3) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.135065ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 2.747168ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 2.826869ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.695348ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.638215ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 2.864ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.929647ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 2.981375ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.996202ms)
Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.169227ms)
Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.108945ms)
Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 3.968203ms)
Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.150872ms)
Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.949174ms)
Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.16552ms)
Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.274092ms)
Aug 17 07:29:28.572: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 1.417426ms)
Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 2.894939ms)
Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.901889ms)
Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.872166ms)
Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.140176ms)
Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.968532ms)
Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.152833ms)
Aug 17 07:29:28.574: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.759086ms)
Aug 17 07:29:28.574: INFO: (4) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.766763ms)
Aug 17 07:29:28.574: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.727925ms)
Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.253171ms)
Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.355639ms)
Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.344909ms)
Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.306602ms)
Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.472563ms)
Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.380994ms)
Aug 17 07:29:28.579: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.090943ms)
Aug 17 07:29:28.579: INFO: (5) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.992541ms)
Aug 17 07:29:28.579: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.243348ms)
Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.486294ms)
Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.554925ms)
Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.811287ms)
Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.199543ms)
Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.241189ms)
Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.770177ms)
Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.374986ms)
Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.931133ms)
Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.883635ms)
Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 6.273199ms)
Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.992822ms)
Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 6.351948ms)
Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 6.448903ms)
Aug 17 07:29:28.584: INFO: (6) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.8257ms)
Aug 17 07:29:28.584: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.405504ms)
Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.6361ms)
Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 2.791288ms)
Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 2.729282ms)
Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.783767ms)
Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 2.910909ms)
Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.378105ms)
Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.536158ms)
Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.923246ms)
Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.698386ms)
Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.42017ms)
Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.321181ms)
Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.496663ms)
Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.707646ms)
Aug 17 07:29:28.587: INFO: (6) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.934041ms)
Aug 17 07:29:28.589: INFO: (7) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 2.393233ms)
Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.188189ms)
Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.393034ms)
Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.205264ms)
Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.707241ms)
Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.645221ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.625596ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.896336ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.730208ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.941359ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.495154ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.403818ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.198504ms)
Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.350017ms)
Aug 17 07:29:28.593: INFO: (7) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.372784ms)
Aug 17 07:29:28.593: INFO: (7) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.608364ms)
Aug 17 07:29:28.595: INFO: (8) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.149763ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.513207ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.489005ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.337965ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.506953ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.699578ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.999122ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.865491ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.07246ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.234967ms)
Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.290496ms)
Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.358626ms)
Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.027511ms)
Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.545587ms)
Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.155141ms)
Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.48565ms)
Aug 17 07:29:28.602: INFO: (9) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.272889ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.543212ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 5.096639ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.935589ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 5.205357ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 5.090779ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.489409ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.264713ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.148458ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.657826ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.411796ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.499559ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.42697ms)
Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.955396ms)
Aug 17 07:29:28.605: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 5.424337ms)
Aug 17 07:29:28.605: INFO: (9) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.76892ms)
Aug 17 07:29:28.606: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 1.623756ms)
Aug 17 07:29:28.606: INFO: (10) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 1.832061ms)
Aug 17 07:29:28.607: INFO: (10) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 2.77441ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.815449ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.583021ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 2.727927ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 2.777213ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 2.6877ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.146871ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.039768ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.085266ms)
Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.216396ms)
Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 3.547292ms)
Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 3.702258ms)
Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 3.797721ms)
Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.881481ms)
Aug 17 07:29:28.612: INFO: (11) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.395786ms)
Aug 17 07:29:28.613: INFO: (11) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.715679ms)
Aug 17 07:29:28.613: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.567987ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.7778ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.665555ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.653768ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.711983ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.665723ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.024835ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.111006ms)
Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.922737ms)
Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.06112ms)
Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.465052ms)
Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.468858ms)
Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.566543ms)
Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.107499ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.037233ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.107201ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.189531ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.404358ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.249398ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.477845ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.444ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.411172ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.486012ms)
Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.646167ms)
Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.015572ms)
Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.938098ms)
Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.030099ms)
Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.021606ms)
Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.103938ms)
Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.096727ms)
Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.637856ms)
Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.1054ms)
Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.087179ms)
Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.43564ms)
Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.899284ms)
Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.285993ms)
Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.011084ms)
Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.132833ms)
Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.224196ms)
Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.069477ms)
Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.197111ms)
Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.350055ms)
Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.545048ms)
Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.525437ms)
Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.367663ms)
Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.54243ms)
Aug 17 07:29:28.628: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.492135ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.309044ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.72559ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.583144ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.677911ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.483321ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.651995ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.589603ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.783993ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.834771ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.115994ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.396166ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.440808ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.011369ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.308008ms)
Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.435636ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.979234ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.178836ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 3.589801ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 3.65464ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 3.707578ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.548928ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 3.593556ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 3.683005ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.485312ms)
Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.457472ms)
Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.447534ms)
Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.621983ms)
Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.759619ms)
Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.944455ms)
Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.037024ms)
Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.538956ms)
Aug 17 07:29:28.636: INFO: (16) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 1.857413ms)
Aug 17 07:29:28.636: INFO: (16) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 2.027725ms)
Aug 17 07:29:28.636: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.045962ms)
Aug 17 07:29:28.637: INFO: (16) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.542542ms)
Aug 17 07:29:28.637: INFO: (16) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.549823ms)
Aug 17 07:29:28.637: INFO: (16) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.655078ms)
Aug 17 07:29:28.638: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.701286ms)
Aug 17 07:29:28.639: INFO: (16) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.983011ms)
Aug 17 07:29:28.639: INFO: (16) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.971659ms)
Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.93879ms)
Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.84889ms)
Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.990281ms)
Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.848674ms)
Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.863902ms)
Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 6.451205ms)
Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 6.314882ms)
Aug 17 07:29:28.642: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 1.929623ms)
Aug 17 07:29:28.642: INFO: (17) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 1.873316ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.468293ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.372892ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.581646ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.677573ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.561702ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.510364ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.948569ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.692796ms)
Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.883508ms)
Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.844997ms)
Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.793831ms)
Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.323207ms)
Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.211601ms)
Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.109895ms)
Aug 17 07:29:28.647: INFO: (18) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 1.333788ms)
Aug 17 07:29:28.647: INFO: (18) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 1.386553ms)
Aug 17 07:29:28.648: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 1.794785ms)
Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.689804ms)
Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.862296ms)
Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.849378ms)
Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.031672ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 3.442594ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 3.412156ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.159543ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 3.593457ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.204084ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.382998ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 3.710118ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.488179ms)
Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 3.307139ms)
Aug 17 07:29:28.652: INFO: (19) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 1.781818ms)
Aug 17 07:29:28.652: INFO: (19) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 1.63131ms)
Aug 17 07:29:28.652: INFO: (19) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 1.587371ms)
Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 2.308154ms)
Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 2.661179ms)
Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.202722ms)
Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 2.626916ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.224052ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.458997ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.077611ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.175332ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.232946ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.943884ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.871388ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.053791ms)
Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.145521ms)
STEP: deleting ReplicationController proxy-service-7wth7 in namespace proxy-6578, will wait for the garbage collector to delete the pods 08/17/23 07:29:28.654
Aug 17 07:29:28.710: INFO: Deleting ReplicationController proxy-service-7wth7 took: 3.274996ms
Aug 17 07:29:28.810: INFO: Terminating ReplicationController proxy-service-7wth7 pods took: 100.437561ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:31.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6578" for this suite. 08/17/23 07:29:31.014
------------------------------
â€¢ [SLOW TEST] [5.577 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:25.441
    Aug 17 07:29:25.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename proxy 08/17/23 07:29:25.442
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:25.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:25.451
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/17/23 07:29:25.485
    STEP: creating replication controller proxy-service-7wth7 in namespace proxy-6578 08/17/23 07:29:25.485
    I0817 07:29:25.489647      21 runners.go:193] Created replication controller with name: proxy-service-7wth7, namespace: proxy-6578, replica count: 1
    I0817 07:29:26.541183      21 runners.go:193] proxy-service-7wth7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0817 07:29:27.542019      21 runners.go:193] proxy-service-7wth7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0817 07:29:28.543200      21 runners.go:193] proxy-service-7wth7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 07:29:28.545: INFO: setup took 3.092024821s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/17/23 07:29:28.545
    Aug 17 07:29:28.550: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.007877ms)
    Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.088487ms)
    Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.691011ms)
    Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.389727ms)
    Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 5.295533ms)
    Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 6.115956ms)
    Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.86715ms)
    Aug 17 07:29:28.551: INFO: (0) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.848258ms)
    Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 8.553312ms)
    Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 8.878974ms)
    Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 9.108717ms)
    Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 9.088598ms)
    Aug 17 07:29:28.554: INFO: (0) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 8.874715ms)
    Aug 17 07:29:28.555: INFO: (0) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 9.478367ms)
    Aug 17 07:29:28.555: INFO: (0) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 9.880642ms)
    Aug 17 07:29:28.555: INFO: (0) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 10.017799ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.271815ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.681602ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.551676ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.974233ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.035736ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.1661ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.985685ms)
    Aug 17 07:29:28.559: INFO: (1) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.384125ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.090609ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.36959ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.416976ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.282596ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.539752ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.785815ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.817824ms)
    Aug 17 07:29:28.560: INFO: (1) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.719136ms)
    Aug 17 07:29:28.565: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.918732ms)
    Aug 17 07:29:28.565: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.103545ms)
    Aug 17 07:29:28.565: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 5.242457ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.416233ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.854642ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.961533ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 5.134516ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 5.254041ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.539877ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 5.115877ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.422462ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.091366ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.323304ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.403394ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.26325ms)
    Aug 17 07:29:28.566: INFO: (2) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.54666ms)
    Aug 17 07:29:28.568: INFO: (3) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.135065ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 2.747168ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 2.826869ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.695348ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.638215ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 2.864ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.929647ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 2.981375ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.996202ms)
    Aug 17 07:29:28.569: INFO: (3) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.169227ms)
    Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.108945ms)
    Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 3.968203ms)
    Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.150872ms)
    Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.949174ms)
    Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.16552ms)
    Aug 17 07:29:28.570: INFO: (3) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.274092ms)
    Aug 17 07:29:28.572: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 1.417426ms)
    Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 2.894939ms)
    Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.901889ms)
    Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.872166ms)
    Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.140176ms)
    Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.968532ms)
    Aug 17 07:29:28.573: INFO: (4) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.152833ms)
    Aug 17 07:29:28.574: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.759086ms)
    Aug 17 07:29:28.574: INFO: (4) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.766763ms)
    Aug 17 07:29:28.574: INFO: (4) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.727925ms)
    Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.253171ms)
    Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.355639ms)
    Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.344909ms)
    Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.306602ms)
    Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.472563ms)
    Aug 17 07:29:28.575: INFO: (4) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.380994ms)
    Aug 17 07:29:28.579: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.090943ms)
    Aug 17 07:29:28.579: INFO: (5) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.992541ms)
    Aug 17 07:29:28.579: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.243348ms)
    Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.486294ms)
    Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.554925ms)
    Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.811287ms)
    Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.199543ms)
    Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.241189ms)
    Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.770177ms)
    Aug 17 07:29:28.580: INFO: (5) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.374986ms)
    Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.931133ms)
    Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.883635ms)
    Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 6.273199ms)
    Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.992822ms)
    Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 6.351948ms)
    Aug 17 07:29:28.581: INFO: (5) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 6.448903ms)
    Aug 17 07:29:28.584: INFO: (6) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.8257ms)
    Aug 17 07:29:28.584: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.405504ms)
    Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.6361ms)
    Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 2.791288ms)
    Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 2.729282ms)
    Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.783767ms)
    Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 2.910909ms)
    Aug 17 07:29:28.585: INFO: (6) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.378105ms)
    Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.536158ms)
    Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.923246ms)
    Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.698386ms)
    Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.42017ms)
    Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.321181ms)
    Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.496663ms)
    Aug 17 07:29:28.586: INFO: (6) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.707646ms)
    Aug 17 07:29:28.587: INFO: (6) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.934041ms)
    Aug 17 07:29:28.589: INFO: (7) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 2.393233ms)
    Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.188189ms)
    Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.393034ms)
    Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.205264ms)
    Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.707241ms)
    Aug 17 07:29:28.591: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.645221ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.625596ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.896336ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.730208ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.941359ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.495154ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.403818ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.198504ms)
    Aug 17 07:29:28.592: INFO: (7) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.350017ms)
    Aug 17 07:29:28.593: INFO: (7) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.372784ms)
    Aug 17 07:29:28.593: INFO: (7) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.608364ms)
    Aug 17 07:29:28.595: INFO: (8) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.149763ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.513207ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.489005ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.337965ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.506953ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.699578ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.999122ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.865491ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.07246ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.234967ms)
    Aug 17 07:29:28.597: INFO: (8) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.290496ms)
    Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.358626ms)
    Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.027511ms)
    Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.545587ms)
    Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.155141ms)
    Aug 17 07:29:28.598: INFO: (8) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.48565ms)
    Aug 17 07:29:28.602: INFO: (9) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.272889ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.543212ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 5.096639ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.935589ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 5.205357ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 5.090779ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.489409ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.264713ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.148458ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.657826ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.411796ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.499559ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.42697ms)
    Aug 17 07:29:28.604: INFO: (9) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.955396ms)
    Aug 17 07:29:28.605: INFO: (9) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 5.424337ms)
    Aug 17 07:29:28.605: INFO: (9) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.76892ms)
    Aug 17 07:29:28.606: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 1.623756ms)
    Aug 17 07:29:28.606: INFO: (10) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 1.832061ms)
    Aug 17 07:29:28.607: INFO: (10) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 2.77441ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.815449ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.583021ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 2.727927ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 2.777213ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 2.6877ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.146871ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.039768ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.085266ms)
    Aug 17 07:29:28.608: INFO: (10) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.216396ms)
    Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 3.547292ms)
    Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 3.702258ms)
    Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 3.797721ms)
    Aug 17 07:29:28.609: INFO: (10) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.881481ms)
    Aug 17 07:29:28.612: INFO: (11) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.395786ms)
    Aug 17 07:29:28.613: INFO: (11) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.715679ms)
    Aug 17 07:29:28.613: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.567987ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.7778ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.665555ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.653768ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.711983ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 4.665723ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.024835ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.111006ms)
    Aug 17 07:29:28.614: INFO: (11) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.922737ms)
    Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.06112ms)
    Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.465052ms)
    Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.468858ms)
    Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.566543ms)
    Aug 17 07:29:28.615: INFO: (11) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.107499ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.037233ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.107201ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.189531ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.404358ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.249398ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.477845ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.444ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.411172ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.486012ms)
    Aug 17 07:29:28.618: INFO: (12) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.646167ms)
    Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.015572ms)
    Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.938098ms)
    Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.030099ms)
    Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 5.021606ms)
    Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 5.103938ms)
    Aug 17 07:29:28.620: INFO: (12) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.096727ms)
    Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.637856ms)
    Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.1054ms)
    Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.087179ms)
    Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.43564ms)
    Aug 17 07:29:28.623: INFO: (13) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.899284ms)
    Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.285993ms)
    Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.011084ms)
    Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.132833ms)
    Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.224196ms)
    Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.069477ms)
    Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.197111ms)
    Aug 17 07:29:28.624: INFO: (13) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.350055ms)
    Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.545048ms)
    Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.525437ms)
    Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.367663ms)
    Aug 17 07:29:28.625: INFO: (13) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.54243ms)
    Aug 17 07:29:28.628: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.492135ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.309044ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.72559ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.583144ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.677911ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.483321ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.651995ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.589603ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.783993ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.834771ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.115994ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.396166ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.440808ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 4.011369ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.308008ms)
    Aug 17 07:29:28.629: INFO: (14) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.435636ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 2.979234ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.178836ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 3.589801ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 3.65464ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 3.707578ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.548928ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 3.593556ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 3.683005ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.485312ms)
    Aug 17 07:29:28.633: INFO: (15) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.457472ms)
    Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.447534ms)
    Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.621983ms)
    Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.759619ms)
    Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 3.944455ms)
    Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.037024ms)
    Aug 17 07:29:28.634: INFO: (15) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.538956ms)
    Aug 17 07:29:28.636: INFO: (16) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 1.857413ms)
    Aug 17 07:29:28.636: INFO: (16) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 2.027725ms)
    Aug 17 07:29:28.636: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.045962ms)
    Aug 17 07:29:28.637: INFO: (16) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.542542ms)
    Aug 17 07:29:28.637: INFO: (16) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 3.549823ms)
    Aug 17 07:29:28.637: INFO: (16) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.655078ms)
    Aug 17 07:29:28.638: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 3.701286ms)
    Aug 17 07:29:28.639: INFO: (16) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.983011ms)
    Aug 17 07:29:28.639: INFO: (16) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.971659ms)
    Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.93879ms)
    Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 5.84889ms)
    Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 5.990281ms)
    Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 5.848674ms)
    Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 5.863902ms)
    Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 6.451205ms)
    Aug 17 07:29:28.640: INFO: (16) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 6.314882ms)
    Aug 17 07:29:28.642: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 1.929623ms)
    Aug 17 07:29:28.642: INFO: (17) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 1.873316ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 4.468293ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.372892ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 4.581646ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 4.677573ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 4.561702ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 4.510364ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.948569ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 4.692796ms)
    Aug 17 07:29:28.645: INFO: (17) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.883508ms)
    Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 4.844997ms)
    Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.793831ms)
    Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 5.323207ms)
    Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 5.211601ms)
    Aug 17 07:29:28.646: INFO: (17) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 5.109895ms)
    Aug 17 07:29:28.647: INFO: (18) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 1.333788ms)
    Aug 17 07:29:28.647: INFO: (18) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 1.386553ms)
    Aug 17 07:29:28.648: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 1.794785ms)
    Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 2.689804ms)
    Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.862296ms)
    Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.849378ms)
    Aug 17 07:29:28.649: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 3.031672ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 3.442594ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 3.412156ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 3.159543ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 3.593457ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 3.204084ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.382998ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 3.710118ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.488179ms)
    Aug 17 07:29:28.650: INFO: (18) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 3.307139ms)
    Aug 17 07:29:28.652: INFO: (19) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:443/proxy/tlsrewritem... (200; 1.781818ms)
    Aug 17 07:29:28.652: INFO: (19) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:1080/proxy/rewriteme">... (200; 1.63131ms)
    Aug 17 07:29:28.652: INFO: (19) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:462/proxy/: tls qux (200; 1.587371ms)
    Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:1080/proxy/rewriteme">test<... (200; 2.308154ms)
    Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname1/proxy/: foo (200; 2.661179ms)
    Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:160/proxy/: foo (200; 2.202722ms)
    Aug 17 07:29:28.653: INFO: (19) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname2/proxy/: tls qux (200; 2.626916ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.224052ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/http:proxy-service-7wth7-g6v74:160/proxy/: foo (200; 3.458997ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/https:proxy-service-7wth7:tlsportname1/proxy/: tls baz (200; 4.077611ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/proxy-service-7wth7:portname2/proxy/: bar (200; 4.175332ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname1/proxy/: foo (200; 4.232946ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/https:proxy-service-7wth7-g6v74:460/proxy/: tls baz (200; 3.943884ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74:162/proxy/: bar (200; 3.871388ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/: <a href="/api/v1/namespaces/proxy-6578/pods/proxy-service-7wth7-g6v74/proxy/rewriteme">test</a> (200; 4.053791ms)
    Aug 17 07:29:28.654: INFO: (19) /api/v1/namespaces/proxy-6578/services/http:proxy-service-7wth7:portname2/proxy/: bar (200; 4.145521ms)
    STEP: deleting ReplicationController proxy-service-7wth7 in namespace proxy-6578, will wait for the garbage collector to delete the pods 08/17/23 07:29:28.654
    Aug 17 07:29:28.710: INFO: Deleting ReplicationController proxy-service-7wth7 took: 3.274996ms
    Aug 17 07:29:28.810: INFO: Terminating ReplicationController proxy-service-7wth7 pods took: 100.437561ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:31.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6578" for this suite. 08/17/23 07:29:31.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:31.019
Aug 17 07:29:31.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename containers 08/17/23 07:29:31.019
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:31.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:31.03
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 17 07:29:31.065: INFO: Waiting up to 5m0s for pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4" in namespace "containers-9122" to be "running"
Aug 17 07:29:31.074: INFO: Pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.12391ms
Aug 17 07:29:33.078: INFO: Pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013424939s
Aug 17 07:29:33.078: INFO: Pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:33.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9122" for this suite. 08/17/23 07:29:33.085
------------------------------
â€¢ [2.069 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:31.019
    Aug 17 07:29:31.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename containers 08/17/23 07:29:31.019
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:31.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:31.03
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 17 07:29:31.065: INFO: Waiting up to 5m0s for pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4" in namespace "containers-9122" to be "running"
    Aug 17 07:29:31.074: INFO: Pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.12391ms
    Aug 17 07:29:33.078: INFO: Pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013424939s
    Aug 17 07:29:33.078: INFO: Pod "client-containers-3bf270a0-3c24-4bd1-98e6-635094e1cca4" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:33.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9122" for this suite. 08/17/23 07:29:33.085
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:33.088
Aug 17 07:29:33.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:29:33.089
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:33.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:33.101
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:29:33.104
Aug 17 07:29:33.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95" in namespace "projected-3504" to be "Succeeded or Failed"
Aug 17 07:29:33.129: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.164087ms
Aug 17 07:29:35.132: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010936278s
Aug 17 07:29:37.132: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010937104s
STEP: Saw pod success 08/17/23 07:29:37.132
Aug 17 07:29:37.132: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95" satisfied condition "Succeeded or Failed"
Aug 17 07:29:37.133: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95 container client-container: <nil>
STEP: delete the pod 08/17/23 07:29:37.137
Aug 17 07:29:37.148: INFO: Waiting for pod downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95 to disappear
Aug 17 07:29:37.151: INFO: Pod downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:37.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3504" for this suite. 08/17/23 07:29:37.154
------------------------------
â€¢ [4.072 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:33.088
    Aug 17 07:29:33.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:29:33.089
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:33.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:33.101
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:29:33.104
    Aug 17 07:29:33.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95" in namespace "projected-3504" to be "Succeeded or Failed"
    Aug 17 07:29:33.129: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.164087ms
    Aug 17 07:29:35.132: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010936278s
    Aug 17 07:29:37.132: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010937104s
    STEP: Saw pod success 08/17/23 07:29:37.132
    Aug 17 07:29:37.132: INFO: Pod "downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95" satisfied condition "Succeeded or Failed"
    Aug 17 07:29:37.133: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:29:37.137
    Aug 17 07:29:37.148: INFO: Waiting for pod downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95 to disappear
    Aug 17 07:29:37.151: INFO: Pod downwardapi-volume-19f99a15-fa1c-4b0c-a5d8-4d7b002e2d95 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:37.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3504" for this suite. 08/17/23 07:29:37.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:37.16
Aug 17 07:29:37.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:29:37.161
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:37.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:37.172
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:29:37.209
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:29:37.492
STEP: Deploying the webhook pod 08/17/23 07:29:37.496
STEP: Wait for the deployment to be ready 08/17/23 07:29:37.501
Aug 17 07:29:37.505: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:29:39.512
STEP: Verifying the service has paired with the endpoint 08/17/23 07:29:39.518
Aug 17 07:29:40.519: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/17/23 07:29:40.521
STEP: create a namespace for the webhook 08/17/23 07:29:40.533
STEP: create a configmap should be unconditionally rejected by the webhook 08/17/23 07:29:40.54
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:40.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7660" for this suite. 08/17/23 07:29:40.633
STEP: Destroying namespace "webhook-7660-markers" for this suite. 08/17/23 07:29:40.642
------------------------------
â€¢ [3.493 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:37.16
    Aug 17 07:29:37.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:29:37.161
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:37.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:37.172
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:29:37.209
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:29:37.492
    STEP: Deploying the webhook pod 08/17/23 07:29:37.496
    STEP: Wait for the deployment to be ready 08/17/23 07:29:37.501
    Aug 17 07:29:37.505: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:29:39.512
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:29:39.518
    Aug 17 07:29:40.519: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/17/23 07:29:40.521
    STEP: create a namespace for the webhook 08/17/23 07:29:40.533
    STEP: create a configmap should be unconditionally rejected by the webhook 08/17/23 07:29:40.54
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:40.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7660" for this suite. 08/17/23 07:29:40.633
    STEP: Destroying namespace "webhook-7660-markers" for this suite. 08/17/23 07:29:40.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:40.654
Aug 17 07:29:40.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename proxy 08/17/23 07:29:40.654
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:40.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:40.666
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 17 07:29:40.668: INFO: Creating pod...
Aug 17 07:29:40.680: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-590" to be "running"
Aug 17 07:29:40.683: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.538314ms
Aug 17 07:29:42.685: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005092489s
Aug 17 07:29:42.685: INFO: Pod "agnhost" satisfied condition "running"
Aug 17 07:29:42.685: INFO: Creating service...
Aug 17 07:29:42.692: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/DELETE
Aug 17 07:29:42.697: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 07:29:42.697: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/GET
Aug 17 07:29:42.699: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 17 07:29:42.699: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/HEAD
Aug 17 07:29:42.700: INFO: http.Client request:HEAD | StatusCode:200
Aug 17 07:29:42.700: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 17 07:29:42.702: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 07:29:42.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/PATCH
Aug 17 07:29:42.703: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 07:29:42.703: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/POST
Aug 17 07:29:42.705: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 07:29:42.705: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/PUT
Aug 17 07:29:42.707: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 17 07:29:42.707: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/DELETE
Aug 17 07:29:42.710: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 07:29:42.710: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/GET
Aug 17 07:29:42.712: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 17 07:29:42.712: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/HEAD
Aug 17 07:29:42.716: INFO: http.Client request:HEAD | StatusCode:200
Aug 17 07:29:42.716: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/OPTIONS
Aug 17 07:29:42.718: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 07:29:42.718: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/PATCH
Aug 17 07:29:42.720: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 07:29:42.720: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/POST
Aug 17 07:29:42.722: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 07:29:42.722: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/PUT
Aug 17 07:29:42.724: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 17 07:29:42.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-590" for this suite. 08/17/23 07:29:42.726
------------------------------
â€¢ [2.075 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:40.654
    Aug 17 07:29:40.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename proxy 08/17/23 07:29:40.654
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:40.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:40.666
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 17 07:29:40.668: INFO: Creating pod...
    Aug 17 07:29:40.680: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-590" to be "running"
    Aug 17 07:29:40.683: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.538314ms
    Aug 17 07:29:42.685: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005092489s
    Aug 17 07:29:42.685: INFO: Pod "agnhost" satisfied condition "running"
    Aug 17 07:29:42.685: INFO: Creating service...
    Aug 17 07:29:42.692: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/DELETE
    Aug 17 07:29:42.697: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 07:29:42.697: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/GET
    Aug 17 07:29:42.699: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 17 07:29:42.699: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/HEAD
    Aug 17 07:29:42.700: INFO: http.Client request:HEAD | StatusCode:200
    Aug 17 07:29:42.700: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 17 07:29:42.702: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 07:29:42.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/PATCH
    Aug 17 07:29:42.703: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 07:29:42.703: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/POST
    Aug 17 07:29:42.705: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 07:29:42.705: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/pods/agnhost/proxy/some/path/with/PUT
    Aug 17 07:29:42.707: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 17 07:29:42.707: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/DELETE
    Aug 17 07:29:42.710: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 07:29:42.710: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/GET
    Aug 17 07:29:42.712: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 17 07:29:42.712: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/HEAD
    Aug 17 07:29:42.716: INFO: http.Client request:HEAD | StatusCode:200
    Aug 17 07:29:42.716: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/OPTIONS
    Aug 17 07:29:42.718: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 07:29:42.718: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/PATCH
    Aug 17 07:29:42.720: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 07:29:42.720: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/POST
    Aug 17 07:29:42.722: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 07:29:42.722: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-590/services/test-service/proxy/some/path/with/PUT
    Aug 17 07:29:42.724: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:29:42.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-590" for this suite. 08/17/23 07:29:42.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:29:42.73
Aug 17 07:29:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename taint-multiple-pods 08/17/23 07:29:42.731
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:42.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:42.755
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 17 07:29:42.758: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 07:30:42.813: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 17 07:30:42.815: INFO: Starting informer...
STEP: Starting pods... 08/17/23 07:30:42.815
Aug 17 07:30:43.026: INFO: Pod1 is running on yst-node2. Tainting Node
Aug 17 07:30:43.232: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7779" to be "running"
Aug 17 07:30:43.235: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42949ms
Aug 17 07:30:45.237: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004849955s
Aug 17 07:30:45.237: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 17 07:30:45.237: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7779" to be "running"
Aug 17 07:30:45.239: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.633955ms
Aug 17 07:30:45.239: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 17 07:30:45.239: INFO: Pod2 is running on yst-node2. Tainting Node
STEP: Trying to apply a taint on the Node 08/17/23 07:30:45.239
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:30:45.248
STEP: Waiting for Pod1 and Pod2 to be deleted 08/17/23 07:30:45.25
Aug 17 07:30:51.534: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 17 07:31:10.966: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:31:10.977
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:31:10.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7779" for this suite. 08/17/23 07:31:10.982
------------------------------
â€¢ [SLOW TEST] [88.256 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:29:42.73
    Aug 17 07:29:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename taint-multiple-pods 08/17/23 07:29:42.731
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:29:42.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:29:42.755
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 17 07:29:42.758: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 07:30:42.813: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 17 07:30:42.815: INFO: Starting informer...
    STEP: Starting pods... 08/17/23 07:30:42.815
    Aug 17 07:30:43.026: INFO: Pod1 is running on yst-node2. Tainting Node
    Aug 17 07:30:43.232: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7779" to be "running"
    Aug 17 07:30:43.235: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42949ms
    Aug 17 07:30:45.237: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004849955s
    Aug 17 07:30:45.237: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 17 07:30:45.237: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7779" to be "running"
    Aug 17 07:30:45.239: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.633955ms
    Aug 17 07:30:45.239: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 17 07:30:45.239: INFO: Pod2 is running on yst-node2. Tainting Node
    STEP: Trying to apply a taint on the Node 08/17/23 07:30:45.239
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:30:45.248
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/17/23 07:30:45.25
    Aug 17 07:30:51.534: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 17 07:31:10.966: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 07:31:10.977
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:31:10.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7779" for this suite. 08/17/23 07:31:10.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:31:10.987
Aug 17 07:31:10.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 07:31:10.988
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:31:10.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:31:11
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/17/23 07:31:11.003
STEP: waiting for pod running 08/17/23 07:31:11.026
Aug 17 07:31:11.026: INFO: Waiting up to 2m0s for pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" in namespace "var-expansion-4875" to be "running"
Aug 17 07:31:11.030: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310526ms
Aug 17 07:31:13.034: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008109859s
Aug 17 07:31:13.034: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" satisfied condition "running"
STEP: creating a file in subpath 08/17/23 07:31:13.034
Aug 17 07:31:13.036: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4875 PodName:var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:31:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:31:13.036: INFO: ExecWithOptions: Clientset creation
Aug 17 07:31:13.036: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4875/pods/var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/17/23 07:31:13.088
Aug 17 07:31:13.090: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4875 PodName:var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:31:13.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:31:13.091: INFO: ExecWithOptions: Clientset creation
Aug 17 07:31:13.091: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4875/pods/var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/17/23 07:31:13.147
Aug 17 07:31:13.654: INFO: Successfully updated pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7"
STEP: waiting for annotated pod running 08/17/23 07:31:13.654
Aug 17 07:31:13.654: INFO: Waiting up to 2m0s for pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" in namespace "var-expansion-4875" to be "running"
Aug 17 07:31:13.657: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.180184ms
Aug 17 07:31:13.657: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" satisfied condition "running"
STEP: deleting the pod gracefully 08/17/23 07:31:13.657
Aug 17 07:31:13.657: INFO: Deleting pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" in namespace "var-expansion-4875"
Aug 17 07:31:13.690: INFO: Wait up to 5m0s for pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 07:31:47.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4875" for this suite. 08/17/23 07:31:47.696
------------------------------
â€¢ [SLOW TEST] [36.713 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:31:10.987
    Aug 17 07:31:10.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 07:31:10.988
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:31:10.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:31:11
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/17/23 07:31:11.003
    STEP: waiting for pod running 08/17/23 07:31:11.026
    Aug 17 07:31:11.026: INFO: Waiting up to 2m0s for pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" in namespace "var-expansion-4875" to be "running"
    Aug 17 07:31:11.030: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310526ms
    Aug 17 07:31:13.034: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008109859s
    Aug 17 07:31:13.034: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" satisfied condition "running"
    STEP: creating a file in subpath 08/17/23 07:31:13.034
    Aug 17 07:31:13.036: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4875 PodName:var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:31:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:31:13.036: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:31:13.036: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4875/pods/var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/17/23 07:31:13.088
    Aug 17 07:31:13.090: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4875 PodName:var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:31:13.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:31:13.091: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:31:13.091: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4875/pods/var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/17/23 07:31:13.147
    Aug 17 07:31:13.654: INFO: Successfully updated pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7"
    STEP: waiting for annotated pod running 08/17/23 07:31:13.654
    Aug 17 07:31:13.654: INFO: Waiting up to 2m0s for pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" in namespace "var-expansion-4875" to be "running"
    Aug 17 07:31:13.657: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.180184ms
    Aug 17 07:31:13.657: INFO: Pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" satisfied condition "running"
    STEP: deleting the pod gracefully 08/17/23 07:31:13.657
    Aug 17 07:31:13.657: INFO: Deleting pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" in namespace "var-expansion-4875"
    Aug 17 07:31:13.690: INFO: Wait up to 5m0s for pod "var-expansion-00e1d46d-03a6-4d16-be32-a19d5ffa40d7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:31:47.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4875" for this suite. 08/17/23 07:31:47.696
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:31:47.7
Aug 17 07:31:47.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename aggregator 08/17/23 07:31:47.701
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:31:47.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:31:47.715
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 17 07:31:47.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/17/23 07:31:47.721
Aug 17 07:31:47.993: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 17 07:31:50.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:31:52.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:31:54.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:31:56.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:31:58.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:00.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:02.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:04.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:06.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:08.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:10.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:12.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:32:14.145: INFO: Waited 115.781342ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/17/23 07:32:14.547
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/17/23 07:32:14.587
STEP: List APIServices 08/17/23 07:32:14.638
Aug 17 07:32:14.691: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 17 07:32:15.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-3126" for this suite. 08/17/23 07:32:15.758
------------------------------
â€¢ [SLOW TEST] [28.074 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:31:47.7
    Aug 17 07:31:47.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename aggregator 08/17/23 07:31:47.701
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:31:47.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:31:47.715
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 17 07:31:47.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/17/23 07:31:47.721
    Aug 17 07:31:47.993: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Aug 17 07:31:50.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:31:52.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:31:54.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:31:56.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:31:58.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:00.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:02.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:04.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:06.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:08.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:10.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:12.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 31, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:32:14.145: INFO: Waited 115.781342ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/17/23 07:32:14.547
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/17/23 07:32:14.587
    STEP: List APIServices 08/17/23 07:32:14.638
    Aug 17 07:32:14.691: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:32:15.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-3126" for this suite. 08/17/23 07:32:15.758
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:32:15.775
Aug 17 07:32:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 07:32:15.776
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:32:15.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:32:15.791
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/17/23 07:32:15.794
Aug 17 07:32:15.819: INFO: created test-pod-1
Aug 17 07:32:15.830: INFO: created test-pod-2
Aug 17 07:32:15.836: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/17/23 07:32:15.836
Aug 17 07:32:15.836: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8359' to be running and ready
Aug 17 07:32:15.846: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 17 07:32:15.846: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 17 07:32:15.846: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 17 07:32:15.846: INFO: 0 / 3 pods in namespace 'pods-8359' are running and ready (0 seconds elapsed)
Aug 17 07:32:15.846: INFO: expected 0 pod replicas in namespace 'pods-8359', 0 are Running and Ready.
Aug 17 07:32:15.846: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
Aug 17 07:32:15.846: INFO: test-pod-1  yst-node2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  }]
Aug 17 07:32:15.846: INFO: test-pod-2  yst-node2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  }]
Aug 17 07:32:15.846: INFO: test-pod-3  yst-node2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  }]
Aug 17 07:32:15.846: INFO: 
Aug 17 07:32:17.855: INFO: 3 / 3 pods in namespace 'pods-8359' are running and ready (2 seconds elapsed)
Aug 17 07:32:17.855: INFO: expected 0 pod replicas in namespace 'pods-8359', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/17/23 07:32:17.868
Aug 17 07:32:17.870: INFO: Pod quantity 3 is different from expected quantity 0
Aug 17 07:32:18.873: INFO: Pod quantity 3 is different from expected quantity 0
Aug 17 07:32:19.873: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 07:32:20.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8359" for this suite. 08/17/23 07:32:20.876
------------------------------
â€¢ [SLOW TEST] [5.104 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:32:15.775
    Aug 17 07:32:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 07:32:15.776
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:32:15.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:32:15.791
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/17/23 07:32:15.794
    Aug 17 07:32:15.819: INFO: created test-pod-1
    Aug 17 07:32:15.830: INFO: created test-pod-2
    Aug 17 07:32:15.836: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/17/23 07:32:15.836
    Aug 17 07:32:15.836: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8359' to be running and ready
    Aug 17 07:32:15.846: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 17 07:32:15.846: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 17 07:32:15.846: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 17 07:32:15.846: INFO: 0 / 3 pods in namespace 'pods-8359' are running and ready (0 seconds elapsed)
    Aug 17 07:32:15.846: INFO: expected 0 pod replicas in namespace 'pods-8359', 0 are Running and Ready.
    Aug 17 07:32:15.846: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
    Aug 17 07:32:15.846: INFO: test-pod-1  yst-node2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  }]
    Aug 17 07:32:15.846: INFO: test-pod-2  yst-node2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  }]
    Aug 17 07:32:15.846: INFO: test-pod-3  yst-node2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:32:15 +0000 UTC  }]
    Aug 17 07:32:15.846: INFO: 
    Aug 17 07:32:17.855: INFO: 3 / 3 pods in namespace 'pods-8359' are running and ready (2 seconds elapsed)
    Aug 17 07:32:17.855: INFO: expected 0 pod replicas in namespace 'pods-8359', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/17/23 07:32:17.868
    Aug 17 07:32:17.870: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 17 07:32:18.873: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 17 07:32:19.873: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:32:20.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8359" for this suite. 08/17/23 07:32:20.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:32:20.88
Aug 17 07:32:20.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename gc 08/17/23 07:32:20.881
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:32:20.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:32:20.892
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/17/23 07:32:20.904
STEP: delete the rc 08/17/23 07:32:25.929
STEP: wait for the rc to be deleted 08/17/23 07:32:25.958
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/17/23 07:32:30.961
STEP: Gathering metrics 08/17/23 07:33:00.972
Aug 17 07:33:00.987: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
Aug 17 07:33:00.989: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 2.039114ms
Aug 17 07:33:00.989: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
Aug 17 07:33:00.989: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
Aug 17 07:33:01.062: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 17 07:33:01.062: INFO: Deleting pod "simpletest.rc-28wx7" in namespace "gc-1506"
Aug 17 07:33:01.069: INFO: Deleting pod "simpletest.rc-2jkz4" in namespace "gc-1506"
Aug 17 07:33:01.079: INFO: Deleting pod "simpletest.rc-2kvcw" in namespace "gc-1506"
Aug 17 07:33:01.085: INFO: Deleting pod "simpletest.rc-2mjbd" in namespace "gc-1506"
Aug 17 07:33:01.095: INFO: Deleting pod "simpletest.rc-2nrjv" in namespace "gc-1506"
Aug 17 07:33:01.109: INFO: Deleting pod "simpletest.rc-2qx9h" in namespace "gc-1506"
Aug 17 07:33:01.117: INFO: Deleting pod "simpletest.rc-2xr9f" in namespace "gc-1506"
Aug 17 07:33:01.128: INFO: Deleting pod "simpletest.rc-48725" in namespace "gc-1506"
Aug 17 07:33:01.142: INFO: Deleting pod "simpletest.rc-4t69q" in namespace "gc-1506"
Aug 17 07:33:01.151: INFO: Deleting pod "simpletest.rc-5bqf8" in namespace "gc-1506"
Aug 17 07:33:01.162: INFO: Deleting pod "simpletest.rc-5fqz9" in namespace "gc-1506"
Aug 17 07:33:01.179: INFO: Deleting pod "simpletest.rc-6p9cp" in namespace "gc-1506"
Aug 17 07:33:01.191: INFO: Deleting pod "simpletest.rc-7l6cr" in namespace "gc-1506"
Aug 17 07:33:01.200: INFO: Deleting pod "simpletest.rc-7q44g" in namespace "gc-1506"
Aug 17 07:33:01.211: INFO: Deleting pod "simpletest.rc-8nnrn" in namespace "gc-1506"
Aug 17 07:33:01.221: INFO: Deleting pod "simpletest.rc-9qt9n" in namespace "gc-1506"
Aug 17 07:33:01.230: INFO: Deleting pod "simpletest.rc-9rwbp" in namespace "gc-1506"
Aug 17 07:33:01.240: INFO: Deleting pod "simpletest.rc-9ttdl" in namespace "gc-1506"
Aug 17 07:33:01.249: INFO: Deleting pod "simpletest.rc-b4x8f" in namespace "gc-1506"
Aug 17 07:33:01.259: INFO: Deleting pod "simpletest.rc-b8dw8" in namespace "gc-1506"
Aug 17 07:33:01.278: INFO: Deleting pod "simpletest.rc-btbp6" in namespace "gc-1506"
Aug 17 07:33:01.294: INFO: Deleting pod "simpletest.rc-btwj8" in namespace "gc-1506"
Aug 17 07:33:01.310: INFO: Deleting pod "simpletest.rc-bzcrd" in namespace "gc-1506"
Aug 17 07:33:01.320: INFO: Deleting pod "simpletest.rc-cb4qb" in namespace "gc-1506"
Aug 17 07:33:01.329: INFO: Deleting pod "simpletest.rc-cg76j" in namespace "gc-1506"
Aug 17 07:33:01.343: INFO: Deleting pod "simpletest.rc-clw72" in namespace "gc-1506"
Aug 17 07:33:01.352: INFO: Deleting pod "simpletest.rc-dfllz" in namespace "gc-1506"
Aug 17 07:33:01.362: INFO: Deleting pod "simpletest.rc-fg6fv" in namespace "gc-1506"
Aug 17 07:33:01.372: INFO: Deleting pod "simpletest.rc-gn6wm" in namespace "gc-1506"
Aug 17 07:33:01.387: INFO: Deleting pod "simpletest.rc-gz4vn" in namespace "gc-1506"
Aug 17 07:33:01.400: INFO: Deleting pod "simpletest.rc-h2p45" in namespace "gc-1506"
Aug 17 07:33:01.408: INFO: Deleting pod "simpletest.rc-hjbmd" in namespace "gc-1506"
Aug 17 07:33:01.417: INFO: Deleting pod "simpletest.rc-hmthw" in namespace "gc-1506"
Aug 17 07:33:01.429: INFO: Deleting pod "simpletest.rc-hs4j4" in namespace "gc-1506"
Aug 17 07:33:01.439: INFO: Deleting pod "simpletest.rc-hsggw" in namespace "gc-1506"
Aug 17 07:33:01.455: INFO: Deleting pod "simpletest.rc-j4zcv" in namespace "gc-1506"
Aug 17 07:33:01.473: INFO: Deleting pod "simpletest.rc-j64f8" in namespace "gc-1506"
Aug 17 07:33:01.492: INFO: Deleting pod "simpletest.rc-j6zlz" in namespace "gc-1506"
Aug 17 07:33:01.500: INFO: Deleting pod "simpletest.rc-jbr99" in namespace "gc-1506"
Aug 17 07:33:01.516: INFO: Deleting pod "simpletest.rc-jdr6p" in namespace "gc-1506"
Aug 17 07:33:01.530: INFO: Deleting pod "simpletest.rc-jghft" in namespace "gc-1506"
Aug 17 07:33:01.542: INFO: Deleting pod "simpletest.rc-jk7f7" in namespace "gc-1506"
Aug 17 07:33:01.559: INFO: Deleting pod "simpletest.rc-jqcbg" in namespace "gc-1506"
Aug 17 07:33:01.576: INFO: Deleting pod "simpletest.rc-ktcj2" in namespace "gc-1506"
Aug 17 07:33:01.596: INFO: Deleting pod "simpletest.rc-kthts" in namespace "gc-1506"
Aug 17 07:33:01.613: INFO: Deleting pod "simpletest.rc-ktwm7" in namespace "gc-1506"
Aug 17 07:33:01.637: INFO: Deleting pod "simpletest.rc-l65z9" in namespace "gc-1506"
Aug 17 07:33:01.669: INFO: Deleting pod "simpletest.rc-l9tgw" in namespace "gc-1506"
Aug 17 07:33:01.695: INFO: Deleting pod "simpletest.rc-lmx5z" in namespace "gc-1506"
Aug 17 07:33:01.721: INFO: Deleting pod "simpletest.rc-mhpc4" in namespace "gc-1506"
Aug 17 07:33:01.731: INFO: Deleting pod "simpletest.rc-mp9d4" in namespace "gc-1506"
Aug 17 07:33:01.745: INFO: Deleting pod "simpletest.rc-mrdnj" in namespace "gc-1506"
Aug 17 07:33:01.796: INFO: Deleting pod "simpletest.rc-mrxcd" in namespace "gc-1506"
Aug 17 07:33:01.808: INFO: Deleting pod "simpletest.rc-mzv4p" in namespace "gc-1506"
Aug 17 07:33:01.826: INFO: Deleting pod "simpletest.rc-n5xkd" in namespace "gc-1506"
Aug 17 07:33:01.848: INFO: Deleting pod "simpletest.rc-nbv27" in namespace "gc-1506"
Aug 17 07:33:01.868: INFO: Deleting pod "simpletest.rc-nknzh" in namespace "gc-1506"
Aug 17 07:33:01.887: INFO: Deleting pod "simpletest.rc-nnss5" in namespace "gc-1506"
Aug 17 07:33:01.930: INFO: Deleting pod "simpletest.rc-nv2d9" in namespace "gc-1506"
Aug 17 07:33:01.943: INFO: Deleting pod "simpletest.rc-p7z2j" in namespace "gc-1506"
Aug 17 07:33:01.980: INFO: Deleting pod "simpletest.rc-ptg97" in namespace "gc-1506"
Aug 17 07:33:01.995: INFO: Deleting pod "simpletest.rc-pwcb9" in namespace "gc-1506"
Aug 17 07:33:02.013: INFO: Deleting pod "simpletest.rc-q9cn9" in namespace "gc-1506"
Aug 17 07:33:02.057: INFO: Deleting pod "simpletest.rc-qc7kr" in namespace "gc-1506"
Aug 17 07:33:02.080: INFO: Deleting pod "simpletest.rc-qg4w9" in namespace "gc-1506"
Aug 17 07:33:02.101: INFO: Deleting pod "simpletest.rc-qr777" in namespace "gc-1506"
Aug 17 07:33:02.125: INFO: Deleting pod "simpletest.rc-r2qzc" in namespace "gc-1506"
Aug 17 07:33:02.155: INFO: Deleting pod "simpletest.rc-r2v8w" in namespace "gc-1506"
Aug 17 07:33:02.168: INFO: Deleting pod "simpletest.rc-r49wk" in namespace "gc-1506"
Aug 17 07:33:02.190: INFO: Deleting pod "simpletest.rc-r66hs" in namespace "gc-1506"
Aug 17 07:33:02.207: INFO: Deleting pod "simpletest.rc-rbqkr" in namespace "gc-1506"
Aug 17 07:33:02.231: INFO: Deleting pod "simpletest.rc-rmrnd" in namespace "gc-1506"
Aug 17 07:33:02.284: INFO: Deleting pod "simpletest.rc-rt54w" in namespace "gc-1506"
Aug 17 07:33:02.329: INFO: Deleting pod "simpletest.rc-shtqz" in namespace "gc-1506"
Aug 17 07:33:02.381: INFO: Deleting pod "simpletest.rc-shvx7" in namespace "gc-1506"
Aug 17 07:33:02.430: INFO: Deleting pod "simpletest.rc-snrqw" in namespace "gc-1506"
Aug 17 07:33:02.478: INFO: Deleting pod "simpletest.rc-tp4z5" in namespace "gc-1506"
Aug 17 07:33:02.527: INFO: Deleting pod "simpletest.rc-v86hh" in namespace "gc-1506"
Aug 17 07:33:02.573: INFO: Deleting pod "simpletest.rc-vhrgk" in namespace "gc-1506"
Aug 17 07:33:02.620: INFO: Deleting pod "simpletest.rc-vhrrm" in namespace "gc-1506"
Aug 17 07:33:02.679: INFO: Deleting pod "simpletest.rc-vjw5h" in namespace "gc-1506"
Aug 17 07:33:02.728: INFO: Deleting pod "simpletest.rc-vkvb7" in namespace "gc-1506"
Aug 17 07:33:02.774: INFO: Deleting pod "simpletest.rc-vq4zm" in namespace "gc-1506"
Aug 17 07:33:02.823: INFO: Deleting pod "simpletest.rc-vsp82" in namespace "gc-1506"
Aug 17 07:33:02.869: INFO: Deleting pod "simpletest.rc-vwf7v" in namespace "gc-1506"
Aug 17 07:33:02.921: INFO: Deleting pod "simpletest.rc-vx8dz" in namespace "gc-1506"
Aug 17 07:33:02.970: INFO: Deleting pod "simpletest.rc-w2xgx" in namespace "gc-1506"
Aug 17 07:33:03.030: INFO: Deleting pod "simpletest.rc-wh8t6" in namespace "gc-1506"
Aug 17 07:33:03.074: INFO: Deleting pod "simpletest.rc-wjnnd" in namespace "gc-1506"
Aug 17 07:33:03.159: INFO: Deleting pod "simpletest.rc-wlrg5" in namespace "gc-1506"
Aug 17 07:33:03.186: INFO: Deleting pod "simpletest.rc-wtkqf" in namespace "gc-1506"
Aug 17 07:33:03.227: INFO: Deleting pod "simpletest.rc-wzgpf" in namespace "gc-1506"
Aug 17 07:33:03.286: INFO: Deleting pod "simpletest.rc-xbxh8" in namespace "gc-1506"
Aug 17 07:33:03.323: INFO: Deleting pod "simpletest.rc-xh2fk" in namespace "gc-1506"
Aug 17 07:33:03.381: INFO: Deleting pod "simpletest.rc-xjkz8" in namespace "gc-1506"
Aug 17 07:33:03.430: INFO: Deleting pod "simpletest.rc-xlbv2" in namespace "gc-1506"
Aug 17 07:33:03.479: INFO: Deleting pod "simpletest.rc-xzgws" in namespace "gc-1506"
Aug 17 07:33:03.525: INFO: Deleting pod "simpletest.rc-z69g6" in namespace "gc-1506"
Aug 17 07:33:03.598: INFO: Deleting pod "simpletest.rc-z8vrd" in namespace "gc-1506"
Aug 17 07:33:03.624: INFO: Deleting pod "simpletest.rc-zkgpj" in namespace "gc-1506"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:03.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1506" for this suite. 08/17/23 07:33:03.718
------------------------------
â€¢ [SLOW TEST] [42.897 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:32:20.88
    Aug 17 07:32:20.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename gc 08/17/23 07:32:20.881
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:32:20.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:32:20.892
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/17/23 07:32:20.904
    STEP: delete the rc 08/17/23 07:32:25.929
    STEP: wait for the rc to be deleted 08/17/23 07:32:25.958
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/17/23 07:32:30.961
    STEP: Gathering metrics 08/17/23 07:33:00.972
    Aug 17 07:33:00.987: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
    Aug 17 07:33:00.989: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 2.039114ms
    Aug 17 07:33:00.989: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
    Aug 17 07:33:00.989: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
    Aug 17 07:33:01.062: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 17 07:33:01.062: INFO: Deleting pod "simpletest.rc-28wx7" in namespace "gc-1506"
    Aug 17 07:33:01.069: INFO: Deleting pod "simpletest.rc-2jkz4" in namespace "gc-1506"
    Aug 17 07:33:01.079: INFO: Deleting pod "simpletest.rc-2kvcw" in namespace "gc-1506"
    Aug 17 07:33:01.085: INFO: Deleting pod "simpletest.rc-2mjbd" in namespace "gc-1506"
    Aug 17 07:33:01.095: INFO: Deleting pod "simpletest.rc-2nrjv" in namespace "gc-1506"
    Aug 17 07:33:01.109: INFO: Deleting pod "simpletest.rc-2qx9h" in namespace "gc-1506"
    Aug 17 07:33:01.117: INFO: Deleting pod "simpletest.rc-2xr9f" in namespace "gc-1506"
    Aug 17 07:33:01.128: INFO: Deleting pod "simpletest.rc-48725" in namespace "gc-1506"
    Aug 17 07:33:01.142: INFO: Deleting pod "simpletest.rc-4t69q" in namespace "gc-1506"
    Aug 17 07:33:01.151: INFO: Deleting pod "simpletest.rc-5bqf8" in namespace "gc-1506"
    Aug 17 07:33:01.162: INFO: Deleting pod "simpletest.rc-5fqz9" in namespace "gc-1506"
    Aug 17 07:33:01.179: INFO: Deleting pod "simpletest.rc-6p9cp" in namespace "gc-1506"
    Aug 17 07:33:01.191: INFO: Deleting pod "simpletest.rc-7l6cr" in namespace "gc-1506"
    Aug 17 07:33:01.200: INFO: Deleting pod "simpletest.rc-7q44g" in namespace "gc-1506"
    Aug 17 07:33:01.211: INFO: Deleting pod "simpletest.rc-8nnrn" in namespace "gc-1506"
    Aug 17 07:33:01.221: INFO: Deleting pod "simpletest.rc-9qt9n" in namespace "gc-1506"
    Aug 17 07:33:01.230: INFO: Deleting pod "simpletest.rc-9rwbp" in namespace "gc-1506"
    Aug 17 07:33:01.240: INFO: Deleting pod "simpletest.rc-9ttdl" in namespace "gc-1506"
    Aug 17 07:33:01.249: INFO: Deleting pod "simpletest.rc-b4x8f" in namespace "gc-1506"
    Aug 17 07:33:01.259: INFO: Deleting pod "simpletest.rc-b8dw8" in namespace "gc-1506"
    Aug 17 07:33:01.278: INFO: Deleting pod "simpletest.rc-btbp6" in namespace "gc-1506"
    Aug 17 07:33:01.294: INFO: Deleting pod "simpletest.rc-btwj8" in namespace "gc-1506"
    Aug 17 07:33:01.310: INFO: Deleting pod "simpletest.rc-bzcrd" in namespace "gc-1506"
    Aug 17 07:33:01.320: INFO: Deleting pod "simpletest.rc-cb4qb" in namespace "gc-1506"
    Aug 17 07:33:01.329: INFO: Deleting pod "simpletest.rc-cg76j" in namespace "gc-1506"
    Aug 17 07:33:01.343: INFO: Deleting pod "simpletest.rc-clw72" in namespace "gc-1506"
    Aug 17 07:33:01.352: INFO: Deleting pod "simpletest.rc-dfllz" in namespace "gc-1506"
    Aug 17 07:33:01.362: INFO: Deleting pod "simpletest.rc-fg6fv" in namespace "gc-1506"
    Aug 17 07:33:01.372: INFO: Deleting pod "simpletest.rc-gn6wm" in namespace "gc-1506"
    Aug 17 07:33:01.387: INFO: Deleting pod "simpletest.rc-gz4vn" in namespace "gc-1506"
    Aug 17 07:33:01.400: INFO: Deleting pod "simpletest.rc-h2p45" in namespace "gc-1506"
    Aug 17 07:33:01.408: INFO: Deleting pod "simpletest.rc-hjbmd" in namespace "gc-1506"
    Aug 17 07:33:01.417: INFO: Deleting pod "simpletest.rc-hmthw" in namespace "gc-1506"
    Aug 17 07:33:01.429: INFO: Deleting pod "simpletest.rc-hs4j4" in namespace "gc-1506"
    Aug 17 07:33:01.439: INFO: Deleting pod "simpletest.rc-hsggw" in namespace "gc-1506"
    Aug 17 07:33:01.455: INFO: Deleting pod "simpletest.rc-j4zcv" in namespace "gc-1506"
    Aug 17 07:33:01.473: INFO: Deleting pod "simpletest.rc-j64f8" in namespace "gc-1506"
    Aug 17 07:33:01.492: INFO: Deleting pod "simpletest.rc-j6zlz" in namespace "gc-1506"
    Aug 17 07:33:01.500: INFO: Deleting pod "simpletest.rc-jbr99" in namespace "gc-1506"
    Aug 17 07:33:01.516: INFO: Deleting pod "simpletest.rc-jdr6p" in namespace "gc-1506"
    Aug 17 07:33:01.530: INFO: Deleting pod "simpletest.rc-jghft" in namespace "gc-1506"
    Aug 17 07:33:01.542: INFO: Deleting pod "simpletest.rc-jk7f7" in namespace "gc-1506"
    Aug 17 07:33:01.559: INFO: Deleting pod "simpletest.rc-jqcbg" in namespace "gc-1506"
    Aug 17 07:33:01.576: INFO: Deleting pod "simpletest.rc-ktcj2" in namespace "gc-1506"
    Aug 17 07:33:01.596: INFO: Deleting pod "simpletest.rc-kthts" in namespace "gc-1506"
    Aug 17 07:33:01.613: INFO: Deleting pod "simpletest.rc-ktwm7" in namespace "gc-1506"
    Aug 17 07:33:01.637: INFO: Deleting pod "simpletest.rc-l65z9" in namespace "gc-1506"
    Aug 17 07:33:01.669: INFO: Deleting pod "simpletest.rc-l9tgw" in namespace "gc-1506"
    Aug 17 07:33:01.695: INFO: Deleting pod "simpletest.rc-lmx5z" in namespace "gc-1506"
    Aug 17 07:33:01.721: INFO: Deleting pod "simpletest.rc-mhpc4" in namespace "gc-1506"
    Aug 17 07:33:01.731: INFO: Deleting pod "simpletest.rc-mp9d4" in namespace "gc-1506"
    Aug 17 07:33:01.745: INFO: Deleting pod "simpletest.rc-mrdnj" in namespace "gc-1506"
    Aug 17 07:33:01.796: INFO: Deleting pod "simpletest.rc-mrxcd" in namespace "gc-1506"
    Aug 17 07:33:01.808: INFO: Deleting pod "simpletest.rc-mzv4p" in namespace "gc-1506"
    Aug 17 07:33:01.826: INFO: Deleting pod "simpletest.rc-n5xkd" in namespace "gc-1506"
    Aug 17 07:33:01.848: INFO: Deleting pod "simpletest.rc-nbv27" in namespace "gc-1506"
    Aug 17 07:33:01.868: INFO: Deleting pod "simpletest.rc-nknzh" in namespace "gc-1506"
    Aug 17 07:33:01.887: INFO: Deleting pod "simpletest.rc-nnss5" in namespace "gc-1506"
    Aug 17 07:33:01.930: INFO: Deleting pod "simpletest.rc-nv2d9" in namespace "gc-1506"
    Aug 17 07:33:01.943: INFO: Deleting pod "simpletest.rc-p7z2j" in namespace "gc-1506"
    Aug 17 07:33:01.980: INFO: Deleting pod "simpletest.rc-ptg97" in namespace "gc-1506"
    Aug 17 07:33:01.995: INFO: Deleting pod "simpletest.rc-pwcb9" in namespace "gc-1506"
    Aug 17 07:33:02.013: INFO: Deleting pod "simpletest.rc-q9cn9" in namespace "gc-1506"
    Aug 17 07:33:02.057: INFO: Deleting pod "simpletest.rc-qc7kr" in namespace "gc-1506"
    Aug 17 07:33:02.080: INFO: Deleting pod "simpletest.rc-qg4w9" in namespace "gc-1506"
    Aug 17 07:33:02.101: INFO: Deleting pod "simpletest.rc-qr777" in namespace "gc-1506"
    Aug 17 07:33:02.125: INFO: Deleting pod "simpletest.rc-r2qzc" in namespace "gc-1506"
    Aug 17 07:33:02.155: INFO: Deleting pod "simpletest.rc-r2v8w" in namespace "gc-1506"
    Aug 17 07:33:02.168: INFO: Deleting pod "simpletest.rc-r49wk" in namespace "gc-1506"
    Aug 17 07:33:02.190: INFO: Deleting pod "simpletest.rc-r66hs" in namespace "gc-1506"
    Aug 17 07:33:02.207: INFO: Deleting pod "simpletest.rc-rbqkr" in namespace "gc-1506"
    Aug 17 07:33:02.231: INFO: Deleting pod "simpletest.rc-rmrnd" in namespace "gc-1506"
    Aug 17 07:33:02.284: INFO: Deleting pod "simpletest.rc-rt54w" in namespace "gc-1506"
    Aug 17 07:33:02.329: INFO: Deleting pod "simpletest.rc-shtqz" in namespace "gc-1506"
    Aug 17 07:33:02.381: INFO: Deleting pod "simpletest.rc-shvx7" in namespace "gc-1506"
    Aug 17 07:33:02.430: INFO: Deleting pod "simpletest.rc-snrqw" in namespace "gc-1506"
    Aug 17 07:33:02.478: INFO: Deleting pod "simpletest.rc-tp4z5" in namespace "gc-1506"
    Aug 17 07:33:02.527: INFO: Deleting pod "simpletest.rc-v86hh" in namespace "gc-1506"
    Aug 17 07:33:02.573: INFO: Deleting pod "simpletest.rc-vhrgk" in namespace "gc-1506"
    Aug 17 07:33:02.620: INFO: Deleting pod "simpletest.rc-vhrrm" in namespace "gc-1506"
    Aug 17 07:33:02.679: INFO: Deleting pod "simpletest.rc-vjw5h" in namespace "gc-1506"
    Aug 17 07:33:02.728: INFO: Deleting pod "simpletest.rc-vkvb7" in namespace "gc-1506"
    Aug 17 07:33:02.774: INFO: Deleting pod "simpletest.rc-vq4zm" in namespace "gc-1506"
    Aug 17 07:33:02.823: INFO: Deleting pod "simpletest.rc-vsp82" in namespace "gc-1506"
    Aug 17 07:33:02.869: INFO: Deleting pod "simpletest.rc-vwf7v" in namespace "gc-1506"
    Aug 17 07:33:02.921: INFO: Deleting pod "simpletest.rc-vx8dz" in namespace "gc-1506"
    Aug 17 07:33:02.970: INFO: Deleting pod "simpletest.rc-w2xgx" in namespace "gc-1506"
    Aug 17 07:33:03.030: INFO: Deleting pod "simpletest.rc-wh8t6" in namespace "gc-1506"
    Aug 17 07:33:03.074: INFO: Deleting pod "simpletest.rc-wjnnd" in namespace "gc-1506"
    Aug 17 07:33:03.159: INFO: Deleting pod "simpletest.rc-wlrg5" in namespace "gc-1506"
    Aug 17 07:33:03.186: INFO: Deleting pod "simpletest.rc-wtkqf" in namespace "gc-1506"
    Aug 17 07:33:03.227: INFO: Deleting pod "simpletest.rc-wzgpf" in namespace "gc-1506"
    Aug 17 07:33:03.286: INFO: Deleting pod "simpletest.rc-xbxh8" in namespace "gc-1506"
    Aug 17 07:33:03.323: INFO: Deleting pod "simpletest.rc-xh2fk" in namespace "gc-1506"
    Aug 17 07:33:03.381: INFO: Deleting pod "simpletest.rc-xjkz8" in namespace "gc-1506"
    Aug 17 07:33:03.430: INFO: Deleting pod "simpletest.rc-xlbv2" in namespace "gc-1506"
    Aug 17 07:33:03.479: INFO: Deleting pod "simpletest.rc-xzgws" in namespace "gc-1506"
    Aug 17 07:33:03.525: INFO: Deleting pod "simpletest.rc-z69g6" in namespace "gc-1506"
    Aug 17 07:33:03.598: INFO: Deleting pod "simpletest.rc-z8vrd" in namespace "gc-1506"
    Aug 17 07:33:03.624: INFO: Deleting pod "simpletest.rc-zkgpj" in namespace "gc-1506"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:03.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1506" for this suite. 08/17/23 07:33:03.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:03.777
Aug 17 07:33:03.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename watch 08/17/23 07:33:03.778
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:03.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:03.854
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/17/23 07:33:03.857
STEP: creating a new configmap 08/17/23 07:33:03.87
STEP: modifying the configmap once 08/17/23 07:33:03.876
STEP: closing the watch once it receives two notifications 08/17/23 07:33:03.892
Aug 17 07:33:03.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045242 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:33:03.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045244 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/17/23 07:33:03.892
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/17/23 07:33:03.906
STEP: deleting the configmap 08/17/23 07:33:03.908
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/17/23 07:33:03.915
Aug 17 07:33:03.915: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045248 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:33:03.915: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045251 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:03.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9971" for this suite. 08/17/23 07:33:03.922
------------------------------
â€¢ [0.150 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:03.777
    Aug 17 07:33:03.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename watch 08/17/23 07:33:03.778
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:03.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:03.854
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/17/23 07:33:03.857
    STEP: creating a new configmap 08/17/23 07:33:03.87
    STEP: modifying the configmap once 08/17/23 07:33:03.876
    STEP: closing the watch once it receives two notifications 08/17/23 07:33:03.892
    Aug 17 07:33:03.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045242 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:33:03.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045244 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/17/23 07:33:03.892
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/17/23 07:33:03.906
    STEP: deleting the configmap 08/17/23 07:33:03.908
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/17/23 07:33:03.915
    Aug 17 07:33:03.915: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045248 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:33:03.915: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9971  04a27167-65f3-4286-8856-272baa83c064 27045251 0 2023-08-17 07:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 07:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:03.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9971" for this suite. 08/17/23 07:33:03.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:03.927
Aug 17 07:33:03.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-runtime 08/17/23 07:33:03.928
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:03.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:03.96
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/17/23 07:33:03.963
STEP: wait for the container to reach Succeeded 08/17/23 07:33:03.978
STEP: get the container status 08/17/23 07:33:09.001
STEP: the container should be terminated 08/17/23 07:33:09.004
STEP: the termination message should be set 08/17/23 07:33:09.004
Aug 17 07:33:09.004: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/17/23 07:33:09.004
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:09.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7388" for this suite. 08/17/23 07:33:09.019
------------------------------
â€¢ [SLOW TEST] [5.094 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:03.927
    Aug 17 07:33:03.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-runtime 08/17/23 07:33:03.928
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:03.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:03.96
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/17/23 07:33:03.963
    STEP: wait for the container to reach Succeeded 08/17/23 07:33:03.978
    STEP: get the container status 08/17/23 07:33:09.001
    STEP: the container should be terminated 08/17/23 07:33:09.004
    STEP: the termination message should be set 08/17/23 07:33:09.004
    Aug 17 07:33:09.004: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/17/23 07:33:09.004
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:09.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7388" for this suite. 08/17/23 07:33:09.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:09.022
Aug 17 07:33:09.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:33:09.023
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:09.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:09.036
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/17/23 07:33:09.044
Aug 17 07:33:09.053: INFO: Waiting up to 5m0s for pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4" in namespace "downward-api-9679" to be "running and ready"
Aug 17 07:33:09.057: INFO: Pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575201ms
Aug 17 07:33:09.057: INFO: The phase of Pod labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:33:11.061: INFO: Pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008548557s
Aug 17 07:33:11.061: INFO: The phase of Pod labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4 is Running (Ready = true)
Aug 17 07:33:11.061: INFO: Pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4" satisfied condition "running and ready"
Aug 17 07:33:11.583: INFO: Successfully updated pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:15.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9679" for this suite. 08/17/23 07:33:15.601
------------------------------
â€¢ [SLOW TEST] [6.584 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:09.022
    Aug 17 07:33:09.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:33:09.023
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:09.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:09.036
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/17/23 07:33:09.044
    Aug 17 07:33:09.053: INFO: Waiting up to 5m0s for pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4" in namespace "downward-api-9679" to be "running and ready"
    Aug 17 07:33:09.057: INFO: Pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575201ms
    Aug 17 07:33:09.057: INFO: The phase of Pod labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:33:11.061: INFO: Pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008548557s
    Aug 17 07:33:11.061: INFO: The phase of Pod labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4 is Running (Ready = true)
    Aug 17 07:33:11.061: INFO: Pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4" satisfied condition "running and ready"
    Aug 17 07:33:11.583: INFO: Successfully updated pod "labelsupdate162145aa-347f-4de7-a33b-ec7d58403cb4"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:15.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9679" for this suite. 08/17/23 07:33:15.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:15.606
Aug 17 07:33:15.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:33:15.607
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:15.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:15.622
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:33:15.624
Aug 17 07:33:15.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917" in namespace "downward-api-7813" to be "Succeeded or Failed"
Aug 17 07:33:15.656: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448717ms
Aug 17 07:33:17.659: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194641s
Aug 17 07:33:19.659: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005656767s
STEP: Saw pod success 08/17/23 07:33:19.659
Aug 17 07:33:19.659: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917" satisfied condition "Succeeded or Failed"
Aug 17 07:33:19.661: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917 container client-container: <nil>
STEP: delete the pod 08/17/23 07:33:19.664
Aug 17 07:33:19.703: INFO: Waiting for pod downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917 to disappear
Aug 17 07:33:19.705: INFO: Pod downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:19.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7813" for this suite. 08/17/23 07:33:19.707
------------------------------
â€¢ [4.104 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:15.606
    Aug 17 07:33:15.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:33:15.607
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:15.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:15.622
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:33:15.624
    Aug 17 07:33:15.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917" in namespace "downward-api-7813" to be "Succeeded or Failed"
    Aug 17 07:33:15.656: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448717ms
    Aug 17 07:33:17.659: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194641s
    Aug 17 07:33:19.659: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005656767s
    STEP: Saw pod success 08/17/23 07:33:19.659
    Aug 17 07:33:19.659: INFO: Pod "downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917" satisfied condition "Succeeded or Failed"
    Aug 17 07:33:19.661: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:33:19.664
    Aug 17 07:33:19.703: INFO: Waiting for pod downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917 to disappear
    Aug 17 07:33:19.705: INFO: Pod downwardapi-volume-fc1a802e-a8d6-4f5a-9d21-3ead75fed917 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:19.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7813" for this suite. 08/17/23 07:33:19.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:19.716
Aug 17 07:33:19.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:33:19.717
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:19.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:19.737
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/17/23 07:33:19.743
STEP: Getting a ResourceQuota 08/17/23 07:33:19.755
STEP: Updating a ResourceQuota 08/17/23 07:33:19.757
STEP: Verifying a ResourceQuota was modified 08/17/23 07:33:19.76
STEP: Deleting a ResourceQuota 08/17/23 07:33:19.762
STEP: Verifying the deleted ResourceQuota 08/17/23 07:33:19.767
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:19.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1137" for this suite. 08/17/23 07:33:19.771
------------------------------
â€¢ [0.060 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:19.716
    Aug 17 07:33:19.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:33:19.717
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:19.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:19.737
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/17/23 07:33:19.743
    STEP: Getting a ResourceQuota 08/17/23 07:33:19.755
    STEP: Updating a ResourceQuota 08/17/23 07:33:19.757
    STEP: Verifying a ResourceQuota was modified 08/17/23 07:33:19.76
    STEP: Deleting a ResourceQuota 08/17/23 07:33:19.762
    STEP: Verifying the deleted ResourceQuota 08/17/23 07:33:19.767
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:19.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1137" for this suite. 08/17/23 07:33:19.771
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:19.776
Aug 17 07:33:19.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:33:19.777
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:19.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:19.789
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-aad642c2-080e-41c1-8f55-6164224ea9d8 08/17/23 07:33:19.792
STEP: Creating a pod to test consume secrets 08/17/23 07:33:19.799
Aug 17 07:33:19.807: INFO: Waiting up to 5m0s for pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05" in namespace "secrets-1213" to be "Succeeded or Failed"
Aug 17 07:33:19.815: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05": Phase="Pending", Reason="", readiness=false. Elapsed: 7.110047ms
Aug 17 07:33:21.818: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010178344s
Aug 17 07:33:23.817: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010042953s
STEP: Saw pod success 08/17/23 07:33:23.817
Aug 17 07:33:23.818: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05" satisfied condition "Succeeded or Failed"
Aug 17 07:33:23.819: INFO: Trying to get logs from node yst-node2 pod pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 07:33:23.823
Aug 17 07:33:23.829: INFO: Waiting for pod pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05 to disappear
Aug 17 07:33:23.832: INFO: Pod pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:23.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1213" for this suite. 08/17/23 07:33:23.835
------------------------------
â€¢ [4.062 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:19.776
    Aug 17 07:33:19.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:33:19.777
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:19.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:19.789
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-aad642c2-080e-41c1-8f55-6164224ea9d8 08/17/23 07:33:19.792
    STEP: Creating a pod to test consume secrets 08/17/23 07:33:19.799
    Aug 17 07:33:19.807: INFO: Waiting up to 5m0s for pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05" in namespace "secrets-1213" to be "Succeeded or Failed"
    Aug 17 07:33:19.815: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05": Phase="Pending", Reason="", readiness=false. Elapsed: 7.110047ms
    Aug 17 07:33:21.818: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010178344s
    Aug 17 07:33:23.817: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010042953s
    STEP: Saw pod success 08/17/23 07:33:23.817
    Aug 17 07:33:23.818: INFO: Pod "pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05" satisfied condition "Succeeded or Failed"
    Aug 17 07:33:23.819: INFO: Trying to get logs from node yst-node2 pod pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:33:23.823
    Aug 17 07:33:23.829: INFO: Waiting for pod pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05 to disappear
    Aug 17 07:33:23.832: INFO: Pod pod-secrets-7171f07b-5f8a-4fa9-83c6-0c4b873b0f05 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:23.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1213" for this suite. 08/17/23 07:33:23.835
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:23.839
Aug 17 07:33:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replication-controller 08/17/23 07:33:23.84
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:23.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:23.853
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20 08/17/23 07:33:23.856
Aug 17 07:33:23.870: INFO: Pod name my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20: Found 0 pods out of 1
Aug 17 07:33:28.876: INFO: Pod name my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20: Found 1 pods out of 1
Aug 17 07:33:28.876: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20" are running
Aug 17 07:33:28.876: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b" in namespace "replication-controller-4659" to be "running"
Aug 17 07:33:28.879: INFO: Pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.469115ms
Aug 17 07:33:28.879: INFO: Pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b" satisfied condition "running"
Aug 17 07:33:28.879: INFO: Pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:23 +0000 UTC Reason: Message:}])
Aug 17 07:33:28.879: INFO: Trying to dial the pod
Aug 17 07:33:33.887: INFO: Controller my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20: Got expected result from replica 1 [my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b]: "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:33.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4659" for this suite. 08/17/23 07:33:33.89
------------------------------
â€¢ [SLOW TEST] [10.054 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:23.839
    Aug 17 07:33:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replication-controller 08/17/23 07:33:23.84
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:23.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:23.853
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20 08/17/23 07:33:23.856
    Aug 17 07:33:23.870: INFO: Pod name my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20: Found 0 pods out of 1
    Aug 17 07:33:28.876: INFO: Pod name my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20: Found 1 pods out of 1
    Aug 17 07:33:28.876: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20" are running
    Aug 17 07:33:28.876: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b" in namespace "replication-controller-4659" to be "running"
    Aug 17 07:33:28.879: INFO: Pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.469115ms
    Aug 17 07:33:28.879: INFO: Pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b" satisfied condition "running"
    Aug 17 07:33:28.879: INFO: Pod "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 07:33:23 +0000 UTC Reason: Message:}])
    Aug 17 07:33:28.879: INFO: Trying to dial the pod
    Aug 17 07:33:33.887: INFO: Controller my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20: Got expected result from replica 1 [my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b]: "my-hostname-basic-5c982c38-60ca-495f-a7ef-0d83f3ccdf20-zvg2b", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:33.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4659" for this suite. 08/17/23 07:33:33.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:33.893
Aug 17 07:33:33.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:33:33.894
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:33.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:33.914
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/17/23 07:33:33.921
Aug 17 07:33:33.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4869 cluster-info'
Aug 17 07:33:34.005: INFO: stderr: ""
Aug 17 07:33:34.005: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:34.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4869" for this suite. 08/17/23 07:33:34.008
------------------------------
â€¢ [0.118 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:33.893
    Aug 17 07:33:33.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:33:33.894
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:33.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:33.914
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/17/23 07:33:33.921
    Aug 17 07:33:33.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-4869 cluster-info'
    Aug 17 07:33:34.005: INFO: stderr: ""
    Aug 17 07:33:34.005: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:34.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4869" for this suite. 08/17/23 07:33:34.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:34.013
Aug 17 07:33:34.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 07:33:34.013
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:34.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:34.029
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 17 07:33:34.031: INFO: Creating deployment "test-recreate-deployment"
Aug 17 07:33:34.045: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 17 07:33:34.052: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 17 07:33:36.056: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 17 07:33:36.057: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 17 07:33:36.061: INFO: Updating deployment test-recreate-deployment
Aug 17 07:33:36.061: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 07:33:36.103: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5300  14739642-57c5-4f50-8b96-7b1c52757a2f 27046909 2 2023-08-17 07:33:34 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f33748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-17 07:33:36 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-17 07:33:36 +0000 UTC,LastTransitionTime:2023-08-17 07:33:34 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 17 07:33:36.105: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5300  cc7d6bfe-e0db-4a23-83fd-0c550afc9482 27046906 1 2023-08-17 07:33:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 14739642-57c5-4f50-8b96-7b1c52757a2f 0xc008380060 0xc008380061}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14739642-57c5-4f50-8b96-7b1c52757a2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083800f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:33:36.105: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 17 07:33:36.105: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5300  f41dbdf8-52a8-4ed4-8a62-88e8c115f9fd 27046897 2 2023-08-17 07:33:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 14739642-57c5-4f50-8b96-7b1c52757a2f 0xc004f33b17 0xc004f33b18}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14739642-57c5-4f50-8b96-7b1c52757a2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f33f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:33:36.107: INFO: Pod "test-recreate-deployment-cff6dc657-fjffx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-fjffx test-recreate-deployment-cff6dc657- deployment-5300  d9dd6764-91e0-4fff-befe-472ef039a7fb 27046908 0 2023-08-17 07:33:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 cc7d6bfe-e0db-4a23-83fd-0c550afc9482 0xc004d97880 0xc004d97881}] [] [{kube-controller-manager Update v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc7d6bfe-e0db-4a23-83fd-0c550afc9482\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmnmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmnmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:,StartTime:2023-08-17 07:33:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:36.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5300" for this suite. 08/17/23 07:33:36.11
------------------------------
â€¢ [2.101 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:34.013
    Aug 17 07:33:34.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 07:33:34.013
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:34.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:34.029
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 17 07:33:34.031: INFO: Creating deployment "test-recreate-deployment"
    Aug 17 07:33:34.045: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 17 07:33:34.052: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 17 07:33:36.056: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 17 07:33:36.057: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 17 07:33:36.061: INFO: Updating deployment test-recreate-deployment
    Aug 17 07:33:36.061: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 07:33:36.103: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5300  14739642-57c5-4f50-8b96-7b1c52757a2f 27046909 2 2023-08-17 07:33:34 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f33748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-17 07:33:36 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-17 07:33:36 +0000 UTC,LastTransitionTime:2023-08-17 07:33:34 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 17 07:33:36.105: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5300  cc7d6bfe-e0db-4a23-83fd-0c550afc9482 27046906 1 2023-08-17 07:33:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 14739642-57c5-4f50-8b96-7b1c52757a2f 0xc008380060 0xc008380061}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14739642-57c5-4f50-8b96-7b1c52757a2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083800f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:33:36.105: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 17 07:33:36.105: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5300  f41dbdf8-52a8-4ed4-8a62-88e8c115f9fd 27046897 2 2023-08-17 07:33:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 14739642-57c5-4f50-8b96-7b1c52757a2f 0xc004f33b17 0xc004f33b18}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14739642-57c5-4f50-8b96-7b1c52757a2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f33f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:33:36.107: INFO: Pod "test-recreate-deployment-cff6dc657-fjffx" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-fjffx test-recreate-deployment-cff6dc657- deployment-5300  d9dd6764-91e0-4fff-befe-472ef039a7fb 27046908 0 2023-08-17 07:33:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 cc7d6bfe-e0db-4a23-83fd-0c550afc9482 0xc004d97880 0xc004d97881}] [] [{kube-controller-manager Update v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc7d6bfe-e0db-4a23-83fd-0c550afc9482\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmnmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmnmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:33:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:,StartTime:2023-08-17 07:33:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:36.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5300" for this suite. 08/17/23 07:33:36.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:36.114
Aug 17 07:33:36.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename disruption 08/17/23 07:33:36.115
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:36.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:36.125
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/17/23 07:33:36.136
STEP: Waiting for all pods to be running 08/17/23 07:33:38.157
Aug 17 07:33:38.160: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:40.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7529" for this suite. 08/17/23 07:33:40.168
------------------------------
â€¢ [4.057 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:36.114
    Aug 17 07:33:36.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename disruption 08/17/23 07:33:36.115
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:36.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:36.125
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/17/23 07:33:36.136
    STEP: Waiting for all pods to be running 08/17/23 07:33:38.157
    Aug 17 07:33:38.160: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:40.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7529" for this suite. 08/17/23 07:33:40.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:40.172
Aug 17 07:33:40.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:33:40.173
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:40.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:40.205
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-31e06e17-d94c-4b0e-90fa-8cb9aaaaea7e 08/17/23 07:33:40.208
STEP: Creating a pod to test consume secrets 08/17/23 07:33:40.214
Aug 17 07:33:40.218: INFO: Waiting up to 5m0s for pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5" in namespace "secrets-204" to be "Succeeded or Failed"
Aug 17 07:33:40.221: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4524ms
Aug 17 07:33:42.226: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007341613s
Aug 17 07:33:44.224: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005732186s
STEP: Saw pod success 08/17/23 07:33:44.224
Aug 17 07:33:44.224: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5" satisfied condition "Succeeded or Failed"
Aug 17 07:33:44.226: INFO: Trying to get logs from node yst-node2 pod pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 07:33:44.23
Aug 17 07:33:44.237: INFO: Waiting for pod pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5 to disappear
Aug 17 07:33:44.239: INFO: Pod pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:33:44.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-204" for this suite. 08/17/23 07:33:44.241
------------------------------
â€¢ [4.075 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:40.172
    Aug 17 07:33:40.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:33:40.173
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:40.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:40.205
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-31e06e17-d94c-4b0e-90fa-8cb9aaaaea7e 08/17/23 07:33:40.208
    STEP: Creating a pod to test consume secrets 08/17/23 07:33:40.214
    Aug 17 07:33:40.218: INFO: Waiting up to 5m0s for pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5" in namespace "secrets-204" to be "Succeeded or Failed"
    Aug 17 07:33:40.221: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4524ms
    Aug 17 07:33:42.226: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007341613s
    Aug 17 07:33:44.224: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005732186s
    STEP: Saw pod success 08/17/23 07:33:44.224
    Aug 17 07:33:44.224: INFO: Pod "pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5" satisfied condition "Succeeded or Failed"
    Aug 17 07:33:44.226: INFO: Trying to get logs from node yst-node2 pod pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:33:44.23
    Aug 17 07:33:44.237: INFO: Waiting for pod pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5 to disappear
    Aug 17 07:33:44.239: INFO: Pod pod-secrets-6d2cd118-1b6a-4896-85c1-6e5bcb26b7f5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:33:44.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-204" for this suite. 08/17/23 07:33:44.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:33:44.248
Aug 17 07:33:44.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:33:44.248
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:44.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:44.262
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-5xthg" 08/17/23 07:33:44.276
Aug 17 07:33:44.289: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard cpu limit of 500m
Aug 17 07:33:44.289: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-5xthg" /status 08/17/23 07:33:44.289
STEP: Confirm /status for "e2e-rq-status-5xthg" resourceQuota via watch 08/17/23 07:33:44.302
Aug 17 07:33:44.304: INFO: observed resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList(nil)
Aug 17 07:33:44.304: INFO: Found resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 17 07:33:44.304: INFO: ResourceQuota "e2e-rq-status-5xthg" /status was updated
STEP: Patching hard spec values for cpu & memory 08/17/23 07:33:44.306
Aug 17 07:33:44.310: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard cpu limit of 1
Aug 17 07:33:44.310: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-5xthg" /status 08/17/23 07:33:44.31
STEP: Confirm /status for "e2e-rq-status-5xthg" resourceQuota via watch 08/17/23 07:33:44.313
Aug 17 07:33:44.314: INFO: observed resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 17 07:33:44.314: INFO: Found resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 17 07:33:44.314: INFO: ResourceQuota "e2e-rq-status-5xthg" /status was patched
STEP: Get "e2e-rq-status-5xthg" /status 08/17/23 07:33:44.314
Aug 17 07:33:44.316: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard cpu of 1
Aug 17 07:33:44.316: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-5xthg" /status before checking Spec is unchanged 08/17/23 07:33:44.317
Aug 17 07:33:44.322: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard cpu of 2
Aug 17 07:33:44.322: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard memory of 2Gi
Aug 17 07:33:44.323: INFO: Found resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 17 07:37:44.327: INFO: ResourceQuota "e2e-rq-status-5xthg" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:37:44.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1939" for this suite. 08/17/23 07:37:44.333
------------------------------
â€¢ [SLOW TEST] [240.090 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:33:44.248
    Aug 17 07:33:44.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:33:44.248
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:33:44.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:33:44.262
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-5xthg" 08/17/23 07:33:44.276
    Aug 17 07:33:44.289: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard cpu limit of 500m
    Aug 17 07:33:44.289: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-5xthg" /status 08/17/23 07:33:44.289
    STEP: Confirm /status for "e2e-rq-status-5xthg" resourceQuota via watch 08/17/23 07:33:44.302
    Aug 17 07:33:44.304: INFO: observed resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList(nil)
    Aug 17 07:33:44.304: INFO: Found resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 17 07:33:44.304: INFO: ResourceQuota "e2e-rq-status-5xthg" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/17/23 07:33:44.306
    Aug 17 07:33:44.310: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard cpu limit of 1
    Aug 17 07:33:44.310: INFO: Resource quota "e2e-rq-status-5xthg" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-5xthg" /status 08/17/23 07:33:44.31
    STEP: Confirm /status for "e2e-rq-status-5xthg" resourceQuota via watch 08/17/23 07:33:44.313
    Aug 17 07:33:44.314: INFO: observed resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 17 07:33:44.314: INFO: Found resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 17 07:33:44.314: INFO: ResourceQuota "e2e-rq-status-5xthg" /status was patched
    STEP: Get "e2e-rq-status-5xthg" /status 08/17/23 07:33:44.314
    Aug 17 07:33:44.316: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard cpu of 1
    Aug 17 07:33:44.316: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-5xthg" /status before checking Spec is unchanged 08/17/23 07:33:44.317
    Aug 17 07:33:44.322: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard cpu of 2
    Aug 17 07:33:44.322: INFO: Resourcequota "e2e-rq-status-5xthg" reports status: hard memory of 2Gi
    Aug 17 07:33:44.323: INFO: Found resourceQuota "e2e-rq-status-5xthg" in namespace "resourcequota-1939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 17 07:37:44.327: INFO: ResourceQuota "e2e-rq-status-5xthg" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:37:44.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1939" for this suite. 08/17/23 07:37:44.333
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:37:44.338
Aug 17 07:37:44.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:37:44.339
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:44.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:44.402
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9412 08/17/23 07:37:44.406
STEP: changing the ExternalName service to type=ClusterIP 08/17/23 07:37:44.412
STEP: creating replication controller externalname-service in namespace services-9412 08/17/23 07:37:44.461
I0817 07:37:44.466061      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9412, replica count: 2
I0817 07:37:47.517586      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 07:37:47.517: INFO: Creating new exec pod
Aug 17 07:37:47.521: INFO: Waiting up to 5m0s for pod "execpodvtvw8" in namespace "services-9412" to be "running"
Aug 17 07:37:47.523: INFO: Pod "execpodvtvw8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552033ms
Aug 17 07:37:49.526: INFO: Pod "execpodvtvw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004606234s
Aug 17 07:37:49.526: INFO: Pod "execpodvtvw8" satisfied condition "running"
Aug 17 07:37:50.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9412 exec execpodvtvw8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 17 07:37:50.668: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 17 07:37:50.668: INFO: stdout: ""
Aug 17 07:37:50.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9412 exec execpodvtvw8 -- /bin/sh -x -c nc -v -z -w 2 10.106.186.65 80'
Aug 17 07:37:50.796: INFO: stderr: "+ nc -v -z -w 2 10.106.186.65 80\nConnection to 10.106.186.65 80 port [tcp/http] succeeded!\n"
Aug 17 07:37:50.796: INFO: stdout: ""
Aug 17 07:37:50.796: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:37:50.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9412" for this suite. 08/17/23 07:37:50.813
------------------------------
â€¢ [SLOW TEST] [6.479 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:37:44.338
    Aug 17 07:37:44.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:37:44.339
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:44.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:44.402
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9412 08/17/23 07:37:44.406
    STEP: changing the ExternalName service to type=ClusterIP 08/17/23 07:37:44.412
    STEP: creating replication controller externalname-service in namespace services-9412 08/17/23 07:37:44.461
    I0817 07:37:44.466061      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9412, replica count: 2
    I0817 07:37:47.517586      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 07:37:47.517: INFO: Creating new exec pod
    Aug 17 07:37:47.521: INFO: Waiting up to 5m0s for pod "execpodvtvw8" in namespace "services-9412" to be "running"
    Aug 17 07:37:47.523: INFO: Pod "execpodvtvw8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552033ms
    Aug 17 07:37:49.526: INFO: Pod "execpodvtvw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004606234s
    Aug 17 07:37:49.526: INFO: Pod "execpodvtvw8" satisfied condition "running"
    Aug 17 07:37:50.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9412 exec execpodvtvw8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 17 07:37:50.668: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 17 07:37:50.668: INFO: stdout: ""
    Aug 17 07:37:50.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9412 exec execpodvtvw8 -- /bin/sh -x -c nc -v -z -w 2 10.106.186.65 80'
    Aug 17 07:37:50.796: INFO: stderr: "+ nc -v -z -w 2 10.106.186.65 80\nConnection to 10.106.186.65 80 port [tcp/http] succeeded!\n"
    Aug 17 07:37:50.796: INFO: stdout: ""
    Aug 17 07:37:50.796: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:37:50.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9412" for this suite. 08/17/23 07:37:50.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:37:50.818
Aug 17 07:37:50.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename disruption 08/17/23 07:37:50.819
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:50.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:50.838
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/17/23 07:37:50.853
STEP: Updating PodDisruptionBudget status 08/17/23 07:37:52.859
STEP: Waiting for all pods to be running 08/17/23 07:37:52.863
Aug 17 07:37:52.865: INFO: running pods: 0 < 1
STEP: locating a running pod 08/17/23 07:37:54.868
STEP: Waiting for the pdb to be processed 08/17/23 07:37:54.874
STEP: Patching PodDisruptionBudget status 08/17/23 07:37:54.879
STEP: Waiting for the pdb to be processed 08/17/23 07:37:54.885
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 07:37:54.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9097" for this suite. 08/17/23 07:37:54.889
------------------------------
â€¢ [4.074 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:37:50.818
    Aug 17 07:37:50.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename disruption 08/17/23 07:37:50.819
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:50.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:50.838
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/17/23 07:37:50.853
    STEP: Updating PodDisruptionBudget status 08/17/23 07:37:52.859
    STEP: Waiting for all pods to be running 08/17/23 07:37:52.863
    Aug 17 07:37:52.865: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/17/23 07:37:54.868
    STEP: Waiting for the pdb to be processed 08/17/23 07:37:54.874
    STEP: Patching PodDisruptionBudget status 08/17/23 07:37:54.879
    STEP: Waiting for the pdb to be processed 08/17/23 07:37:54.885
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:37:54.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9097" for this suite. 08/17/23 07:37:54.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:37:54.894
Aug 17 07:37:54.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:37:54.895
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:54.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:54.917
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-0ca626fa-054e-4708-90ed-e86edbc23af2 08/17/23 07:37:54.932
STEP: Creating configMap with name cm-test-opt-upd-312bae8e-2342-4e01-9a9b-01731a8fc673 08/17/23 07:37:54.935
STEP: Creating the pod 08/17/23 07:37:54.938
Aug 17 07:37:54.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73" in namespace "projected-8290" to be "running and ready"
Aug 17 07:37:54.945: INFO: Pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.219686ms
Aug 17 07:37:54.945: INFO: The phase of Pod pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:37:56.947: INFO: Pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73": Phase="Running", Reason="", readiness=true. Elapsed: 2.004439189s
Aug 17 07:37:56.948: INFO: The phase of Pod pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73 is Running (Ready = true)
Aug 17 07:37:56.948: INFO: Pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0ca626fa-054e-4708-90ed-e86edbc23af2 08/17/23 07:37:56.963
STEP: Updating configmap cm-test-opt-upd-312bae8e-2342-4e01-9a9b-01731a8fc673 08/17/23 07:37:56.966
STEP: Creating configMap with name cm-test-opt-create-125d9cec-1c69-4731-8d5b-ea70a0de864b 08/17/23 07:37:56.969
STEP: waiting to observe update in volume 08/17/23 07:37:56.971
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:37:58.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8290" for this suite. 08/17/23 07:37:58.99
------------------------------
â€¢ [4.098 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:37:54.894
    Aug 17 07:37:54.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:37:54.895
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:54.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:54.917
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-0ca626fa-054e-4708-90ed-e86edbc23af2 08/17/23 07:37:54.932
    STEP: Creating configMap with name cm-test-opt-upd-312bae8e-2342-4e01-9a9b-01731a8fc673 08/17/23 07:37:54.935
    STEP: Creating the pod 08/17/23 07:37:54.938
    Aug 17 07:37:54.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73" in namespace "projected-8290" to be "running and ready"
    Aug 17 07:37:54.945: INFO: Pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.219686ms
    Aug 17 07:37:54.945: INFO: The phase of Pod pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:37:56.947: INFO: Pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73": Phase="Running", Reason="", readiness=true. Elapsed: 2.004439189s
    Aug 17 07:37:56.948: INFO: The phase of Pod pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73 is Running (Ready = true)
    Aug 17 07:37:56.948: INFO: Pod "pod-projected-configmaps-4184d99b-4a9c-494a-8843-ae146daefe73" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0ca626fa-054e-4708-90ed-e86edbc23af2 08/17/23 07:37:56.963
    STEP: Updating configmap cm-test-opt-upd-312bae8e-2342-4e01-9a9b-01731a8fc673 08/17/23 07:37:56.966
    STEP: Creating configMap with name cm-test-opt-create-125d9cec-1c69-4731-8d5b-ea70a0de864b 08/17/23 07:37:56.969
    STEP: waiting to observe update in volume 08/17/23 07:37:56.971
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:37:58.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8290" for this suite. 08/17/23 07:37:58.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:37:58.993
Aug 17 07:37:58.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 07:37:58.994
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.009
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/17/23 07:37:59.011
STEP: getting 08/17/23 07:37:59.031
STEP: listing in namespace 08/17/23 07:37:59.033
STEP: patching 08/17/23 07:37:59.034
STEP: deleting 08/17/23 07:37:59.097
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:37:59.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-686" for this suite. 08/17/23 07:37:59.107
------------------------------
â€¢ [0.118 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:37:58.993
    Aug 17 07:37:58.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 07:37:58.994
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.009
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/17/23 07:37:59.011
    STEP: getting 08/17/23 07:37:59.031
    STEP: listing in namespace 08/17/23 07:37:59.033
    STEP: patching 08/17/23 07:37:59.034
    STEP: deleting 08/17/23 07:37:59.097
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:37:59.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-686" for this suite. 08/17/23 07:37:59.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:37:59.113
Aug 17 07:37:59.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 07:37:59.114
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.132
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 17 07:37:59.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:37:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9825" for this suite. 08/17/23 07:37:59.697
------------------------------
â€¢ [0.588 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:37:59.113
    Aug 17 07:37:59.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 07:37:59.114
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.132
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 17 07:37:59.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:37:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9825" for this suite. 08/17/23 07:37:59.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:37:59.701
Aug 17 07:37:59.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:37:59.702
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.714
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-xk52v"  08/17/23 07:37:59.719
Aug 17 07:37:59.725: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-xk52v"  08/17/23 07:37:59.725
Aug 17 07:37:59.732: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 07:37:59.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4581" for this suite. 08/17/23 07:37:59.739
------------------------------
â€¢ [0.040 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:37:59.701
    Aug 17 07:37:59.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:37:59.702
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.714
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-xk52v"  08/17/23 07:37:59.719
    Aug 17 07:37:59.725: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-xk52v"  08/17/23 07:37:59.725
    Aug 17 07:37:59.732: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:37:59.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4581" for this suite. 08/17/23 07:37:59.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:37:59.742
Aug 17 07:37:59.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:37:59.743
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.752
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/17/23 07:37:59.754
Aug 17 07:37:59.767: INFO: Waiting up to 5m0s for pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd" in namespace "downward-api-8226" to be "Succeeded or Failed"
Aug 17 07:37:59.768: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.540588ms
Aug 17 07:38:01.772: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005200545s
Aug 17 07:38:03.771: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004605699s
STEP: Saw pod success 08/17/23 07:38:03.771
Aug 17 07:38:03.771: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd" satisfied condition "Succeeded or Failed"
Aug 17 07:38:03.773: INFO: Trying to get logs from node yst-node2 pod downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd container dapi-container: <nil>
STEP: delete the pod 08/17/23 07:38:03.777
Aug 17 07:38:03.783: INFO: Waiting for pod downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd to disappear
Aug 17 07:38:03.786: INFO: Pod downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:03.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8226" for this suite. 08/17/23 07:38:03.789
------------------------------
â€¢ [4.049 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:37:59.742
    Aug 17 07:37:59.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:37:59.743
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:37:59.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:37:59.752
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/17/23 07:37:59.754
    Aug 17 07:37:59.767: INFO: Waiting up to 5m0s for pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd" in namespace "downward-api-8226" to be "Succeeded or Failed"
    Aug 17 07:37:59.768: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.540588ms
    Aug 17 07:38:01.772: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005200545s
    Aug 17 07:38:03.771: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004605699s
    STEP: Saw pod success 08/17/23 07:38:03.771
    Aug 17 07:38:03.771: INFO: Pod "downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd" satisfied condition "Succeeded or Failed"
    Aug 17 07:38:03.773: INFO: Trying to get logs from node yst-node2 pod downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd container dapi-container: <nil>
    STEP: delete the pod 08/17/23 07:38:03.777
    Aug 17 07:38:03.783: INFO: Waiting for pod downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd to disappear
    Aug 17 07:38:03.786: INFO: Pod downward-api-05afbf58-c4ce-4058-9913-b69049f54bdd no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:03.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8226" for this suite. 08/17/23 07:38:03.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:03.792
Aug 17 07:38:03.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:38:03.792
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:03.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:03.807
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 17 07:38:03.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 07:38:07.334
Aug 17 07:38:07.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 create -f -'
Aug 17 07:38:08.577: INFO: stderr: ""
Aug 17 07:38:08.577: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 17 07:38:08.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 delete e2e-test-crd-publish-openapi-1231-crds test-cr'
Aug 17 07:38:08.657: INFO: stderr: ""
Aug 17 07:38:08.657: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 17 07:38:08.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 apply -f -'
Aug 17 07:38:08.976: INFO: stderr: ""
Aug 17 07:38:08.976: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 17 07:38:08.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 delete e2e-test-crd-publish-openapi-1231-crds test-cr'
Aug 17 07:38:09.061: INFO: stderr: ""
Aug 17 07:38:09.061: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/17/23 07:38:09.061
Aug 17 07:38:09.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 explain e2e-test-crd-publish-openapi-1231-crds'
Aug 17 07:38:09.383: INFO: stderr: ""
Aug 17 07:38:09.383: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1231-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:11.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4264" for this suite. 08/17/23 07:38:11.343
------------------------------
â€¢ [SLOW TEST] [7.556 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:03.792
    Aug 17 07:38:03.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:38:03.792
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:03.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:03.807
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 17 07:38:03.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 07:38:07.334
    Aug 17 07:38:07.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 create -f -'
    Aug 17 07:38:08.577: INFO: stderr: ""
    Aug 17 07:38:08.577: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 17 07:38:08.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 delete e2e-test-crd-publish-openapi-1231-crds test-cr'
    Aug 17 07:38:08.657: INFO: stderr: ""
    Aug 17 07:38:08.657: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 17 07:38:08.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 apply -f -'
    Aug 17 07:38:08.976: INFO: stderr: ""
    Aug 17 07:38:08.976: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 17 07:38:08.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 --namespace=crd-publish-openapi-4264 delete e2e-test-crd-publish-openapi-1231-crds test-cr'
    Aug 17 07:38:09.061: INFO: stderr: ""
    Aug 17 07:38:09.061: INFO: stdout: "e2e-test-crd-publish-openapi-1231-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/17/23 07:38:09.061
    Aug 17 07:38:09.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4264 explain e2e-test-crd-publish-openapi-1231-crds'
    Aug 17 07:38:09.383: INFO: stderr: ""
    Aug 17 07:38:09.383: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1231-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:11.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4264" for this suite. 08/17/23 07:38:11.343
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:11.348
Aug 17 07:38:11.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename certificates 08/17/23 07:38:11.348
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:11.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:11.359
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/17/23 07:38:12.096
STEP: getting /apis/certificates.k8s.io 08/17/23 07:38:12.098
STEP: getting /apis/certificates.k8s.io/v1 08/17/23 07:38:12.098
STEP: creating 08/17/23 07:38:12.099
STEP: getting 08/17/23 07:38:12.106
STEP: listing 08/17/23 07:38:12.108
STEP: watching 08/17/23 07:38:12.109
Aug 17 07:38:12.109: INFO: starting watch
STEP: patching 08/17/23 07:38:12.11
STEP: updating 08/17/23 07:38:12.113
Aug 17 07:38:12.115: INFO: waiting for watch events with expected annotations
Aug 17 07:38:12.115: INFO: saw patched and updated annotations
STEP: getting /approval 08/17/23 07:38:12.115
STEP: patching /approval 08/17/23 07:38:12.117
STEP: updating /approval 08/17/23 07:38:12.119
STEP: getting /status 08/17/23 07:38:12.122
STEP: patching /status 08/17/23 07:38:12.124
STEP: updating /status 08/17/23 07:38:12.127
STEP: deleting 08/17/23 07:38:12.131
STEP: deleting a collection 08/17/23 07:38:12.136
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:12.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-6634" for this suite. 08/17/23 07:38:12.143
------------------------------
â€¢ [0.797 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:11.348
    Aug 17 07:38:11.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename certificates 08/17/23 07:38:11.348
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:11.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:11.359
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/17/23 07:38:12.096
    STEP: getting /apis/certificates.k8s.io 08/17/23 07:38:12.098
    STEP: getting /apis/certificates.k8s.io/v1 08/17/23 07:38:12.098
    STEP: creating 08/17/23 07:38:12.099
    STEP: getting 08/17/23 07:38:12.106
    STEP: listing 08/17/23 07:38:12.108
    STEP: watching 08/17/23 07:38:12.109
    Aug 17 07:38:12.109: INFO: starting watch
    STEP: patching 08/17/23 07:38:12.11
    STEP: updating 08/17/23 07:38:12.113
    Aug 17 07:38:12.115: INFO: waiting for watch events with expected annotations
    Aug 17 07:38:12.115: INFO: saw patched and updated annotations
    STEP: getting /approval 08/17/23 07:38:12.115
    STEP: patching /approval 08/17/23 07:38:12.117
    STEP: updating /approval 08/17/23 07:38:12.119
    STEP: getting /status 08/17/23 07:38:12.122
    STEP: patching /status 08/17/23 07:38:12.124
    STEP: updating /status 08/17/23 07:38:12.127
    STEP: deleting 08/17/23 07:38:12.131
    STEP: deleting a collection 08/17/23 07:38:12.136
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:12.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-6634" for this suite. 08/17/23 07:38:12.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:12.146
Aug 17 07:38:12.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:38:12.147
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:12.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:12.157
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/17/23 07:38:12.158
STEP: Creating a ResourceQuota 08/17/23 07:38:17.161
STEP: Ensuring resource quota status is calculated 08/17/23 07:38:17.165
STEP: Creating a ReplicaSet 08/17/23 07:38:19.167
STEP: Ensuring resource quota status captures replicaset creation 08/17/23 07:38:19.173
STEP: Deleting a ReplicaSet 08/17/23 07:38:21.176
STEP: Ensuring resource quota status released usage 08/17/23 07:38:21.179
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:23.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7226" for this suite. 08/17/23 07:38:23.187
------------------------------
â€¢ [SLOW TEST] [11.044 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:12.146
    Aug 17 07:38:12.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:38:12.147
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:12.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:12.157
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/17/23 07:38:12.158
    STEP: Creating a ResourceQuota 08/17/23 07:38:17.161
    STEP: Ensuring resource quota status is calculated 08/17/23 07:38:17.165
    STEP: Creating a ReplicaSet 08/17/23 07:38:19.167
    STEP: Ensuring resource quota status captures replicaset creation 08/17/23 07:38:19.173
    STEP: Deleting a ReplicaSet 08/17/23 07:38:21.176
    STEP: Ensuring resource quota status released usage 08/17/23 07:38:21.179
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:23.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7226" for this suite. 08/17/23 07:38:23.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:23.191
Aug 17 07:38:23.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:38:23.192
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:23.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:23.206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:38:23.208
Aug 17 07:38:23.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc" in namespace "downward-api-6700" to be "Succeeded or Failed"
Aug 17 07:38:23.223: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121726ms
Aug 17 07:38:25.226: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004993763s
Aug 17 07:38:27.227: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006126018s
STEP: Saw pod success 08/17/23 07:38:27.227
Aug 17 07:38:27.227: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc" satisfied condition "Succeeded or Failed"
Aug 17 07:38:27.229: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc container client-container: <nil>
STEP: delete the pod 08/17/23 07:38:27.234
Aug 17 07:38:27.239: INFO: Waiting for pod downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc to disappear
Aug 17 07:38:27.240: INFO: Pod downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:27.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6700" for this suite. 08/17/23 07:38:27.243
------------------------------
â€¢ [4.055 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:23.191
    Aug 17 07:38:23.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:38:23.192
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:23.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:23.206
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:38:23.208
    Aug 17 07:38:23.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc" in namespace "downward-api-6700" to be "Succeeded or Failed"
    Aug 17 07:38:23.223: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121726ms
    Aug 17 07:38:25.226: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004993763s
    Aug 17 07:38:27.227: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006126018s
    STEP: Saw pod success 08/17/23 07:38:27.227
    Aug 17 07:38:27.227: INFO: Pod "downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc" satisfied condition "Succeeded or Failed"
    Aug 17 07:38:27.229: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc container client-container: <nil>
    STEP: delete the pod 08/17/23 07:38:27.234
    Aug 17 07:38:27.239: INFO: Waiting for pod downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc to disappear
    Aug 17 07:38:27.240: INFO: Pod downwardapi-volume-9abdb7ef-0edd-4177-9b1b-ac4d2369eadc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:27.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6700" for this suite. 08/17/23 07:38:27.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:27.247
Aug 17 07:38:27.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:38:27.247
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:27.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:27.258
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:38:27.26
Aug 17 07:38:27.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769" in namespace "projected-7717" to be "Succeeded or Failed"
Aug 17 07:38:27.289: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769": Phase="Pending", Reason="", readiness=false. Elapsed: 7.216625ms
Aug 17 07:38:29.291: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009884856s
Aug 17 07:38:31.292: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010059812s
STEP: Saw pod success 08/17/23 07:38:31.292
Aug 17 07:38:31.292: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769" satisfied condition "Succeeded or Failed"
Aug 17 07:38:31.293: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769 container client-container: <nil>
STEP: delete the pod 08/17/23 07:38:31.296
Aug 17 07:38:31.301: INFO: Waiting for pod downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769 to disappear
Aug 17 07:38:31.304: INFO: Pod downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:31.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7717" for this suite. 08/17/23 07:38:31.306
------------------------------
â€¢ [4.062 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:27.247
    Aug 17 07:38:27.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:38:27.247
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:27.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:27.258
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:38:27.26
    Aug 17 07:38:27.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769" in namespace "projected-7717" to be "Succeeded or Failed"
    Aug 17 07:38:27.289: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769": Phase="Pending", Reason="", readiness=false. Elapsed: 7.216625ms
    Aug 17 07:38:29.291: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009884856s
    Aug 17 07:38:31.292: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010059812s
    STEP: Saw pod success 08/17/23 07:38:31.292
    Aug 17 07:38:31.292: INFO: Pod "downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769" satisfied condition "Succeeded or Failed"
    Aug 17 07:38:31.293: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:38:31.296
    Aug 17 07:38:31.301: INFO: Waiting for pod downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769 to disappear
    Aug 17 07:38:31.304: INFO: Pod downwardapi-volume-53276597-374a-4c24-886d-6f95d4ef2769 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:31.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7717" for this suite. 08/17/23 07:38:31.306
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:31.309
Aug 17 07:38:31.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename daemonsets 08/17/23 07:38:31.31
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:31.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:31.333
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 08/17/23 07:38:31.357
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:38:31.359
Aug 17 07:38:31.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:38:31.367: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:38:32.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 07:38:32.373: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:38:33.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:38:33.373: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/17/23 07:38:33.375
Aug 17 07:38:33.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 07:38:33.384: INFO: Node yst-node1 is running 0 daemon pod, expected 1
Aug 17 07:38:34.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 07:38:34.389: INFO: Node yst-node1 is running 0 daemon pod, expected 1
Aug 17 07:38:35.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 07:38:35.405: INFO: Node yst-node1 is running 0 daemon pod, expected 1
Aug 17 07:38:36.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:38:36.389: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:38:36.391
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4544, will wait for the garbage collector to delete the pods 08/17/23 07:38:36.391
Aug 17 07:38:36.445: INFO: Deleting DaemonSet.extensions daemon-set took: 2.922657ms
Aug 17 07:38:36.546: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.848137ms
Aug 17 07:38:39.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:38:39.448: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 07:38:39.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27051286"},"items":null}

Aug 17 07:38:39.451: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27051286"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:39.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4544" for this suite. 08/17/23 07:38:39.46
------------------------------
â€¢ [SLOW TEST] [8.154 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:31.309
    Aug 17 07:38:31.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename daemonsets 08/17/23 07:38:31.31
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:31.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:31.333
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 08/17/23 07:38:31.357
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:38:31.359
    Aug 17 07:38:31.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:38:31.367: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:38:32.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 07:38:32.373: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:38:33.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:38:33.373: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/17/23 07:38:33.375
    Aug 17 07:38:33.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 07:38:33.384: INFO: Node yst-node1 is running 0 daemon pod, expected 1
    Aug 17 07:38:34.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 07:38:34.389: INFO: Node yst-node1 is running 0 daemon pod, expected 1
    Aug 17 07:38:35.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 07:38:35.405: INFO: Node yst-node1 is running 0 daemon pod, expected 1
    Aug 17 07:38:36.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:38:36.389: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:38:36.391
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4544, will wait for the garbage collector to delete the pods 08/17/23 07:38:36.391
    Aug 17 07:38:36.445: INFO: Deleting DaemonSet.extensions daemon-set took: 2.922657ms
    Aug 17 07:38:36.546: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.848137ms
    Aug 17 07:38:39.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:38:39.448: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 07:38:39.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27051286"},"items":null}

    Aug 17 07:38:39.451: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27051286"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:39.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4544" for this suite. 08/17/23 07:38:39.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:39.464
Aug 17 07:38:39.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename gc 08/17/23 07:38:39.464
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:39.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:39.475
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/17/23 07:38:39.478
STEP: delete the rc 08/17/23 07:38:44.498
STEP: wait for all pods to be garbage collected 08/17/23 07:38:44.5
STEP: Gathering metrics 08/17/23 07:38:49.505
Aug 17 07:38:49.521: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
Aug 17 07:38:49.523: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.793339ms
Aug 17 07:38:49.523: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
Aug 17 07:38:49.523: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
Aug 17 07:38:49.579: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 07:38:49.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3398" for this suite. 08/17/23 07:38:49.581
------------------------------
â€¢ [SLOW TEST] [10.120 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:39.464
    Aug 17 07:38:39.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename gc 08/17/23 07:38:39.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:39.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:39.475
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/17/23 07:38:39.478
    STEP: delete the rc 08/17/23 07:38:44.498
    STEP: wait for all pods to be garbage collected 08/17/23 07:38:44.5
    STEP: Gathering metrics 08/17/23 07:38:49.505
    Aug 17 07:38:49.521: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
    Aug 17 07:38:49.523: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.793339ms
    Aug 17 07:38:49.523: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
    Aug 17 07:38:49.523: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
    Aug 17 07:38:49.579: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:38:49.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3398" for this suite. 08/17/23 07:38:49.581
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:38:49.584
Aug 17 07:38:49.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-pred 08/17/23 07:38:49.585
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:49.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:49.602
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 07:38:49.604: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 07:38:49.620: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 07:38:49.623: INFO: 
Logging pods the apiserver thinks is on node yst-master before test
Aug 17 07:38:49.636: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container app ready: true, restart count 1
Aug 17 07:38:49.636: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container chartmuseum ready: true, restart count 2
Aug 17 07:38:49.636: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container cluster-server ready: true, restart count 7
Aug 17 07:38:49.636: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container proxy ready: true, restart count 8
Aug 17 07:38:49.636: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container keycloak ready: true, restart count 2
Aug 17 07:38:49.636: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container postgres ready: true, restart count 2
Aug 17 07:38:49.636: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container keycloak-observer ready: true, restart count 6
Aug 17 07:38:49.636: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container manual-webserver ready: true, restart count 1
Aug 17 07:38:49.636: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Aug 17 07:38:49.636: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Aug 17 07:38:49.636: INFO: 	Container kube-state-metrics ready: true, restart count 1
Aug 17 07:38:49.636: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:38:49.636: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:38:49.636: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
Aug 17 07:38:49.636: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container apiserver ready: true, restart count 2
Aug 17 07:38:49.636: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container manager ready: true, restart count 4
Aug 17 07:38:49.636: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:38:49.636: INFO: 	Container etcd-date ready: false, restart count 0
Aug 17 07:38:49.636: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:38:49.636: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container core ready: true, restart count 4
Aug 17 07:38:49.636: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container database ready: true, restart count 2
Aug 17 07:38:49.636: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container jobservice ready: true, restart count 11
Aug 17 07:38:49.636: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container nginx ready: true, restart count 9
Aug 17 07:38:49.636: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container notary-signer ready: true, restart count 6
Aug 17 07:38:49.636: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kiali ready: true, restart count 1
Aug 17 07:38:49.636: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kube-event-exporter ready: true, restart count 2
Aug 17 07:38:49.636: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container minio ready: true, restart count 2
Aug 17 07:38:49.636: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container index-clear ready: false, restart count 0
Aug 17 07:38:49.636: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container etcd-check ready: false, restart count 0
Aug 17 07:38:49.636: INFO: 	Container pgdata-backup-node ready: false, restart count 0
Aug 17 07:38:49.636: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container manager ready: true, restart count 5
Aug 17 07:38:49.636: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:38:49.636: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:38:49.636: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container backup-check ready: false, restart count 1
Aug 17 07:38:49.636: INFO: 	Container backup-date ready: false, restart count 0
Aug 17 07:38:49.636: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Aug 17 07:38:49.636: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:38:49.636: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container coredns ready: true, restart count 1
Aug 17 07:38:49.636: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container etcd ready: true, restart count 2
Aug 17 07:38:49.636: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kube-apiserver ready: true, restart count 2
Aug 17 07:38:49.636: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kube-controller-manager ready: true, restart count 1
Aug 17 07:38:49.636: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:38:49.636: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container kube-scheduler ready: true, restart count 1
Aug 17 07:38:49.636: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container metrics-server ready: true, restart count 3
Aug 17 07:38:49.636: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:38:49.636: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:38:49.636: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container acc-tomcat ready: true, restart count 1
Aug 17 07:38:49.636: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.636: INFO: 	Container exporter ready: true, restart count 2
Aug 17 07:38:49.636: INFO: 	Container server ready: true, restart count 2
Aug 17 07:38:49.636: INFO: 
Logging pods the apiserver thinks is on node yst-node1 before test
Aug 17 07:38:49.648: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container webserver ready: true, restart count 1
Aug 17 07:38:49.648: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container gateway ready: true, restart count 7
Aug 17 07:38:49.648: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container helm-server ready: true, restart count 7
Aug 17 07:38:49.648: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container app ready: true, restart count 2
Aug 17 07:38:49.648: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container thanos ready: true, restart count 2
Aug 17 07:38:49.648: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:38:49.648: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:38:49.648: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:38:49.648: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container server ready: true, restart count 5
Aug 17 07:38:49.648: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container alertmanager ready: true, restart count 3
Aug 17 07:38:49.648: INFO: 	Container config-reloader ready: true, restart count 2
Aug 17 07:38:49.648: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container auth-server ready: true, restart count 5
Aug 17 07:38:49.648: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container blackbox-exporter ready: true, restart count 1
Aug 17 07:38:49.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:38:49.648: INFO: 	Container module-configmap-reloader ready: true, restart count 1
Aug 17 07:38:49.648: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:38:49.648: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:38:49.648: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container cronhpa-controller ready: true, restart count 1
Aug 17 07:38:49.648: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container default-http-backend ready: true, restart count 1
Aug 17 07:38:49.648: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:38:49.648: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container chartmuseum ready: true, restart count 1
Aug 17 07:38:49.648: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container notary-server ready: true, restart count 7
Aug 17 07:38:49.648: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container portal ready: true, restart count 2
Aug 17 07:38:49.648: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container redis ready: true, restart count 2
Aug 17 07:38:49.648: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container registry ready: true, restart count 2
Aug 17 07:38:49.648: INFO: 	Container registryctl ready: true, restart count 2
Aug 17 07:38:49.648: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container trivy ready: true, restart count 2
Aug 17 07:38:49.648: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container server ready: true, restart count 7
Aug 17 07:38:49.648: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container logstash ready: true, restart count 2
Aug 17 07:38:49.648: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container agent ready: true, restart count 2
Aug 17 07:38:49.648: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container prometheus-adapter ready: true, restart count 3
Aug 17 07:38:49.648: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Aug 17 07:38:49.648: INFO: 	Container prometheus-operator ready: true, restart count 1
Aug 17 07:38:49.648: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container manager ready: true, restart count 7
Aug 17 07:38:49.648: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container thanos-store ready: true, restart count 16
Aug 17 07:38:49.648: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container timescaledb ready: true, restart count 2
Aug 17 07:38:49.648: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
Aug 17 07:38:49.648: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container workflow-controller ready: true, restart count 7
Aug 17 07:38:49.648: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container apiserver ready: true, restart count 1
Aug 17 07:38:49.648: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:38:49.648: INFO: coredns-69754fcccf-f55mq from kube-system started at 2023-08-17 07:14:52 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container coredns ready: true, restart count 0
Aug 17 07:38:49.648: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:38:49.648: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:38:49.648: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 07:38:49.648: INFO: 
Logging pods the apiserver thinks is on node yst-node2 before test
Aug 17 07:38:49.665: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Aug 17 07:38:49.665: INFO: 	Container node-exporter ready: true, restart count 2
Aug 17 07:38:49.665: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container filebeat ready: true, restart count 2
Aug 17 07:38:49.665: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container opensearch ready: true, restart count 1
Aug 17 07:38:49.665: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container config-reloader ready: true, restart count 1
Aug 17 07:38:49.665: INFO: 	Container prometheus ready: false, restart count 1
Aug 17 07:38:49.665: INFO: 	Container thanos-sidecar ready: true, restart count 1
Aug 17 07:38:49.665: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container nginx-ingress-controller ready: false, restart count 2
Aug 17 07:38:49.665: INFO: pod-csi-inline-volumes from csiinlinevolumes-686 started at 2023-08-17 07:37:59 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Aug 17 07:38:49.665: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container calico-node ready: true, restart count 2
Aug 17 07:38:49.665: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container coredns ready: false, restart count 1
Aug 17 07:38:49.665: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 17 07:38:49.665: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 07:38:49.665: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container e2e ready: true, restart count 0
Aug 17 07:38:49.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:38:49.665: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
Aug 17 07:38:49.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 07:38:49.665: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 07:38:49.665
Aug 17 07:38:49.669: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5572" to be "running"
Aug 17 07:38:49.670: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395957ms
Aug 17 07:38:51.673: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004374109s
Aug 17 07:38:51.673: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 07:38:51.675
STEP: Trying to apply a random label on the found node. 08/17/23 07:38:51.723
STEP: verifying the node has the label kubernetes.io/e2e-1312514b-ab05-4186-9597-4e7c85566777 95 08/17/23 07:38:51.739
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/17/23 07:38:51.742
Aug 17 07:38:51.745: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5572" to be "not pending"
Aug 17 07:38:51.746: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.500756ms
Aug 17 07:38:53.750: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004875697s
Aug 17 07:38:53.750: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.60.200.177 on the node which pod4 resides and expect not scheduled 08/17/23 07:38:53.75
Aug 17 07:38:53.753: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5572" to be "not pending"
Aug 17 07:38:53.755: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780834ms
Aug 17 07:38:55.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004681856s
Aug 17 07:38:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005112069s
Aug 17 07:38:59.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005086046s
Aug 17 07:39:01.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003653747s
Aug 17 07:39:03.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00418389s
Aug 17 07:39:05.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004778181s
Aug 17 07:39:07.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005085294s
Aug 17 07:39:09.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004756619s
Aug 17 07:39:11.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004371458s
Aug 17 07:39:13.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.003914349s
Aug 17 07:39:15.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004082233s
Aug 17 07:39:17.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004275758s
Aug 17 07:39:19.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005465043s
Aug 17 07:39:21.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004367237s
Aug 17 07:39:23.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004277664s
Aug 17 07:39:25.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004485542s
Aug 17 07:39:27.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005388714s
Aug 17 07:39:29.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00465855s
Aug 17 07:39:31.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004803115s
Aug 17 07:39:33.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00579292s
Aug 17 07:39:35.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004764698s
Aug 17 07:39:37.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004137243s
Aug 17 07:39:39.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.0055321s
Aug 17 07:39:41.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004474499s
Aug 17 07:39:43.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005885398s
Aug 17 07:39:45.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004243203s
Aug 17 07:39:47.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004124931s
Aug 17 07:39:49.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005283435s
Aug 17 07:39:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004695758s
Aug 17 07:39:53.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005198192s
Aug 17 07:39:55.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004756895s
Aug 17 07:39:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004947826s
Aug 17 07:39:59.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004609361s
Aug 17 07:40:01.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004231765s
Aug 17 07:40:03.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004738407s
Aug 17 07:40:05.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004595647s
Aug 17 07:40:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00424427s
Aug 17 07:40:09.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00457318s
Aug 17 07:40:11.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005157147s
Aug 17 07:40:13.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005002027s
Aug 17 07:40:15.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004924339s
Aug 17 07:40:17.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005319591s
Aug 17 07:40:19.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00460895s
Aug 17 07:40:21.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004398511s
Aug 17 07:40:23.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00532053s
Aug 17 07:40:25.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004302475s
Aug 17 07:40:27.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006019433s
Aug 17 07:40:29.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005499397s
Aug 17 07:40:31.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005062608s
Aug 17 07:40:33.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005337071s
Aug 17 07:40:35.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005857123s
Aug 17 07:40:37.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005575659s
Aug 17 07:40:39.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006276312s
Aug 17 07:40:41.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004768313s
Aug 17 07:40:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005139467s
Aug 17 07:40:45.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005658285s
Aug 17 07:40:47.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004916516s
Aug 17 07:40:49.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005356118s
Aug 17 07:40:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00465508s
Aug 17 07:40:53.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005575945s
Aug 17 07:40:55.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005758102s
Aug 17 07:40:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005091739s
Aug 17 07:40:59.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005530462s
Aug 17 07:41:01.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.004624107s
Aug 17 07:41:03.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004406729s
Aug 17 07:41:05.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005566051s
Aug 17 07:41:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004212657s
Aug 17 07:41:09.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004172222s
Aug 17 07:41:11.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.004615194s
Aug 17 07:41:13.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004736935s
Aug 17 07:41:15.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005959307s
Aug 17 07:41:17.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005280071s
Aug 17 07:41:19.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006241925s
Aug 17 07:41:21.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004936227s
Aug 17 07:41:23.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005292506s
Aug 17 07:41:25.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005271274s
Aug 17 07:41:27.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004378026s
Aug 17 07:41:29.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004087785s
Aug 17 07:41:31.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004149118s
Aug 17 07:41:33.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.003766958s
Aug 17 07:41:35.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.003971032s
Aug 17 07:41:37.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005706261s
Aug 17 07:41:39.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006607113s
Aug 17 07:41:41.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005125097s
Aug 17 07:41:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00543982s
Aug 17 07:41:45.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006307349s
Aug 17 07:41:47.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.004199304s
Aug 17 07:41:49.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005868043s
Aug 17 07:41:51.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004420608s
Aug 17 07:41:53.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004069785s
Aug 17 07:41:55.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005588824s
Aug 17 07:41:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004891703s
Aug 17 07:41:59.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.004585395s
Aug 17 07:42:01.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005169968s
Aug 17 07:42:03.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005133954s
Aug 17 07:42:05.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005202274s
Aug 17 07:42:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00439803s
Aug 17 07:42:09.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.003873842s
Aug 17 07:42:11.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004479395s
Aug 17 07:42:13.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004090234s
Aug 17 07:42:15.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005345297s
Aug 17 07:42:17.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.004208777s
Aug 17 07:42:19.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004175308s
Aug 17 07:42:21.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00439191s
Aug 17 07:42:23.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.004132958s
Aug 17 07:42:25.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004155748s
Aug 17 07:42:27.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005833313s
Aug 17 07:42:29.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005819808s
Aug 17 07:42:31.780: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.027114537s
Aug 17 07:42:33.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004251429s
Aug 17 07:42:35.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005015941s
Aug 17 07:42:37.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.005344864s
Aug 17 07:42:39.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.006130893s
Aug 17 07:42:41.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004571674s
Aug 17 07:42:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005110321s
Aug 17 07:42:45.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005224472s
Aug 17 07:42:47.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005428975s
Aug 17 07:42:49.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006118187s
Aug 17 07:42:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005274169s
Aug 17 07:42:53.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005429831s
Aug 17 07:42:55.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006019333s
Aug 17 07:42:57.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004056462s
Aug 17 07:42:59.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004279069s
Aug 17 07:43:01.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.004508111s
Aug 17 07:43:03.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004223129s
Aug 17 07:43:05.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004416769s
Aug 17 07:43:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004158579s
Aug 17 07:43:09.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004310102s
Aug 17 07:43:11.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005712984s
Aug 17 07:43:13.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004021167s
Aug 17 07:43:15.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004297794s
Aug 17 07:43:17.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004841965s
Aug 17 07:43:19.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00662861s
Aug 17 07:43:21.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006289324s
Aug 17 07:43:23.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.006008488s
Aug 17 07:43:25.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005976209s
Aug 17 07:43:27.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004280932s
Aug 17 07:43:29.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005649011s
Aug 17 07:43:31.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00445041s
Aug 17 07:43:33.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004556103s
Aug 17 07:43:35.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004186885s
Aug 17 07:43:37.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004501793s
Aug 17 07:43:39.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004399534s
Aug 17 07:43:41.772: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018656731s
Aug 17 07:43:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005515678s
Aug 17 07:43:45.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004611177s
Aug 17 07:43:47.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004245843s
Aug 17 07:43:49.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004523041s
Aug 17 07:43:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.004558263s
Aug 17 07:43:53.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004429242s
Aug 17 07:43:53.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007077044s
STEP: removing the label kubernetes.io/e2e-1312514b-ab05-4186-9597-4e7c85566777 off the node yst-node2 08/17/23 07:43:53.76
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1312514b-ab05-4186-9597-4e7c85566777 08/17/23 07:43:53.769
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:43:53.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5572" for this suite. 08/17/23 07:43:53.776
------------------------------
â€¢ [SLOW TEST] [304.195 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:38:49.584
    Aug 17 07:38:49.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-pred 08/17/23 07:38:49.585
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:38:49.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:38:49.602
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 07:38:49.604: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 07:38:49.620: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 07:38:49.623: INFO: 
    Logging pods the apiserver thinks is on node yst-master before test
    Aug 17 07:38:49.636: INFO: alert-apiserver-67bffd5d79-4jkdw from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container app ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: chartmuseum-chartmuseum-fd9b485cc-98wvk from acc-global started at 2023-07-04 02:30:08 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container chartmuseum ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: cluster-server-9775c76cc-9g6hh from acc-global started at 2023-07-04 02:29:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container cluster-server ready: true, restart count 7
    Aug 17 07:38:49.636: INFO: gateway-proxy-v8vpk from acc-global started at 2023-07-04 02:30:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container proxy ready: true, restart count 8
    Aug 17 07:38:49.636: INFO: keycloak-64b4b5569c-x5j9n from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container keycloak ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: keycloak-db-5cc8f7f7d5-x6xbq from acc-global started at 2023-07-04 02:29:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container postgres ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: keycloak-observer-86b5cc8d77-qs2lj from acc-global started at 2023-07-04 02:29:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container keycloak-observer ready: true, restart count 6
    Aug 17 07:38:49.636: INFO: manual-webserver-66754cc96-5rf5n from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container manual-webserver ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: acc-kube-state-metrics-86c68f6799-bpbfb from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: 	Container kube-state-metrics ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: acc-node-exporter-4nxg2 from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: accordion-data-provisioner-868fbcb9c9-94xx4 from acc-system started at 2023-07-04 02:12:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container nfs-client-provisioner ready: true, restart count 6
    Aug 17 07:38:49.636: INFO: cicd-apiserver-59d9f5977b-qj6tl from acc-system started at 2023-07-04 02:27:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container apiserver ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: cicd-trigger-manager-7f78f48b64-9t4dx from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container manager ready: true, restart count 4
    Aug 17 07:38:49.636: INFO: etcd-backup-28203360-wgtp4 from acc-system started at 2023-08-16 16:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:38:49.636: INFO: 	Container etcd-date ready: false, restart count 0
    Aug 17 07:38:49.636: INFO: filebeat-filebeat-wt7kd from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: harbor-core-c69dc8568-dxzjg from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container core ready: true, restart count 4
    Aug 17 07:38:49.636: INFO: harbor-database-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container database ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: harbor-jobservice-66b66b4b9-f22t7 from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container jobservice ready: true, restart count 11
    Aug 17 07:38:49.636: INFO: harbor-nginx-7bb6fcf984-xgzmf from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container nginx ready: true, restart count 9
    Aug 17 07:38:49.636: INFO: harbor-notary-signer-cbbf7c95d-nlmpk from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container notary-signer ready: true, restart count 6
    Aug 17 07:38:49.636: INFO: kiali-6d6fd7b96c-4qtgj from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kiali ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: kube-event-exporter-57bb5b5d55-gnbnd from acc-system started at 2023-07-04 02:18:07 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kube-event-exporter ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: minio-0 from acc-system started at 2023-07-04 02:20:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container minio ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: opensearch-index-clear-28203300-cjkd4 from acc-system started at 2023-08-16 15:00:00 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container index-clear ready: false, restart count 0
    Aug 17 07:38:49.636: INFO: pgdata-backup-node-28203420-px2mm from acc-system started at 2023-08-16 17:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container etcd-check ready: false, restart count 0
    Aug 17 07:38:49.636: INFO: 	Container pgdata-backup-node ready: false, restart count 0
    Aug 17 07:38:49.636: INFO: scouter-manager-6bd5945b66-zmwv2 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container manager ready: true, restart count 5
    Aug 17 07:38:49.636: INFO: thanos-compactor-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: user-ingress-controller-ltkfd from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: yaml-backup-28203480-kl9lv from acc-system started at 2023-08-16 18:00:00 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container backup-check ready: false, restart count 1
    Aug 17 07:38:49.636: INFO: 	Container backup-date ready: false, restart count 0
    Aug 17 07:38:49.636: INFO: calico-kube-controllers-59f6c5b776-ld46r from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: calico-node-rvq9v from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: coredns-69754fcccf-r7xb2 from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container coredns ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: etcd-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container etcd ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: kube-apiserver-yst-master from kube-system started at 2023-08-17 06:41:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kube-apiserver ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: kube-controller-manager-yst-master from kube-system started at 2023-07-04 02:01:32 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: kube-proxy-mtcmj from kube-system started at 2023-08-17 06:16:44 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: kube-scheduler-yst-master from kube-system started at 2023-08-09 07:18:45 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container kube-scheduler ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: metrics-server-d88c6d6d7-6n64h from kube-system started at 2023-07-04 02:29:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container metrics-server ready: true, restart count 3
    Aug 17 07:38:49.636: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:38:49.636: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:38:49.636: INFO: acc-tomcat-ddb885997-2w79z from test started at 2023-08-09 07:25:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container acc-tomcat ready: true, restart count 1
    Aug 17 07:38:49.636: INFO: scouter-server-64f47bc954-r47gx from test started at 2023-07-19 01:38:58 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.636: INFO: 	Container exporter ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: 	Container server ready: true, restart count 2
    Aug 17 07:38:49.636: INFO: 
    Logging pods the apiserver thinks is on node yst-node1 before test
    Aug 17 07:38:49.648: INFO: console-7fb6dfb4b6-gfzpf from acc-global started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container webserver ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: gateway-6f8686745f-7sthl from acc-global started at 2023-07-04 02:30:25 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container gateway ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: helm-server-5776765f78-ggckc from acc-global started at 2023-07-04 02:31:05 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container helm-server ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: monitoring-apiserver-db555b567-mbxz5 from acc-global started at 2023-07-04 02:31:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container app ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: thanos-querier-74bcdf4595-4d8sj from acc-global started at 2023-07-04 02:32:46 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container thanos ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: acc-node-exporter-6gbgm from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: accordion-ingress-controller-56bfbf9c76-hz58x from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: alert-server-57f98d8ff9-vkk9v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container server ready: true, restart count 5
    Aug 17 07:38:49.648: INFO: alertmanager-main-0 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container alertmanager ready: true, restart count 3
    Aug 17 07:38:49.648: INFO: 	Container config-reloader ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: auth-server-f67b484c7-n8c9m from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container auth-server ready: true, restart count 5
    Aug 17 07:38:49.648: INFO: blackbox-exporter-f48cd58c9-w6p8v from acc-system started at 2023-08-09 07:25:20 +0000 UTC (3 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container blackbox-exporter ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: 	Container module-configmap-reloader ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: cicd-manager-56f888c97f-wpbd7 from acc-system started at 2023-07-04 02:25:31 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: cicd-template-sync-manager-957d5cff4-6mqz6 from acc-system started at 2023-07-04 02:27:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: cronhpa-controller-66bcb7c54c-z5mwq from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container cronhpa-controller ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: default-http-backend-586b89f667-v7mjh from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container default-http-backend ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: filebeat-filebeat-g4crt from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: harbor-chartmuseum-777994dc68-nk8fn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container chartmuseum ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: harbor-notary-server-799b956658-k4cjz from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container notary-server ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: harbor-portal-77c89f9f8d-jvtfl from acc-system started at 2023-07-04 02:19:01 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container portal ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: harbor-redis-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container redis ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: harbor-registry-64856bdc84-pq8r2 from acc-system started at 2023-07-04 02:19:05 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container registry ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: 	Container registryctl ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: harbor-trivy-0 from acc-system started at 2023-07-04 02:19:02 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container trivy ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: log-server-78fd7c75c8-76s7s from acc-system started at 2023-07-04 02:33:34 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container server ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: logstash-logstash-0 from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container logstash ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: member-agent-64c8644464-8mqkv from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container agent ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: prometheus-adapter-dcc8655bc-697t4 from acc-system started at 2023-07-04 02:16:21 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container prometheus-adapter ready: true, restart count 3
    Aug 17 07:38:49.648: INFO: prometheus-operator-779799477d-hg559 from acc-system started at 2023-08-09 07:25:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: 	Container prometheus-operator ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: registry-manager-75ff67db55-mzct2 from acc-system started at 2023-07-04 02:23:41 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container manager ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: thanos-store-0 from acc-system started at 2023-07-04 02:20:11 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container thanos-store ready: true, restart count 16
    Aug 17 07:38:49.648: INFO: tsdb-timescaledb-0 from acc-system started at 2023-07-04 02:17:50 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container timescaledb ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: user-ingress-controller-r7wrt from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container nginx-ingress-controller ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: workflow-controller-746b98c5c-5t6dl from acc-system started at 2023-07-04 02:24:37 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container workflow-controller ready: true, restart count 7
    Aug 17 07:38:49.648: INFO: xlog-apiserver-b78f4959d-wcfnn from acc-system started at 2023-08-09 07:25:20 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container apiserver ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: calico-node-hnssl from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:38:49.648: INFO: coredns-69754fcccf-f55mq from kube-system started at 2023-08-17 07:14:52 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 07:38:49.648: INFO: kube-proxy-vl6rl from kube-system started at 2023-08-17 06:16:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:38:49.648: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-zl574 from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:38:49.648: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 07:38:49.648: INFO: 
    Logging pods the apiserver thinks is on node yst-node2 before test
    Aug 17 07:38:49.665: INFO: acc-node-exporter-n94lf from acc-system started at 2023-07-04 02:16:20 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
    Aug 17 07:38:49.665: INFO: 	Container node-exporter ready: true, restart count 2
    Aug 17 07:38:49.665: INFO: filebeat-filebeat-ttr4s from acc-system started at 2023-07-04 02:17:26 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container filebeat ready: true, restart count 2
    Aug 17 07:38:49.665: INFO: opensearch-cluster-master-0 from acc-system started at 2023-08-09 07:34:39 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container opensearch ready: true, restart count 1
    Aug 17 07:38:49.665: INFO: prometheus-prometheus-operator-prometheus-0 from acc-system started at 2023-08-09 07:34:30 +0000 UTC (3 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container config-reloader ready: true, restart count 1
    Aug 17 07:38:49.665: INFO: 	Container prometheus ready: false, restart count 1
    Aug 17 07:38:49.665: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Aug 17 07:38:49.665: INFO: user-ingress-controller-knj75 from acc-system started at 2023-07-04 02:16:51 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container nginx-ingress-controller ready: false, restart count 2
    Aug 17 07:38:49.665: INFO: pod-csi-inline-volumes from csiinlinevolumes-686 started at 2023-08-17 07:37:59 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Aug 17 07:38:49.665: INFO: calico-node-kh6f4 from kube-system started at 2023-07-04 02:29:04 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container calico-node ready: true, restart count 2
    Aug 17 07:38:49.665: INFO: coredns-69754fcccf-bx54l from kube-system started at 2023-08-17 06:16:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container coredns ready: false, restart count 1
    Aug 17 07:38:49.665: INFO: kube-proxy-znvf7 from kube-system started at 2023-08-17 06:16:30 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container kube-proxy ready: true, restart count 1
    Aug 17 07:38:49.665: INFO: sonobuoy from sonobuoy started at 2023-08-17 06:44:24 +0000 UTC (1 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 07:38:49.665: INFO: sonobuoy-e2e-job-ab8d33ed8e20458b from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 07:38:49.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:38:49.665: INFO: sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-5sqfn from sonobuoy started at 2023-08-17 06:44:31 +0000 UTC (2 container statuses recorded)
    Aug 17 07:38:49.665: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 07:38:49.665: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 07:38:49.665
    Aug 17 07:38:49.669: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5572" to be "running"
    Aug 17 07:38:49.670: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395957ms
    Aug 17 07:38:51.673: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004374109s
    Aug 17 07:38:51.673: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 07:38:51.675
    STEP: Trying to apply a random label on the found node. 08/17/23 07:38:51.723
    STEP: verifying the node has the label kubernetes.io/e2e-1312514b-ab05-4186-9597-4e7c85566777 95 08/17/23 07:38:51.739
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/17/23 07:38:51.742
    Aug 17 07:38:51.745: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5572" to be "not pending"
    Aug 17 07:38:51.746: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.500756ms
    Aug 17 07:38:53.750: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004875697s
    Aug 17 07:38:53.750: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.60.200.177 on the node which pod4 resides and expect not scheduled 08/17/23 07:38:53.75
    Aug 17 07:38:53.753: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5572" to be "not pending"
    Aug 17 07:38:53.755: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780834ms
    Aug 17 07:38:55.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004681856s
    Aug 17 07:38:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005112069s
    Aug 17 07:38:59.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005086046s
    Aug 17 07:39:01.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003653747s
    Aug 17 07:39:03.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00418389s
    Aug 17 07:39:05.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004778181s
    Aug 17 07:39:07.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005085294s
    Aug 17 07:39:09.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004756619s
    Aug 17 07:39:11.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004371458s
    Aug 17 07:39:13.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.003914349s
    Aug 17 07:39:15.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004082233s
    Aug 17 07:39:17.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004275758s
    Aug 17 07:39:19.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005465043s
    Aug 17 07:39:21.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004367237s
    Aug 17 07:39:23.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004277664s
    Aug 17 07:39:25.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004485542s
    Aug 17 07:39:27.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005388714s
    Aug 17 07:39:29.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00465855s
    Aug 17 07:39:31.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004803115s
    Aug 17 07:39:33.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00579292s
    Aug 17 07:39:35.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004764698s
    Aug 17 07:39:37.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004137243s
    Aug 17 07:39:39.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.0055321s
    Aug 17 07:39:41.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004474499s
    Aug 17 07:39:43.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005885398s
    Aug 17 07:39:45.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004243203s
    Aug 17 07:39:47.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004124931s
    Aug 17 07:39:49.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005283435s
    Aug 17 07:39:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004695758s
    Aug 17 07:39:53.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005198192s
    Aug 17 07:39:55.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004756895s
    Aug 17 07:39:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004947826s
    Aug 17 07:39:59.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004609361s
    Aug 17 07:40:01.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004231765s
    Aug 17 07:40:03.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004738407s
    Aug 17 07:40:05.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004595647s
    Aug 17 07:40:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00424427s
    Aug 17 07:40:09.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00457318s
    Aug 17 07:40:11.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005157147s
    Aug 17 07:40:13.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005002027s
    Aug 17 07:40:15.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004924339s
    Aug 17 07:40:17.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005319591s
    Aug 17 07:40:19.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00460895s
    Aug 17 07:40:21.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004398511s
    Aug 17 07:40:23.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00532053s
    Aug 17 07:40:25.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004302475s
    Aug 17 07:40:27.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006019433s
    Aug 17 07:40:29.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005499397s
    Aug 17 07:40:31.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005062608s
    Aug 17 07:40:33.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005337071s
    Aug 17 07:40:35.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005857123s
    Aug 17 07:40:37.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005575659s
    Aug 17 07:40:39.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006276312s
    Aug 17 07:40:41.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004768313s
    Aug 17 07:40:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005139467s
    Aug 17 07:40:45.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005658285s
    Aug 17 07:40:47.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004916516s
    Aug 17 07:40:49.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005356118s
    Aug 17 07:40:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00465508s
    Aug 17 07:40:53.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005575945s
    Aug 17 07:40:55.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005758102s
    Aug 17 07:40:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005091739s
    Aug 17 07:40:59.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005530462s
    Aug 17 07:41:01.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.004624107s
    Aug 17 07:41:03.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004406729s
    Aug 17 07:41:05.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005566051s
    Aug 17 07:41:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004212657s
    Aug 17 07:41:09.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004172222s
    Aug 17 07:41:11.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.004615194s
    Aug 17 07:41:13.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004736935s
    Aug 17 07:41:15.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005959307s
    Aug 17 07:41:17.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005280071s
    Aug 17 07:41:19.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006241925s
    Aug 17 07:41:21.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004936227s
    Aug 17 07:41:23.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005292506s
    Aug 17 07:41:25.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005271274s
    Aug 17 07:41:27.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004378026s
    Aug 17 07:41:29.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004087785s
    Aug 17 07:41:31.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004149118s
    Aug 17 07:41:33.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.003766958s
    Aug 17 07:41:35.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.003971032s
    Aug 17 07:41:37.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005706261s
    Aug 17 07:41:39.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006607113s
    Aug 17 07:41:41.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005125097s
    Aug 17 07:41:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00543982s
    Aug 17 07:41:45.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006307349s
    Aug 17 07:41:47.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.004199304s
    Aug 17 07:41:49.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005868043s
    Aug 17 07:41:51.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004420608s
    Aug 17 07:41:53.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004069785s
    Aug 17 07:41:55.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005588824s
    Aug 17 07:41:57.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004891703s
    Aug 17 07:41:59.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.004585395s
    Aug 17 07:42:01.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005169968s
    Aug 17 07:42:03.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005133954s
    Aug 17 07:42:05.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005202274s
    Aug 17 07:42:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00439803s
    Aug 17 07:42:09.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.003873842s
    Aug 17 07:42:11.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004479395s
    Aug 17 07:42:13.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004090234s
    Aug 17 07:42:15.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005345297s
    Aug 17 07:42:17.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.004208777s
    Aug 17 07:42:19.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004175308s
    Aug 17 07:42:21.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00439191s
    Aug 17 07:42:23.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.004132958s
    Aug 17 07:42:25.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004155748s
    Aug 17 07:42:27.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005833313s
    Aug 17 07:42:29.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005819808s
    Aug 17 07:42:31.780: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.027114537s
    Aug 17 07:42:33.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004251429s
    Aug 17 07:42:35.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005015941s
    Aug 17 07:42:37.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.005344864s
    Aug 17 07:42:39.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.006130893s
    Aug 17 07:42:41.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004571674s
    Aug 17 07:42:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005110321s
    Aug 17 07:42:45.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005224472s
    Aug 17 07:42:47.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005428975s
    Aug 17 07:42:49.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006118187s
    Aug 17 07:42:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005274169s
    Aug 17 07:42:53.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005429831s
    Aug 17 07:42:55.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006019333s
    Aug 17 07:42:57.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004056462s
    Aug 17 07:42:59.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004279069s
    Aug 17 07:43:01.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.004508111s
    Aug 17 07:43:03.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004223129s
    Aug 17 07:43:05.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004416769s
    Aug 17 07:43:07.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004158579s
    Aug 17 07:43:09.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004310102s
    Aug 17 07:43:11.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005712984s
    Aug 17 07:43:13.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004021167s
    Aug 17 07:43:15.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004297794s
    Aug 17 07:43:17.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004841965s
    Aug 17 07:43:19.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00662861s
    Aug 17 07:43:21.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006289324s
    Aug 17 07:43:23.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.006008488s
    Aug 17 07:43:25.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005976209s
    Aug 17 07:43:27.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004280932s
    Aug 17 07:43:29.759: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005649011s
    Aug 17 07:43:31.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00445041s
    Aug 17 07:43:33.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004556103s
    Aug 17 07:43:35.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004186885s
    Aug 17 07:43:37.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004501793s
    Aug 17 07:43:39.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004399534s
    Aug 17 07:43:41.772: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018656731s
    Aug 17 07:43:43.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005515678s
    Aug 17 07:43:45.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004611177s
    Aug 17 07:43:47.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004245843s
    Aug 17 07:43:49.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004523041s
    Aug 17 07:43:51.758: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.004558263s
    Aug 17 07:43:53.757: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004429242s
    Aug 17 07:43:53.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007077044s
    STEP: removing the label kubernetes.io/e2e-1312514b-ab05-4186-9597-4e7c85566777 off the node yst-node2 08/17/23 07:43:53.76
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1312514b-ab05-4186-9597-4e7c85566777 08/17/23 07:43:53.769
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:43:53.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5572" for this suite. 08/17/23 07:43:53.776
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:43:53.779
Aug 17 07:43:53.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename watch 08/17/23 07:43:53.78
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:43:53.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:43:53.791
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/17/23 07:43:53.796
STEP: creating a watch on configmaps with label B 08/17/23 07:43:53.798
STEP: creating a watch on configmaps with label A or B 08/17/23 07:43:53.799
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/17/23 07:43:53.8
Aug 17 07:43:53.808: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053985 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:43:53.808: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053985 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/17/23 07:43:53.808
Aug 17 07:43:53.817: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053990 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:43:53.817: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053990 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/17/23 07:43:53.817
Aug 17 07:43:53.827: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053992 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:43:53.828: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053992 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/17/23 07:43:53.828
Aug 17 07:43:53.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053993 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:43:53.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053993 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/17/23 07:43:53.834
Aug 17 07:43:53.836: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27053995 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:43:53.836: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27053995 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/17/23 07:44:03.837
Aug 17 07:44:03.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27054155 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:44:03.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27054155 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 07:44:13.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4582" for this suite. 08/17/23 07:44:13.845
------------------------------
â€¢ [SLOW TEST] [20.073 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:43:53.779
    Aug 17 07:43:53.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename watch 08/17/23 07:43:53.78
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:43:53.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:43:53.791
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/17/23 07:43:53.796
    STEP: creating a watch on configmaps with label B 08/17/23 07:43:53.798
    STEP: creating a watch on configmaps with label A or B 08/17/23 07:43:53.799
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/17/23 07:43:53.8
    Aug 17 07:43:53.808: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053985 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:43:53.808: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053985 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/17/23 07:43:53.808
    Aug 17 07:43:53.817: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053990 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:43:53.817: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053990 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/17/23 07:43:53.817
    Aug 17 07:43:53.827: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053992 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:43:53.828: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053992 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/17/23 07:43:53.828
    Aug 17 07:43:53.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053993 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:43:53.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4582  3c468915-76f6-48a0-99b4-cc024f9297f0 27053993 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/17/23 07:43:53.834
    Aug 17 07:43:53.836: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27053995 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:43:53.836: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27053995 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/17/23 07:44:03.837
    Aug 17 07:44:03.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27054155 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:44:03.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4582  79dcf569-7e2f-49d8-82bb-43d08a04ea37 27054155 0 2023-08-17 07:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 07:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:44:13.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4582" for this suite. 08/17/23 07:44:13.845
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:44:13.853
Aug 17 07:44:13.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sysctl 08/17/23 07:44:13.854
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:13.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:13.879
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/17/23 07:44:13.881
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:44:13.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2998" for this suite. 08/17/23 07:44:13.896
------------------------------
â€¢ [0.052 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:44:13.853
    Aug 17 07:44:13.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sysctl 08/17/23 07:44:13.854
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:13.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:13.879
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/17/23 07:44:13.881
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:44:13.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2998" for this suite. 08/17/23 07:44:13.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:44:13.905
Aug 17 07:44:13.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:44:13.906
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:13.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:13.932
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2b8f1052-e1dd-4f44-9a08-36fedfd8abe2 08/17/23 07:44:13.955
STEP: Creating the pod 08/17/23 07:44:13.973
Aug 17 07:44:13.988: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12" in namespace "projected-9082" to be "running and ready"
Aug 17 07:44:14.008: INFO: Pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12": Phase="Pending", Reason="", readiness=false. Elapsed: 20.140486ms
Aug 17 07:44:14.008: INFO: The phase of Pod pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:44:16.011: INFO: Pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12": Phase="Running", Reason="", readiness=true. Elapsed: 2.023046408s
Aug 17 07:44:16.011: INFO: The phase of Pod pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12 is Running (Ready = true)
Aug 17 07:44:16.011: INFO: Pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-2b8f1052-e1dd-4f44-9a08-36fedfd8abe2 08/17/23 07:44:16.024
STEP: waiting to observe update in volume 08/17/23 07:44:16.027
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:44:18.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9082" for this suite. 08/17/23 07:44:18.041
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:44:13.905
    Aug 17 07:44:13.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:44:13.906
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:13.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:13.932
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-2b8f1052-e1dd-4f44-9a08-36fedfd8abe2 08/17/23 07:44:13.955
    STEP: Creating the pod 08/17/23 07:44:13.973
    Aug 17 07:44:13.988: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12" in namespace "projected-9082" to be "running and ready"
    Aug 17 07:44:14.008: INFO: Pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12": Phase="Pending", Reason="", readiness=false. Elapsed: 20.140486ms
    Aug 17 07:44:14.008: INFO: The phase of Pod pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:44:16.011: INFO: Pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12": Phase="Running", Reason="", readiness=true. Elapsed: 2.023046408s
    Aug 17 07:44:16.011: INFO: The phase of Pod pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12 is Running (Ready = true)
    Aug 17 07:44:16.011: INFO: Pod "pod-projected-configmaps-4fc37e4a-cd21-478e-b1d9-0a78bab53d12" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-2b8f1052-e1dd-4f44-9a08-36fedfd8abe2 08/17/23 07:44:16.024
    STEP: waiting to observe update in volume 08/17/23 07:44:16.027
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:44:18.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9082" for this suite. 08/17/23 07:44:18.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:44:18.046
Aug 17 07:44:18.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 07:44:18.047
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:18.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:18.055
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 17 07:44:18.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: creating the pod 08/17/23 07:44:18.105
STEP: submitting the pod to kubernetes 08/17/23 07:44:18.105
Aug 17 07:44:18.117: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7" in namespace "pods-1040" to be "running and ready"
Aug 17 07:44:18.119: INFO: Pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161167ms
Aug 17 07:44:18.119: INFO: The phase of Pod pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:44:20.122: INFO: Pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004848621s
Aug 17 07:44:20.122: INFO: The phase of Pod pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7 is Running (Ready = true)
Aug 17 07:44:20.122: INFO: Pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 07:44:20.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1040" for this suite. 08/17/23 07:44:20.195
------------------------------
â€¢ [2.153 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:44:18.046
    Aug 17 07:44:18.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 07:44:18.047
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:18.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:18.055
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 17 07:44:18.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: creating the pod 08/17/23 07:44:18.105
    STEP: submitting the pod to kubernetes 08/17/23 07:44:18.105
    Aug 17 07:44:18.117: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7" in namespace "pods-1040" to be "running and ready"
    Aug 17 07:44:18.119: INFO: Pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161167ms
    Aug 17 07:44:18.119: INFO: The phase of Pod pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:44:20.122: INFO: Pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004848621s
    Aug 17 07:44:20.122: INFO: The phase of Pod pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7 is Running (Ready = true)
    Aug 17 07:44:20.122: INFO: Pod "pod-exec-websocket-9a1bffab-0919-48cf-a1cb-88b991b220f7" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:44:20.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1040" for this suite. 08/17/23 07:44:20.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:44:20.2
Aug 17 07:44:20.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename job 08/17/23 07:44:20.201
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:20.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:20.222
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/17/23 07:44:20.226
STEP: Ensuring active pods == parallelism 08/17/23 07:44:20.232
STEP: delete a job 08/17/23 07:44:22.235
STEP: deleting Job.batch foo in namespace job-8608, will wait for the garbage collector to delete the pods 08/17/23 07:44:22.235
Aug 17 07:44:22.291: INFO: Deleting Job.batch foo took: 3.603403ms
Aug 17 07:44:22.391: INFO: Terminating Job.batch foo pods took: 100.256609ms
STEP: Ensuring job was deleted 08/17/23 07:44:54.591
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 07:44:54.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8608" for this suite. 08/17/23 07:44:54.596
------------------------------
â€¢ [SLOW TEST] [34.400 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:44:20.2
    Aug 17 07:44:20.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename job 08/17/23 07:44:20.201
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:20.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:20.222
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/17/23 07:44:20.226
    STEP: Ensuring active pods == parallelism 08/17/23 07:44:20.232
    STEP: delete a job 08/17/23 07:44:22.235
    STEP: deleting Job.batch foo in namespace job-8608, will wait for the garbage collector to delete the pods 08/17/23 07:44:22.235
    Aug 17 07:44:22.291: INFO: Deleting Job.batch foo took: 3.603403ms
    Aug 17 07:44:22.391: INFO: Terminating Job.batch foo pods took: 100.256609ms
    STEP: Ensuring job was deleted 08/17/23 07:44:54.591
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:44:54.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8608" for this suite. 08/17/23 07:44:54.596
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:44:54.601
Aug 17 07:44:54.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:44:54.602
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:54.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:54.62
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Aug 17 07:44:54.649: INFO: created pod
Aug 17 07:44:54.649: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8515" to be "Succeeded or Failed"
Aug 17 07:44:54.651: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.56997ms
Aug 17 07:44:56.654: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579613s
Aug 17 07:44:58.655: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005716833s
STEP: Saw pod success 08/17/23 07:44:58.655
Aug 17 07:44:58.655: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 17 07:45:28.656: INFO: polling logs
Aug 17 07:45:28.660: INFO: Pod logs: 
I0817 07:44:55.736931       1 log.go:198] OK: Got token
I0817 07:44:55.736970       1 log.go:198] validating with in-cluster discovery
I0817 07:44:55.737274       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0817 07:44:55.737301       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8515:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692258894, NotBefore:1692258294, IssuedAt:1692258294, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8515", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"33123ac8-6dec-4467-b10b-e70df9215ba2"}}}
I0817 07:44:55.747949       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0817 07:44:55.753866       1 log.go:198] OK: Validated signature on JWT
I0817 07:44:55.753953       1 log.go:198] OK: Got valid claims from token!
I0817 07:44:55.753973       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8515:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692258894, NotBefore:1692258294, IssuedAt:1692258294, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8515", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"33123ac8-6dec-4467-b10b-e70df9215ba2"}}}

Aug 17 07:45:28.660: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:28.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8515" for this suite. 08/17/23 07:45:28.666
------------------------------
â€¢ [SLOW TEST] [34.068 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:44:54.601
    Aug 17 07:44:54.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 07:44:54.602
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:44:54.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:44:54.62
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Aug 17 07:44:54.649: INFO: created pod
    Aug 17 07:44:54.649: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8515" to be "Succeeded or Failed"
    Aug 17 07:44:54.651: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.56997ms
    Aug 17 07:44:56.654: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579613s
    Aug 17 07:44:58.655: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005716833s
    STEP: Saw pod success 08/17/23 07:44:58.655
    Aug 17 07:44:58.655: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 17 07:45:28.656: INFO: polling logs
    Aug 17 07:45:28.660: INFO: Pod logs: 
    I0817 07:44:55.736931       1 log.go:198] OK: Got token
    I0817 07:44:55.736970       1 log.go:198] validating with in-cluster discovery
    I0817 07:44:55.737274       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0817 07:44:55.737301       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8515:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692258894, NotBefore:1692258294, IssuedAt:1692258294, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8515", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"33123ac8-6dec-4467-b10b-e70df9215ba2"}}}
    I0817 07:44:55.747949       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0817 07:44:55.753866       1 log.go:198] OK: Validated signature on JWT
    I0817 07:44:55.753953       1 log.go:198] OK: Got valid claims from token!
    I0817 07:44:55.753973       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8515:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692258894, NotBefore:1692258294, IssuedAt:1692258294, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8515", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"33123ac8-6dec-4467-b10b-e70df9215ba2"}}}

    Aug 17 07:45:28.660: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:28.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8515" for this suite. 08/17/23 07:45:28.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:28.67
Aug 17 07:45:28.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:45:28.67
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:28.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:28.682
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:45:28.713
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:45:29.129
STEP: Deploying the webhook pod 08/17/23 07:45:29.134
STEP: Wait for the deployment to be ready 08/17/23 07:45:29.139
Aug 17 07:45:29.154: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:45:31.16
STEP: Verifying the service has paired with the endpoint 08/17/23 07:45:31.168
Aug 17 07:45:32.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 07:45:32.17
STEP: create a pod 08/17/23 07:45:32.182
Aug 17 07:45:32.186: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4955" to be "running"
Aug 17 07:45:32.189: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778223ms
Aug 17 07:45:34.192: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005338285s
Aug 17 07:45:34.192: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/17/23 07:45:34.192
Aug 17 07:45:34.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=webhook-4955 attach --namespace=webhook-4955 to-be-attached-pod -i -c=container1'
Aug 17 07:45:34.286: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:34.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4955" for this suite. 08/17/23 07:45:34.316
STEP: Destroying namespace "webhook-4955-markers" for this suite. 08/17/23 07:45:34.32
------------------------------
â€¢ [SLOW TEST] [5.655 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:28.67
    Aug 17 07:45:28.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:45:28.67
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:28.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:28.682
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:45:28.713
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:45:29.129
    STEP: Deploying the webhook pod 08/17/23 07:45:29.134
    STEP: Wait for the deployment to be ready 08/17/23 07:45:29.139
    Aug 17 07:45:29.154: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:45:31.16
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:45:31.168
    Aug 17 07:45:32.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 07:45:32.17
    STEP: create a pod 08/17/23 07:45:32.182
    Aug 17 07:45:32.186: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4955" to be "running"
    Aug 17 07:45:32.189: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778223ms
    Aug 17 07:45:34.192: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005338285s
    Aug 17 07:45:34.192: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/17/23 07:45:34.192
    Aug 17 07:45:34.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=webhook-4955 attach --namespace=webhook-4955 to-be-attached-pod -i -c=container1'
    Aug 17 07:45:34.286: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:34.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4955" for this suite. 08/17/23 07:45:34.316
    STEP: Destroying namespace "webhook-4955-markers" for this suite. 08/17/23 07:45:34.32
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:34.325
Aug 17 07:45:34.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename events 08/17/23 07:45:34.326
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:34.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:34.348
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/17/23 07:45:34.351
STEP: listing all events in all namespaces 08/17/23 07:45:34.356
STEP: patching the test event 08/17/23 07:45:34.36
STEP: fetching the test event 08/17/23 07:45:34.373
STEP: updating the test event 08/17/23 07:45:34.375
STEP: getting the test event 08/17/23 07:45:34.382
STEP: deleting the test event 08/17/23 07:45:34.384
STEP: listing all events in all namespaces 08/17/23 07:45:34.388
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:34.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8840" for this suite. 08/17/23 07:45:34.397
------------------------------
â€¢ [0.074 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:34.325
    Aug 17 07:45:34.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename events 08/17/23 07:45:34.326
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:34.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:34.348
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/17/23 07:45:34.351
    STEP: listing all events in all namespaces 08/17/23 07:45:34.356
    STEP: patching the test event 08/17/23 07:45:34.36
    STEP: fetching the test event 08/17/23 07:45:34.373
    STEP: updating the test event 08/17/23 07:45:34.375
    STEP: getting the test event 08/17/23 07:45:34.382
    STEP: deleting the test event 08/17/23 07:45:34.384
    STEP: listing all events in all namespaces 08/17/23 07:45:34.388
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:34.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8840" for this suite. 08/17/23 07:45:34.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:34.401
Aug 17 07:45:34.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 07:45:34.402
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:34.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:34.423
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/17/23 07:45:34.439
STEP: watching for Pod to be ready 08/17/23 07:45:34.444
Aug 17 07:45:34.445: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 17 07:45:34.448: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
Aug 17 07:45:34.456: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
Aug 17 07:45:34.909: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
Aug 17 07:45:35.784: INFO: Found Pod pod-test in namespace pods-462 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/17/23 07:45:35.786
STEP: getting the Pod and ensuring that it's patched 08/17/23 07:45:35.793
STEP: replacing the Pod's status Ready condition to False 08/17/23 07:45:35.795
STEP: check the Pod again to ensure its Ready conditions are False 08/17/23 07:45:35.802
STEP: deleting the Pod via a Collection with a LabelSelector 08/17/23 07:45:35.802
STEP: watching for the Pod to be deleted 08/17/23 07:45:35.807
Aug 17 07:45:35.808: INFO: observed event type MODIFIED
Aug 17 07:45:37.768: INFO: observed event type MODIFIED
Aug 17 07:45:37.972: INFO: observed event type MODIFIED
Aug 17 07:45:38.807: INFO: observed event type MODIFIED
Aug 17 07:45:38.812: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:38.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-462" for this suite. 08/17/23 07:45:38.819
------------------------------
â€¢ [4.420 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:34.401
    Aug 17 07:45:34.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 07:45:34.402
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:34.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:34.423
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/17/23 07:45:34.439
    STEP: watching for Pod to be ready 08/17/23 07:45:34.444
    Aug 17 07:45:34.445: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 17 07:45:34.448: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
    Aug 17 07:45:34.456: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
    Aug 17 07:45:34.909: INFO: observed Pod pod-test in namespace pods-462 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
    Aug 17 07:45:35.784: INFO: Found Pod pod-test in namespace pods-462 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 07:45:34 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/17/23 07:45:35.786
    STEP: getting the Pod and ensuring that it's patched 08/17/23 07:45:35.793
    STEP: replacing the Pod's status Ready condition to False 08/17/23 07:45:35.795
    STEP: check the Pod again to ensure its Ready conditions are False 08/17/23 07:45:35.802
    STEP: deleting the Pod via a Collection with a LabelSelector 08/17/23 07:45:35.802
    STEP: watching for the Pod to be deleted 08/17/23 07:45:35.807
    Aug 17 07:45:35.808: INFO: observed event type MODIFIED
    Aug 17 07:45:37.768: INFO: observed event type MODIFIED
    Aug 17 07:45:37.972: INFO: observed event type MODIFIED
    Aug 17 07:45:38.807: INFO: observed event type MODIFIED
    Aug 17 07:45:38.812: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:38.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-462" for this suite. 08/17/23 07:45:38.819
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:38.822
Aug 17 07:45:38.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:45:38.822
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:38.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:38.84
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 17 07:45:38.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 07:45:41.394
Aug 17 07:45:41.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 create -f -'
Aug 17 07:45:42.608: INFO: stderr: ""
Aug 17 07:45:42.608: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 17 07:45:42.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 delete e2e-test-crd-publish-openapi-1110-crds test-cr'
Aug 17 07:45:42.692: INFO: stderr: ""
Aug 17 07:45:42.692: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 17 07:45:42.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 apply -f -'
Aug 17 07:45:43.010: INFO: stderr: ""
Aug 17 07:45:43.010: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 17 07:45:43.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 delete e2e-test-crd-publish-openapi-1110-crds test-cr'
Aug 17 07:45:43.099: INFO: stderr: ""
Aug 17 07:45:43.099: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/17/23 07:45:43.099
Aug 17 07:45:43.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 explain e2e-test-crd-publish-openapi-1110-crds'
Aug 17 07:45:43.430: INFO: stderr: ""
Aug 17 07:45:43.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1110-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:45.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4245" for this suite. 08/17/23 07:45:45.994
------------------------------
â€¢ [SLOW TEST] [7.176 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:38.822
    Aug 17 07:45:38.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:45:38.822
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:38.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:38.84
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 17 07:45:38.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 07:45:41.394
    Aug 17 07:45:41.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 create -f -'
    Aug 17 07:45:42.608: INFO: stderr: ""
    Aug 17 07:45:42.608: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 17 07:45:42.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 delete e2e-test-crd-publish-openapi-1110-crds test-cr'
    Aug 17 07:45:42.692: INFO: stderr: ""
    Aug 17 07:45:42.692: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 17 07:45:42.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 apply -f -'
    Aug 17 07:45:43.010: INFO: stderr: ""
    Aug 17 07:45:43.010: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 17 07:45:43.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 --namespace=crd-publish-openapi-4245 delete e2e-test-crd-publish-openapi-1110-crds test-cr'
    Aug 17 07:45:43.099: INFO: stderr: ""
    Aug 17 07:45:43.099: INFO: stdout: "e2e-test-crd-publish-openapi-1110-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/17/23 07:45:43.099
    Aug 17 07:45:43.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-4245 explain e2e-test-crd-publish-openapi-1110-crds'
    Aug 17 07:45:43.430: INFO: stderr: ""
    Aug 17 07:45:43.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1110-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:45.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4245" for this suite. 08/17/23 07:45:45.994
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:45.998
Aug 17 07:45:45.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 07:45:45.999
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:46.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:46.026
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 17 07:45:46.028: INFO: Creating deployment "webserver-deployment"
Aug 17 07:45:46.036: INFO: Waiting for observed generation 1
Aug 17 07:45:48.042: INFO: Waiting for all required pods to come up
Aug 17 07:45:48.045: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/17/23 07:45:48.045
Aug 17 07:45:48.045: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9v6qz" in namespace "deployment-6014" to be "running"
Aug 17 07:45:48.047: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.886038ms
Aug 17 07:45:50.050: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004853386s
Aug 17 07:45:50.050: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz" satisfied condition "running"
Aug 17 07:45:50.050: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 17 07:45:50.054: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 17 07:45:50.059: INFO: Updating deployment webserver-deployment
Aug 17 07:45:50.059: INFO: Waiting for observed generation 2
Aug 17 07:45:52.064: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 17 07:45:52.065: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 17 07:45:52.067: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 17 07:45:52.072: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 17 07:45:52.072: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 17 07:45:52.073: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 17 07:45:52.075: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 17 07:45:52.075: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 17 07:45:52.080: INFO: Updating deployment webserver-deployment
Aug 17 07:45:52.080: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 17 07:45:52.082: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 17 07:45:52.085: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 07:45:52.093: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6014  4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39 27056446 3 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d44aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 07:45:47 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-17 07:45:50 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 17 07:45:52.098: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6014  3ee275cd-a663-410d-8bba-e66c32b13495 27056450 3 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39 0xc003d45797 0xc003d45798}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d458a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:45:52.098: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 17 07:45:52.098: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6014  4381df3c-e25a-49ca-92d3-34452533bfc6 27056447 3 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39 0xc003d456a7 0xc003d456a8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d45738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:45:52.104: INFO: Pod "webserver-deployment-7f5969cbc7-2bv7b" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2bv7b webserver-deployment-7f5969cbc7- deployment-6014  da2cd168-b567-4b26-aa8b-b2aa62e99cce 27056268 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:314cfdf1afaa50e2c8b28e8b31c4480f20c62c7adf974546c47444d137b0d8ce cni.projectcalico.org/podIP:172.32.123.175/32 cni.projectcalico.org/podIPs:172.32.123.175/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0035ebc47 0xc0035ebc48}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gv4kh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gv4kh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.175,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d06d19b0057b9c2fa3fdcb0133980299c87adc9cb4ef156d43fff51f9cfaad82,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.104: INFO: Pod "webserver-deployment-7f5969cbc7-859sd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-859sd webserver-deployment-7f5969cbc7- deployment-6014  adbbfbc9-5c0f-42df-ab30-5f89ed9cb710 27056459 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0035ebe47 0xc0035ebe48}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4vp7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4vp7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9v6qz webserver-deployment-7f5969cbc7- deployment-6014  3fbc735e-f0d2-4619-81b8-96d8da9a67ac 27056294 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:676b988860328f897484e14b68f8b668f591cf666dff270a9d7f7aa40b13f0f7 cni.projectcalico.org/podIP:172.32.238.88/32 cni.projectcalico.org/podIPs:172.32.238.88/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0035ebfd7 0xc0035ebfd8}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfgz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfgz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.88,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://843fd6cd04e8e453f70ebb3c6d8168c9f2cc93fcbbdb6adfb236b0ea7fbe5744,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-f885r" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f885r webserver-deployment-7f5969cbc7- deployment-6014  127f7470-793f-4e13-bc22-e1e224ca6ddd 27056261 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d758a7b94e41d466d4dbbf9b18d28558a8e3b906ea746714013a2f1f6e162a01 cni.projectcalico.org/podIP:172.32.3.44/32 cni.projectcalico.org/podIPs:172.32.3.44/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0004db4e7 0xc0004db4e8}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mh69k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mh69k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:172.32.3.44,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3ef54767c61e407b1ba5348207b840b89088e63718fd772630dca12f283e8a5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.3.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-gvwgl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gvwgl webserver-deployment-7f5969cbc7- deployment-6014  963afe58-7c52-47d0-9d56-21d01104d09f 27056464 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104a7b7 0xc00104a7b8}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jghgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jghgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-hj5pl" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hj5pl webserver-deployment-7f5969cbc7- deployment-6014  44ed501a-d379-4bab-aed5-b5af5c25b9c9 27056271 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ad0fe4b83569a73ca49b3cfa888ca7eed67b1413641acf26b4ec002aabb9763d cni.projectcalico.org/podIP:172.32.123.133/32 cni.projectcalico.org/podIPs:172.32.123.133/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104abf7 0xc00104abf8}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d98mj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d98mj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.133,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1ce41962ee5281bf99e271760efd76fdf0d872fac6249918b3230eaf5ccd8eb0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-m67qz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m67qz webserver-deployment-7f5969cbc7- deployment-6014  3afc7faf-0b11-4a28-8271-f8866c1bbf49 27056259 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:78dca36635d8702827ed0ce5ee39550a0836c4a08b23004987f87ba3651ae4ed cni.projectcalico.org/podIP:172.32.3.38/32 cni.projectcalico.org/podIPs:172.32.3.38/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104b357 0xc00104b358}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvtbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvtbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:172.32.3.38,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65a0e362f72532a63cb1c4ad91615c59a1978b0a667b1de53e63ad8108c8294c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.3.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-nghnl" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nghnl webserver-deployment-7f5969cbc7- deployment-6014  b846cbf3-e17e-4798-add7-0508368a0044 27056273 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4f5a306cc22abf9cc3a468da5e35b05e17e459c189cba0564d9ba990994f7b28 cni.projectcalico.org/podIP:172.32.123.168/32 cni.projectcalico.org/podIPs:172.32.123.168/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104b977 0xc00104b978}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggcf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggcf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.168,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bd503a467d6607ed9d89dd6c10ab9d212efea6937522ba8fb0b7c51821148ea0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-sv4nd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sv4nd webserver-deployment-7f5969cbc7- deployment-6014  8a482024-36d8-47a8-9c61-a6bd5acca958 27056453 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00a5760c7 0xc00a5760c8}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p22lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p22lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-wkbcp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wkbcp webserver-deployment-7f5969cbc7- deployment-6014  b35056cb-5682-469f-a456-ef1a4dee3abb 27056264 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f2c46a57bc65ae522640f51e1dfa11bff6b8170eecb96b65be5a072b1a46c634 cni.projectcalico.org/podIP:172.32.3.11/32 cni.projectcalico.org/podIPs:172.32.3.11/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00a576327 0xc00a576328}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tg9b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tg9b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:172.32.3.11,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b5bc76b074c91bcab9727876934e0896fbf987ee01c1d91510854316b66a8431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.3.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-zkn5d" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zkn5d webserver-deployment-7f5969cbc7- deployment-6014  d7895ad1-d6dc-40e2-af33-d393a207b466 27056283 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d375b7536a408f1aafe92098bd4e095d8e3cefe7ec7f94fce8873c348b33e05c cni.projectcalico.org/podIP:172.32.238.118/32 cni.projectcalico.org/podIPs:172.32.238.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00a576527 0xc00a576528}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlf4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlf4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.118,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://891013ba11d73ea5e68fba39fb977688976a5f9b4e55121b51d47eab6979eb1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-d9f79cb5-8bfbs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8bfbs webserver-deployment-d9f79cb5- deployment-6014  2d50c330-0895-495f-a51a-1c9be1493488 27056383 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:94d05ee7a134376e28638faae7dc9db045f26e4cbc2c3bcedbe95881da9389fc cni.projectcalico.org/podIP:172.32.3.45/32 cni.projectcalico.org/podIPs:172.32.3.45/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576747 0xc00a576748}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfjlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfjlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-d9f79cb5-94pbm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-94pbm webserver-deployment-d9f79cb5- deployment-6014  b0df14ba-0668-48d4-a63e-b7be4c4ebc83 27056460 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576947 0xc00a576948}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmgsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmgsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-d9f79cb5-98j4m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-98j4m webserver-deployment-d9f79cb5- deployment-6014  22685391-ed4e-4e55-8170-e47476250ee8 27056374 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:40160d227f0ff4eb2fdc1d8f1527f1bb3b474f891dcf17f3b39d2412e2fc3e5e cni.projectcalico.org/podIP:172.32.238.83/32 cni.projectcalico.org/podIPs:172.32.238.83/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576a8f 0xc00a576ac0}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtw4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtw4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-fwq7j" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fwq7j webserver-deployment-d9f79cb5- deployment-6014  7adde66b-04a5-405e-b329-30182bee4615 27056373 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7f877d5cc84f6cef84099bde73c9331edf5f4c127cfe678561dd8b29d81e9bd5 cni.projectcalico.org/podIP:172.32.123.129/32 cni.projectcalico.org/podIPs:172.32.123.129/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576cd7 0xc00a576cd8}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhpw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhpw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-spqqp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-spqqp webserver-deployment-d9f79cb5- deployment-6014  24c6b47e-64d9-4573-b847-432ea95578e5 27056461 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576ee7 0xc00a576ee8}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxjrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxjrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-sztrs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sztrs webserver-deployment-d9f79cb5- deployment-6014  08332e39-c31d-4914-b638-6ac8698583cc 27056382 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b88b2d218cdcacb773762a40701a2b4ba2f67a915e50166148fe97d3c67b8692 cni.projectcalico.org/podIP:172.32.123.191/32 cni.projectcalico.org/podIPs:172.32.123.191/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a57702f 0xc00a577040}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rv47k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rv47k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-xhx6v" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xhx6v webserver-deployment-d9f79cb5- deployment-6014  2bdd4f0d-9ade-4f0f-8fa7-182fd91f1030 27056381 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:bf68051a93f72bc53a7afc1da048d59d7ab03392182b365b55e2f99f27c73a79 cni.projectcalico.org/podIP:172.32.238.110/32 cni.projectcalico.org/podIPs:172.32.238.110/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a577257 0xc00a577258}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g246z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g246z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-z5f87" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z5f87 webserver-deployment-d9f79cb5- deployment-6014  148bb0f4-6ca7-499f-8669-a08749f3ac35 27056458 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a577497 0xc00a577498}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xp56d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xp56d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:52.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6014" for this suite. 08/17/23 07:45:52.113
------------------------------
â€¢ [SLOW TEST] [6.127 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:45.998
    Aug 17 07:45:45.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 07:45:45.999
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:46.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:46.026
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 17 07:45:46.028: INFO: Creating deployment "webserver-deployment"
    Aug 17 07:45:46.036: INFO: Waiting for observed generation 1
    Aug 17 07:45:48.042: INFO: Waiting for all required pods to come up
    Aug 17 07:45:48.045: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/17/23 07:45:48.045
    Aug 17 07:45:48.045: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9v6qz" in namespace "deployment-6014" to be "running"
    Aug 17 07:45:48.047: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.886038ms
    Aug 17 07:45:50.050: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004853386s
    Aug 17 07:45:50.050: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz" satisfied condition "running"
    Aug 17 07:45:50.050: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 17 07:45:50.054: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 17 07:45:50.059: INFO: Updating deployment webserver-deployment
    Aug 17 07:45:50.059: INFO: Waiting for observed generation 2
    Aug 17 07:45:52.064: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 17 07:45:52.065: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 17 07:45:52.067: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 17 07:45:52.072: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 17 07:45:52.072: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 17 07:45:52.073: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 17 07:45:52.075: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 17 07:45:52.075: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 17 07:45:52.080: INFO: Updating deployment webserver-deployment
    Aug 17 07:45:52.080: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 17 07:45:52.082: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 17 07:45:52.085: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 07:45:52.093: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6014  4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39 27056446 3 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d44aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 07:45:47 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-17 07:45:50 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 17 07:45:52.098: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6014  3ee275cd-a663-410d-8bba-e66c32b13495 27056450 3 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39 0xc003d45797 0xc003d45798}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d458a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:45:52.098: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 17 07:45:52.098: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6014  4381df3c-e25a-49ca-92d3-34452533bfc6 27056447 3 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39 0xc003d456a7 0xc003d456a8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ce7d85f-a78e-4d3e-b286-fc7e6e97ea39\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d45738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:45:52.104: INFO: Pod "webserver-deployment-7f5969cbc7-2bv7b" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2bv7b webserver-deployment-7f5969cbc7- deployment-6014  da2cd168-b567-4b26-aa8b-b2aa62e99cce 27056268 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:314cfdf1afaa50e2c8b28e8b31c4480f20c62c7adf974546c47444d137b0d8ce cni.projectcalico.org/podIP:172.32.123.175/32 cni.projectcalico.org/podIPs:172.32.123.175/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0035ebc47 0xc0035ebc48}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gv4kh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gv4kh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.175,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d06d19b0057b9c2fa3fdcb0133980299c87adc9cb4ef156d43fff51f9cfaad82,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.104: INFO: Pod "webserver-deployment-7f5969cbc7-859sd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-859sd webserver-deployment-7f5969cbc7- deployment-6014  adbbfbc9-5c0f-42df-ab30-5f89ed9cb710 27056459 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0035ebe47 0xc0035ebe48}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4vp7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4vp7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-9v6qz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9v6qz webserver-deployment-7f5969cbc7- deployment-6014  3fbc735e-f0d2-4619-81b8-96d8da9a67ac 27056294 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:676b988860328f897484e14b68f8b668f591cf666dff270a9d7f7aa40b13f0f7 cni.projectcalico.org/podIP:172.32.238.88/32 cni.projectcalico.org/podIPs:172.32.238.88/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0035ebfd7 0xc0035ebfd8}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfgz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfgz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.88,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://843fd6cd04e8e453f70ebb3c6d8168c9f2cc93fcbbdb6adfb236b0ea7fbe5744,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-f885r" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f885r webserver-deployment-7f5969cbc7- deployment-6014  127f7470-793f-4e13-bc22-e1e224ca6ddd 27056261 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d758a7b94e41d466d4dbbf9b18d28558a8e3b906ea746714013a2f1f6e162a01 cni.projectcalico.org/podIP:172.32.3.44/32 cni.projectcalico.org/podIPs:172.32.3.44/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc0004db4e7 0xc0004db4e8}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mh69k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mh69k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:172.32.3.44,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3ef54767c61e407b1ba5348207b840b89088e63718fd772630dca12f283e8a5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.3.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-gvwgl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gvwgl webserver-deployment-7f5969cbc7- deployment-6014  963afe58-7c52-47d0-9d56-21d01104d09f 27056464 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104a7b7 0xc00104a7b8}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jghgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jghgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-hj5pl" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hj5pl webserver-deployment-7f5969cbc7- deployment-6014  44ed501a-d379-4bab-aed5-b5af5c25b9c9 27056271 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ad0fe4b83569a73ca49b3cfa888ca7eed67b1413641acf26b4ec002aabb9763d cni.projectcalico.org/podIP:172.32.123.133/32 cni.projectcalico.org/podIPs:172.32.123.133/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104abf7 0xc00104abf8}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d98mj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d98mj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.133,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1ce41962ee5281bf99e271760efd76fdf0d872fac6249918b3230eaf5ccd8eb0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.105: INFO: Pod "webserver-deployment-7f5969cbc7-m67qz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m67qz webserver-deployment-7f5969cbc7- deployment-6014  3afc7faf-0b11-4a28-8271-f8866c1bbf49 27056259 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:78dca36635d8702827ed0ce5ee39550a0836c4a08b23004987f87ba3651ae4ed cni.projectcalico.org/podIP:172.32.3.38/32 cni.projectcalico.org/podIPs:172.32.3.38/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104b357 0xc00104b358}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvtbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvtbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:172.32.3.38,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65a0e362f72532a63cb1c4ad91615c59a1978b0a667b1de53e63ad8108c8294c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.3.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-nghnl" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nghnl webserver-deployment-7f5969cbc7- deployment-6014  b846cbf3-e17e-4798-add7-0508368a0044 27056273 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4f5a306cc22abf9cc3a468da5e35b05e17e459c189cba0564d9ba990994f7b28 cni.projectcalico.org/podIP:172.32.123.168/32 cni.projectcalico.org/podIPs:172.32.123.168/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00104b977 0xc00104b978}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.123.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggcf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggcf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:172.32.123.168,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bd503a467d6607ed9d89dd6c10ab9d212efea6937522ba8fb0b7c51821148ea0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.123.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-sv4nd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sv4nd webserver-deployment-7f5969cbc7- deployment-6014  8a482024-36d8-47a8-9c61-a6bd5acca958 27056453 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00a5760c7 0xc00a5760c8}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p22lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p22lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-wkbcp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wkbcp webserver-deployment-7f5969cbc7- deployment-6014  b35056cb-5682-469f-a456-ef1a4dee3abb 27056264 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f2c46a57bc65ae522640f51e1dfa11bff6b8170eecb96b65be5a072b1a46c634 cni.projectcalico.org/podIP:172.32.3.11/32 cni.projectcalico.org/podIPs:172.32.3.11/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00a576327 0xc00a576328}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.3.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tg9b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tg9b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:172.32.3.11,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b5bc76b074c91bcab9727876934e0896fbf987ee01c1d91510854316b66a8431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.3.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-7f5969cbc7-zkn5d" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zkn5d webserver-deployment-7f5969cbc7- deployment-6014  d7895ad1-d6dc-40e2-af33-d393a207b466 27056283 0 2023-08-17 07:45:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d375b7536a408f1aafe92098bd4e095d8e3cefe7ec7f94fce8873c348b33e05c cni.projectcalico.org/podIP:172.32.238.118/32 cni.projectcalico.org/podIPs:172.32.238.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 4381df3c-e25a-49ca-92d3-34452533bfc6 0xc00a576527 0xc00a576528}] [] [{calico Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4381df3c-e25a-49ca-92d3-34452533bfc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlf4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlf4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.118,StartTime:2023-08-17 07:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:45:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://891013ba11d73ea5e68fba39fb977688976a5f9b4e55121b51d47eab6979eb1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-d9f79cb5-8bfbs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8bfbs webserver-deployment-d9f79cb5- deployment-6014  2d50c330-0895-495f-a51a-1c9be1493488 27056383 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:94d05ee7a134376e28638faae7dc9db045f26e4cbc2c3bcedbe95881da9389fc cni.projectcalico.org/podIP:172.32.3.45/32 cni.projectcalico.org/podIPs:172.32.3.45/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576747 0xc00a576748}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfjlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfjlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.175,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-d9f79cb5-94pbm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-94pbm webserver-deployment-d9f79cb5- deployment-6014  b0df14ba-0668-48d4-a63e-b7be4c4ebc83 27056460 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576947 0xc00a576948}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmgsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmgsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.106: INFO: Pod "webserver-deployment-d9f79cb5-98j4m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-98j4m webserver-deployment-d9f79cb5- deployment-6014  22685391-ed4e-4e55-8170-e47476250ee8 27056374 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:40160d227f0ff4eb2fdc1d8f1527f1bb3b474f891dcf17f3b39d2412e2fc3e5e cni.projectcalico.org/podIP:172.32.238.83/32 cni.projectcalico.org/podIPs:172.32.238.83/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576a8f 0xc00a576ac0}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtw4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtw4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-fwq7j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fwq7j webserver-deployment-d9f79cb5- deployment-6014  7adde66b-04a5-405e-b329-30182bee4615 27056373 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7f877d5cc84f6cef84099bde73c9331edf5f4c127cfe678561dd8b29d81e9bd5 cni.projectcalico.org/podIP:172.32.123.129/32 cni.projectcalico.org/podIPs:172.32.123.129/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576cd7 0xc00a576cd8}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhpw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhpw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-spqqp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-spqqp webserver-deployment-d9f79cb5- deployment-6014  24c6b47e-64d9-4573-b847-432ea95578e5 27056461 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a576ee7 0xc00a576ee8}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxjrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxjrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-sztrs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sztrs webserver-deployment-d9f79cb5- deployment-6014  08332e39-c31d-4914-b638-6ac8698583cc 27056382 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b88b2d218cdcacb773762a40701a2b4ba2f67a915e50166148fe97d3c67b8692 cni.projectcalico.org/podIP:172.32.123.191/32 cni.projectcalico.org/podIPs:172.32.123.191/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a57702f 0xc00a577040}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rv47k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rv47k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.176,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-xhx6v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xhx6v webserver-deployment-d9f79cb5- deployment-6014  2bdd4f0d-9ade-4f0f-8fa7-182fd91f1030 27056381 0 2023-08-17 07:45:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:bf68051a93f72bc53a7afc1da048d59d7ab03392182b365b55e2f99f27c73a79 cni.projectcalico.org/podIP:172.32.238.110/32 cni.projectcalico.org/podIPs:172.32.238.110/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a577257 0xc00a577258}] [] [{calico Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:45:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g246z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g246z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:,StartTime:2023-08-17 07:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 07:45:52.107: INFO: Pod "webserver-deployment-d9f79cb5-z5f87" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z5f87 webserver-deployment-d9f79cb5- deployment-6014  148bb0f4-6ca7-499f-8669-a08749f3ac35 27056458 0 2023-08-17 07:45:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3ee275cd-a663-410d-8bba-e66c32b13495 0xc00a577497 0xc00a577498}] [] [{kube-controller-manager Update v1 2023-08-17 07:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ee275cd-a663-410d-8bba-e66c32b13495\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xp56d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xp56d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:45:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:52.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6014" for this suite. 08/17/23 07:45:52.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:52.127
Aug 17 07:45:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename security-context 08/17/23 07:45:52.128
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:52.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:52.147
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 07:45:52.15
Aug 17 07:45:52.174: INFO: Waiting up to 5m0s for pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b" in namespace "security-context-4179" to be "Succeeded or Failed"
Aug 17 07:45:52.180: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.687906ms
Aug 17 07:45:54.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00824199s
Aug 17 07:45:56.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008929534s
Aug 17 07:45:58.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008479402s
STEP: Saw pod success 08/17/23 07:45:58.183
Aug 17 07:45:58.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b" satisfied condition "Succeeded or Failed"
Aug 17 07:45:58.185: INFO: Trying to get logs from node yst-node2 pod security-context-85df7a37-1b24-427f-ac56-9e0975ae521b container test-container: <nil>
STEP: delete the pod 08/17/23 07:45:58.188
Aug 17 07:45:58.193: INFO: Waiting for pod security-context-85df7a37-1b24-427f-ac56-9e0975ae521b to disappear
Aug 17 07:45:58.195: INFO: Pod security-context-85df7a37-1b24-427f-ac56-9e0975ae521b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:58.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4179" for this suite. 08/17/23 07:45:58.198
------------------------------
â€¢ [SLOW TEST] [6.074 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:52.127
    Aug 17 07:45:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename security-context 08/17/23 07:45:52.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:52.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:52.147
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 07:45:52.15
    Aug 17 07:45:52.174: INFO: Waiting up to 5m0s for pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b" in namespace "security-context-4179" to be "Succeeded or Failed"
    Aug 17 07:45:52.180: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.687906ms
    Aug 17 07:45:54.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00824199s
    Aug 17 07:45:56.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008929534s
    Aug 17 07:45:58.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008479402s
    STEP: Saw pod success 08/17/23 07:45:58.183
    Aug 17 07:45:58.183: INFO: Pod "security-context-85df7a37-1b24-427f-ac56-9e0975ae521b" satisfied condition "Succeeded or Failed"
    Aug 17 07:45:58.185: INFO: Trying to get logs from node yst-node2 pod security-context-85df7a37-1b24-427f-ac56-9e0975ae521b container test-container: <nil>
    STEP: delete the pod 08/17/23 07:45:58.188
    Aug 17 07:45:58.193: INFO: Waiting for pod security-context-85df7a37-1b24-427f-ac56-9e0975ae521b to disappear
    Aug 17 07:45:58.195: INFO: Pod security-context-85df7a37-1b24-427f-ac56-9e0975ae521b no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:58.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4179" for this suite. 08/17/23 07:45:58.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:58.202
Aug 17 07:45:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:45:58.202
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:58.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:58.224
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:45:58.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2063" for this suite. 08/17/23 07:45:58.313
------------------------------
â€¢ [0.115 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:58.202
    Aug 17 07:45:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:45:58.202
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:58.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:58.224
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:45:58.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2063" for this suite. 08/17/23 07:45:58.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:45:58.316
Aug 17 07:45:58.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:45:58.317
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:58.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:58.328
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:45:58.33
Aug 17 07:45:58.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca" in namespace "projected-7011" to be "Succeeded or Failed"
Aug 17 07:45:58.347: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875619ms
Aug 17 07:46:00.351: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008261839s
Aug 17 07:46:02.350: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008139851s
STEP: Saw pod success 08/17/23 07:46:02.35
Aug 17 07:46:02.351: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca" satisfied condition "Succeeded or Failed"
Aug 17 07:46:02.352: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca container client-container: <nil>
STEP: delete the pod 08/17/23 07:46:02.356
Aug 17 07:46:02.363: INFO: Waiting for pod downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca to disappear
Aug 17 07:46:02.365: INFO: Pod downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7011" for this suite. 08/17/23 07:46:02.367
------------------------------
â€¢ [4.054 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:45:58.316
    Aug 17 07:45:58.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:45:58.317
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:45:58.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:45:58.328
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:45:58.33
    Aug 17 07:45:58.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca" in namespace "projected-7011" to be "Succeeded or Failed"
    Aug 17 07:45:58.347: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875619ms
    Aug 17 07:46:00.351: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008261839s
    Aug 17 07:46:02.350: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008139851s
    STEP: Saw pod success 08/17/23 07:46:02.35
    Aug 17 07:46:02.351: INFO: Pod "downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca" satisfied condition "Succeeded or Failed"
    Aug 17 07:46:02.352: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca container client-container: <nil>
    STEP: delete the pod 08/17/23 07:46:02.356
    Aug 17 07:46:02.363: INFO: Waiting for pod downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca to disappear
    Aug 17 07:46:02.365: INFO: Pod downwardapi-volume-e3b4a114-2ec4-454a-ae94-3c7218221bca no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7011" for this suite. 08/17/23 07:46:02.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:02.371
Aug 17 07:46:02.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:46:02.372
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:02.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:02.398
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-005679d0-95b2-4a2e-a2ab-d20e784ff5f1 08/17/23 07:46:02.402
STEP: Creating secret with name secret-projected-all-test-volume-174a2a25-7ac6-409e-b98e-cc47f91c6e80 08/17/23 07:46:02.416
STEP: Creating a pod to test Check all projections for projected volume plugin 08/17/23 07:46:02.421
Aug 17 07:46:02.426: INFO: Waiting up to 5m0s for pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f" in namespace "projected-6966" to be "Succeeded or Failed"
Aug 17 07:46:02.427: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.630599ms
Aug 17 07:46:04.430: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003971105s
Aug 17 07:46:06.430: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00465094s
STEP: Saw pod success 08/17/23 07:46:06.43
Aug 17 07:46:06.430: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f" satisfied condition "Succeeded or Failed"
Aug 17 07:46:06.432: INFO: Trying to get logs from node yst-node2 pod projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f container projected-all-volume-test: <nil>
STEP: delete the pod 08/17/23 07:46:06.435
Aug 17 07:46:06.441: INFO: Waiting for pod projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f to disappear
Aug 17 07:46:06.442: INFO: Pod projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:06.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6966" for this suite. 08/17/23 07:46:06.445
------------------------------
â€¢ [4.076 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:02.371
    Aug 17 07:46:02.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:46:02.372
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:02.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:02.398
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-005679d0-95b2-4a2e-a2ab-d20e784ff5f1 08/17/23 07:46:02.402
    STEP: Creating secret with name secret-projected-all-test-volume-174a2a25-7ac6-409e-b98e-cc47f91c6e80 08/17/23 07:46:02.416
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/17/23 07:46:02.421
    Aug 17 07:46:02.426: INFO: Waiting up to 5m0s for pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f" in namespace "projected-6966" to be "Succeeded or Failed"
    Aug 17 07:46:02.427: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.630599ms
    Aug 17 07:46:04.430: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003971105s
    Aug 17 07:46:06.430: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00465094s
    STEP: Saw pod success 08/17/23 07:46:06.43
    Aug 17 07:46:06.430: INFO: Pod "projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f" satisfied condition "Succeeded or Failed"
    Aug 17 07:46:06.432: INFO: Trying to get logs from node yst-node2 pod projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f container projected-all-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:46:06.435
    Aug 17 07:46:06.441: INFO: Waiting for pod projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f to disappear
    Aug 17 07:46:06.442: INFO: Pod projected-volume-12379cb1-1ea6-454a-a357-eb3ebdc8994f no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:06.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6966" for this suite. 08/17/23 07:46:06.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:06.448
Aug 17 07:46:06.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:46:06.449
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:06.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:06.462
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-be41b425-19fe-4c09-8801-2ec9e433a094 08/17/23 07:46:06.474
STEP: Creating secret with name s-test-opt-upd-6ecc1a02-e500-429b-a41e-0e21b577ed56 08/17/23 07:46:06.478
STEP: Creating the pod 08/17/23 07:46:06.481
Aug 17 07:46:06.487: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02" in namespace "projected-1254" to be "running and ready"
Aug 17 07:46:06.489: INFO: Pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971935ms
Aug 17 07:46:06.489: INFO: The phase of Pod pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:46:08.493: INFO: Pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02": Phase="Running", Reason="", readiness=true. Elapsed: 2.005396126s
Aug 17 07:46:08.493: INFO: The phase of Pod pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02 is Running (Ready = true)
Aug 17 07:46:08.493: INFO: Pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-be41b425-19fe-4c09-8801-2ec9e433a094 08/17/23 07:46:08.504
STEP: Updating secret s-test-opt-upd-6ecc1a02-e500-429b-a41e-0e21b577ed56 08/17/23 07:46:08.507
STEP: Creating secret with name s-test-opt-create-5a2e1cec-97a1-4fc1-9bff-5853214eac1c 08/17/23 07:46:08.509
STEP: waiting to observe update in volume 08/17/23 07:46:08.513
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:12.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1254" for this suite. 08/17/23 07:46:12.533
------------------------------
â€¢ [SLOW TEST] [6.089 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:06.448
    Aug 17 07:46:06.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:46:06.449
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:06.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:06.462
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-be41b425-19fe-4c09-8801-2ec9e433a094 08/17/23 07:46:06.474
    STEP: Creating secret with name s-test-opt-upd-6ecc1a02-e500-429b-a41e-0e21b577ed56 08/17/23 07:46:06.478
    STEP: Creating the pod 08/17/23 07:46:06.481
    Aug 17 07:46:06.487: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02" in namespace "projected-1254" to be "running and ready"
    Aug 17 07:46:06.489: INFO: Pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971935ms
    Aug 17 07:46:06.489: INFO: The phase of Pod pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:46:08.493: INFO: Pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02": Phase="Running", Reason="", readiness=true. Elapsed: 2.005396126s
    Aug 17 07:46:08.493: INFO: The phase of Pod pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02 is Running (Ready = true)
    Aug 17 07:46:08.493: INFO: Pod "pod-projected-secrets-3ffbe86f-e43f-4ce3-ab23-87cead1a9b02" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-be41b425-19fe-4c09-8801-2ec9e433a094 08/17/23 07:46:08.504
    STEP: Updating secret s-test-opt-upd-6ecc1a02-e500-429b-a41e-0e21b577ed56 08/17/23 07:46:08.507
    STEP: Creating secret with name s-test-opt-create-5a2e1cec-97a1-4fc1-9bff-5853214eac1c 08/17/23 07:46:08.509
    STEP: waiting to observe update in volume 08/17/23 07:46:08.513
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:12.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1254" for this suite. 08/17/23 07:46:12.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:12.538
Aug 17 07:46:12.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:46:12.538
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:12.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:12.549
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/17/23 07:46:12.572
Aug 17 07:46:12.572: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c" in namespace "kubelet-test-52" to be "completed"
Aug 17 07:46:12.579: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.174706ms
Aug 17 07:46:14.582: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009638132s
Aug 17 07:46:16.582: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009335751s
Aug 17 07:46:16.582: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:16.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-52" for this suite. 08/17/23 07:46:16.589
------------------------------
â€¢ [4.055 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:12.538
    Aug 17 07:46:12.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:46:12.538
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:12.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:12.549
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/17/23 07:46:12.572
    Aug 17 07:46:12.572: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c" in namespace "kubelet-test-52" to be "completed"
    Aug 17 07:46:12.579: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.174706ms
    Aug 17 07:46:14.582: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009638132s
    Aug 17 07:46:16.582: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009335751s
    Aug 17 07:46:16.582: INFO: Pod "agnhost-host-aliasese9febf4c-fddb-4e56-9d4e-403943e8c22c" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:16.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-52" for this suite. 08/17/23 07:46:16.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:16.593
Aug 17 07:46:16.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename watch 08/17/23 07:46:16.593
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:16.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:16.606
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/17/23 07:46:16.614
STEP: starting a background goroutine to produce watch events 08/17/23 07:46:16.622
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/17/23 07:46:16.622
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-507" for this suite. 08/17/23 07:46:19.447
------------------------------
â€¢ [2.904 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:16.593
    Aug 17 07:46:16.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename watch 08/17/23 07:46:16.593
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:16.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:16.606
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/17/23 07:46:16.614
    STEP: starting a background goroutine to produce watch events 08/17/23 07:46:16.622
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/17/23 07:46:16.622
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-507" for this suite. 08/17/23 07:46:19.447
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:19.497
Aug 17 07:46:19.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:46:19.498
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:19.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:19.51
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/17/23 07:46:19.512
STEP: Getting a ResourceQuota 08/17/23 07:46:19.518
STEP: Listing all ResourceQuotas with LabelSelector 08/17/23 07:46:19.536
STEP: Patching the ResourceQuota 08/17/23 07:46:19.538
STEP: Deleting a Collection of ResourceQuotas 08/17/23 07:46:19.543
STEP: Verifying the deleted ResourceQuota 08/17/23 07:46:19.546
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:19.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6370" for this suite. 08/17/23 07:46:19.55
------------------------------
â€¢ [0.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:19.497
    Aug 17 07:46:19.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:46:19.498
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:19.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:19.51
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/17/23 07:46:19.512
    STEP: Getting a ResourceQuota 08/17/23 07:46:19.518
    STEP: Listing all ResourceQuotas with LabelSelector 08/17/23 07:46:19.536
    STEP: Patching the ResourceQuota 08/17/23 07:46:19.538
    STEP: Deleting a Collection of ResourceQuotas 08/17/23 07:46:19.543
    STEP: Verifying the deleted ResourceQuota 08/17/23 07:46:19.546
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:19.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6370" for this suite. 08/17/23 07:46:19.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:19.554
Aug 17 07:46:19.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename security-context 08/17/23 07:46:19.555
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:19.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:19.564
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 07:46:19.57
Aug 17 07:46:19.584: INFO: Waiting up to 5m0s for pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32" in namespace "security-context-4660" to be "Succeeded or Failed"
Aug 17 07:46:19.588: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426523ms
Aug 17 07:46:21.592: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007871346s
Aug 17 07:46:23.591: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007373055s
STEP: Saw pod success 08/17/23 07:46:23.591
Aug 17 07:46:23.591: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32" satisfied condition "Succeeded or Failed"
Aug 17 07:46:23.593: INFO: Trying to get logs from node yst-node2 pod security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32 container test-container: <nil>
STEP: delete the pod 08/17/23 07:46:23.597
Aug 17 07:46:23.602: INFO: Waiting for pod security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32 to disappear
Aug 17 07:46:23.604: INFO: Pod security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:23.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4660" for this suite. 08/17/23 07:46:23.607
------------------------------
â€¢ [4.058 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:19.554
    Aug 17 07:46:19.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename security-context 08/17/23 07:46:19.555
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:19.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:19.564
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 07:46:19.57
    Aug 17 07:46:19.584: INFO: Waiting up to 5m0s for pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32" in namespace "security-context-4660" to be "Succeeded or Failed"
    Aug 17 07:46:19.588: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426523ms
    Aug 17 07:46:21.592: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007871346s
    Aug 17 07:46:23.591: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007373055s
    STEP: Saw pod success 08/17/23 07:46:23.591
    Aug 17 07:46:23.591: INFO: Pod "security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32" satisfied condition "Succeeded or Failed"
    Aug 17 07:46:23.593: INFO: Trying to get logs from node yst-node2 pod security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32 container test-container: <nil>
    STEP: delete the pod 08/17/23 07:46:23.597
    Aug 17 07:46:23.602: INFO: Waiting for pod security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32 to disappear
    Aug 17 07:46:23.604: INFO: Pod security-context-35a5f5de-75de-4be0-8bda-0bd0f8c3ea32 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:23.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4660" for this suite. 08/17/23 07:46:23.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:23.613
Aug 17 07:46:23.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:46:23.614
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:23.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:23.625
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/17/23 07:46:23.627
Aug 17 07:46:23.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: mark a version not serverd 08/17/23 07:46:29.432
STEP: check the unserved version gets removed 08/17/23 07:46:29.443
STEP: check the other version is not changed 08/17/23 07:46:31.941
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:36.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2873" for this suite. 08/17/23 07:46:36.736
------------------------------
â€¢ [SLOW TEST] [13.127 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:23.613
    Aug 17 07:46:23.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:46:23.614
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:23.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:23.625
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/17/23 07:46:23.627
    Aug 17 07:46:23.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: mark a version not serverd 08/17/23 07:46:29.432
    STEP: check the unserved version gets removed 08/17/23 07:46:29.443
    STEP: check the other version is not changed 08/17/23 07:46:31.941
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:36.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2873" for this suite. 08/17/23 07:46:36.736
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:36.74
Aug 17 07:46:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:46:36.741
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:36.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:36.754
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-656e172c-755c-48da-950f-b9599f2508fd 08/17/23 07:46:36.757
STEP: Creating a pod to test consume secrets 08/17/23 07:46:36.765
Aug 17 07:46:36.783: INFO: Waiting up to 5m0s for pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5" in namespace "secrets-5451" to be "Succeeded or Failed"
Aug 17 07:46:36.786: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.210889ms
Aug 17 07:46:38.789: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006360241s
Aug 17 07:46:40.789: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005989614s
STEP: Saw pod success 08/17/23 07:46:40.789
Aug 17 07:46:40.789: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5" satisfied condition "Succeeded or Failed"
Aug 17 07:46:40.791: INFO: Trying to get logs from node yst-node2 pod pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 07:46:40.794
Aug 17 07:46:40.800: INFO: Waiting for pod pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5 to disappear
Aug 17 07:46:40.801: INFO: Pod pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:40.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5451" for this suite. 08/17/23 07:46:40.804
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:36.74
    Aug 17 07:46:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:46:36.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:36.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:36.754
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-656e172c-755c-48da-950f-b9599f2508fd 08/17/23 07:46:36.757
    STEP: Creating a pod to test consume secrets 08/17/23 07:46:36.765
    Aug 17 07:46:36.783: INFO: Waiting up to 5m0s for pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5" in namespace "secrets-5451" to be "Succeeded or Failed"
    Aug 17 07:46:36.786: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.210889ms
    Aug 17 07:46:38.789: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006360241s
    Aug 17 07:46:40.789: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005989614s
    STEP: Saw pod success 08/17/23 07:46:40.789
    Aug 17 07:46:40.789: INFO: Pod "pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5" satisfied condition "Succeeded or Failed"
    Aug 17 07:46:40.791: INFO: Trying to get logs from node yst-node2 pod pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:46:40.794
    Aug 17 07:46:40.800: INFO: Waiting for pod pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5 to disappear
    Aug 17 07:46:40.801: INFO: Pod pod-secrets-20efdb25-c4d2-4b67-9b6c-2d88d4f746c5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:40.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5451" for this suite. 08/17/23 07:46:40.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:40.807
Aug 17 07:46:40.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:46:40.808
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:40.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:40.821
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-7279/configmap-test-366450ec-d60f-49cd-9721-1de32ab6a7a0 08/17/23 07:46:40.824
STEP: Creating a pod to test consume configMaps 08/17/23 07:46:40.838
Aug 17 07:46:40.852: INFO: Waiting up to 5m0s for pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309" in namespace "configmap-7279" to be "Succeeded or Failed"
Aug 17 07:46:40.854: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056324ms
Aug 17 07:46:42.856: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004523681s
Aug 17 07:46:44.857: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00473325s
STEP: Saw pod success 08/17/23 07:46:44.857
Aug 17 07:46:44.857: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309" satisfied condition "Succeeded or Failed"
Aug 17 07:46:44.858: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309 container env-test: <nil>
STEP: delete the pod 08/17/23 07:46:44.862
Aug 17 07:46:44.867: INFO: Waiting for pod pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309 to disappear
Aug 17 07:46:44.868: INFO: Pod pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:44.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7279" for this suite. 08/17/23 07:46:44.871
------------------------------
â€¢ [4.066 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:40.807
    Aug 17 07:46:40.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:46:40.808
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:40.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:40.821
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-7279/configmap-test-366450ec-d60f-49cd-9721-1de32ab6a7a0 08/17/23 07:46:40.824
    STEP: Creating a pod to test consume configMaps 08/17/23 07:46:40.838
    Aug 17 07:46:40.852: INFO: Waiting up to 5m0s for pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309" in namespace "configmap-7279" to be "Succeeded or Failed"
    Aug 17 07:46:40.854: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056324ms
    Aug 17 07:46:42.856: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004523681s
    Aug 17 07:46:44.857: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00473325s
    STEP: Saw pod success 08/17/23 07:46:44.857
    Aug 17 07:46:44.857: INFO: Pod "pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309" satisfied condition "Succeeded or Failed"
    Aug 17 07:46:44.858: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309 container env-test: <nil>
    STEP: delete the pod 08/17/23 07:46:44.862
    Aug 17 07:46:44.867: INFO: Waiting for pod pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309 to disappear
    Aug 17 07:46:44.868: INFO: Pod pod-configmaps-d42cf082-d11e-412a-b9bd-b9db976bf309 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:44.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7279" for this suite. 08/17/23 07:46:44.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:44.874
Aug 17 07:46:44.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename events 08/17/23 07:46:44.874
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:44.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:44.886
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/17/23 07:46:44.888
Aug 17 07:46:44.893: INFO: created test-event-1
Aug 17 07:46:44.899: INFO: created test-event-2
Aug 17 07:46:44.902: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/17/23 07:46:44.902
STEP: delete collection of events 08/17/23 07:46:44.904
Aug 17 07:46:44.904: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/17/23 07:46:44.916
Aug 17 07:46:44.917: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 17 07:46:44.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2815" for this suite. 08/17/23 07:46:44.923
------------------------------
â€¢ [0.053 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:44.874
    Aug 17 07:46:44.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename events 08/17/23 07:46:44.874
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:44.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:44.886
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/17/23 07:46:44.888
    Aug 17 07:46:44.893: INFO: created test-event-1
    Aug 17 07:46:44.899: INFO: created test-event-2
    Aug 17 07:46:44.902: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/17/23 07:46:44.902
    STEP: delete collection of events 08/17/23 07:46:44.904
    Aug 17 07:46:44.904: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/17/23 07:46:44.916
    Aug 17 07:46:44.917: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:46:44.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2815" for this suite. 08/17/23 07:46:44.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:46:44.926
Aug 17 07:46:44.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-preemption 08/17/23 07:46:44.927
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:44.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:44.935
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 07:46:44.952: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 07:47:45.000: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/17/23 07:47:45.002
Aug 17 07:47:45.014: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 17 07:47:45.017: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 17 07:47:45.028: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 17 07:47:45.031: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 17 07:47:45.042: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 17 07:47:45.046: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/17/23 07:47:45.046
Aug 17 07:47:45.046: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8100" to be "running"
Aug 17 07:47:45.052: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041069ms
Aug 17 07:47:47.055: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.00903176s
Aug 17 07:47:47.055: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 17 07:47:47.055: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
Aug 17 07:47:47.057: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.884487ms
Aug 17 07:47:47.057: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:47:47.057: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
Aug 17 07:47:47.059: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.767288ms
Aug 17 07:47:47.059: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:47:47.059: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
Aug 17 07:47:47.060: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.569374ms
Aug 17 07:47:47.060: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:47:47.060: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
Aug 17 07:47:47.062: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.41416ms
Aug 17 07:47:47.062: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 07:47:47.062: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
Aug 17 07:47:47.063: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.364348ms
Aug 17 07:47:47.063: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/17/23 07:47:47.063
Aug 17 07:47:47.067: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8100" to be "running"
Aug 17 07:47:47.068: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.665052ms
Aug 17 07:47:49.070: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003675032s
Aug 17 07:47:51.071: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00468878s
Aug 17 07:47:53.071: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004410576s
Aug 17 07:47:53.071: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:47:53.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8100" for this suite. 08/17/23 07:47:53.119
------------------------------
â€¢ [SLOW TEST] [68.195 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:46:44.926
    Aug 17 07:46:44.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 07:46:44.927
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:46:44.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:46:44.935
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 07:46:44.952: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 07:47:45.000: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/17/23 07:47:45.002
    Aug 17 07:47:45.014: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 17 07:47:45.017: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 17 07:47:45.028: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 17 07:47:45.031: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 17 07:47:45.042: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 17 07:47:45.046: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/17/23 07:47:45.046
    Aug 17 07:47:45.046: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8100" to be "running"
    Aug 17 07:47:45.052: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041069ms
    Aug 17 07:47:47.055: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.00903176s
    Aug 17 07:47:47.055: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 17 07:47:47.055: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
    Aug 17 07:47:47.057: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.884487ms
    Aug 17 07:47:47.057: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:47:47.057: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
    Aug 17 07:47:47.059: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.767288ms
    Aug 17 07:47:47.059: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:47:47.059: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
    Aug 17 07:47:47.060: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.569374ms
    Aug 17 07:47:47.060: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:47:47.060: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
    Aug 17 07:47:47.062: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.41416ms
    Aug 17 07:47:47.062: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 07:47:47.062: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8100" to be "running"
    Aug 17 07:47:47.063: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.364348ms
    Aug 17 07:47:47.063: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/17/23 07:47:47.063
    Aug 17 07:47:47.067: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8100" to be "running"
    Aug 17 07:47:47.068: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.665052ms
    Aug 17 07:47:49.070: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003675032s
    Aug 17 07:47:51.071: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00468878s
    Aug 17 07:47:53.071: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004410576s
    Aug 17 07:47:53.071: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:47:53.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8100" for this suite. 08/17/23 07:47:53.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:47:53.123
Aug 17 07:47:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:47:53.124
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:47:53.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:47:53.137
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/17/23 07:47:53.139
STEP: Creating a ResourceQuota 08/17/23 07:47:58.141
STEP: Ensuring resource quota status is calculated 08/17/23 07:47:58.144
STEP: Creating a ReplicationController 08/17/23 07:48:00.147
STEP: Ensuring resource quota status captures replication controller creation 08/17/23 07:48:00.155
STEP: Deleting a ReplicationController 08/17/23 07:48:02.158
STEP: Ensuring resource quota status released usage 08/17/23 07:48:02.16
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:48:04.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9238" for this suite. 08/17/23 07:48:04.166
------------------------------
â€¢ [SLOW TEST] [11.046 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:47:53.123
    Aug 17 07:47:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:47:53.124
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:47:53.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:47:53.137
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/17/23 07:47:53.139
    STEP: Creating a ResourceQuota 08/17/23 07:47:58.141
    STEP: Ensuring resource quota status is calculated 08/17/23 07:47:58.144
    STEP: Creating a ReplicationController 08/17/23 07:48:00.147
    STEP: Ensuring resource quota status captures replication controller creation 08/17/23 07:48:00.155
    STEP: Deleting a ReplicationController 08/17/23 07:48:02.158
    STEP: Ensuring resource quota status released usage 08/17/23 07:48:02.16
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:48:04.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9238" for this suite. 08/17/23 07:48:04.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:48:04.17
Aug 17 07:48:04.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sysctl 08/17/23 07:48:04.171
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:04.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:04.182
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/17/23 07:48:04.184
STEP: Watching for error events or started pod 08/17/23 07:48:04.199
STEP: Waiting for pod completion 08/17/23 07:48:06.202
Aug 17 07:48:06.202: INFO: Waiting up to 3m0s for pod "sysctl-4add3945-cf8e-462c-9b84-589368833463" in namespace "sysctl-4855" to be "completed"
Aug 17 07:48:06.204: INFO: Pod "sysctl-4add3945-cf8e-462c-9b84-589368833463": Phase="Pending", Reason="", readiness=false. Elapsed: 1.828083ms
Aug 17 07:48:08.208: INFO: Pod "sysctl-4add3945-cf8e-462c-9b84-589368833463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005434677s
Aug 17 07:48:08.208: INFO: Pod "sysctl-4add3945-cf8e-462c-9b84-589368833463" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/17/23 07:48:08.21
STEP: Getting logs from the pod 08/17/23 07:48:08.21
STEP: Checking that the sysctl is actually updated 08/17/23 07:48:08.216
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:48:08.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4855" for this suite. 08/17/23 07:48:08.218
------------------------------
â€¢ [4.051 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:48:04.17
    Aug 17 07:48:04.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sysctl 08/17/23 07:48:04.171
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:04.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:04.182
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/17/23 07:48:04.184
    STEP: Watching for error events or started pod 08/17/23 07:48:04.199
    STEP: Waiting for pod completion 08/17/23 07:48:06.202
    Aug 17 07:48:06.202: INFO: Waiting up to 3m0s for pod "sysctl-4add3945-cf8e-462c-9b84-589368833463" in namespace "sysctl-4855" to be "completed"
    Aug 17 07:48:06.204: INFO: Pod "sysctl-4add3945-cf8e-462c-9b84-589368833463": Phase="Pending", Reason="", readiness=false. Elapsed: 1.828083ms
    Aug 17 07:48:08.208: INFO: Pod "sysctl-4add3945-cf8e-462c-9b84-589368833463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005434677s
    Aug 17 07:48:08.208: INFO: Pod "sysctl-4add3945-cf8e-462c-9b84-589368833463" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/17/23 07:48:08.21
    STEP: Getting logs from the pod 08/17/23 07:48:08.21
    STEP: Checking that the sysctl is actually updated 08/17/23 07:48:08.216
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:48:08.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4855" for this suite. 08/17/23 07:48:08.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:48:08.222
Aug 17 07:48:08.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:48:08.223
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:08.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:08.235
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1292 08/17/23 07:48:08.239
STEP: changing the ExternalName service to type=NodePort 08/17/23 07:48:08.25
STEP: creating replication controller externalname-service in namespace services-1292 08/17/23 07:48:08.272
I0817 07:48:08.277198      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1292, replica count: 2
I0817 07:48:11.328092      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 07:48:11.328: INFO: Creating new exec pod
Aug 17 07:48:11.331: INFO: Waiting up to 5m0s for pod "execpodw7tnt" in namespace "services-1292" to be "running"
Aug 17 07:48:11.333: INFO: Pod "execpodw7tnt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717024ms
Aug 17 07:48:13.336: INFO: Pod "execpodw7tnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.004443508s
Aug 17 07:48:13.336: INFO: Pod "execpodw7tnt" satisfied condition "running"
Aug 17 07:48:14.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 17 07:48:14.480: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 17 07:48:14.480: INFO: stdout: ""
Aug 17 07:48:14.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 10.101.197.144 80'
Aug 17 07:48:14.615: INFO: stderr: "+ nc -v -z -w 2 10.101.197.144 80\nConnection to 10.101.197.144 80 port [tcp/http] succeeded!\n"
Aug 17 07:48:14.615: INFO: stdout: ""
Aug 17 07:48:14.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.176 32167'
Aug 17 07:48:14.757: INFO: stderr: "+ nc -v -z -w 2 10.60.200.176 32167\nConnection to 10.60.200.176 32167 port [tcp/*] succeeded!\n"
Aug 17 07:48:14.757: INFO: stdout: ""
Aug 17 07:48:14.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.177 32167'
Aug 17 07:48:14.886: INFO: stderr: "+ nc -v -z -w 2 10.60.200.177 32167\nConnection to 10.60.200.177 32167 port [tcp/*] succeeded!\n"
Aug 17 07:48:14.886: INFO: stdout: ""
Aug 17 07:48:14.886: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:48:14.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1292" for this suite. 08/17/23 07:48:14.902
------------------------------
â€¢ [SLOW TEST] [6.683 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:48:08.222
    Aug 17 07:48:08.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:48:08.223
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:08.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:08.235
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1292 08/17/23 07:48:08.239
    STEP: changing the ExternalName service to type=NodePort 08/17/23 07:48:08.25
    STEP: creating replication controller externalname-service in namespace services-1292 08/17/23 07:48:08.272
    I0817 07:48:08.277198      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1292, replica count: 2
    I0817 07:48:11.328092      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 07:48:11.328: INFO: Creating new exec pod
    Aug 17 07:48:11.331: INFO: Waiting up to 5m0s for pod "execpodw7tnt" in namespace "services-1292" to be "running"
    Aug 17 07:48:11.333: INFO: Pod "execpodw7tnt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717024ms
    Aug 17 07:48:13.336: INFO: Pod "execpodw7tnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.004443508s
    Aug 17 07:48:13.336: INFO: Pod "execpodw7tnt" satisfied condition "running"
    Aug 17 07:48:14.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 17 07:48:14.480: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 17 07:48:14.480: INFO: stdout: ""
    Aug 17 07:48:14.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 10.101.197.144 80'
    Aug 17 07:48:14.615: INFO: stderr: "+ nc -v -z -w 2 10.101.197.144 80\nConnection to 10.101.197.144 80 port [tcp/http] succeeded!\n"
    Aug 17 07:48:14.615: INFO: stdout: ""
    Aug 17 07:48:14.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.176 32167'
    Aug 17 07:48:14.757: INFO: stderr: "+ nc -v -z -w 2 10.60.200.176 32167\nConnection to 10.60.200.176 32167 port [tcp/*] succeeded!\n"
    Aug 17 07:48:14.757: INFO: stdout: ""
    Aug 17 07:48:14.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-1292 exec execpodw7tnt -- /bin/sh -x -c nc -v -z -w 2 10.60.200.177 32167'
    Aug 17 07:48:14.886: INFO: stderr: "+ nc -v -z -w 2 10.60.200.177 32167\nConnection to 10.60.200.177 32167 port [tcp/*] succeeded!\n"
    Aug 17 07:48:14.886: INFO: stdout: ""
    Aug 17 07:48:14.886: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:48:14.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1292" for this suite. 08/17/23 07:48:14.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:48:14.906
Aug 17 07:48:14.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-runtime 08/17/23 07:48:14.906
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:14.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:14.923
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/17/23 07:48:14.926
STEP: wait for the container to reach Failed 08/17/23 07:48:14.934
STEP: get the container status 08/17/23 07:48:17.943
STEP: the container should be terminated 08/17/23 07:48:17.945
STEP: the termination message should be set 08/17/23 07:48:17.945
Aug 17 07:48:17.945: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/17/23 07:48:17.945
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 07:48:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1825" for this suite. 08/17/23 07:48:17.955
------------------------------
â€¢ [3.051 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:48:14.906
    Aug 17 07:48:14.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-runtime 08/17/23 07:48:14.906
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:14.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:14.923
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/17/23 07:48:14.926
    STEP: wait for the container to reach Failed 08/17/23 07:48:14.934
    STEP: get the container status 08/17/23 07:48:17.943
    STEP: the container should be terminated 08/17/23 07:48:17.945
    STEP: the termination message should be set 08/17/23 07:48:17.945
    Aug 17 07:48:17.945: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/17/23 07:48:17.945
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:48:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1825" for this suite. 08/17/23 07:48:17.955
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:48:17.957
Aug 17 07:48:17.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename ingressclass 08/17/23 07:48:17.958
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:17.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:17.966
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/17/23 07:48:17.969
STEP: getting /apis/networking.k8s.io 08/17/23 07:48:17.973
STEP: getting /apis/networking.k8s.iov1 08/17/23 07:48:17.976
STEP: creating 08/17/23 07:48:17.977
STEP: getting 08/17/23 07:48:18.006
STEP: listing 08/17/23 07:48:18.008
STEP: watching 08/17/23 07:48:18.009
Aug 17 07:48:18.009: INFO: starting watch
STEP: patching 08/17/23 07:48:18.01
STEP: updating 08/17/23 07:48:18.013
Aug 17 07:48:18.046: INFO: waiting for watch events with expected annotations
Aug 17 07:48:18.046: INFO: saw patched and updated annotations
STEP: deleting 08/17/23 07:48:18.046
STEP: deleting a collection 08/17/23 07:48:18.056
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 17 07:48:18.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-8091" for this suite. 08/17/23 07:48:18.065
------------------------------
â€¢ [0.111 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:48:17.957
    Aug 17 07:48:17.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename ingressclass 08/17/23 07:48:17.958
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:17.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:17.966
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/17/23 07:48:17.969
    STEP: getting /apis/networking.k8s.io 08/17/23 07:48:17.973
    STEP: getting /apis/networking.k8s.iov1 08/17/23 07:48:17.976
    STEP: creating 08/17/23 07:48:17.977
    STEP: getting 08/17/23 07:48:18.006
    STEP: listing 08/17/23 07:48:18.008
    STEP: watching 08/17/23 07:48:18.009
    Aug 17 07:48:18.009: INFO: starting watch
    STEP: patching 08/17/23 07:48:18.01
    STEP: updating 08/17/23 07:48:18.013
    Aug 17 07:48:18.046: INFO: waiting for watch events with expected annotations
    Aug 17 07:48:18.046: INFO: saw patched and updated annotations
    STEP: deleting 08/17/23 07:48:18.046
    STEP: deleting a collection 08/17/23 07:48:18.056
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:48:18.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-8091" for this suite. 08/17/23 07:48:18.065
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:48:18.068
Aug 17 07:48:18.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 07:48:18.069
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:18.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:18.086
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/17/23 07:48:35.09
STEP: Creating a ResourceQuota 08/17/23 07:48:40.093
STEP: Ensuring resource quota status is calculated 08/17/23 07:48:40.096
STEP: Creating a ConfigMap 08/17/23 07:48:42.098
STEP: Ensuring resource quota status captures configMap creation 08/17/23 07:48:42.104
STEP: Deleting a ConfigMap 08/17/23 07:48:44.106
STEP: Ensuring resource quota status released usage 08/17/23 07:48:44.109
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 07:48:46.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6240" for this suite. 08/17/23 07:48:46.115
------------------------------
â€¢ [SLOW TEST] [28.050 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:48:18.068
    Aug 17 07:48:18.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 07:48:18.069
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:18.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:18.086
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/17/23 07:48:35.09
    STEP: Creating a ResourceQuota 08/17/23 07:48:40.093
    STEP: Ensuring resource quota status is calculated 08/17/23 07:48:40.096
    STEP: Creating a ConfigMap 08/17/23 07:48:42.098
    STEP: Ensuring resource quota status captures configMap creation 08/17/23 07:48:42.104
    STEP: Deleting a ConfigMap 08/17/23 07:48:44.106
    STEP: Ensuring resource quota status released usage 08/17/23 07:48:44.109
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:48:46.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6240" for this suite. 08/17/23 07:48:46.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:48:46.119
Aug 17 07:48:46.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 07:48:46.12
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:46.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:46.133
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6 in namespace container-probe-2175 08/17/23 07:48:46.135
Aug 17 07:48:46.146: INFO: Waiting up to 5m0s for pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6" in namespace "container-probe-2175" to be "not pending"
Aug 17 07:48:46.149: INFO: Pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924614ms
Aug 17 07:48:48.152: INFO: Pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005951943s
Aug 17 07:48:48.152: INFO: Pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6" satisfied condition "not pending"
Aug 17 07:48:48.152: INFO: Started pod liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6 in namespace container-probe-2175
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:48:48.152
Aug 17 07:48:48.153: INFO: Initial restart count of pod liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6 is 0
STEP: deleting the pod 08/17/23 07:52:48.494
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 07:52:48.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2175" for this suite. 08/17/23 07:52:48.504
------------------------------
â€¢ [SLOW TEST] [242.389 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:48:46.119
    Aug 17 07:48:46.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 07:48:46.12
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:48:46.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:48:46.133
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6 in namespace container-probe-2175 08/17/23 07:48:46.135
    Aug 17 07:48:46.146: INFO: Waiting up to 5m0s for pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6" in namespace "container-probe-2175" to be "not pending"
    Aug 17 07:48:46.149: INFO: Pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924614ms
    Aug 17 07:48:48.152: INFO: Pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005951943s
    Aug 17 07:48:48.152: INFO: Pod "liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6" satisfied condition "not pending"
    Aug 17 07:48:48.152: INFO: Started pod liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6 in namespace container-probe-2175
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:48:48.152
    Aug 17 07:48:48.153: INFO: Initial restart count of pod liveness-adabf3fc-4634-41b5-8aaf-b2949e719fd6 is 0
    STEP: deleting the pod 08/17/23 07:52:48.494
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:52:48.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2175" for this suite. 08/17/23 07:52:48.504
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:52:48.508
Aug 17 07:52:48.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:52:48.509
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:52:48.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:52:48.53
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:52:48.532
Aug 17 07:52:48.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba" in namespace "projected-5558" to be "Succeeded or Failed"
Aug 17 07:52:48.544: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.097459ms
Aug 17 07:52:50.548: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006304998s
Aug 17 07:52:52.547: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006071121s
STEP: Saw pod success 08/17/23 07:52:52.547
Aug 17 07:52:52.547: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba" satisfied condition "Succeeded or Failed"
Aug 17 07:52:52.549: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba container client-container: <nil>
STEP: delete the pod 08/17/23 07:52:52.56
Aug 17 07:52:52.566: INFO: Waiting for pod downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba to disappear
Aug 17 07:52:52.568: INFO: Pod downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 07:52:52.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5558" for this suite. 08/17/23 07:52:52.57
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:52:48.508
    Aug 17 07:52:48.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:52:48.509
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:52:48.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:52:48.53
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:52:48.532
    Aug 17 07:52:48.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba" in namespace "projected-5558" to be "Succeeded or Failed"
    Aug 17 07:52:48.544: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.097459ms
    Aug 17 07:52:50.548: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006304998s
    Aug 17 07:52:52.547: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006071121s
    STEP: Saw pod success 08/17/23 07:52:52.547
    Aug 17 07:52:52.547: INFO: Pod "downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba" satisfied condition "Succeeded or Failed"
    Aug 17 07:52:52.549: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba container client-container: <nil>
    STEP: delete the pod 08/17/23 07:52:52.56
    Aug 17 07:52:52.566: INFO: Waiting for pod downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba to disappear
    Aug 17 07:52:52.568: INFO: Pod downwardapi-volume-a4f0868a-ae65-4dd7-bfb7-91ef27dfdeba no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:52:52.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5558" for this suite. 08/17/23 07:52:52.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:52:52.574
Aug 17 07:52:52.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:52:52.574
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:52:52.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:52:52.587
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:52:52.606
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:52:52.958
STEP: Deploying the webhook pod 08/17/23 07:52:52.963
STEP: Wait for the deployment to be ready 08/17/23 07:52:52.969
Aug 17 07:52:52.972: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/17/23 07:52:54.978
STEP: Verifying the service has paired with the endpoint 08/17/23 07:52:54.988
Aug 17 07:52:55.988: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/17/23 07:52:55.991
STEP: Creating a custom resource definition that should be denied by the webhook 08/17/23 07:52:56
Aug 17 07:52:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:52:56.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9900" for this suite. 08/17/23 07:52:56.045
STEP: Destroying namespace "webhook-9900-markers" for this suite. 08/17/23 07:52:56.05
------------------------------
â€¢ [3.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:52:52.574
    Aug 17 07:52:52.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:52:52.574
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:52:52.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:52:52.587
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:52:52.606
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:52:52.958
    STEP: Deploying the webhook pod 08/17/23 07:52:52.963
    STEP: Wait for the deployment to be ready 08/17/23 07:52:52.969
    Aug 17 07:52:52.972: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/17/23 07:52:54.978
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:52:54.988
    Aug 17 07:52:55.988: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/17/23 07:52:55.991
    STEP: Creating a custom resource definition that should be denied by the webhook 08/17/23 07:52:56
    Aug 17 07:52:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:52:56.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9900" for this suite. 08/17/23 07:52:56.045
    STEP: Destroying namespace "webhook-9900-markers" for this suite. 08/17/23 07:52:56.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:52:56.054
Aug 17 07:52:56.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename daemonsets 08/17/23 07:52:56.055
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:52:56.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:52:56.067
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 08/17/23 07:52:56.094
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:52:56.096
Aug 17 07:52:56.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:52:56.102: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:52:57.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 07:52:57.119: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:52:58.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:52:58.107: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/17/23 07:52:58.11
Aug 17 07:52:58.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 07:52:58.122: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:52:59.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 07:52:59.127: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 07:53:00.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 17 07:53:00.128: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/17/23 07:53:00.128
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:53:00.134
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9440, will wait for the garbage collector to delete the pods 08/17/23 07:53:00.134
Aug 17 07:53:00.190: INFO: Deleting DaemonSet.extensions daemon-set took: 3.058779ms
Aug 17 07:53:00.291: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.066655ms
Aug 17 07:53:02.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 07:53:02.294: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 07:53:02.296: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27063021"},"items":null}

Aug 17 07:53:02.297: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27063021"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:53:02.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9440" for this suite. 08/17/23 07:53:02.306
------------------------------
â€¢ [SLOW TEST] [6.255 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:52:56.054
    Aug 17 07:52:56.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename daemonsets 08/17/23 07:52:56.055
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:52:56.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:52:56.067
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 08/17/23 07:52:56.094
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 07:52:56.096
    Aug 17 07:52:56.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:52:56.102: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:52:57.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 07:52:57.119: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:52:58.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:52:58.107: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/17/23 07:52:58.11
    Aug 17 07:52:58.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 07:52:58.122: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:52:59.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 07:52:59.127: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 07:53:00.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 17 07:53:00.128: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/17/23 07:53:00.128
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 07:53:00.134
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9440, will wait for the garbage collector to delete the pods 08/17/23 07:53:00.134
    Aug 17 07:53:00.190: INFO: Deleting DaemonSet.extensions daemon-set took: 3.058779ms
    Aug 17 07:53:00.291: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.066655ms
    Aug 17 07:53:02.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 07:53:02.294: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 07:53:02.296: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27063021"},"items":null}

    Aug 17 07:53:02.297: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27063021"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:53:02.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9440" for this suite. 08/17/23 07:53:02.306
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:53:02.309
Aug 17 07:53:02.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 07:53:02.31
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:53:02.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:53:02.319
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 07:54:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4757" for this suite. 08/17/23 07:54:02.345
------------------------------
â€¢ [SLOW TEST] [60.038 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:53:02.309
    Aug 17 07:53:02.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 07:53:02.31
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:53:02.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:53:02.319
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:54:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4757" for this suite. 08/17/23 07:54:02.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:54:02.348
Aug 17 07:54:02.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:54:02.349
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:02.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:02.363
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/17/23 07:54:02.368
Aug 17 07:54:02.368: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 17 07:54:02.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
Aug 17 07:54:03.570: INFO: stderr: ""
Aug 17 07:54:03.570: INFO: stdout: "service/agnhost-replica created\n"
Aug 17 07:54:03.570: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 17 07:54:03.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
Aug 17 07:54:03.886: INFO: stderr: ""
Aug 17 07:54:03.886: INFO: stdout: "service/agnhost-primary created\n"
Aug 17 07:54:03.886: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 17 07:54:03.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
Aug 17 07:54:04.195: INFO: stderr: ""
Aug 17 07:54:04.195: INFO: stdout: "service/frontend created\n"
Aug 17 07:54:04.195: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 17 07:54:04.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
Aug 17 07:54:04.494: INFO: stderr: ""
Aug 17 07:54:04.494: INFO: stdout: "deployment.apps/frontend created\n"
Aug 17 07:54:04.494: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 17 07:54:04.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
Aug 17 07:54:04.813: INFO: stderr: ""
Aug 17 07:54:04.813: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 17 07:54:04.813: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 17 07:54:04.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
Aug 17 07:54:05.142: INFO: stderr: ""
Aug 17 07:54:05.142: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/17/23 07:54:05.142
Aug 17 07:54:05.142: INFO: Waiting for all frontend pods to be Running.
Aug 17 07:54:10.193: INFO: Waiting for frontend to serve content.
Aug 17 07:54:10.199: INFO: Trying to add a new entry to the guestbook.
Aug 17 07:54:10.206: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/17/23 07:54:10.21
Aug 17 07:54:10.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
Aug 17 07:54:10.306: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:54:10.306: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 07:54:10.306
Aug 17 07:54:10.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
Aug 17 07:54:10.401: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:54:10.401: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 07:54:10.401
Aug 17 07:54:10.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
Aug 17 07:54:10.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:54:10.495: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 07:54:10.495
Aug 17 07:54:10.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
Aug 17 07:54:10.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:54:10.572: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 07:54:10.573
Aug 17 07:54:10.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
Aug 17 07:54:10.660: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:54:10.660: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 07:54:10.66
Aug 17 07:54:10.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
Aug 17 07:54:10.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 07:54:10.752: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:54:10.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9728" for this suite. 08/17/23 07:54:10.755
------------------------------
â€¢ [SLOW TEST] [8.411 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:54:02.348
    Aug 17 07:54:02.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:54:02.349
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:02.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:02.363
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/17/23 07:54:02.368
    Aug 17 07:54:02.368: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 17 07:54:02.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
    Aug 17 07:54:03.570: INFO: stderr: ""
    Aug 17 07:54:03.570: INFO: stdout: "service/agnhost-replica created\n"
    Aug 17 07:54:03.570: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 17 07:54:03.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
    Aug 17 07:54:03.886: INFO: stderr: ""
    Aug 17 07:54:03.886: INFO: stdout: "service/agnhost-primary created\n"
    Aug 17 07:54:03.886: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 17 07:54:03.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
    Aug 17 07:54:04.195: INFO: stderr: ""
    Aug 17 07:54:04.195: INFO: stdout: "service/frontend created\n"
    Aug 17 07:54:04.195: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 17 07:54:04.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
    Aug 17 07:54:04.494: INFO: stderr: ""
    Aug 17 07:54:04.494: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 17 07:54:04.494: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 17 07:54:04.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
    Aug 17 07:54:04.813: INFO: stderr: ""
    Aug 17 07:54:04.813: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 17 07:54:04.813: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 17 07:54:04.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 create -f -'
    Aug 17 07:54:05.142: INFO: stderr: ""
    Aug 17 07:54:05.142: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/17/23 07:54:05.142
    Aug 17 07:54:05.142: INFO: Waiting for all frontend pods to be Running.
    Aug 17 07:54:10.193: INFO: Waiting for frontend to serve content.
    Aug 17 07:54:10.199: INFO: Trying to add a new entry to the guestbook.
    Aug 17 07:54:10.206: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/17/23 07:54:10.21
    Aug 17 07:54:10.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
    Aug 17 07:54:10.306: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:54:10.306: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 07:54:10.306
    Aug 17 07:54:10.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
    Aug 17 07:54:10.401: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:54:10.401: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 07:54:10.401
    Aug 17 07:54:10.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
    Aug 17 07:54:10.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:54:10.495: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 07:54:10.495
    Aug 17 07:54:10.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
    Aug 17 07:54:10.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:54:10.572: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 07:54:10.573
    Aug 17 07:54:10.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
    Aug 17 07:54:10.660: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:54:10.660: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 07:54:10.66
    Aug 17 07:54:10.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9728 delete --grace-period=0 --force -f -'
    Aug 17 07:54:10.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 07:54:10.752: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:54:10.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9728" for this suite. 08/17/23 07:54:10.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:54:10.76
Aug 17 07:54:10.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:54:10.761
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:10.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:10.779
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/17/23 07:54:10.856
STEP: waiting for available Endpoint 08/17/23 07:54:10.87
STEP: listing all Endpoints 08/17/23 07:54:10.873
STEP: updating the Endpoint 08/17/23 07:54:10.889
STEP: fetching the Endpoint 08/17/23 07:54:10.901
STEP: patching the Endpoint 08/17/23 07:54:10.904
STEP: fetching the Endpoint 08/17/23 07:54:10.914
STEP: deleting the Endpoint by Collection 08/17/23 07:54:10.92
STEP: waiting for Endpoint deletion 08/17/23 07:54:10.926
STEP: fetching the Endpoint 08/17/23 07:54:10.927
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:54:10.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5014" for this suite. 08/17/23 07:54:10.933
------------------------------
â€¢ [0.176 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:54:10.76
    Aug 17 07:54:10.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:54:10.761
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:10.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:10.779
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/17/23 07:54:10.856
    STEP: waiting for available Endpoint 08/17/23 07:54:10.87
    STEP: listing all Endpoints 08/17/23 07:54:10.873
    STEP: updating the Endpoint 08/17/23 07:54:10.889
    STEP: fetching the Endpoint 08/17/23 07:54:10.901
    STEP: patching the Endpoint 08/17/23 07:54:10.904
    STEP: fetching the Endpoint 08/17/23 07:54:10.914
    STEP: deleting the Endpoint by Collection 08/17/23 07:54:10.92
    STEP: waiting for Endpoint deletion 08/17/23 07:54:10.926
    STEP: fetching the Endpoint 08/17/23 07:54:10.927
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:54:10.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5014" for this suite. 08/17/23 07:54:10.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:54:10.936
Aug 17 07:54:10.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 07:54:10.937
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:10.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:10.956
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-015cddcb-35e1-4400-9ac1-e385132673f7 08/17/23 07:54:10.958
STEP: Creating a pod to test consume configMaps 08/17/23 07:54:10.968
Aug 17 07:54:10.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96" in namespace "configmap-7512" to be "Succeeded or Failed"
Aug 17 07:54:10.989: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568814ms
Aug 17 07:54:12.991: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007151865s
Aug 17 07:54:14.991: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006844224s
STEP: Saw pod success 08/17/23 07:54:14.991
Aug 17 07:54:14.991: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96" satisfied condition "Succeeded or Failed"
Aug 17 07:54:14.993: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 07:54:14.996
Aug 17 07:54:15.001: INFO: Waiting for pod pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96 to disappear
Aug 17 07:54:15.002: INFO: Pod pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 07:54:15.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7512" for this suite. 08/17/23 07:54:15.004
------------------------------
â€¢ [4.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:54:10.936
    Aug 17 07:54:10.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 07:54:10.937
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:10.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:10.956
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-015cddcb-35e1-4400-9ac1-e385132673f7 08/17/23 07:54:10.958
    STEP: Creating a pod to test consume configMaps 08/17/23 07:54:10.968
    Aug 17 07:54:10.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96" in namespace "configmap-7512" to be "Succeeded or Failed"
    Aug 17 07:54:10.989: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568814ms
    Aug 17 07:54:12.991: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007151865s
    Aug 17 07:54:14.991: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006844224s
    STEP: Saw pod success 08/17/23 07:54:14.991
    Aug 17 07:54:14.991: INFO: Pod "pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96" satisfied condition "Succeeded or Failed"
    Aug 17 07:54:14.993: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 07:54:14.996
    Aug 17 07:54:15.001: INFO: Waiting for pod pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96 to disappear
    Aug 17 07:54:15.002: INFO: Pod pod-configmaps-ac407821-6476-4d4f-a989-cd10b9399b96 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:54:15.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7512" for this suite. 08/17/23 07:54:15.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:54:15.008
Aug 17 07:54:15.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename disruption 08/17/23 07:54:15.008
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:15.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:15.04
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/17/23 07:54:15.044
STEP: Waiting for the pdb to be processed 08/17/23 07:54:15.075
STEP: First trying to evict a pod which shouldn't be evictable 08/17/23 07:54:15.115
STEP: Waiting for all pods to be running 08/17/23 07:54:15.115
Aug 17 07:54:15.122: INFO: pods: 0 < 3
STEP: locating a running pod 08/17/23 07:54:17.126
STEP: Updating the pdb to allow a pod to be evicted 08/17/23 07:54:17.133
STEP: Waiting for the pdb to be processed 08/17/23 07:54:17.137
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 07:54:19.141
STEP: Waiting for all pods to be running 08/17/23 07:54:19.141
STEP: Waiting for the pdb to observed all healthy pods 08/17/23 07:54:19.143
STEP: Patching the pdb to disallow a pod to be evicted 08/17/23 07:54:19.154
STEP: Waiting for the pdb to be processed 08/17/23 07:54:19.167
STEP: Waiting for all pods to be running 08/17/23 07:54:19.175
Aug 17 07:54:19.182: INFO: running pods: 2 < 3
STEP: locating a running pod 08/17/23 07:54:21.185
STEP: Deleting the pdb to allow a pod to be evicted 08/17/23 07:54:21.191
STEP: Waiting for the pdb to be deleted 08/17/23 07:54:21.194
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 07:54:21.196
STEP: Waiting for all pods to be running 08/17/23 07:54:21.196
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 07:54:21.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4003" for this suite. 08/17/23 07:54:21.207
------------------------------
â€¢ [SLOW TEST] [6.204 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:54:15.008
    Aug 17 07:54:15.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename disruption 08/17/23 07:54:15.008
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:15.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:15.04
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/17/23 07:54:15.044
    STEP: Waiting for the pdb to be processed 08/17/23 07:54:15.075
    STEP: First trying to evict a pod which shouldn't be evictable 08/17/23 07:54:15.115
    STEP: Waiting for all pods to be running 08/17/23 07:54:15.115
    Aug 17 07:54:15.122: INFO: pods: 0 < 3
    STEP: locating a running pod 08/17/23 07:54:17.126
    STEP: Updating the pdb to allow a pod to be evicted 08/17/23 07:54:17.133
    STEP: Waiting for the pdb to be processed 08/17/23 07:54:17.137
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 07:54:19.141
    STEP: Waiting for all pods to be running 08/17/23 07:54:19.141
    STEP: Waiting for the pdb to observed all healthy pods 08/17/23 07:54:19.143
    STEP: Patching the pdb to disallow a pod to be evicted 08/17/23 07:54:19.154
    STEP: Waiting for the pdb to be processed 08/17/23 07:54:19.167
    STEP: Waiting for all pods to be running 08/17/23 07:54:19.175
    Aug 17 07:54:19.182: INFO: running pods: 2 < 3
    STEP: locating a running pod 08/17/23 07:54:21.185
    STEP: Deleting the pdb to allow a pod to be evicted 08/17/23 07:54:21.191
    STEP: Waiting for the pdb to be deleted 08/17/23 07:54:21.194
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 07:54:21.196
    STEP: Waiting for all pods to be running 08/17/23 07:54:21.196
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:54:21.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4003" for this suite. 08/17/23 07:54:21.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:54:21.212
Aug 17 07:54:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 07:54:21.212
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:21.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:21.228
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 17 07:54:21.242: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 17 07:54:26.246: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 07:54:26.246
Aug 17 07:54:26.246: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 17 07:54:28.249: INFO: Creating deployment "test-rollover-deployment"
Aug 17 07:54:28.254: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 17 07:54:30.260: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 17 07:54:30.264: INFO: Ensure that both replica sets have 1 created replica
Aug 17 07:54:30.267: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 17 07:54:30.273: INFO: Updating deployment test-rollover-deployment
Aug 17 07:54:30.273: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 17 07:54:32.278: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 17 07:54:32.282: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 17 07:54:32.285: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 07:54:32.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:54:34.290: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 07:54:34.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:54:36.290: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 07:54:36.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:54:38.291: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 07:54:38.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:54:40.291: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 07:54:40.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 07:54:42.290: INFO: 
Aug 17 07:54:42.290: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 07:54:42.294: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8098  bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 27064871 2 2023-08-17 07:54:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033e8828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 07:54:28 +0000 UTC,LastTransitionTime:2023-08-17 07:54:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-17 07:54:41 +0000 UTC,LastTransitionTime:2023-08-17 07:54:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 07:54:42.295: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8098  3adee5a7-cc8b-45e9-8733-69ba8de6c198 27064861 2 2023-08-17 07:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 0xc0046c6e27 0xc0046c6e28}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8f6d8d-4537-45ed-af7f-ce135f2bbcab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046c6ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:54:42.295: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 17 07:54:42.296: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8098  38a0b955-ede4-4d99-a8c6-d2537b54c210 27064870 2 2023-08-17 07:54:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 0xc0046c6cf7 0xc0046c6cf8}] [] [{e2e.test Update apps/v1 2023-08-17 07:54:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8f6d8d-4537-45ed-af7f-ce135f2bbcab\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046c6db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:54:42.296: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8098  bd835647-0429-4bc6-8157-49d0bc10ddd1 27064749 2 2023-08-17 07:54:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 0xc0046c6f47 0xc0046c6f48}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8f6d8d-4537-45ed-af7f-ce135f2bbcab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046c6ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:54:42.297: INFO: Pod "test-rollover-deployment-6c6df9974f-fd6n7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-fd6n7 test-rollover-deployment-6c6df9974f- deployment-8098  25b293ff-7d73-4a90-baac-9fc721d44373 27064779 0 2023-08-17 07:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:3eadcfccb93913e4bc16b143eff04b1fdf44b469c97110d1b159bb470a92e742 cni.projectcalico.org/podIP:172.32.238.107/32 cni.projectcalico.org/podIPs:172.32.238.107/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 3adee5a7-cc8b-45e9-8733-69ba8de6c198 0xc0033e8bc7 0xc0033e8bc8}] [] [{calico Update v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3adee5a7-cc8b-45e9-8733-69ba8de6c198\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:54:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7f2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7f2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.107,StartTime:2023-08-17 07:54:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:54:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://ecbc0c5473a07971c3f1a91bdf8dc00b827985464b8665c751e5583d0a63ba52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 07:54:42.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8098" for this suite. 08/17/23 07:54:42.299
------------------------------
â€¢ [SLOW TEST] [21.090 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:54:21.212
    Aug 17 07:54:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 07:54:21.212
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:21.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:21.228
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 17 07:54:21.242: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 17 07:54:26.246: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 07:54:26.246
    Aug 17 07:54:26.246: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 17 07:54:28.249: INFO: Creating deployment "test-rollover-deployment"
    Aug 17 07:54:28.254: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 17 07:54:30.260: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 17 07:54:30.264: INFO: Ensure that both replica sets have 1 created replica
    Aug 17 07:54:30.267: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 17 07:54:30.273: INFO: Updating deployment test-rollover-deployment
    Aug 17 07:54:30.273: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 17 07:54:32.278: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 17 07:54:32.282: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 17 07:54:32.285: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 07:54:32.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:54:34.290: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 07:54:34.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:54:36.290: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 07:54:36.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:54:38.291: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 07:54:38.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:54:40.291: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 07:54:40.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 7, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 7, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 07:54:42.290: INFO: 
    Aug 17 07:54:42.290: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 07:54:42.294: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8098  bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 27064871 2 2023-08-17 07:54:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033e8828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 07:54:28 +0000 UTC,LastTransitionTime:2023-08-17 07:54:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-17 07:54:41 +0000 UTC,LastTransitionTime:2023-08-17 07:54:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 07:54:42.295: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8098  3adee5a7-cc8b-45e9-8733-69ba8de6c198 27064861 2 2023-08-17 07:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 0xc0046c6e27 0xc0046c6e28}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8f6d8d-4537-45ed-af7f-ce135f2bbcab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046c6ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:54:42.295: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 17 07:54:42.296: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8098  38a0b955-ede4-4d99-a8c6-d2537b54c210 27064870 2 2023-08-17 07:54:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 0xc0046c6cf7 0xc0046c6cf8}] [] [{e2e.test Update apps/v1 2023-08-17 07:54:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8f6d8d-4537-45ed-af7f-ce135f2bbcab\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046c6db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:54:42.296: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8098  bd835647-0429-4bc6-8157-49d0bc10ddd1 27064749 2 2023-08-17 07:54:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bc8f6d8d-4537-45ed-af7f-ce135f2bbcab 0xc0046c6f47 0xc0046c6f48}] [] [{kube-controller-manager Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8f6d8d-4537-45ed-af7f-ce135f2bbcab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046c6ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:54:42.297: INFO: Pod "test-rollover-deployment-6c6df9974f-fd6n7" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-fd6n7 test-rollover-deployment-6c6df9974f- deployment-8098  25b293ff-7d73-4a90-baac-9fc721d44373 27064779 0 2023-08-17 07:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:3eadcfccb93913e4bc16b143eff04b1fdf44b469c97110d1b159bb470a92e742 cni.projectcalico.org/podIP:172.32.238.107/32 cni.projectcalico.org/podIPs:172.32.238.107/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 3adee5a7-cc8b-45e9-8733-69ba8de6c198 0xc0033e8bc7 0xc0033e8bc8}] [] [{calico Update v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3adee5a7-cc8b-45e9-8733-69ba8de6c198\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:54:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7f2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7f2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:54:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.107,StartTime:2023-08-17 07:54:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:54:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://ecbc0c5473a07971c3f1a91bdf8dc00b827985464b8665c751e5583d0a63ba52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:54:42.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8098" for this suite. 08/17/23 07:54:42.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:54:42.303
Aug 17 07:54:42.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename endpointslicemirroring 08/17/23 07:54:42.304
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:42.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:42.325
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/17/23 07:54:42.339
STEP: mirroring an update to a custom Endpoint 08/17/23 07:54:42.345
Aug 17 07:54:42.349: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/17/23 07:54:44.351
Aug 17 07:54:44.357: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 17 07:54:46.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6062" for this suite. 08/17/23 07:54:46.363
------------------------------
â€¢ [4.063 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:54:42.303
    Aug 17 07:54:42.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename endpointslicemirroring 08/17/23 07:54:42.304
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:42.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:42.325
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/17/23 07:54:42.339
    STEP: mirroring an update to a custom Endpoint 08/17/23 07:54:42.345
    Aug 17 07:54:42.349: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/17/23 07:54:44.351
    Aug 17 07:54:44.357: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:54:46.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6062" for this suite. 08/17/23 07:54:46.363
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:54:46.366
Aug 17 07:54:46.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pod-network-test 08/17/23 07:54:46.367
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:46.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:46.385
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9509 08/17/23 07:54:46.387
STEP: creating a selector 08/17/23 07:54:46.387
STEP: Creating the service pods in kubernetes 08/17/23 07:54:46.388
Aug 17 07:54:46.388: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 07:54:46.410: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9509" to be "running and ready"
Aug 17 07:54:46.411: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.337977ms
Aug 17 07:54:46.411: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:54:48.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006127063s
Aug 17 07:54:48.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:54:50.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.003982614s
Aug 17 07:54:50.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:54:52.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004407335s
Aug 17 07:54:52.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:54:54.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004677429s
Aug 17 07:54:54.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:54:56.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004099956s
Aug 17 07:54:56.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:54:58.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005291102s
Aug 17 07:54:58.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:55:00.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005641657s
Aug 17 07:55:00.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:55:02.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004753665s
Aug 17 07:55:02.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:55:04.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00394894s
Aug 17 07:55:04.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:55:06.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00371233s
Aug 17 07:55:06.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 07:55:08.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004923309s
Aug 17 07:55:08.415: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 07:55:08.415: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 07:55:08.416: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9509" to be "running and ready"
Aug 17 07:55:08.418: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.497618ms
Aug 17 07:55:08.418: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 07:55:08.418: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 17 07:55:08.419: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9509" to be "running and ready"
Aug 17 07:55:08.421: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.41948ms
Aug 17 07:55:08.421: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 17 07:55:08.421: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 07:55:08.422
Aug 17 07:55:08.425: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9509" to be "running"
Aug 17 07:55:08.427: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455565ms
Aug 17 07:55:10.429: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003940464s
Aug 17 07:55:10.429: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 07:55:10.431: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 17 07:55:10.431: INFO: Breadth first check of 172.32.3.27 on host 10.60.200.175...
Aug 17 07:55:10.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.80:9080/dial?request=hostname&protocol=http&host=172.32.3.27&port=8083&tries=1'] Namespace:pod-network-test-9509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:55:10.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:55:10.434: INFO: ExecWithOptions: Clientset creation
Aug 17 07:55:10.434: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.80%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.32.3.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 07:55:10.492: INFO: Waiting for responses: map[]
Aug 17 07:55:10.492: INFO: reached 172.32.3.27 after 0/1 tries
Aug 17 07:55:10.492: INFO: Breadth first check of 172.32.123.167 on host 10.60.200.176...
Aug 17 07:55:10.494: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.80:9080/dial?request=hostname&protocol=http&host=172.32.123.167&port=8083&tries=1'] Namespace:pod-network-test-9509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:55:10.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:55:10.494: INFO: ExecWithOptions: Clientset creation
Aug 17 07:55:10.494: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.80%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.32.123.167%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 07:55:10.542: INFO: Waiting for responses: map[]
Aug 17 07:55:10.542: INFO: reached 172.32.123.167 after 0/1 tries
Aug 17 07:55:10.542: INFO: Breadth first check of 172.32.238.95 on host 10.60.200.177...
Aug 17 07:55:10.544: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.80:9080/dial?request=hostname&protocol=http&host=172.32.238.95&port=8083&tries=1'] Namespace:pod-network-test-9509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 07:55:10.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:55:10.545: INFO: ExecWithOptions: Clientset creation
Aug 17 07:55:10.545: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.80%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.32.238.95%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 07:55:10.593: INFO: Waiting for responses: map[]
Aug 17 07:55:10.593: INFO: reached 172.32.238.95 after 0/1 tries
Aug 17 07:55:10.593: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:10.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9509" for this suite. 08/17/23 07:55:10.596
------------------------------
â€¢ [SLOW TEST] [24.233 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:54:46.366
    Aug 17 07:54:46.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 07:54:46.367
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:54:46.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:54:46.385
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9509 08/17/23 07:54:46.387
    STEP: creating a selector 08/17/23 07:54:46.387
    STEP: Creating the service pods in kubernetes 08/17/23 07:54:46.388
    Aug 17 07:54:46.388: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 07:54:46.410: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9509" to be "running and ready"
    Aug 17 07:54:46.411: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.337977ms
    Aug 17 07:54:46.411: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:54:48.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006127063s
    Aug 17 07:54:48.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:54:50.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.003982614s
    Aug 17 07:54:50.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:54:52.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004407335s
    Aug 17 07:54:52.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:54:54.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004677429s
    Aug 17 07:54:54.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:54:56.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004099956s
    Aug 17 07:54:56.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:54:58.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005291102s
    Aug 17 07:54:58.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:55:00.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005641657s
    Aug 17 07:55:00.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:55:02.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004753665s
    Aug 17 07:55:02.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:55:04.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00394894s
    Aug 17 07:55:04.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:55:06.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00371233s
    Aug 17 07:55:06.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 07:55:08.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004923309s
    Aug 17 07:55:08.415: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 07:55:08.415: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 07:55:08.416: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9509" to be "running and ready"
    Aug 17 07:55:08.418: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.497618ms
    Aug 17 07:55:08.418: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 07:55:08.418: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 17 07:55:08.419: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9509" to be "running and ready"
    Aug 17 07:55:08.421: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.41948ms
    Aug 17 07:55:08.421: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 17 07:55:08.421: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 07:55:08.422
    Aug 17 07:55:08.425: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9509" to be "running"
    Aug 17 07:55:08.427: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455565ms
    Aug 17 07:55:10.429: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003940464s
    Aug 17 07:55:10.429: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 07:55:10.431: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 17 07:55:10.431: INFO: Breadth first check of 172.32.3.27 on host 10.60.200.175...
    Aug 17 07:55:10.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.80:9080/dial?request=hostname&protocol=http&host=172.32.3.27&port=8083&tries=1'] Namespace:pod-network-test-9509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:55:10.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:55:10.434: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:55:10.434: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.80%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.32.3.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 07:55:10.492: INFO: Waiting for responses: map[]
    Aug 17 07:55:10.492: INFO: reached 172.32.3.27 after 0/1 tries
    Aug 17 07:55:10.492: INFO: Breadth first check of 172.32.123.167 on host 10.60.200.176...
    Aug 17 07:55:10.494: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.80:9080/dial?request=hostname&protocol=http&host=172.32.123.167&port=8083&tries=1'] Namespace:pod-network-test-9509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:55:10.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:55:10.494: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:55:10.494: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.80%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.32.123.167%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 07:55:10.542: INFO: Waiting for responses: map[]
    Aug 17 07:55:10.542: INFO: reached 172.32.123.167 after 0/1 tries
    Aug 17 07:55:10.542: INFO: Breadth first check of 172.32.238.95 on host 10.60.200.177...
    Aug 17 07:55:10.544: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.80:9080/dial?request=hostname&protocol=http&host=172.32.238.95&port=8083&tries=1'] Namespace:pod-network-test-9509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 07:55:10.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:55:10.545: INFO: ExecWithOptions: Clientset creation
    Aug 17 07:55:10.545: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.80%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.32.238.95%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 07:55:10.593: INFO: Waiting for responses: map[]
    Aug 17 07:55:10.593: INFO: reached 172.32.238.95 after 0/1 tries
    Aug 17 07:55:10.593: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:10.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9509" for this suite. 08/17/23 07:55:10.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:10.599
Aug 17 07:55:10.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 07:55:10.6
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:10.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:10.611
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 07:55:10.64
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:55:11.031
STEP: Deploying the webhook pod 08/17/23 07:55:11.035
STEP: Wait for the deployment to be ready 08/17/23 07:55:11.042
Aug 17 07:55:11.046: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 07:55:13.054
STEP: Verifying the service has paired with the endpoint 08/17/23 07:55:13.062
Aug 17 07:55:14.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 17 07:55:14.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6676-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 07:55:14.571
STEP: Creating a custom resource while v1 is storage version 08/17/23 07:55:14.58
STEP: Patching Custom Resource Definition to set v2 as storage 08/17/23 07:55:16.59
STEP: Patching the custom resource while v2 is storage version 08/17/23 07:55:16.605
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:17.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1795" for this suite. 08/17/23 07:55:17.19
STEP: Destroying namespace "webhook-1795-markers" for this suite. 08/17/23 07:55:17.193
------------------------------
â€¢ [SLOW TEST] [6.600 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:10.599
    Aug 17 07:55:10.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 07:55:10.6
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:10.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:10.611
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 07:55:10.64
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 07:55:11.031
    STEP: Deploying the webhook pod 08/17/23 07:55:11.035
    STEP: Wait for the deployment to be ready 08/17/23 07:55:11.042
    Aug 17 07:55:11.046: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 07:55:13.054
    STEP: Verifying the service has paired with the endpoint 08/17/23 07:55:13.062
    Aug 17 07:55:14.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 17 07:55:14.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6676-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 07:55:14.571
    STEP: Creating a custom resource while v1 is storage version 08/17/23 07:55:14.58
    STEP: Patching Custom Resource Definition to set v2 as storage 08/17/23 07:55:16.59
    STEP: Patching the custom resource while v2 is storage version 08/17/23 07:55:16.605
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:17.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1795" for this suite. 08/17/23 07:55:17.19
    STEP: Destroying namespace "webhook-1795-markers" for this suite. 08/17/23 07:55:17.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:17.199
Aug 17 07:55:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename security-context-test 08/17/23 07:55:17.2
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:17.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:17.215
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 17 07:55:17.252: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f" in namespace "security-context-test-8264" to be "Succeeded or Failed"
Aug 17 07:55:17.254: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084822ms
Aug 17 07:55:19.258: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f": Phase="Running", Reason="", readiness=false. Elapsed: 2.005660374s
Aug 17 07:55:21.258: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005289122s
Aug 17 07:55:21.258: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:21.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8264" for this suite. 08/17/23 07:55:21.261
------------------------------
â€¢ [4.066 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:17.199
    Aug 17 07:55:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename security-context-test 08/17/23 07:55:17.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:17.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:17.215
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 17 07:55:17.252: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f" in namespace "security-context-test-8264" to be "Succeeded or Failed"
    Aug 17 07:55:17.254: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084822ms
    Aug 17 07:55:19.258: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f": Phase="Running", Reason="", readiness=false. Elapsed: 2.005660374s
    Aug 17 07:55:21.258: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005289122s
    Aug 17 07:55:21.258: INFO: Pod "busybox-user-65534-a4388544-0cf3-4679-8de9-cc70f7bdf02f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:21.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8264" for this suite. 08/17/23 07:55:21.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:21.266
Aug 17 07:55:21.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:55:21.267
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:21.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:21.281
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/17/23 07:55:21.283
Aug 17 07:55:21.294: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64" in namespace "projected-8866" to be "Succeeded or Failed"
Aug 17 07:55:21.299: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64": Phase="Pending", Reason="", readiness=false. Elapsed: 5.142558ms
Aug 17 07:55:23.303: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009134929s
Aug 17 07:55:25.304: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010023207s
STEP: Saw pod success 08/17/23 07:55:25.304
Aug 17 07:55:25.304: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64" satisfied condition "Succeeded or Failed"
Aug 17 07:55:25.306: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64 container client-container: <nil>
STEP: delete the pod 08/17/23 07:55:25.31
Aug 17 07:55:25.317: INFO: Waiting for pod downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64 to disappear
Aug 17 07:55:25.318: INFO: Pod downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:25.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8866" for this suite. 08/17/23 07:55:25.321
------------------------------
â€¢ [4.060 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:21.266
    Aug 17 07:55:21.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:55:21.267
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:21.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:21.281
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/17/23 07:55:21.283
    Aug 17 07:55:21.294: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64" in namespace "projected-8866" to be "Succeeded or Failed"
    Aug 17 07:55:21.299: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64": Phase="Pending", Reason="", readiness=false. Elapsed: 5.142558ms
    Aug 17 07:55:23.303: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009134929s
    Aug 17 07:55:25.304: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010023207s
    STEP: Saw pod success 08/17/23 07:55:25.304
    Aug 17 07:55:25.304: INFO: Pod "downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64" satisfied condition "Succeeded or Failed"
    Aug 17 07:55:25.306: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64 container client-container: <nil>
    STEP: delete the pod 08/17/23 07:55:25.31
    Aug 17 07:55:25.317: INFO: Waiting for pod downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64 to disappear
    Aug 17 07:55:25.318: INFO: Pod downwardapi-volume-5935043b-e0df-4061-aa18-16b19d28bf64 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:25.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8866" for this suite. 08/17/23 07:55:25.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:25.327
Aug 17 07:55:25.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 07:55:25.327
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:25.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:25.339
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1179.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1179.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/17/23 07:55:25.342
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1179.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1179.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/17/23 07:55:25.342
STEP: creating a pod to probe /etc/hosts 08/17/23 07:55:25.342
STEP: submitting the pod to kubernetes 08/17/23 07:55:25.342
Aug 17 07:55:25.359: INFO: Waiting up to 15m0s for pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347" in namespace "dns-1179" to be "running"
Aug 17 07:55:25.366: INFO: Pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347": Phase="Pending", Reason="", readiness=false. Elapsed: 7.329308ms
Aug 17 07:55:27.370: INFO: Pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347": Phase="Running", Reason="", readiness=true. Elapsed: 2.011447677s
Aug 17 07:55:27.370: INFO: Pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347" satisfied condition "running"
STEP: retrieving the pod 08/17/23 07:55:27.37
STEP: looking for the results for each expected name from probers 08/17/23 07:55:27.373
Aug 17 07:55:27.383: INFO: DNS probes using dns-1179/dns-test-e4d11464-110c-48ed-b019-11efe4b59347 succeeded

STEP: deleting the pod 08/17/23 07:55:27.383
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:27.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1179" for this suite. 08/17/23 07:55:27.585
------------------------------
â€¢ [2.272 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:25.327
    Aug 17 07:55:25.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 07:55:25.327
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:25.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:25.339
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1179.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1179.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/17/23 07:55:25.342
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1179.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1179.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/17/23 07:55:25.342
    STEP: creating a pod to probe /etc/hosts 08/17/23 07:55:25.342
    STEP: submitting the pod to kubernetes 08/17/23 07:55:25.342
    Aug 17 07:55:25.359: INFO: Waiting up to 15m0s for pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347" in namespace "dns-1179" to be "running"
    Aug 17 07:55:25.366: INFO: Pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347": Phase="Pending", Reason="", readiness=false. Elapsed: 7.329308ms
    Aug 17 07:55:27.370: INFO: Pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347": Phase="Running", Reason="", readiness=true. Elapsed: 2.011447677s
    Aug 17 07:55:27.370: INFO: Pod "dns-test-e4d11464-110c-48ed-b019-11efe4b59347" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 07:55:27.37
    STEP: looking for the results for each expected name from probers 08/17/23 07:55:27.373
    Aug 17 07:55:27.383: INFO: DNS probes using dns-1179/dns-test-e4d11464-110c-48ed-b019-11efe4b59347 succeeded

    STEP: deleting the pod 08/17/23 07:55:27.383
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:27.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1179" for this suite. 08/17/23 07:55:27.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:27.599
Aug 17 07:55:27.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename disruption 08/17/23 07:55:27.6
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:27.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:27.679
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:27.689
Aug 17 07:55:27.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename disruption-2 08/17/23 07:55:27.69
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:27.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:27.792
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/17/23 07:55:27.837
STEP: Waiting for the pdb to be processed 08/17/23 07:55:27.902
STEP: Waiting for the pdb to be processed 08/17/23 07:55:29.935
STEP: listing a collection of PDBs across all namespaces 08/17/23 07:55:29.941
STEP: listing a collection of PDBs in namespace disruption-5611 08/17/23 07:55:29.944
STEP: deleting a collection of PDBs 08/17/23 07:55:29.947
STEP: Waiting for the PDB collection to be deleted 08/17/23 07:55:30.012
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:30.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:30.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4465" for this suite. 08/17/23 07:55:30.02
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5611" for this suite. 08/17/23 07:55:30.067
------------------------------
â€¢ [2.474 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:27.599
    Aug 17 07:55:27.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename disruption 08/17/23 07:55:27.6
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:27.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:27.679
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:27.689
    Aug 17 07:55:27.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename disruption-2 08/17/23 07:55:27.69
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:27.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:27.792
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/17/23 07:55:27.837
    STEP: Waiting for the pdb to be processed 08/17/23 07:55:27.902
    STEP: Waiting for the pdb to be processed 08/17/23 07:55:29.935
    STEP: listing a collection of PDBs across all namespaces 08/17/23 07:55:29.941
    STEP: listing a collection of PDBs in namespace disruption-5611 08/17/23 07:55:29.944
    STEP: deleting a collection of PDBs 08/17/23 07:55:29.947
    STEP: Waiting for the PDB collection to be deleted 08/17/23 07:55:30.012
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:30.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:30.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4465" for this suite. 08/17/23 07:55:30.02
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5611" for this suite. 08/17/23 07:55:30.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:30.073
Aug 17 07:55:30.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 07:55:30.074
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:30.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:30.138
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:30.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-838" for this suite. 08/17/23 07:55:30.162
------------------------------
â€¢ [0.102 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:30.073
    Aug 17 07:55:30.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 07:55:30.074
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:30.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:30.138
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:30.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-838" for this suite. 08/17/23 07:55:30.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:30.182
Aug 17 07:55:30.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 07:55:30.183
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:30.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:30.196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 07:55:30.199
Aug 17 07:55:30.223: INFO: Waiting up to 5m0s for pod "pod-a942408f-8566-4a7f-8075-23545c31a88e" in namespace "emptydir-424" to be "Succeeded or Failed"
Aug 17 07:55:30.227: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006734ms
Aug 17 07:55:32.320: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0966821s
Aug 17 07:55:34.230: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006723528s
STEP: Saw pod success 08/17/23 07:55:34.23
Aug 17 07:55:34.230: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e" satisfied condition "Succeeded or Failed"
Aug 17 07:55:34.232: INFO: Trying to get logs from node yst-node2 pod pod-a942408f-8566-4a7f-8075-23545c31a88e container test-container: <nil>
STEP: delete the pod 08/17/23 07:55:34.237
Aug 17 07:55:34.243: INFO: Waiting for pod pod-a942408f-8566-4a7f-8075-23545c31a88e to disappear
Aug 17 07:55:34.245: INFO: Pod pod-a942408f-8566-4a7f-8075-23545c31a88e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:34.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-424" for this suite. 08/17/23 07:55:34.248
------------------------------
â€¢ [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:30.182
    Aug 17 07:55:30.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 07:55:30.183
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:30.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:30.196
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 07:55:30.199
    Aug 17 07:55:30.223: INFO: Waiting up to 5m0s for pod "pod-a942408f-8566-4a7f-8075-23545c31a88e" in namespace "emptydir-424" to be "Succeeded or Failed"
    Aug 17 07:55:30.227: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006734ms
    Aug 17 07:55:32.320: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0966821s
    Aug 17 07:55:34.230: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006723528s
    STEP: Saw pod success 08/17/23 07:55:34.23
    Aug 17 07:55:34.230: INFO: Pod "pod-a942408f-8566-4a7f-8075-23545c31a88e" satisfied condition "Succeeded or Failed"
    Aug 17 07:55:34.232: INFO: Trying to get logs from node yst-node2 pod pod-a942408f-8566-4a7f-8075-23545c31a88e container test-container: <nil>
    STEP: delete the pod 08/17/23 07:55:34.237
    Aug 17 07:55:34.243: INFO: Waiting for pod pod-a942408f-8566-4a7f-8075-23545c31a88e to disappear
    Aug 17 07:55:34.245: INFO: Pod pod-a942408f-8566-4a7f-8075-23545c31a88e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:34.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-424" for this suite. 08/17/23 07:55:34.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:34.252
Aug 17 07:55:34.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename deployment 08/17/23 07:55:34.252
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:34.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:34.289
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 17 07:55:34.298: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 17 07:55:39.302: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 07:55:39.302
Aug 17 07:55:39.302: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/17/23 07:55:39.309
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 07:55:39.315: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8492  8c00911a-26c0-45d8-89c1-436c70a131ac 27066682 1 2023-08-17 07:55:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-17 07:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00502eb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 17 07:55:39.317: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Aug 17 07:55:39.317: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 17 07:55:39.318: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8492  032af221-8557-4ca0-924e-7781c47bf222 27066685 1 2023-08-17 07:55:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8c00911a-26c0-45d8-89c1-436c70a131ac 0xc008cca927 0xc008cca928}] [] [{e2e.test Update apps/v1 2023-08-17 07:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:55:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 07:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8c00911a-26c0-45d8-89c1-436c70a131ac\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008cca9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 07:55:39.320: INFO: Pod "test-cleanup-controller-4nnkr" is available:
&Pod{ObjectMeta:{test-cleanup-controller-4nnkr test-cleanup-controller- deployment-8492  335d2a3b-4199-4940-8345-98a0d208d772 27066653 0 2023-08-17 07:55:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:4005051bbb7db207b950cb2168b45a8c325a2d130bc362cd418c82a1c4ba4c90 cni.projectcalico.org/podIP:172.32.238.112/32 cni.projectcalico.org/podIPs:172.32.238.112/32] [{apps/v1 ReplicaSet test-cleanup-controller 032af221-8557-4ca0-924e-7781c47bf222 0xc008ccacc7 0xc008ccacc8}] [] [{calico Update v1 2023-08-17 07:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"032af221-8557-4ca0-924e-7781c47bf222\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:55:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76s66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76s66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.112,StartTime:2023-08-17 07:55:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:55:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://270c741981a304138e53a39294bbeb5ad7a2eb1b33b0699f9f22e113a21ab248,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:39.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8492" for this suite. 08/17/23 07:55:39.328
------------------------------
â€¢ [SLOW TEST] [5.084 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:34.252
    Aug 17 07:55:34.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename deployment 08/17/23 07:55:34.252
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:34.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:34.289
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 17 07:55:34.298: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 17 07:55:39.302: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 07:55:39.302
    Aug 17 07:55:39.302: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/17/23 07:55:39.309
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 07:55:39.315: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8492  8c00911a-26c0-45d8-89c1-436c70a131ac 27066682 1 2023-08-17 07:55:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-17 07:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00502eb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 17 07:55:39.317: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Aug 17 07:55:39.317: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Aug 17 07:55:39.318: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8492  032af221-8557-4ca0-924e-7781c47bf222 27066685 1 2023-08-17 07:55:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8c00911a-26c0-45d8-89c1-436c70a131ac 0xc008cca927 0xc008cca928}] [] [{e2e.test Update apps/v1 2023-08-17 07:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 07:55:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 07:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8c00911a-26c0-45d8-89c1-436c70a131ac\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008cca9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 07:55:39.320: INFO: Pod "test-cleanup-controller-4nnkr" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-4nnkr test-cleanup-controller- deployment-8492  335d2a3b-4199-4940-8345-98a0d208d772 27066653 0 2023-08-17 07:55:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:4005051bbb7db207b950cb2168b45a8c325a2d130bc362cd418c82a1c4ba4c90 cni.projectcalico.org/podIP:172.32.238.112/32 cni.projectcalico.org/podIPs:172.32.238.112/32] [{apps/v1 ReplicaSet test-cleanup-controller 032af221-8557-4ca0-924e-7781c47bf222 0xc008ccacc7 0xc008ccacc8}] [] [{calico Update v1 2023-08-17 07:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 07:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"032af221-8557-4ca0-924e-7781c47bf222\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 07:55:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.32.238.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76s66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76s66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:yst-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 07:55:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.60.200.177,PodIP:172.32.238.112,StartTime:2023-08-17 07:55:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 07:55:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://270c741981a304138e53a39294bbeb5ad7a2eb1b33b0699f9f22e113a21ab248,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.32.238.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:39.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8492" for this suite. 08/17/23 07:55:39.328
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:39.336
Aug 17 07:55:39.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replicaset 08/17/23 07:55:39.337
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:39.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:39.361
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/17/23 07:55:39.367
Aug 17 07:55:39.379: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 07:55:44.381: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 07:55:44.381
STEP: getting scale subresource 08/17/23 07:55:44.382
STEP: updating a scale subresource 08/17/23 07:55:44.385
STEP: verifying the replicaset Spec.Replicas was modified 08/17/23 07:55:44.389
STEP: Patch a scale subresource 08/17/23 07:55:44.394
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:55:44.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9869" for this suite. 08/17/23 07:55:44.408
------------------------------
â€¢ [SLOW TEST] [5.075 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:39.336
    Aug 17 07:55:39.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replicaset 08/17/23 07:55:39.337
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:39.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:39.361
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/17/23 07:55:39.367
    Aug 17 07:55:39.379: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 07:55:44.381: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 07:55:44.381
    STEP: getting scale subresource 08/17/23 07:55:44.382
    STEP: updating a scale subresource 08/17/23 07:55:44.385
    STEP: verifying the replicaset Spec.Replicas was modified 08/17/23 07:55:44.389
    STEP: Patch a scale subresource 08/17/23 07:55:44.394
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:55:44.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9869" for this suite. 08/17/23 07:55:44.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:55:44.412
Aug 17 07:55:44.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 07:55:44.413
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:44.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:44.431
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 in namespace container-probe-7624 08/17/23 07:55:44.434
Aug 17 07:55:44.442: INFO: Waiting up to 5m0s for pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79" in namespace "container-probe-7624" to be "not pending"
Aug 17 07:55:44.446: INFO: Pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79": Phase="Pending", Reason="", readiness=false. Elapsed: 3.548941ms
Aug 17 07:55:46.449: INFO: Pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79": Phase="Running", Reason="", readiness=true. Elapsed: 2.006891363s
Aug 17 07:55:46.449: INFO: Pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79" satisfied condition "not pending"
Aug 17 07:55:46.449: INFO: Started pod busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 in namespace container-probe-7624
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:55:46.449
Aug 17 07:55:46.451: INFO: Initial restart count of pod busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 is 0
Aug 17 07:56:36.535: INFO: Restart count of pod container-probe-7624/busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 is now 1 (50.083951886s elapsed)
STEP: deleting the pod 08/17/23 07:56:36.535
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 07:56:36.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7624" for this suite. 08/17/23 07:56:36.544
------------------------------
â€¢ [SLOW TEST] [52.135 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:55:44.412
    Aug 17 07:55:44.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 07:55:44.413
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:55:44.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:55:44.431
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 in namespace container-probe-7624 08/17/23 07:55:44.434
    Aug 17 07:55:44.442: INFO: Waiting up to 5m0s for pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79" in namespace "container-probe-7624" to be "not pending"
    Aug 17 07:55:44.446: INFO: Pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79": Phase="Pending", Reason="", readiness=false. Elapsed: 3.548941ms
    Aug 17 07:55:46.449: INFO: Pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79": Phase="Running", Reason="", readiness=true. Elapsed: 2.006891363s
    Aug 17 07:55:46.449: INFO: Pod "busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79" satisfied condition "not pending"
    Aug 17 07:55:46.449: INFO: Started pod busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 in namespace container-probe-7624
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:55:46.449
    Aug 17 07:55:46.451: INFO: Initial restart count of pod busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 is 0
    Aug 17 07:56:36.535: INFO: Restart count of pod container-probe-7624/busybox-877e1fbc-7f9a-4f38-9ede-29789ad49c79 is now 1 (50.083951886s elapsed)
    STEP: deleting the pod 08/17/23 07:56:36.535
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:56:36.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7624" for this suite. 08/17/23 07:56:36.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:56:36.548
Aug 17 07:56:36.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename namespaces 08/17/23 07:56:36.548
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:56:36.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:56:36.557
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/17/23 07:56:36.587
STEP: patching the Namespace 08/17/23 07:56:36.607
STEP: get the Namespace and ensuring it has the label 08/17/23 07:56:36.622
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:56:36.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6329" for this suite. 08/17/23 07:56:36.639
STEP: Destroying namespace "nspatchtest-976684b5-de07-49b6-90b3-b4a7c7b5ff55-4492" for this suite. 08/17/23 07:56:36.643
------------------------------
â€¢ [0.099 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:56:36.548
    Aug 17 07:56:36.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename namespaces 08/17/23 07:56:36.548
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:56:36.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:56:36.557
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/17/23 07:56:36.587
    STEP: patching the Namespace 08/17/23 07:56:36.607
    STEP: get the Namespace and ensuring it has the label 08/17/23 07:56:36.622
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:56:36.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6329" for this suite. 08/17/23 07:56:36.639
    STEP: Destroying namespace "nspatchtest-976684b5-de07-49b6-90b3-b4a7c7b5ff55-4492" for this suite. 08/17/23 07:56:36.643
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:56:36.646
Aug 17 07:56:36.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-preemption 08/17/23 07:56:36.647
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:56:36.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:56:36.662
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 07:56:36.679: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 07:57:36.725: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:57:36.727
Aug 17 07:57:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 07:57:36.727
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:57:36.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:57:36.736
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/17/23 07:57:36.739
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 07:57:36.739
Aug 17 07:57:36.756: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6185" to be "running"
Aug 17 07:57:36.759: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281708ms
Aug 17 07:57:38.763: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006660468s
Aug 17 07:57:38.763: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 07:57:38.773
Aug 17 07:57:38.801: INFO: found a healthy node: yst-node2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 17 07:57:44.851: INFO: pods created so far: [1 1 1]
Aug 17 07:57:44.851: INFO: length of pods created so far: 3
Aug 17 07:57:46.857: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 17 07:57:53.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:57:53.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6185" for this suite. 08/17/23 07:57:53.905
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-18" for this suite. 08/17/23 07:57:53.908
------------------------------
â€¢ [SLOW TEST] [77.264 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:56:36.646
    Aug 17 07:56:36.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 07:56:36.647
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:56:36.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:56:36.662
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 07:56:36.679: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 07:57:36.725: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:57:36.727
    Aug 17 07:57:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 07:57:36.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:57:36.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:57:36.736
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/17/23 07:57:36.739
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 07:57:36.739
    Aug 17 07:57:36.756: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6185" to be "running"
    Aug 17 07:57:36.759: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281708ms
    Aug 17 07:57:38.763: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006660468s
    Aug 17 07:57:38.763: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 07:57:38.773
    Aug 17 07:57:38.801: INFO: found a healthy node: yst-node2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 17 07:57:44.851: INFO: pods created so far: [1 1 1]
    Aug 17 07:57:44.851: INFO: length of pods created so far: 3
    Aug 17 07:57:46.857: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:57:53.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:57:53.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6185" for this suite. 08/17/23 07:57:53.905
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-18" for this suite. 08/17/23 07:57:53.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:57:53.911
Aug 17 07:57:53.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 07:57:53.912
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:57:53.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:57:53.921
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-8aaf0fcc-ac19-4f90-a85b-6d2bf748a3a5 08/17/23 07:57:53.925
STEP: Creating a pod to test consume secrets 08/17/23 07:57:53.929
Aug 17 07:57:53.942: INFO: Waiting up to 5m0s for pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c" in namespace "secrets-77" to be "Succeeded or Failed"
Aug 17 07:57:53.944: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.393938ms
Aug 17 07:57:55.948: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005943689s
Aug 17 07:57:57.947: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004599955s
STEP: Saw pod success 08/17/23 07:57:57.947
Aug 17 07:57:57.947: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c" satisfied condition "Succeeded or Failed"
Aug 17 07:57:57.948: INFO: Trying to get logs from node yst-node2 pod pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 07:57:57.958
Aug 17 07:57:57.964: INFO: Waiting for pod pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c to disappear
Aug 17 07:57:57.965: INFO: Pod pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 07:57:57.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-77" for this suite. 08/17/23 07:57:57.968
------------------------------
â€¢ [4.060 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:57:53.911
    Aug 17 07:57:53.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 07:57:53.912
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:57:53.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:57:53.921
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-8aaf0fcc-ac19-4f90-a85b-6d2bf748a3a5 08/17/23 07:57:53.925
    STEP: Creating a pod to test consume secrets 08/17/23 07:57:53.929
    Aug 17 07:57:53.942: INFO: Waiting up to 5m0s for pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c" in namespace "secrets-77" to be "Succeeded or Failed"
    Aug 17 07:57:53.944: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.393938ms
    Aug 17 07:57:55.948: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005943689s
    Aug 17 07:57:57.947: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004599955s
    STEP: Saw pod success 08/17/23 07:57:57.947
    Aug 17 07:57:57.947: INFO: Pod "pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c" satisfied condition "Succeeded or Failed"
    Aug 17 07:57:57.948: INFO: Trying to get logs from node yst-node2 pod pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:57:57.958
    Aug 17 07:57:57.964: INFO: Waiting for pod pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c to disappear
    Aug 17 07:57:57.965: INFO: Pod pod-secrets-e1bcbe24-1642-4b23-ba27-5fd60e8b0d8c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:57:57.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-77" for this suite. 08/17/23 07:57:57.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:57:57.972
Aug 17 07:57:57.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 07:57:57.973
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:57:57.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:57:57.988
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/17/23 07:57:57.99
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/17/23 07:57:57.998
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/17/23 07:57:57.998
STEP: creating a pod to probe DNS 08/17/23 07:57:57.998
STEP: submitting the pod to kubernetes 08/17/23 07:57:57.998
Aug 17 07:57:58.007: INFO: Waiting up to 15m0s for pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914" in namespace "dns-5127" to be "running"
Aug 17 07:57:58.008: INFO: Pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428624ms
Aug 17 07:58:00.010: INFO: Pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914": Phase="Running", Reason="", readiness=true. Elapsed: 2.003437523s
Aug 17 07:58:00.010: INFO: Pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914" satisfied condition "running"
STEP: retrieving the pod 08/17/23 07:58:00.01
STEP: looking for the results for each expected name from probers 08/17/23 07:58:00.012
Aug 17 07:58:00.019: INFO: DNS probes using dns-5127/dns-test-edd39168-a266-408f-8142-4c0a8d36a914 succeeded

STEP: deleting the pod 08/17/23 07:58:00.019
STEP: deleting the test headless service 08/17/23 07:58:00.025
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:00.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5127" for this suite. 08/17/23 07:58:00.037
------------------------------
â€¢ [2.071 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:57:57.972
    Aug 17 07:57:57.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 07:57:57.973
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:57:57.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:57:57.988
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/17/23 07:57:57.99
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/17/23 07:57:57.998
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5127.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/17/23 07:57:57.998
    STEP: creating a pod to probe DNS 08/17/23 07:57:57.998
    STEP: submitting the pod to kubernetes 08/17/23 07:57:57.998
    Aug 17 07:57:58.007: INFO: Waiting up to 15m0s for pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914" in namespace "dns-5127" to be "running"
    Aug 17 07:57:58.008: INFO: Pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428624ms
    Aug 17 07:58:00.010: INFO: Pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914": Phase="Running", Reason="", readiness=true. Elapsed: 2.003437523s
    Aug 17 07:58:00.010: INFO: Pod "dns-test-edd39168-a266-408f-8142-4c0a8d36a914" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 07:58:00.01
    STEP: looking for the results for each expected name from probers 08/17/23 07:58:00.012
    Aug 17 07:58:00.019: INFO: DNS probes using dns-5127/dns-test-edd39168-a266-408f-8142-4c0a8d36a914 succeeded

    STEP: deleting the pod 08/17/23 07:58:00.019
    STEP: deleting the test headless service 08/17/23 07:58:00.025
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:00.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5127" for this suite. 08/17/23 07:58:00.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:00.043
Aug 17 07:58:00.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:58:00.044
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:00.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:00.065
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:00.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5990" for this suite. 08/17/23 07:58:00.097
------------------------------
â€¢ [0.060 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:00.043
    Aug 17 07:58:00.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:58:00.044
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:00.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:00.065
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:00.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5990" for this suite. 08/17/23 07:58:00.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:00.103
Aug 17 07:58:00.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 07:58:00.104
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:00.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:00.119
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/17/23 07:58:00.123
Aug 17 07:58:00.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3993 api-versions'
Aug 17 07:58:00.205: INFO: stderr: ""
Aug 17 07:58:00.205: INFO: stdout: "activator.accordions.co.kr/v1beta1\nadmissionregistration.k8s.io/v1\nalert.accordions.co.kr/v1beta1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nargoproj.io/v1alpha1\nauth.accordions.co.kr/v1beta1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.alibabacloud.com/v1beta1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncicd.accordions.co.kr/v1beta1\ncluster.accordions.co.kr/v1beta1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndiscovery.k8s.io/v1\nevent.accordions.co.kr/v1beta1\nevents.k8s.io/v1\nextensions.istio.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.accordions.co.kr/v1beta1\nkeycloak.accordions.co.kr/v1beta1\nlog.accordions.co.kr/v1beta1\nmanagement.accordions.co.kr/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.accordions.co.kr/v1\nmonitoring.accordions.co.kr/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.istio.io/v1alpha3\nnetworking.istio.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsecurity.istio.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntelemetry.istio.io/v1alpha1\ntoken.accordions.co.kr/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:00.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3993" for this suite. 08/17/23 07:58:00.209
------------------------------
â€¢ [0.111 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:00.103
    Aug 17 07:58:00.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 07:58:00.104
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:00.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:00.119
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/17/23 07:58:00.123
    Aug 17 07:58:00.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3993 api-versions'
    Aug 17 07:58:00.205: INFO: stderr: ""
    Aug 17 07:58:00.205: INFO: stdout: "activator.accordions.co.kr/v1beta1\nadmissionregistration.k8s.io/v1\nalert.accordions.co.kr/v1beta1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nargoproj.io/v1alpha1\nauth.accordions.co.kr/v1beta1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.alibabacloud.com/v1beta1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncicd.accordions.co.kr/v1beta1\ncluster.accordions.co.kr/v1beta1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndiscovery.k8s.io/v1\nevent.accordions.co.kr/v1beta1\nevents.k8s.io/v1\nextensions.istio.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.accordions.co.kr/v1beta1\nkeycloak.accordions.co.kr/v1beta1\nlog.accordions.co.kr/v1beta1\nmanagement.accordions.co.kr/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.accordions.co.kr/v1\nmonitoring.accordions.co.kr/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.istio.io/v1alpha3\nnetworking.istio.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsecurity.istio.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntelemetry.istio.io/v1alpha1\ntoken.accordions.co.kr/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:00.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3993" for this suite. 08/17/23 07:58:00.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:00.214
Aug 17 07:58:00.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 07:58:00.215
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:00.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:00.238
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 17 07:58:00.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: creating the pod 08/17/23 07:58:00.241
STEP: submitting the pod to kubernetes 08/17/23 07:58:00.241
Aug 17 07:58:00.245: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f" in namespace "pods-6884" to be "running and ready"
Aug 17 07:58:00.247: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404951ms
Aug 17 07:58:00.247: INFO: The phase of Pod pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:58:02.249: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004525898s
Aug 17 07:58:02.249: INFO: The phase of Pod pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:58:04.250: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f": Phase="Running", Reason="", readiness=true. Elapsed: 4.005423415s
Aug 17 07:58:04.250: INFO: The phase of Pod pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f is Running (Ready = true)
Aug 17 07:58:04.250: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:04.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6884" for this suite. 08/17/23 07:58:04.26
------------------------------
â€¢ [4.049 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:00.214
    Aug 17 07:58:00.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 07:58:00.215
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:00.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:00.238
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 17 07:58:00.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: creating the pod 08/17/23 07:58:00.241
    STEP: submitting the pod to kubernetes 08/17/23 07:58:00.241
    Aug 17 07:58:00.245: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f" in namespace "pods-6884" to be "running and ready"
    Aug 17 07:58:00.247: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404951ms
    Aug 17 07:58:00.247: INFO: The phase of Pod pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:58:02.249: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004525898s
    Aug 17 07:58:02.249: INFO: The phase of Pod pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:58:04.250: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f": Phase="Running", Reason="", readiness=true. Elapsed: 4.005423415s
    Aug 17 07:58:04.250: INFO: The phase of Pod pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f is Running (Ready = true)
    Aug 17 07:58:04.250: INFO: Pod "pod-logs-websocket-fd92879d-a236-4596-88d4-8069d90b212f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:04.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6884" for this suite. 08/17/23 07:58:04.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:04.264
Aug 17 07:58:04.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 07:58:04.265
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:04.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:04.273
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/17/23 07:58:04.279
STEP: submitting the pod to kubernetes 08/17/23 07:58:04.28
Aug 17 07:58:04.303: INFO: Waiting up to 5m0s for pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" in namespace "pods-6009" to be "running and ready"
Aug 17 07:58:04.313: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.810322ms
Aug 17 07:58:04.313: INFO: The phase of Pod pod-update-27223882-5558-423c-b366-a065b6996d4d is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:58:06.316: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013171237s
Aug 17 07:58:06.316: INFO: The phase of Pod pod-update-27223882-5558-423c-b366-a065b6996d4d is Running (Ready = true)
Aug 17 07:58:06.316: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/17/23 07:58:06.318
STEP: updating the pod 08/17/23 07:58:06.32
Aug 17 07:58:06.827: INFO: Successfully updated pod "pod-update-27223882-5558-423c-b366-a065b6996d4d"
Aug 17 07:58:06.827: INFO: Waiting up to 5m0s for pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" in namespace "pods-6009" to be "running"
Aug 17 07:58:06.830: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.294481ms
Aug 17 07:58:06.830: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/17/23 07:58:06.83
Aug 17 07:58:06.831: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:06.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6009" for this suite. 08/17/23 07:58:06.834
------------------------------
â€¢ [2.574 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:04.264
    Aug 17 07:58:04.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 07:58:04.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:04.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:04.273
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/17/23 07:58:04.279
    STEP: submitting the pod to kubernetes 08/17/23 07:58:04.28
    Aug 17 07:58:04.303: INFO: Waiting up to 5m0s for pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" in namespace "pods-6009" to be "running and ready"
    Aug 17 07:58:04.313: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.810322ms
    Aug 17 07:58:04.313: INFO: The phase of Pod pod-update-27223882-5558-423c-b366-a065b6996d4d is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:58:06.316: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013171237s
    Aug 17 07:58:06.316: INFO: The phase of Pod pod-update-27223882-5558-423c-b366-a065b6996d4d is Running (Ready = true)
    Aug 17 07:58:06.316: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/17/23 07:58:06.318
    STEP: updating the pod 08/17/23 07:58:06.32
    Aug 17 07:58:06.827: INFO: Successfully updated pod "pod-update-27223882-5558-423c-b366-a065b6996d4d"
    Aug 17 07:58:06.827: INFO: Waiting up to 5m0s for pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" in namespace "pods-6009" to be "running"
    Aug 17 07:58:06.830: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.294481ms
    Aug 17 07:58:06.830: INFO: Pod "pod-update-27223882-5558-423c-b366-a065b6996d4d" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/17/23 07:58:06.83
    Aug 17 07:58:06.831: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:06.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6009" for this suite. 08/17/23 07:58:06.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:06.838
Aug 17 07:58:06.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename downward-api 08/17/23 07:58:06.839
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:06.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:06.856
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/17/23 07:58:06.859
Aug 17 07:58:06.869: INFO: Waiting up to 5m0s for pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7" in namespace "downward-api-3995" to be "Succeeded or Failed"
Aug 17 07:58:06.873: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961047ms
Aug 17 07:58:08.876: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007270993s
Aug 17 07:58:10.875: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005903261s
STEP: Saw pod success 08/17/23 07:58:10.875
Aug 17 07:58:10.875: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7" satisfied condition "Succeeded or Failed"
Aug 17 07:58:10.876: INFO: Trying to get logs from node yst-node2 pod downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7 container dapi-container: <nil>
STEP: delete the pod 08/17/23 07:58:10.88
Aug 17 07:58:10.885: INFO: Waiting for pod downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7 to disappear
Aug 17 07:58:10.887: INFO: Pod downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:10.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3995" for this suite. 08/17/23 07:58:10.889
------------------------------
â€¢ [4.054 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:06.838
    Aug 17 07:58:06.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename downward-api 08/17/23 07:58:06.839
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:06.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:06.856
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/17/23 07:58:06.859
    Aug 17 07:58:06.869: INFO: Waiting up to 5m0s for pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7" in namespace "downward-api-3995" to be "Succeeded or Failed"
    Aug 17 07:58:06.873: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961047ms
    Aug 17 07:58:08.876: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007270993s
    Aug 17 07:58:10.875: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005903261s
    STEP: Saw pod success 08/17/23 07:58:10.875
    Aug 17 07:58:10.875: INFO: Pod "downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7" satisfied condition "Succeeded or Failed"
    Aug 17 07:58:10.876: INFO: Trying to get logs from node yst-node2 pod downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 07:58:10.88
    Aug 17 07:58:10.885: INFO: Waiting for pod downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7 to disappear
    Aug 17 07:58:10.887: INFO: Pod downward-api-d6691dfc-df81-484e-9cca-b39d51ceb5e7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:10.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3995" for this suite. 08/17/23 07:58:10.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:10.892
Aug 17 07:58:10.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename limitrange 08/17/23 07:58:10.893
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:10.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:10.905
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-w4t79" in namespace "limitrange-2966" 08/17/23 07:58:10.908
STEP: Creating another limitRange in another namespace 08/17/23 07:58:10.911
Aug 17 07:58:10.927: INFO: Namespace "e2e-limitrange-w4t79-4454" created
Aug 17 07:58:10.927: INFO: Creating LimitRange "e2e-limitrange-w4t79" in namespace "e2e-limitrange-w4t79-4454"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-w4t79" 08/17/23 07:58:10.93
Aug 17 07:58:10.941: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-w4t79" in "limitrange-2966" namespace 08/17/23 07:58:10.942
Aug 17 07:58:10.948: INFO: LimitRange "e2e-limitrange-w4t79" has been patched
STEP: Delete LimitRange "e2e-limitrange-w4t79" by Collection with labelSelector: "e2e-limitrange-w4t79=patched" 08/17/23 07:58:10.948
STEP: Confirm that the limitRange "e2e-limitrange-w4t79" has been deleted 08/17/23 07:58:10.956
Aug 17 07:58:10.956: INFO: Requesting list of LimitRange to confirm quantity
Aug 17 07:58:10.958: INFO: Found 0 LimitRange with label "e2e-limitrange-w4t79=patched"
Aug 17 07:58:10.958: INFO: LimitRange "e2e-limitrange-w4t79" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-w4t79" 08/17/23 07:58:10.958
Aug 17 07:58:10.959: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2966" for this suite. 08/17/23 07:58:10.962
STEP: Destroying namespace "e2e-limitrange-w4t79-4454" for this suite. 08/17/23 07:58:10.965
------------------------------
â€¢ [0.076 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:10.892
    Aug 17 07:58:10.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename limitrange 08/17/23 07:58:10.893
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:10.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:10.905
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-w4t79" in namespace "limitrange-2966" 08/17/23 07:58:10.908
    STEP: Creating another limitRange in another namespace 08/17/23 07:58:10.911
    Aug 17 07:58:10.927: INFO: Namespace "e2e-limitrange-w4t79-4454" created
    Aug 17 07:58:10.927: INFO: Creating LimitRange "e2e-limitrange-w4t79" in namespace "e2e-limitrange-w4t79-4454"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-w4t79" 08/17/23 07:58:10.93
    Aug 17 07:58:10.941: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-w4t79" in "limitrange-2966" namespace 08/17/23 07:58:10.942
    Aug 17 07:58:10.948: INFO: LimitRange "e2e-limitrange-w4t79" has been patched
    STEP: Delete LimitRange "e2e-limitrange-w4t79" by Collection with labelSelector: "e2e-limitrange-w4t79=patched" 08/17/23 07:58:10.948
    STEP: Confirm that the limitRange "e2e-limitrange-w4t79" has been deleted 08/17/23 07:58:10.956
    Aug 17 07:58:10.956: INFO: Requesting list of LimitRange to confirm quantity
    Aug 17 07:58:10.958: INFO: Found 0 LimitRange with label "e2e-limitrange-w4t79=patched"
    Aug 17 07:58:10.958: INFO: LimitRange "e2e-limitrange-w4t79" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-w4t79" 08/17/23 07:58:10.958
    Aug 17 07:58:10.959: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2966" for this suite. 08/17/23 07:58:10.962
    STEP: Destroying namespace "e2e-limitrange-w4t79-4454" for this suite. 08/17/23 07:58:10.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:10.969
Aug 17 07:58:10.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename watch 08/17/23 07:58:10.969
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:10.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:10.982
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/17/23 07:58:10.986
STEP: creating a new configmap 08/17/23 07:58:10.987
STEP: modifying the configmap once 08/17/23 07:58:10.991
STEP: changing the label value of the configmap 08/17/23 07:58:11
STEP: Expecting to observe a delete notification for the watched object 08/17/23 07:58:11.008
Aug 17 07:58:11.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069386 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:58:11.008: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069390 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:58:11.008: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069394 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/17/23 07:58:11.008
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/17/23 07:58:11.013
STEP: changing the label value of the configmap back 08/17/23 07:58:21.014
STEP: modifying the configmap a third time 08/17/23 07:58:21.019
STEP: deleting the configmap 08/17/23 07:58:21.023
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/17/23 07:58:21.026
Aug 17 07:58:21.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069674 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:58:21.026: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069675 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 07:58:21.026: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069676 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:21.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4278" for this suite. 08/17/23 07:58:21.029
------------------------------
â€¢ [SLOW TEST] [10.064 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:10.969
    Aug 17 07:58:10.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename watch 08/17/23 07:58:10.969
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:10.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:10.982
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/17/23 07:58:10.986
    STEP: creating a new configmap 08/17/23 07:58:10.987
    STEP: modifying the configmap once 08/17/23 07:58:10.991
    STEP: changing the label value of the configmap 08/17/23 07:58:11
    STEP: Expecting to observe a delete notification for the watched object 08/17/23 07:58:11.008
    Aug 17 07:58:11.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069386 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:58:11.008: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069390 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:58:11.008: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069394 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/17/23 07:58:11.008
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/17/23 07:58:11.013
    STEP: changing the label value of the configmap back 08/17/23 07:58:21.014
    STEP: modifying the configmap a third time 08/17/23 07:58:21.019
    STEP: deleting the configmap 08/17/23 07:58:21.023
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/17/23 07:58:21.026
    Aug 17 07:58:21.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069674 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:58:21.026: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069675 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 07:58:21.026: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4278  0cfec254-ed12-4adf-8cf7-78fc5524c196 27069676 0 2023-08-17 07:58:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 07:58:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:21.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4278" for this suite. 08/17/23 07:58:21.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:21.033
Aug 17 07:58:21.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 07:58:21.034
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:21.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:21.052
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-ea898b72-a6aa-4aef-a5c4-e8eabeb66564 08/17/23 07:58:21.055
STEP: Creating a pod to test consume secrets 08/17/23 07:58:21.06
Aug 17 07:58:21.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0" in namespace "projected-5881" to be "Succeeded or Failed"
Aug 17 07:58:21.068: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.629127ms
Aug 17 07:58:23.072: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005070714s
Aug 17 07:58:25.071: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004117128s
STEP: Saw pod success 08/17/23 07:58:25.071
Aug 17 07:58:25.071: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0" satisfied condition "Succeeded or Failed"
Aug 17 07:58:25.072: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 07:58:25.076
Aug 17 07:58:25.109: INFO: Waiting for pod pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0 to disappear
Aug 17 07:58:25.111: INFO: Pod pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:25.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5881" for this suite. 08/17/23 07:58:25.114
------------------------------
â€¢ [4.083 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:21.033
    Aug 17 07:58:21.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 07:58:21.034
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:21.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:21.052
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-ea898b72-a6aa-4aef-a5c4-e8eabeb66564 08/17/23 07:58:21.055
    STEP: Creating a pod to test consume secrets 08/17/23 07:58:21.06
    Aug 17 07:58:21.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0" in namespace "projected-5881" to be "Succeeded or Failed"
    Aug 17 07:58:21.068: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.629127ms
    Aug 17 07:58:23.072: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005070714s
    Aug 17 07:58:25.071: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004117128s
    STEP: Saw pod success 08/17/23 07:58:25.071
    Aug 17 07:58:25.071: INFO: Pod "pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0" satisfied condition "Succeeded or Failed"
    Aug 17 07:58:25.072: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 07:58:25.076
    Aug 17 07:58:25.109: INFO: Waiting for pod pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0 to disappear
    Aug 17 07:58:25.111: INFO: Pod pod-projected-secrets-a6866412-3791-418e-b552-16e80e2f88a0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:25.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5881" for this suite. 08/17/23 07:58:25.114
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:25.117
Aug 17 07:58:25.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:58:25.118
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:25.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:25.146
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 17 07:58:25.161: INFO: Waiting up to 5m0s for pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e" in namespace "kubelet-test-5601" to be "running and ready"
Aug 17 07:58:25.163: INFO: Pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09396ms
Aug 17 07:58:25.163: INFO: The phase of Pod busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:58:27.166: INFO: Pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004841635s
Aug 17 07:58:27.166: INFO: The phase of Pod busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e is Running (Ready = true)
Aug 17 07:58:27.166: INFO: Pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:27.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5601" for this suite. 08/17/23 07:58:27.174
------------------------------
â€¢ [2.061 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:25.117
    Aug 17 07:58:25.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 07:58:25.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:25.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:25.146
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 17 07:58:25.161: INFO: Waiting up to 5m0s for pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e" in namespace "kubelet-test-5601" to be "running and ready"
    Aug 17 07:58:25.163: INFO: Pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09396ms
    Aug 17 07:58:25.163: INFO: The phase of Pod busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:58:27.166: INFO: Pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004841635s
    Aug 17 07:58:27.166: INFO: The phase of Pod busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e is Running (Ready = true)
    Aug 17 07:58:27.166: INFO: Pod "busybox-readonly-fseebe28f8-0c08-489e-920e-ff3a2267413e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:27.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5601" for this suite. 08/17/23 07:58:27.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:27.179
Aug 17 07:58:27.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:58:27.18
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:27.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:27.201
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/17/23 07:58:27.204
Aug 17 07:58:27.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/17/23 07:58:36.302
Aug 17 07:58:36.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 07:58:38.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:46.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9884" for this suite. 08/17/23 07:58:47.011
------------------------------
â€¢ [SLOW TEST] [19.835 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:27.179
    Aug 17 07:58:27.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 07:58:27.18
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:27.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:27.201
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/17/23 07:58:27.204
    Aug 17 07:58:27.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/17/23 07:58:36.302
    Aug 17 07:58:36.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 07:58:38.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:46.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9884" for this suite. 08/17/23 07:58:47.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:47.015
Aug 17 07:58:47.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename limitrange 08/17/23 07:58:47.016
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:47.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:47.03
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/17/23 07:58:47.034
STEP: Setting up watch 08/17/23 07:58:47.034
STEP: Submitting a LimitRange 08/17/23 07:58:47.157
STEP: Verifying LimitRange creation was observed 08/17/23 07:58:47.16
STEP: Fetching the LimitRange to ensure it has proper values 08/17/23 07:58:47.16
Aug 17 07:58:47.162: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 17 07:58:47.162: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/17/23 07:58:47.162
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/17/23 07:58:47.165
Aug 17 07:58:47.169: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 17 07:58:47.169: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/17/23 07:58:47.169
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/17/23 07:58:47.172
Aug 17 07:58:47.175: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 17 07:58:47.175: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/17/23 07:58:47.175
STEP: Failing to create a Pod with more than max resources 08/17/23 07:58:47.177
STEP: Updating a LimitRange 08/17/23 07:58:47.179
STEP: Verifying LimitRange updating is effective 08/17/23 07:58:47.185
STEP: Creating a Pod with less than former min resources 08/17/23 07:58:49.188
STEP: Failing to create a Pod with more than max resources 08/17/23 07:58:49.19
STEP: Deleting a LimitRange 08/17/23 07:58:49.193
STEP: Verifying the LimitRange was deleted 08/17/23 07:58:49.197
Aug 17 07:58:54.200: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/17/23 07:58:54.2
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 17 07:58:54.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-9550" for this suite. 08/17/23 07:58:54.207
------------------------------
â€¢ [SLOW TEST] [7.196 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:47.015
    Aug 17 07:58:47.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename limitrange 08/17/23 07:58:47.016
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:47.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:47.03
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/17/23 07:58:47.034
    STEP: Setting up watch 08/17/23 07:58:47.034
    STEP: Submitting a LimitRange 08/17/23 07:58:47.157
    STEP: Verifying LimitRange creation was observed 08/17/23 07:58:47.16
    STEP: Fetching the LimitRange to ensure it has proper values 08/17/23 07:58:47.16
    Aug 17 07:58:47.162: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 17 07:58:47.162: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/17/23 07:58:47.162
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/17/23 07:58:47.165
    Aug 17 07:58:47.169: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 17 07:58:47.169: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/17/23 07:58:47.169
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/17/23 07:58:47.172
    Aug 17 07:58:47.175: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 17 07:58:47.175: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/17/23 07:58:47.175
    STEP: Failing to create a Pod with more than max resources 08/17/23 07:58:47.177
    STEP: Updating a LimitRange 08/17/23 07:58:47.179
    STEP: Verifying LimitRange updating is effective 08/17/23 07:58:47.185
    STEP: Creating a Pod with less than former min resources 08/17/23 07:58:49.188
    STEP: Failing to create a Pod with more than max resources 08/17/23 07:58:49.19
    STEP: Deleting a LimitRange 08/17/23 07:58:49.193
    STEP: Verifying the LimitRange was deleted 08/17/23 07:58:49.197
    Aug 17 07:58:54.200: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/17/23 07:58:54.2
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:58:54.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-9550" for this suite. 08/17/23 07:58:54.207
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:58:54.212
Aug 17 07:58:54.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 07:58:54.212
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:54.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:54.226
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-a740d769-9979-4776-a9b6-fbdf58115068 in namespace container-probe-8391 08/17/23 07:58:54.228
Aug 17 07:58:54.242: INFO: Waiting up to 5m0s for pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068" in namespace "container-probe-8391" to be "not pending"
Aug 17 07:58:54.244: INFO: Pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949336ms
Aug 17 07:58:56.247: INFO: Pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068": Phase="Running", Reason="", readiness=true. Elapsed: 2.005174723s
Aug 17 07:58:56.247: INFO: Pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068" satisfied condition "not pending"
Aug 17 07:58:56.247: INFO: Started pod liveness-a740d769-9979-4776-a9b6-fbdf58115068 in namespace container-probe-8391
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:58:56.247
Aug 17 07:58:56.249: INFO: Initial restart count of pod liveness-a740d769-9979-4776-a9b6-fbdf58115068 is 0
Aug 17 07:59:16.281: INFO: Restart count of pod container-probe-8391/liveness-a740d769-9979-4776-a9b6-fbdf58115068 is now 1 (20.031536608s elapsed)
STEP: deleting the pod 08/17/23 07:59:16.281
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 07:59:16.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8391" for this suite. 08/17/23 07:59:16.289
------------------------------
â€¢ [SLOW TEST] [22.080 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:58:54.212
    Aug 17 07:58:54.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 07:58:54.212
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:58:54.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:58:54.226
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-a740d769-9979-4776-a9b6-fbdf58115068 in namespace container-probe-8391 08/17/23 07:58:54.228
    Aug 17 07:58:54.242: INFO: Waiting up to 5m0s for pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068" in namespace "container-probe-8391" to be "not pending"
    Aug 17 07:58:54.244: INFO: Pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949336ms
    Aug 17 07:58:56.247: INFO: Pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068": Phase="Running", Reason="", readiness=true. Elapsed: 2.005174723s
    Aug 17 07:58:56.247: INFO: Pod "liveness-a740d769-9979-4776-a9b6-fbdf58115068" satisfied condition "not pending"
    Aug 17 07:58:56.247: INFO: Started pod liveness-a740d769-9979-4776-a9b6-fbdf58115068 in namespace container-probe-8391
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 07:58:56.247
    Aug 17 07:58:56.249: INFO: Initial restart count of pod liveness-a740d769-9979-4776-a9b6-fbdf58115068 is 0
    Aug 17 07:59:16.281: INFO: Restart count of pod container-probe-8391/liveness-a740d769-9979-4776-a9b6-fbdf58115068 is now 1 (20.031536608s elapsed)
    STEP: deleting the pod 08/17/23 07:59:16.281
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:59:16.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8391" for this suite. 08/17/23 07:59:16.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:59:16.292
Aug 17 07:59:16.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 07:59:16.292
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:59:16.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:59:16.306
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 17 07:59:16.323: INFO: Waiting up to 5m0s for pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525" in namespace "container-probe-1118" to be "running and ready"
Aug 17 07:59:16.326: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Pending", Reason="", readiness=false. Elapsed: 3.187955ms
Aug 17 07:59:16.327: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 07:59:18.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 2.005401097s
Aug 17 07:59:18.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:20.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 4.005649556s
Aug 17 07:59:20.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:22.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 6.005754798s
Aug 17 07:59:22.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:24.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 8.005777969s
Aug 17 07:59:24.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:26.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 10.005972275s
Aug 17 07:59:26.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:28.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 12.005602062s
Aug 17 07:59:28.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:30.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 14.005921954s
Aug 17 07:59:30.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:32.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 16.005689844s
Aug 17 07:59:32.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:34.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 18.005595481s
Aug 17 07:59:34.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:36.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 20.005684722s
Aug 17 07:59:36.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
Aug 17 07:59:38.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=true. Elapsed: 22.005820729s
Aug 17 07:59:38.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = true)
Aug 17 07:59:38.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525" satisfied condition "running and ready"
Aug 17 07:59:38.331: INFO: Container started at 2023-08-17 07:59:17 +0000 UTC, pod became ready at 2023-08-17 07:59:36 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 07:59:38.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1118" for this suite. 08/17/23 07:59:38.333
------------------------------
â€¢ [SLOW TEST] [22.045 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:59:16.292
    Aug 17 07:59:16.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 07:59:16.292
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:59:16.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:59:16.306
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 17 07:59:16.323: INFO: Waiting up to 5m0s for pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525" in namespace "container-probe-1118" to be "running and ready"
    Aug 17 07:59:16.326: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Pending", Reason="", readiness=false. Elapsed: 3.187955ms
    Aug 17 07:59:16.327: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 07:59:18.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 2.005401097s
    Aug 17 07:59:18.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:20.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 4.005649556s
    Aug 17 07:59:20.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:22.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 6.005754798s
    Aug 17 07:59:22.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:24.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 8.005777969s
    Aug 17 07:59:24.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:26.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 10.005972275s
    Aug 17 07:59:26.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:28.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 12.005602062s
    Aug 17 07:59:28.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:30.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 14.005921954s
    Aug 17 07:59:30.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:32.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 16.005689844s
    Aug 17 07:59:32.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:34.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 18.005595481s
    Aug 17 07:59:34.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:36.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=false. Elapsed: 20.005684722s
    Aug 17 07:59:36.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = false)
    Aug 17 07:59:38.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525": Phase="Running", Reason="", readiness=true. Elapsed: 22.005820729s
    Aug 17 07:59:38.329: INFO: The phase of Pod test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525 is Running (Ready = true)
    Aug 17 07:59:38.329: INFO: Pod "test-webserver-49556f00-7beb-4e5d-9d1d-80324083a525" satisfied condition "running and ready"
    Aug 17 07:59:38.331: INFO: Container started at 2023-08-17 07:59:17 +0000 UTC, pod became ready at 2023-08-17 07:59:36 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 07:59:38.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1118" for this suite. 08/17/23 07:59:38.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 07:59:38.337
Aug 17 07:59:38.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 07:59:38.338
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:59:38.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:59:38.405
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2437 08/17/23 07:59:38.407
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/17/23 07:59:38.411
Aug 17 07:59:38.416: INFO: Found 0 stateful pods, waiting for 3
Aug 17 07:59:48.419: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:59:48.419: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:59:48.419: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 07:59:48.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 07:59:48.566: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 07:59:48.566: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 07:59:48.566: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 07:59:58.574
Aug 17 07:59:58.588: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/17/23 07:59:58.588
STEP: Updating Pods in reverse ordinal order 08/17/23 08:00:08.596
Aug 17 08:00:08.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 08:00:08.730: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 08:00:08.730: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 08:00:08.730: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/17/23 08:00:18.743
Aug 17 08:00:18.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 08:00:18.869: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 08:00:18.869: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 08:00:18.869: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 08:00:28.920: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/17/23 08:00:38.93
Aug 17 08:00:38.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 08:00:39.066: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 08:00:39.066: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 08:00:39.066: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 08:00:49.077: INFO: Deleting all statefulset in ns statefulset-2437
Aug 17 08:00:49.079: INFO: Scaling statefulset ss2 to 0
Aug 17 08:00:59.088: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 08:00:59.089: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 08:00:59.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2437" for this suite. 08/17/23 08:00:59.103
------------------------------
â€¢ [SLOW TEST] [80.768 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 07:59:38.337
    Aug 17 07:59:38.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 07:59:38.338
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 07:59:38.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 07:59:38.405
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2437 08/17/23 07:59:38.407
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/17/23 07:59:38.411
    Aug 17 07:59:38.416: INFO: Found 0 stateful pods, waiting for 3
    Aug 17 07:59:48.419: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:59:48.419: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:59:48.419: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 07:59:48.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 07:59:48.566: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 07:59:48.566: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 07:59:48.566: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 07:59:58.574
    Aug 17 07:59:58.588: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/17/23 07:59:58.588
    STEP: Updating Pods in reverse ordinal order 08/17/23 08:00:08.596
    Aug 17 08:00:08.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 08:00:08.730: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 08:00:08.730: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 08:00:08.730: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/17/23 08:00:18.743
    Aug 17 08:00:18.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 08:00:18.869: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 08:00:18.869: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 08:00:18.869: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 08:00:28.920: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/17/23 08:00:38.93
    Aug 17 08:00:38.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=statefulset-2437 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 08:00:39.066: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 08:00:39.066: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 08:00:39.066: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 08:00:49.077: INFO: Deleting all statefulset in ns statefulset-2437
    Aug 17 08:00:49.079: INFO: Scaling statefulset ss2 to 0
    Aug 17 08:00:59.088: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 08:00:59.089: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:00:59.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2437" for this suite. 08/17/23 08:00:59.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:00:59.106
Aug 17 08:00:59.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 08:00:59.107
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:00:59.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:00:59.12
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/17/23 08:00:59.122
Aug 17 08:00:59.160: INFO: Waiting up to 5m0s for pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64" in namespace "var-expansion-3163" to be "Succeeded or Failed"
Aug 17 08:00:59.198: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64": Phase="Pending", Reason="", readiness=false. Elapsed: 37.891155ms
Aug 17 08:01:01.202: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042507972s
Aug 17 08:01:03.200: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040699843s
STEP: Saw pod success 08/17/23 08:01:03.201
Aug 17 08:01:03.201: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64" satisfied condition "Succeeded or Failed"
Aug 17 08:01:03.202: INFO: Trying to get logs from node yst-node2 pod var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64 container dapi-container: <nil>
STEP: delete the pod 08/17/23 08:01:03.211
Aug 17 08:01:03.218: INFO: Waiting for pod var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64 to disappear
Aug 17 08:01:03.220: INFO: Pod var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 08:01:03.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3163" for this suite. 08/17/23 08:01:03.223
------------------------------
â€¢ [4.121 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:00:59.106
    Aug 17 08:00:59.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 08:00:59.107
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:00:59.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:00:59.12
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/17/23 08:00:59.122
    Aug 17 08:00:59.160: INFO: Waiting up to 5m0s for pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64" in namespace "var-expansion-3163" to be "Succeeded or Failed"
    Aug 17 08:00:59.198: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64": Phase="Pending", Reason="", readiness=false. Elapsed: 37.891155ms
    Aug 17 08:01:01.202: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042507972s
    Aug 17 08:01:03.200: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040699843s
    STEP: Saw pod success 08/17/23 08:01:03.201
    Aug 17 08:01:03.201: INFO: Pod "var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64" satisfied condition "Succeeded or Failed"
    Aug 17 08:01:03.202: INFO: Trying to get logs from node yst-node2 pod var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 08:01:03.211
    Aug 17 08:01:03.218: INFO: Waiting for pod var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64 to disappear
    Aug 17 08:01:03.220: INFO: Pod var-expansion-971b53e2-cf2e-4e5d-9429-8d5599733e64 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:01:03.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3163" for this suite. 08/17/23 08:01:03.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:01:03.228
Aug 17 08:01:03.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-probe 08/17/23 08:01:03.229
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:01:03.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:01:03.249
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-d1d39e8c-da25-4790-93c3-4dc29c889525 in namespace container-probe-5433 08/17/23 08:01:03.251
Aug 17 08:01:03.259: INFO: Waiting up to 5m0s for pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525" in namespace "container-probe-5433" to be "not pending"
Aug 17 08:01:03.262: INFO: Pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140477ms
Aug 17 08:01:05.264: INFO: Pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525": Phase="Running", Reason="", readiness=true. Elapsed: 2.005334048s
Aug 17 08:01:05.264: INFO: Pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525" satisfied condition "not pending"
Aug 17 08:01:05.264: INFO: Started pod busybox-d1d39e8c-da25-4790-93c3-4dc29c889525 in namespace container-probe-5433
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 08:01:05.264
Aug 17 08:01:05.266: INFO: Initial restart count of pod busybox-d1d39e8c-da25-4790-93c3-4dc29c889525 is 0
STEP: deleting the pod 08/17/23 08:05:05.604
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 08:05:05.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5433" for this suite. 08/17/23 08:05:05.758
------------------------------
â€¢ [SLOW TEST] [242.545 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:01:03.228
    Aug 17 08:01:03.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-probe 08/17/23 08:01:03.229
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:01:03.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:01:03.249
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-d1d39e8c-da25-4790-93c3-4dc29c889525 in namespace container-probe-5433 08/17/23 08:01:03.251
    Aug 17 08:01:03.259: INFO: Waiting up to 5m0s for pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525" in namespace "container-probe-5433" to be "not pending"
    Aug 17 08:01:03.262: INFO: Pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140477ms
    Aug 17 08:01:05.264: INFO: Pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525": Phase="Running", Reason="", readiness=true. Elapsed: 2.005334048s
    Aug 17 08:01:05.264: INFO: Pod "busybox-d1d39e8c-da25-4790-93c3-4dc29c889525" satisfied condition "not pending"
    Aug 17 08:01:05.264: INFO: Started pod busybox-d1d39e8c-da25-4790-93c3-4dc29c889525 in namespace container-probe-5433
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 08:01:05.264
    Aug 17 08:01:05.266: INFO: Initial restart count of pod busybox-d1d39e8c-da25-4790-93c3-4dc29c889525 is 0
    STEP: deleting the pod 08/17/23 08:05:05.604
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:05:05.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5433" for this suite. 08/17/23 08:05:05.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:05:05.775
Aug 17 08:05:05.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 08:05:05.775
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:05.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:05.852
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 08:05:05.854
Aug 17 08:05:05.861: INFO: Waiting up to 5m0s for pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71" in namespace "emptydir-7370" to be "Succeeded or Failed"
Aug 17 08:05:05.864: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049934ms
Aug 17 08:05:07.867: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005484392s
Aug 17 08:05:09.867: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005990443s
STEP: Saw pod success 08/17/23 08:05:09.867
Aug 17 08:05:09.867: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71" satisfied condition "Succeeded or Failed"
Aug 17 08:05:09.869: INFO: Trying to get logs from node yst-node2 pod pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71 container test-container: <nil>
STEP: delete the pod 08/17/23 08:05:09.88
Aug 17 08:05:09.895: INFO: Waiting for pod pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71 to disappear
Aug 17 08:05:09.896: INFO: Pod pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 08:05:09.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7370" for this suite. 08/17/23 08:05:09.899
------------------------------
â€¢ [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:05:05.775
    Aug 17 08:05:05.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 08:05:05.775
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:05.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:05.852
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 08:05:05.854
    Aug 17 08:05:05.861: INFO: Waiting up to 5m0s for pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71" in namespace "emptydir-7370" to be "Succeeded or Failed"
    Aug 17 08:05:05.864: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049934ms
    Aug 17 08:05:07.867: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005484392s
    Aug 17 08:05:09.867: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005990443s
    STEP: Saw pod success 08/17/23 08:05:09.867
    Aug 17 08:05:09.867: INFO: Pod "pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71" satisfied condition "Succeeded or Failed"
    Aug 17 08:05:09.869: INFO: Trying to get logs from node yst-node2 pod pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71 container test-container: <nil>
    STEP: delete the pod 08/17/23 08:05:09.88
    Aug 17 08:05:09.895: INFO: Waiting for pod pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71 to disappear
    Aug 17 08:05:09.896: INFO: Pod pod-b9e46d44-0e8d-4267-b3cf-55e3b73dce71 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:05:09.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7370" for this suite. 08/17/23 08:05:09.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:05:09.902
Aug 17 08:05:09.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename server-version 08/17/23 08:05:09.903
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:09.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:09.922
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/17/23 08:05:09.925
STEP: Confirm major version 08/17/23 08:05:09.926
Aug 17 08:05:09.926: INFO: Major version: 1
STEP: Confirm minor version 08/17/23 08:05:09.926
Aug 17 08:05:09.926: INFO: cleanMinorVersion: 26
Aug 17 08:05:09.926: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 17 08:05:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-7310" for this suite. 08/17/23 08:05:09.933
------------------------------
â€¢ [0.041 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:05:09.902
    Aug 17 08:05:09.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename server-version 08/17/23 08:05:09.903
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:09.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:09.922
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/17/23 08:05:09.925
    STEP: Confirm major version 08/17/23 08:05:09.926
    Aug 17 08:05:09.926: INFO: Major version: 1
    STEP: Confirm minor version 08/17/23 08:05:09.926
    Aug 17 08:05:09.926: INFO: cleanMinorVersion: 26
    Aug 17 08:05:09.926: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:05:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-7310" for this suite. 08/17/23 08:05:09.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:05:09.944
Aug 17 08:05:09.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename namespaces 08/17/23 08:05:09.944
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:10.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:10.013
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/17/23 08:05:10.023
Aug 17 08:05:10.034: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/17/23 08:05:10.034
Aug 17 08:05:10.045: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/17/23 08:05:10.045
Aug 17 08:05:10.053: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:05:10.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8930" for this suite. 08/17/23 08:05:10.057
------------------------------
â€¢ [0.117 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:05:09.944
    Aug 17 08:05:09.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename namespaces 08/17/23 08:05:09.944
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:10.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:10.013
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/17/23 08:05:10.023
    Aug 17 08:05:10.034: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/17/23 08:05:10.034
    Aug 17 08:05:10.045: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/17/23 08:05:10.045
    Aug 17 08:05:10.053: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:05:10.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8930" for this suite. 08/17/23 08:05:10.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:05:10.061
Aug 17 08:05:10.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename prestop 08/17/23 08:05:10.061
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:10.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:10.087
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5310 08/17/23 08:05:10.089
STEP: Waiting for pods to come up. 08/17/23 08:05:10.096
Aug 17 08:05:10.096: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5310" to be "running"
Aug 17 08:05:10.099: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072528ms
Aug 17 08:05:12.100: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.003985259s
Aug 17 08:05:12.100: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5310 08/17/23 08:05:12.103
Aug 17 08:05:12.108: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5310" to be "running"
Aug 17 08:05:12.109: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515853ms
Aug 17 08:05:14.112: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00397077s
Aug 17 08:05:14.112: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/17/23 08:05:14.112
Aug 17 08:05:19.119: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/17/23 08:05:19.12
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 17 08:05:19.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-5310" for this suite. 08/17/23 08:05:19.129
------------------------------
â€¢ [SLOW TEST] [9.073 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:05:10.061
    Aug 17 08:05:10.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename prestop 08/17/23 08:05:10.061
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:10.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:10.087
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5310 08/17/23 08:05:10.089
    STEP: Waiting for pods to come up. 08/17/23 08:05:10.096
    Aug 17 08:05:10.096: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5310" to be "running"
    Aug 17 08:05:10.099: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072528ms
    Aug 17 08:05:12.100: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.003985259s
    Aug 17 08:05:12.100: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5310 08/17/23 08:05:12.103
    Aug 17 08:05:12.108: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5310" to be "running"
    Aug 17 08:05:12.109: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515853ms
    Aug 17 08:05:14.112: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00397077s
    Aug 17 08:05:14.112: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/17/23 08:05:14.112
    Aug 17 08:05:19.119: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/17/23 08:05:19.12
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:05:19.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-5310" for this suite. 08/17/23 08:05:19.129
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:05:19.134
Aug 17 08:05:19.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 08:05:19.135
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:19.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:19.149
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/17/23 08:05:19.157
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1366;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1366;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +notcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_tcp@PTR;sleep 1; done
 08/17/23 08:05:19.184
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1366;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1366;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +notcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_tcp@PTR;sleep 1; done
 08/17/23 08:05:19.184
STEP: creating a pod to probe DNS 08/17/23 08:05:19.184
STEP: submitting the pod to kubernetes 08/17/23 08:05:19.184
Aug 17 08:05:19.190: INFO: Waiting up to 15m0s for pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046" in namespace "dns-1366" to be "running"
Aug 17 08:05:19.191: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55566ms
Aug 17 08:05:21.195: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004995454s
Aug 17 08:05:23.195: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046": Phase="Running", Reason="", readiness=true. Elapsed: 4.005232102s
Aug 17 08:05:23.195: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046" satisfied condition "running"
STEP: retrieving the pod 08/17/23 08:05:23.195
STEP: looking for the results for each expected name from probers 08/17/23 08:05:23.197
Aug 17 08:05:23.200: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.202: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.205: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.207: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.209: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.211: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.213: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.215: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.223: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.225: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.227: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.229: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.232: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.234: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.236: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.237: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:23.246: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

Aug 17 08:05:28.250: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.252: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.255: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.259: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.261: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.263: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.273: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.275: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.276: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.280: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.283: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.284: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:28.291: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

Aug 17 08:05:33.251: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.253: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.256: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.258: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.260: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.269: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.293: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.295: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.297: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.302: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.305: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.308: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.309: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.311: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:33.318: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

Aug 17 08:05:38.249: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.251: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.253: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.254: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.258: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.259: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.261: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.269: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.270: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.272: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.273: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.274: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.278: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.279: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:38.285: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

Aug 17 08:05:43.249: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.252: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.253: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.255: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.258: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.259: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.261: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.309: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.311: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.312: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.314: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.316: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.321: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:43.326: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

Aug 17 08:05:48.248: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.250: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.252: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.253: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.258: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.260: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.268: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.270: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.272: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.273: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.275: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.279: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.281: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
Aug 17 08:05:48.288: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

Aug 17 08:05:53.289: INFO: DNS probes using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 succeeded

STEP: deleting the pod 08/17/23 08:05:53.289
STEP: deleting the test service 08/17/23 08:05:53.295
STEP: deleting the test headless service 08/17/23 08:05:53.319
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 08:05:53.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1366" for this suite. 08/17/23 08:05:53.327
------------------------------
â€¢ [SLOW TEST] [34.200 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:05:19.134
    Aug 17 08:05:19.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 08:05:19.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:19.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:19.149
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/17/23 08:05:19.157
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1366;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1366;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +notcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_tcp@PTR;sleep 1; done
     08/17/23 08:05:19.184
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1366;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1366;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1366.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1366.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1366.svc;check="$$(dig +notcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.236.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.236.196_tcp@PTR;sleep 1; done
     08/17/23 08:05:19.184
    STEP: creating a pod to probe DNS 08/17/23 08:05:19.184
    STEP: submitting the pod to kubernetes 08/17/23 08:05:19.184
    Aug 17 08:05:19.190: INFO: Waiting up to 15m0s for pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046" in namespace "dns-1366" to be "running"
    Aug 17 08:05:19.191: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55566ms
    Aug 17 08:05:21.195: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004995454s
    Aug 17 08:05:23.195: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046": Phase="Running", Reason="", readiness=true. Elapsed: 4.005232102s
    Aug 17 08:05:23.195: INFO: Pod "dns-test-b0b43aab-1f61-4454-b19a-302be5295046" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 08:05:23.195
    STEP: looking for the results for each expected name from probers 08/17/23 08:05:23.197
    Aug 17 08:05:23.200: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.202: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.205: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.207: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.209: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.211: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.213: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.215: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.223: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.225: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.227: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.229: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.232: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.234: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.236: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.237: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:23.246: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

    Aug 17 08:05:28.250: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.252: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.255: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.259: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.261: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.263: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.273: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.275: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.276: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.280: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.283: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.284: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:28.291: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

    Aug 17 08:05:33.251: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.253: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.256: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.258: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.260: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.269: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.293: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.295: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.297: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.302: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.305: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.308: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.309: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.311: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:33.318: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

    Aug 17 08:05:38.249: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.251: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.253: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.254: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.258: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.259: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.261: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.269: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.270: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.272: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.273: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.274: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.278: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.279: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:38.285: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

    Aug 17 08:05:43.249: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.252: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.253: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.255: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.258: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.259: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.261: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.309: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.311: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.312: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.314: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.316: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.321: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:43.326: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

    Aug 17 08:05:48.248: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.250: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.252: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.253: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.258: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.260: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.268: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.270: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.272: INFO: Unable to read jessie_udp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.273: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366 from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.275: INFO: Unable to read jessie_udp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.279: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.281: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc from pod dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046: the server could not find the requested resource (get pods dns-test-b0b43aab-1f61-4454-b19a-302be5295046)
    Aug 17 08:05:48.288: INFO: Lookups using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1366 wheezy_tcp@dns-test-service.dns-1366 wheezy_udp@dns-test-service.dns-1366.svc wheezy_tcp@dns-test-service.dns-1366.svc wheezy_udp@_http._tcp.dns-test-service.dns-1366.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1366.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1366 jessie_tcp@dns-test-service.dns-1366 jessie_udp@dns-test-service.dns-1366.svc jessie_tcp@dns-test-service.dns-1366.svc jessie_udp@_http._tcp.dns-test-service.dns-1366.svc jessie_tcp@_http._tcp.dns-test-service.dns-1366.svc]

    Aug 17 08:05:53.289: INFO: DNS probes using dns-1366/dns-test-b0b43aab-1f61-4454-b19a-302be5295046 succeeded

    STEP: deleting the pod 08/17/23 08:05:53.289
    STEP: deleting the test service 08/17/23 08:05:53.295
    STEP: deleting the test headless service 08/17/23 08:05:53.319
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:05:53.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1366" for this suite. 08/17/23 08:05:53.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:05:53.336
Aug 17 08:05:53.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 08:05:53.338
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:53.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:53.35
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-bf4b8d15-4176-4b8a-8fe2-35677c9cf24c 08/17/23 08:05:53.352
STEP: Creating a pod to test consume secrets 08/17/23 08:05:53.388
Aug 17 08:05:53.397: INFO: Waiting up to 5m0s for pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e" in namespace "secrets-6765" to be "Succeeded or Failed"
Aug 17 08:05:53.400: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195785ms
Aug 17 08:05:55.402: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00541092s
Aug 17 08:05:57.403: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005897307s
STEP: Saw pod success 08/17/23 08:05:57.403
Aug 17 08:05:57.403: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e" satisfied condition "Succeeded or Failed"
Aug 17 08:05:57.404: INFO: Trying to get logs from node yst-node2 pod pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 08:05:57.408
Aug 17 08:05:57.413: INFO: Waiting for pod pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e to disappear
Aug 17 08:05:57.415: INFO: Pod pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 08:05:57.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6765" for this suite. 08/17/23 08:05:57.418
------------------------------
â€¢ [4.085 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:05:53.336
    Aug 17 08:05:53.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 08:05:53.338
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:53.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:53.35
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-bf4b8d15-4176-4b8a-8fe2-35677c9cf24c 08/17/23 08:05:53.352
    STEP: Creating a pod to test consume secrets 08/17/23 08:05:53.388
    Aug 17 08:05:53.397: INFO: Waiting up to 5m0s for pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e" in namespace "secrets-6765" to be "Succeeded or Failed"
    Aug 17 08:05:53.400: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195785ms
    Aug 17 08:05:55.402: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00541092s
    Aug 17 08:05:57.403: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005897307s
    STEP: Saw pod success 08/17/23 08:05:57.403
    Aug 17 08:05:57.403: INFO: Pod "pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e" satisfied condition "Succeeded or Failed"
    Aug 17 08:05:57.404: INFO: Trying to get logs from node yst-node2 pod pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 08:05:57.408
    Aug 17 08:05:57.413: INFO: Waiting for pod pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e to disappear
    Aug 17 08:05:57.415: INFO: Pod pod-secrets-78b8d8c3-50e6-47fc-8e16-072d5864698e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:05:57.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6765" for this suite. 08/17/23 08:05:57.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:05:57.422
Aug 17 08:05:57.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 08:05:57.422
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:57.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:57.442
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-5287469e-bd00-4713-a586-ae51858dfae6 08/17/23 08:05:57.444
STEP: Creating a pod to test consume secrets 08/17/23 08:05:57.46
Aug 17 08:05:57.468: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927" in namespace "projected-5513" to be "Succeeded or Failed"
Aug 17 08:05:57.471: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349803ms
Aug 17 08:05:59.474: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005613133s
Aug 17 08:06:01.474: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006115764s
STEP: Saw pod success 08/17/23 08:06:01.474
Aug 17 08:06:01.474: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927" satisfied condition "Succeeded or Failed"
Aug 17 08:06:01.476: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 08:06:01.48
Aug 17 08:06:01.657: INFO: Waiting for pod pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927 to disappear
Aug 17 08:06:01.659: INFO: Pod pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:01.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5513" for this suite. 08/17/23 08:06:01.662
------------------------------
â€¢ [4.246 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:05:57.422
    Aug 17 08:05:57.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 08:05:57.422
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:05:57.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:05:57.442
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-5287469e-bd00-4713-a586-ae51858dfae6 08/17/23 08:05:57.444
    STEP: Creating a pod to test consume secrets 08/17/23 08:05:57.46
    Aug 17 08:05:57.468: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927" in namespace "projected-5513" to be "Succeeded or Failed"
    Aug 17 08:05:57.471: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349803ms
    Aug 17 08:05:59.474: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005613133s
    Aug 17 08:06:01.474: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006115764s
    STEP: Saw pod success 08/17/23 08:06:01.474
    Aug 17 08:06:01.474: INFO: Pod "pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927" satisfied condition "Succeeded or Failed"
    Aug 17 08:06:01.476: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 08:06:01.48
    Aug 17 08:06:01.657: INFO: Waiting for pod pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927 to disappear
    Aug 17 08:06:01.659: INFO: Pod pod-projected-secrets-b423e8db-bbda-4601-8d2d-78f0c5b49927 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:01.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5513" for this suite. 08/17/23 08:06:01.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:01.669
Aug 17 08:06:01.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 08:06:01.67
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:01.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:01.785
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 08:06:01.833
Aug 17 08:06:01.986: INFO: Waiting up to 5m0s for pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb" in namespace "emptydir-2734" to be "Succeeded or Failed"
Aug 17 08:06:01.988: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10549ms
Aug 17 08:06:03.991: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005043696s
Aug 17 08:06:05.991: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004909598s
STEP: Saw pod success 08/17/23 08:06:05.991
Aug 17 08:06:05.991: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb" satisfied condition "Succeeded or Failed"
Aug 17 08:06:05.993: INFO: Trying to get logs from node yst-node2 pod pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb container test-container: <nil>
STEP: delete the pod 08/17/23 08:06:05.996
Aug 17 08:06:06.001: INFO: Waiting for pod pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb to disappear
Aug 17 08:06:06.002: INFO: Pod pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:06.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2734" for this suite. 08/17/23 08:06:06.005
------------------------------
â€¢ [4.341 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:01.669
    Aug 17 08:06:01.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 08:06:01.67
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:01.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:01.785
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 08:06:01.833
    Aug 17 08:06:01.986: INFO: Waiting up to 5m0s for pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb" in namespace "emptydir-2734" to be "Succeeded or Failed"
    Aug 17 08:06:01.988: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10549ms
    Aug 17 08:06:03.991: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005043696s
    Aug 17 08:06:05.991: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004909598s
    STEP: Saw pod success 08/17/23 08:06:05.991
    Aug 17 08:06:05.991: INFO: Pod "pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb" satisfied condition "Succeeded or Failed"
    Aug 17 08:06:05.993: INFO: Trying to get logs from node yst-node2 pod pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb container test-container: <nil>
    STEP: delete the pod 08/17/23 08:06:05.996
    Aug 17 08:06:06.001: INFO: Waiting for pod pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb to disappear
    Aug 17 08:06:06.002: INFO: Pod pod-c0a89de4-fdc8-4aa1-832a-12f3286984eb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:06.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2734" for this suite. 08/17/23 08:06:06.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:06.01
Aug 17 08:06:06.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 08:06:06.011
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:06.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:06.029
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/17/23 08:06:06.031
Aug 17 08:06:06.037: INFO: Waiting up to 5m0s for pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2" in namespace "pods-3568" to be "running and ready"
Aug 17 08:06:06.043: INFO: Pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.480423ms
Aug 17 08:06:06.043: INFO: The phase of Pod pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 08:06:08.046: INFO: Pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008262006s
Aug 17 08:06:08.046: INFO: The phase of Pod pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2 is Running (Ready = true)
Aug 17 08:06:08.046: INFO: Pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2" satisfied condition "running and ready"
Aug 17 08:06:08.052: INFO: Pod pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2 has hostIP: 10.60.200.177
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:08.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3568" for this suite. 08/17/23 08:06:08.066
------------------------------
â€¢ [2.060 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:06.01
    Aug 17 08:06:06.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 08:06:06.011
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:06.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:06.029
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/17/23 08:06:06.031
    Aug 17 08:06:06.037: INFO: Waiting up to 5m0s for pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2" in namespace "pods-3568" to be "running and ready"
    Aug 17 08:06:06.043: INFO: Pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.480423ms
    Aug 17 08:06:06.043: INFO: The phase of Pod pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 08:06:08.046: INFO: Pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008262006s
    Aug 17 08:06:08.046: INFO: The phase of Pod pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2 is Running (Ready = true)
    Aug 17 08:06:08.046: INFO: Pod "pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2" satisfied condition "running and ready"
    Aug 17 08:06:08.052: INFO: Pod pod-hostip-d6b53814-0686-414e-adaf-7a04bec09ce2 has hostIP: 10.60.200.177
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:08.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3568" for this suite. 08/17/23 08:06:08.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:08.072
Aug 17 08:06:08.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename gc 08/17/23 08:06:08.073
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:08.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:08.137
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 17 08:06:08.173: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e0f80a40-50f2-48f6-91b2-69723f6b7582", Controller:(*bool)(0xc002bc0a42), BlockOwnerDeletion:(*bool)(0xc002bc0a43)}}
Aug 17 08:06:08.179: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8ab24273-5bb9-482f-a2d4-5d6cac6ec70c", Controller:(*bool)(0xc002bc0d22), BlockOwnerDeletion:(*bool)(0xc002bc0d23)}}
Aug 17 08:06:08.183: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5812b502-786b-4ce9-ade2-a6c072381ad2", Controller:(*bool)(0xc0005c581a), BlockOwnerDeletion:(*bool)(0xc0005c581b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:13.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2693" for this suite. 08/17/23 08:06:13.194
------------------------------
â€¢ [SLOW TEST] [5.126 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:08.072
    Aug 17 08:06:08.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename gc 08/17/23 08:06:08.073
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:08.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:08.137
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 17 08:06:08.173: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e0f80a40-50f2-48f6-91b2-69723f6b7582", Controller:(*bool)(0xc002bc0a42), BlockOwnerDeletion:(*bool)(0xc002bc0a43)}}
    Aug 17 08:06:08.179: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8ab24273-5bb9-482f-a2d4-5d6cac6ec70c", Controller:(*bool)(0xc002bc0d22), BlockOwnerDeletion:(*bool)(0xc002bc0d23)}}
    Aug 17 08:06:08.183: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5812b502-786b-4ce9-ade2-a6c072381ad2", Controller:(*bool)(0xc0005c581a), BlockOwnerDeletion:(*bool)(0xc0005c581b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:13.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2693" for this suite. 08/17/23 08:06:13.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:13.199
Aug 17 08:06:13.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replicaset 08/17/23 08:06:13.199
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:13.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:13.212
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/17/23 08:06:13.214
STEP: Verify that the required pods have come up 08/17/23 08:06:13.23
Aug 17 08:06:13.235: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 17 08:06:18.238: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/17/23 08:06:18.238
Aug 17 08:06:18.240: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/17/23 08:06:18.24
STEP: DeleteCollection of the ReplicaSets 08/17/23 08:06:18.244
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/17/23 08:06:18.251
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:18.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1197" for this suite. 08/17/23 08:06:18.259
------------------------------
â€¢ [SLOW TEST] [5.064 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:13.199
    Aug 17 08:06:13.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replicaset 08/17/23 08:06:13.199
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:13.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:13.212
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/17/23 08:06:13.214
    STEP: Verify that the required pods have come up 08/17/23 08:06:13.23
    Aug 17 08:06:13.235: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 17 08:06:18.238: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/17/23 08:06:18.238
    Aug 17 08:06:18.240: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/17/23 08:06:18.24
    STEP: DeleteCollection of the ReplicaSets 08/17/23 08:06:18.244
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/17/23 08:06:18.251
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:18.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1197" for this suite. 08/17/23 08:06:18.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:18.264
Aug 17 08:06:18.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pods 08/17/23 08:06:18.265
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:18.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:18.307
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/17/23 08:06:18.312
STEP: submitting the pod to kubernetes 08/17/23 08:06:18.312
STEP: verifying QOS class is set on the pod 08/17/23 08:06:18.331
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:18.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3722" for this suite. 08/17/23 08:06:18.338
------------------------------
â€¢ [0.080 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:18.264
    Aug 17 08:06:18.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pods 08/17/23 08:06:18.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:18.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:18.307
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/17/23 08:06:18.312
    STEP: submitting the pod to kubernetes 08/17/23 08:06:18.312
    STEP: verifying QOS class is set on the pod 08/17/23 08:06:18.331
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:18.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3722" for this suite. 08/17/23 08:06:18.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:18.344
Aug 17 08:06:18.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 08:06:18.345
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:18.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:18.365
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-b1246473-8e81-43cd-a69d-405ada3226d0 08/17/23 08:06:18.367
STEP: Creating a pod to test consume secrets 08/17/23 08:06:18.382
Aug 17 08:06:18.393: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554" in namespace "projected-532" to be "Succeeded or Failed"
Aug 17 08:06:18.396: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50183ms
Aug 17 08:06:20.399: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005668296s
Aug 17 08:06:22.399: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005949403s
STEP: Saw pod success 08/17/23 08:06:22.399
Aug 17 08:06:22.400: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554" satisfied condition "Succeeded or Failed"
Aug 17 08:06:22.402: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 08:06:22.407
Aug 17 08:06:22.413: INFO: Waiting for pod pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554 to disappear
Aug 17 08:06:22.414: INFO: Pod pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:22.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-532" for this suite. 08/17/23 08:06:22.417
------------------------------
â€¢ [4.077 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:18.344
    Aug 17 08:06:18.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 08:06:18.345
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:18.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:18.365
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-b1246473-8e81-43cd-a69d-405ada3226d0 08/17/23 08:06:18.367
    STEP: Creating a pod to test consume secrets 08/17/23 08:06:18.382
    Aug 17 08:06:18.393: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554" in namespace "projected-532" to be "Succeeded or Failed"
    Aug 17 08:06:18.396: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50183ms
    Aug 17 08:06:20.399: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005668296s
    Aug 17 08:06:22.399: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005949403s
    STEP: Saw pod success 08/17/23 08:06:22.399
    Aug 17 08:06:22.400: INFO: Pod "pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554" satisfied condition "Succeeded or Failed"
    Aug 17 08:06:22.402: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 08:06:22.407
    Aug 17 08:06:22.413: INFO: Waiting for pod pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554 to disappear
    Aug 17 08:06:22.414: INFO: Pod pod-projected-secrets-dc0b5795-c31e-4ba7-a153-7d0b9a412554 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:22.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-532" for this suite. 08/17/23 08:06:22.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:22.421
Aug 17 08:06:22.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 08:06:22.422
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:22.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:22.435
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/17/23 08:06:22.438
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_tcp@PTR;sleep 1; done
 08/17/23 08:06:22.467
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_tcp@PTR;sleep 1; done
 08/17/23 08:06:22.467
STEP: creating a pod to probe DNS 08/17/23 08:06:22.467
STEP: submitting the pod to kubernetes 08/17/23 08:06:22.467
Aug 17 08:06:22.476: INFO: Waiting up to 15m0s for pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806" in namespace "dns-7071" to be "running"
Aug 17 08:06:22.479: INFO: Pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519061ms
Aug 17 08:06:24.483: INFO: Pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806": Phase="Running", Reason="", readiness=true. Elapsed: 2.007165758s
Aug 17 08:06:24.483: INFO: Pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806" satisfied condition "running"
STEP: retrieving the pod 08/17/23 08:06:24.483
STEP: looking for the results for each expected name from probers 08/17/23 08:06:24.485
Aug 17 08:06:24.487: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.489: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.491: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.492: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.499: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.501: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.502: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.504: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:24.511: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

Aug 17 08:06:29.515: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.517: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.520: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.522: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.533: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.536: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.537: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:29.547: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

Aug 17 08:06:34.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.518: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.520: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.522: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.530: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.534: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.535: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:34.542: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

Aug 17 08:06:39.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.518: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.522: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.528: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.560: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.564: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.571: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.580: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:39.595: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

Aug 17 08:06:44.515: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.517: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.519: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.521: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.532: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.535: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:44.544: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

Aug 17 08:06:49.514: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.516: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.520: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.534: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
Aug 17 08:06:49.540: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

Aug 17 08:06:54.542: INFO: DNS probes using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 succeeded

STEP: deleting the pod 08/17/23 08:06:54.542
STEP: deleting the test service 08/17/23 08:06:54.546
STEP: deleting the test headless service 08/17/23 08:06:54.562
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 08:06:54.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7071" for this suite. 08/17/23 08:06:54.573
------------------------------
â€¢ [SLOW TEST] [32.154 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:22.421
    Aug 17 08:06:22.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 08:06:22.422
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:22.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:22.435
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/17/23 08:06:22.438
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_tcp@PTR;sleep 1; done
     08/17/23 08:06:22.467
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7071.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7071.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7071.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.33.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.33.125_tcp@PTR;sleep 1; done
     08/17/23 08:06:22.467
    STEP: creating a pod to probe DNS 08/17/23 08:06:22.467
    STEP: submitting the pod to kubernetes 08/17/23 08:06:22.467
    Aug 17 08:06:22.476: INFO: Waiting up to 15m0s for pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806" in namespace "dns-7071" to be "running"
    Aug 17 08:06:22.479: INFO: Pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519061ms
    Aug 17 08:06:24.483: INFO: Pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806": Phase="Running", Reason="", readiness=true. Elapsed: 2.007165758s
    Aug 17 08:06:24.483: INFO: Pod "dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 08:06:24.483
    STEP: looking for the results for each expected name from probers 08/17/23 08:06:24.485
    Aug 17 08:06:24.487: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.489: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.491: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.492: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.499: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.501: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.502: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.504: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:24.511: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

    Aug 17 08:06:29.515: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.517: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.520: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.522: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.533: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.536: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.537: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:29.547: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

    Aug 17 08:06:34.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.518: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.520: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.522: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.530: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.534: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.535: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:34.542: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

    Aug 17 08:06:39.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.518: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.522: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.528: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.560: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.564: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.571: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.580: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:39.595: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

    Aug 17 08:06:44.515: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.517: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.519: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.521: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.532: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.535: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:44.544: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

    Aug 17 08:06:49.514: INFO: Unable to read wheezy_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.516: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.520: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.534: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local from pod dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806: the server could not find the requested resource (get pods dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806)
    Aug 17 08:06:49.540: INFO: Lookups using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 failed for: [wheezy_udp@dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@dns-test-service.dns-7071.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_udp@dns-test-service.dns-7071.svc.cluster.local jessie_tcp@dns-test-service.dns-7071.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7071.svc.cluster.local]

    Aug 17 08:06:54.542: INFO: DNS probes using dns-7071/dns-test-5d2d3fc0-90f5-4f53-9347-edba27186806 succeeded

    STEP: deleting the pod 08/17/23 08:06:54.542
    STEP: deleting the test service 08/17/23 08:06:54.546
    STEP: deleting the test headless service 08/17/23 08:06:54.562
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:06:54.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7071" for this suite. 08/17/23 08:06:54.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:06:54.577
Aug 17 08:06:54.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 08:06:54.578
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:54.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:54.599
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/17/23 08:06:54.603
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
 08/17/23 08:06:54.608
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
 08/17/23 08:06:54.608
STEP: creating a pod to probe DNS 08/17/23 08:06:54.608
STEP: submitting the pod to kubernetes 08/17/23 08:06:54.608
Aug 17 08:06:54.619: INFO: Waiting up to 15m0s for pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f" in namespace "dns-7872" to be "running"
Aug 17 08:06:54.622: INFO: Pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.883745ms
Aug 17 08:06:56.625: INFO: Pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005884744s
Aug 17 08:06:56.625: INFO: Pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f" satisfied condition "running"
STEP: retrieving the pod 08/17/23 08:06:56.625
STEP: looking for the results for each expected name from probers 08/17/23 08:06:56.627
Aug 17 08:06:56.631: INFO: DNS probes using dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f succeeded

STEP: deleting the pod 08/17/23 08:06:56.631
STEP: changing the externalName to bar.example.com 08/17/23 08:06:56.64
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
 08/17/23 08:06:56.648
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
 08/17/23 08:06:56.648
STEP: creating a second pod to probe DNS 08/17/23 08:06:56.648
STEP: submitting the pod to kubernetes 08/17/23 08:06:56.648
Aug 17 08:06:56.690: INFO: Waiting up to 15m0s for pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27" in namespace "dns-7872" to be "running"
Aug 17 08:06:56.692: INFO: Pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683257ms
Aug 17 08:06:58.695: INFO: Pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27": Phase="Running", Reason="", readiness=true. Elapsed: 2.00472381s
Aug 17 08:06:58.695: INFO: Pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27" satisfied condition "running"
STEP: retrieving the pod 08/17/23 08:06:58.695
STEP: looking for the results for each expected name from probers 08/17/23 08:06:58.697
Aug 17 08:06:58.701: INFO: DNS probes using dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27 succeeded

STEP: deleting the pod 08/17/23 08:06:58.701
STEP: changing the service to type=ClusterIP 08/17/23 08:06:58.706
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
 08/17/23 08:06:58.715
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
 08/17/23 08:06:58.716
STEP: creating a third pod to probe DNS 08/17/23 08:06:58.716
STEP: submitting the pod to kubernetes 08/17/23 08:06:58.718
Aug 17 08:06:58.722: INFO: Waiting up to 15m0s for pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8" in namespace "dns-7872" to be "running"
Aug 17 08:06:58.724: INFO: Pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61648ms
Aug 17 08:07:00.728: INFO: Pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005687335s
Aug 17 08:07:00.728: INFO: Pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8" satisfied condition "running"
STEP: retrieving the pod 08/17/23 08:07:00.728
STEP: looking for the results for each expected name from probers 08/17/23 08:07:00.73
Aug 17 08:07:00.735: INFO: DNS probes using dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8 succeeded

STEP: deleting the pod 08/17/23 08:07:00.735
STEP: deleting the test externalName service 08/17/23 08:07:00.741
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:00.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7872" for this suite. 08/17/23 08:07:00.753
------------------------------
â€¢ [SLOW TEST] [6.180 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:06:54.577
    Aug 17 08:06:54.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 08:06:54.578
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:06:54.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:06:54.599
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/17/23 08:06:54.603
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
     08/17/23 08:06:54.608
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
     08/17/23 08:06:54.608
    STEP: creating a pod to probe DNS 08/17/23 08:06:54.608
    STEP: submitting the pod to kubernetes 08/17/23 08:06:54.608
    Aug 17 08:06:54.619: INFO: Waiting up to 15m0s for pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f" in namespace "dns-7872" to be "running"
    Aug 17 08:06:54.622: INFO: Pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.883745ms
    Aug 17 08:06:56.625: INFO: Pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005884744s
    Aug 17 08:06:56.625: INFO: Pod "dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 08:06:56.625
    STEP: looking for the results for each expected name from probers 08/17/23 08:06:56.627
    Aug 17 08:06:56.631: INFO: DNS probes using dns-test-695c19eb-0dbb-4d93-a864-9801cd409c9f succeeded

    STEP: deleting the pod 08/17/23 08:06:56.631
    STEP: changing the externalName to bar.example.com 08/17/23 08:06:56.64
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
     08/17/23 08:06:56.648
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
     08/17/23 08:06:56.648
    STEP: creating a second pod to probe DNS 08/17/23 08:06:56.648
    STEP: submitting the pod to kubernetes 08/17/23 08:06:56.648
    Aug 17 08:06:56.690: INFO: Waiting up to 15m0s for pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27" in namespace "dns-7872" to be "running"
    Aug 17 08:06:56.692: INFO: Pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683257ms
    Aug 17 08:06:58.695: INFO: Pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27": Phase="Running", Reason="", readiness=true. Elapsed: 2.00472381s
    Aug 17 08:06:58.695: INFO: Pod "dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 08:06:58.695
    STEP: looking for the results for each expected name from probers 08/17/23 08:06:58.697
    Aug 17 08:06:58.701: INFO: DNS probes using dns-test-766fb3c0-176a-4a1f-9a14-6161fadb9f27 succeeded

    STEP: deleting the pod 08/17/23 08:06:58.701
    STEP: changing the service to type=ClusterIP 08/17/23 08:06:58.706
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
     08/17/23 08:06:58.715
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7872.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7872.svc.cluster.local; sleep 1; done
     08/17/23 08:06:58.716
    STEP: creating a third pod to probe DNS 08/17/23 08:06:58.716
    STEP: submitting the pod to kubernetes 08/17/23 08:06:58.718
    Aug 17 08:06:58.722: INFO: Waiting up to 15m0s for pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8" in namespace "dns-7872" to be "running"
    Aug 17 08:06:58.724: INFO: Pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61648ms
    Aug 17 08:07:00.728: INFO: Pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005687335s
    Aug 17 08:07:00.728: INFO: Pod "dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 08:07:00.728
    STEP: looking for the results for each expected name from probers 08/17/23 08:07:00.73
    Aug 17 08:07:00.735: INFO: DNS probes using dns-test-2da555a3-188a-499d-8126-e7c8dab3ddb8 succeeded

    STEP: deleting the pod 08/17/23 08:07:00.735
    STEP: deleting the test externalName service 08/17/23 08:07:00.741
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:00.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7872" for this suite. 08/17/23 08:07:00.753
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:00.758
Aug 17 08:07:00.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 08:07:00.758
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:00.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:00.775
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/17/23 08:07:00.779
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/17/23 08:07:00.78
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 08:07:00.78
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/17/23 08:07:00.78
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/17/23 08:07:00.782
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 08:07:00.782
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 08:07:00.784
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:00.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3615" for this suite. 08/17/23 08:07:00.789
------------------------------
â€¢ [0.036 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:00.758
    Aug 17 08:07:00.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 08:07:00.758
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:00.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:00.775
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/17/23 08:07:00.779
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/17/23 08:07:00.78
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 08:07:00.78
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/17/23 08:07:00.78
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/17/23 08:07:00.782
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 08:07:00.782
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 08:07:00.784
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:00.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3615" for this suite. 08/17/23 08:07:00.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:00.794
Aug 17 08:07:00.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 08:07:00.795
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:00.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:00.825
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 08:07:00.841
Aug 17 08:07:00.846: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2697" to be "running and ready"
Aug 17 08:07:00.848: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.910508ms
Aug 17 08:07:00.848: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 08:07:02.851: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004812231s
Aug 17 08:07:02.851: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 08:07:02.851: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/17/23 08:07:02.852
Aug 17 08:07:02.855: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2697" to be "running and ready"
Aug 17 08:07:02.857: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833549ms
Aug 17 08:07:02.857: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 08:07:04.859: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004119007s
Aug 17 08:07:04.859: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 17 08:07:04.859: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/17/23 08:07:04.865
STEP: delete the pod with lifecycle hook 08/17/23 08:07:04.87
Aug 17 08:07:04.873: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 08:07:04.875: INFO: Pod pod-with-poststart-http-hook still exists
Aug 17 08:07:06.876: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 08:07:06.878: INFO: Pod pod-with-poststart-http-hook still exists
Aug 17 08:07:08.877: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 08:07:08.879: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:08.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2697" for this suite. 08/17/23 08:07:08.881
------------------------------
â€¢ [SLOW TEST] [8.091 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:00.794
    Aug 17 08:07:00.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 08:07:00.795
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:00.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:00.825
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 08:07:00.841
    Aug 17 08:07:00.846: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2697" to be "running and ready"
    Aug 17 08:07:00.848: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.910508ms
    Aug 17 08:07:00.848: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 08:07:02.851: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004812231s
    Aug 17 08:07:02.851: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 08:07:02.851: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/17/23 08:07:02.852
    Aug 17 08:07:02.855: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2697" to be "running and ready"
    Aug 17 08:07:02.857: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833549ms
    Aug 17 08:07:02.857: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 08:07:04.859: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004119007s
    Aug 17 08:07:04.859: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 17 08:07:04.859: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/17/23 08:07:04.865
    STEP: delete the pod with lifecycle hook 08/17/23 08:07:04.87
    Aug 17 08:07:04.873: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 17 08:07:04.875: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 17 08:07:06.876: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 17 08:07:06.878: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 17 08:07:08.877: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 17 08:07:08.879: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:08.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2697" for this suite. 08/17/23 08:07:08.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:08.885
Aug 17 08:07:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 08:07:08.886
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:08.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:08.914
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 08:07:08.927
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 08:07:09.391
STEP: Deploying the webhook pod 08/17/23 08:07:09.395
STEP: Wait for the deployment to be ready 08/17/23 08:07:09.4
Aug 17 08:07:09.407: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/17/23 08:07:11.414
STEP: Verifying the service has paired with the endpoint 08/17/23 08:07:11.45
Aug 17 08:07:12.451: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/17/23 08:07:12.453
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:07:12.463
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/17/23 08:07:12.469
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:07:12.474
STEP: Patching a validating webhook configuration's rules to include the create operation 08/17/23 08:07:12.479
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:07:12.483
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:12.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6272" for this suite. 08/17/23 08:07:12.512
STEP: Destroying namespace "webhook-6272-markers" for this suite. 08/17/23 08:07:12.516
------------------------------
â€¢ [3.634 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:08.885
    Aug 17 08:07:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 08:07:08.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:08.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:08.914
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 08:07:08.927
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 08:07:09.391
    STEP: Deploying the webhook pod 08/17/23 08:07:09.395
    STEP: Wait for the deployment to be ready 08/17/23 08:07:09.4
    Aug 17 08:07:09.407: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/17/23 08:07:11.414
    STEP: Verifying the service has paired with the endpoint 08/17/23 08:07:11.45
    Aug 17 08:07:12.451: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/17/23 08:07:12.453
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:07:12.463
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/17/23 08:07:12.469
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:07:12.474
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/17/23 08:07:12.479
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:07:12.483
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:12.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6272" for this suite. 08/17/23 08:07:12.512
    STEP: Destroying namespace "webhook-6272-markers" for this suite. 08/17/23 08:07:12.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:12.52
Aug 17 08:07:12.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 08:07:12.521
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:12.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:12.549
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 08:07:12.554
Aug 17 08:07:12.559: INFO: Waiting up to 5m0s for pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd" in namespace "emptydir-4652" to be "Succeeded or Failed"
Aug 17 08:07:12.563: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.957061ms
Aug 17 08:07:14.567: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007952083s
Aug 17 08:07:16.565: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006169957s
STEP: Saw pod success 08/17/23 08:07:16.565
Aug 17 08:07:16.565: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd" satisfied condition "Succeeded or Failed"
Aug 17 08:07:16.567: INFO: Trying to get logs from node yst-node2 pod pod-078159b5-224d-4e2b-9293-5f34890e88dd container test-container: <nil>
STEP: delete the pod 08/17/23 08:07:16.57
Aug 17 08:07:16.576: INFO: Waiting for pod pod-078159b5-224d-4e2b-9293-5f34890e88dd to disappear
Aug 17 08:07:16.578: INFO: Pod pod-078159b5-224d-4e2b-9293-5f34890e88dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:16.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4652" for this suite. 08/17/23 08:07:16.58
------------------------------
â€¢ [4.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:12.52
    Aug 17 08:07:12.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 08:07:12.521
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:12.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:12.549
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 08:07:12.554
    Aug 17 08:07:12.559: INFO: Waiting up to 5m0s for pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd" in namespace "emptydir-4652" to be "Succeeded or Failed"
    Aug 17 08:07:12.563: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.957061ms
    Aug 17 08:07:14.567: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007952083s
    Aug 17 08:07:16.565: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006169957s
    STEP: Saw pod success 08/17/23 08:07:16.565
    Aug 17 08:07:16.565: INFO: Pod "pod-078159b5-224d-4e2b-9293-5f34890e88dd" satisfied condition "Succeeded or Failed"
    Aug 17 08:07:16.567: INFO: Trying to get logs from node yst-node2 pod pod-078159b5-224d-4e2b-9293-5f34890e88dd container test-container: <nil>
    STEP: delete the pod 08/17/23 08:07:16.57
    Aug 17 08:07:16.576: INFO: Waiting for pod pod-078159b5-224d-4e2b-9293-5f34890e88dd to disappear
    Aug 17 08:07:16.578: INFO: Pod pod-078159b5-224d-4e2b-9293-5f34890e88dd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:16.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4652" for this suite. 08/17/23 08:07:16.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:16.585
Aug 17 08:07:16.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename controllerrevisions 08/17/23 08:07:16.585
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:16.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:16.595
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-g9l27-daemon-set" 08/17/23 08:07:16.63
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 08:07:16.634
Aug 17 08:07:16.643: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 0
Aug 17 08:07:16.643: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 08:07:17.648: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 0
Aug 17 08:07:17.648: INFO: Node yst-master is running 0 daemon pod, expected 1
Aug 17 08:07:18.649: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 3
Aug 17 08:07:18.649: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-g9l27-daemon-set
STEP: Confirm DaemonSet "e2e-g9l27-daemon-set" successfully created with "daemonset-name=e2e-g9l27-daemon-set" label 08/17/23 08:07:18.65
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-g9l27-daemon-set" 08/17/23 08:07:18.653
Aug 17 08:07:18.656: INFO: Located ControllerRevision: "e2e-g9l27-daemon-set-64465d76"
STEP: Patching ControllerRevision "e2e-g9l27-daemon-set-64465d76" 08/17/23 08:07:18.657
Aug 17 08:07:18.660: INFO: e2e-g9l27-daemon-set-64465d76 has been patched
STEP: Create a new ControllerRevision 08/17/23 08:07:18.66
Aug 17 08:07:18.662: INFO: Created ControllerRevision: e2e-g9l27-daemon-set-56589f9b8b
STEP: Confirm that there are two ControllerRevisions 08/17/23 08:07:18.662
Aug 17 08:07:18.662: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 08:07:18.664: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-g9l27-daemon-set-64465d76" 08/17/23 08:07:18.664
STEP: Confirm that there is only one ControllerRevision 08/17/23 08:07:18.666
Aug 17 08:07:18.666: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 08:07:18.667: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-g9l27-daemon-set-56589f9b8b" 08/17/23 08:07:18.668
Aug 17 08:07:18.671: INFO: e2e-g9l27-daemon-set-56589f9b8b has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/17/23 08:07:18.671
W0817 08:07:18.674835      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/17/23 08:07:18.674
Aug 17 08:07:18.674: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 08:07:19.676: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 08:07:19.678: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-g9l27-daemon-set-56589f9b8b=updated" 08/17/23 08:07:19.678
STEP: Confirm that there is only one ControllerRevision 08/17/23 08:07:19.682
Aug 17 08:07:19.682: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 08:07:19.683: INFO: Found 1 ControllerRevisions
Aug 17 08:07:19.684: INFO: ControllerRevision "e2e-g9l27-daemon-set-7d948fbcff" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-g9l27-daemon-set" 08/17/23 08:07:19.686
STEP: deleting DaemonSet.extensions e2e-g9l27-daemon-set in namespace controllerrevisions-8699, will wait for the garbage collector to delete the pods 08/17/23 08:07:19.686
Aug 17 08:07:19.741: INFO: Deleting DaemonSet.extensions e2e-g9l27-daemon-set took: 3.25579ms
Aug 17 08:07:19.842: INFO: Terminating DaemonSet.extensions e2e-g9l27-daemon-set pods took: 101.108613ms
Aug 17 08:07:21.444: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 0
Aug 17 08:07:21.444: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-g9l27-daemon-set
Aug 17 08:07:21.446: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27077838"},"items":null}

Aug 17 08:07:21.447: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27077838"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:21.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-8699" for this suite. 08/17/23 08:07:21.46
------------------------------
â€¢ [4.879 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:16.585
    Aug 17 08:07:16.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename controllerrevisions 08/17/23 08:07:16.585
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:16.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:16.595
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-g9l27-daemon-set" 08/17/23 08:07:16.63
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 08:07:16.634
    Aug 17 08:07:16.643: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 0
    Aug 17 08:07:16.643: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 08:07:17.648: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 0
    Aug 17 08:07:17.648: INFO: Node yst-master is running 0 daemon pod, expected 1
    Aug 17 08:07:18.649: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 3
    Aug 17 08:07:18.649: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-g9l27-daemon-set
    STEP: Confirm DaemonSet "e2e-g9l27-daemon-set" successfully created with "daemonset-name=e2e-g9l27-daemon-set" label 08/17/23 08:07:18.65
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-g9l27-daemon-set" 08/17/23 08:07:18.653
    Aug 17 08:07:18.656: INFO: Located ControllerRevision: "e2e-g9l27-daemon-set-64465d76"
    STEP: Patching ControllerRevision "e2e-g9l27-daemon-set-64465d76" 08/17/23 08:07:18.657
    Aug 17 08:07:18.660: INFO: e2e-g9l27-daemon-set-64465d76 has been patched
    STEP: Create a new ControllerRevision 08/17/23 08:07:18.66
    Aug 17 08:07:18.662: INFO: Created ControllerRevision: e2e-g9l27-daemon-set-56589f9b8b
    STEP: Confirm that there are two ControllerRevisions 08/17/23 08:07:18.662
    Aug 17 08:07:18.662: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 08:07:18.664: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-g9l27-daemon-set-64465d76" 08/17/23 08:07:18.664
    STEP: Confirm that there is only one ControllerRevision 08/17/23 08:07:18.666
    Aug 17 08:07:18.666: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 08:07:18.667: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-g9l27-daemon-set-56589f9b8b" 08/17/23 08:07:18.668
    Aug 17 08:07:18.671: INFO: e2e-g9l27-daemon-set-56589f9b8b has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/17/23 08:07:18.671
    W0817 08:07:18.674835      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/17/23 08:07:18.674
    Aug 17 08:07:18.674: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 08:07:19.676: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 08:07:19.678: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-g9l27-daemon-set-56589f9b8b=updated" 08/17/23 08:07:19.678
    STEP: Confirm that there is only one ControllerRevision 08/17/23 08:07:19.682
    Aug 17 08:07:19.682: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 08:07:19.683: INFO: Found 1 ControllerRevisions
    Aug 17 08:07:19.684: INFO: ControllerRevision "e2e-g9l27-daemon-set-7d948fbcff" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-g9l27-daemon-set" 08/17/23 08:07:19.686
    STEP: deleting DaemonSet.extensions e2e-g9l27-daemon-set in namespace controllerrevisions-8699, will wait for the garbage collector to delete the pods 08/17/23 08:07:19.686
    Aug 17 08:07:19.741: INFO: Deleting DaemonSet.extensions e2e-g9l27-daemon-set took: 3.25579ms
    Aug 17 08:07:19.842: INFO: Terminating DaemonSet.extensions e2e-g9l27-daemon-set pods took: 101.108613ms
    Aug 17 08:07:21.444: INFO: Number of nodes with available pods controlled by daemonset e2e-g9l27-daemon-set: 0
    Aug 17 08:07:21.444: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-g9l27-daemon-set
    Aug 17 08:07:21.446: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27077838"},"items":null}

    Aug 17 08:07:21.447: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27077838"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:21.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-8699" for this suite. 08/17/23 08:07:21.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:21.464
Aug 17 08:07:21.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 08:07:21.465
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:21.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:21.474
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/17/23 08:07:21.476
STEP: Creating a ResourceQuota 08/17/23 08:07:26.478
STEP: Ensuring resource quota status is calculated 08/17/23 08:07:26.481
STEP: Creating a Service 08/17/23 08:07:28.485
STEP: Creating a NodePort Service 08/17/23 08:07:28.496
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/17/23 08:07:28.509
STEP: Ensuring resource quota status captures service creation 08/17/23 08:07:28.523
STEP: Deleting Services 08/17/23 08:07:30.526
STEP: Ensuring resource quota status released usage 08/17/23 08:07:30.544
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9143" for this suite. 08/17/23 08:07:32.55
------------------------------
â€¢ [SLOW TEST] [11.089 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:21.464
    Aug 17 08:07:21.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 08:07:21.465
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:21.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:21.474
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/17/23 08:07:21.476
    STEP: Creating a ResourceQuota 08/17/23 08:07:26.478
    STEP: Ensuring resource quota status is calculated 08/17/23 08:07:26.481
    STEP: Creating a Service 08/17/23 08:07:28.485
    STEP: Creating a NodePort Service 08/17/23 08:07:28.496
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/17/23 08:07:28.509
    STEP: Ensuring resource quota status captures service creation 08/17/23 08:07:28.523
    STEP: Deleting Services 08/17/23 08:07:30.526
    STEP: Ensuring resource quota status released usage 08/17/23 08:07:30.544
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9143" for this suite. 08/17/23 08:07:32.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:32.554
Aug 17 08:07:32.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 08:07:32.554
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:32.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:32.565
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/17/23 08:07:32.571
Aug 17 08:07:32.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 create -f -'
Aug 17 08:07:33.651: INFO: stderr: ""
Aug 17 08:07:33.651: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 08:07:33.651
Aug 17 08:07:33.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 08:07:33.732: INFO: stderr: ""
Aug 17 08:07:33.732: INFO: stdout: "update-demo-nautilus-ftc6h update-demo-nautilus-v9tpx "
Aug 17 08:07:33.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-ftc6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 08:07:33.817: INFO: stderr: ""
Aug 17 08:07:33.817: INFO: stdout: ""
Aug 17 08:07:33.817: INFO: update-demo-nautilus-ftc6h is created but not running
Aug 17 08:07:38.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 08:07:38.908: INFO: stderr: ""
Aug 17 08:07:38.908: INFO: stdout: "update-demo-nautilus-ftc6h update-demo-nautilus-v9tpx "
Aug 17 08:07:38.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-ftc6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 08:07:38.993: INFO: stderr: ""
Aug 17 08:07:38.993: INFO: stdout: "true"
Aug 17 08:07:38.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-ftc6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 08:07:39.077: INFO: stderr: ""
Aug 17 08:07:39.077: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 08:07:39.077: INFO: validating pod update-demo-nautilus-ftc6h
Aug 17 08:07:39.080: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 08:07:39.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 08:07:39.080: INFO: update-demo-nautilus-ftc6h is verified up and running
Aug 17 08:07:39.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 08:07:39.161: INFO: stderr: ""
Aug 17 08:07:39.161: INFO: stdout: "true"
Aug 17 08:07:39.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 08:07:39.243: INFO: stderr: ""
Aug 17 08:07:39.243: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 08:07:39.243: INFO: validating pod update-demo-nautilus-v9tpx
Aug 17 08:07:39.246: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 08:07:39.246: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 08:07:39.246: INFO: update-demo-nautilus-v9tpx is verified up and running
STEP: scaling down the replication controller 08/17/23 08:07:39.246
Aug 17 08:07:39.248: INFO: scanned /root for discovery docs: <nil>
Aug 17 08:07:39.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 17 08:07:40.347: INFO: stderr: ""
Aug 17 08:07:40.347: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 08:07:40.347
Aug 17 08:07:40.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 08:07:40.431: INFO: stderr: ""
Aug 17 08:07:40.431: INFO: stdout: "update-demo-nautilus-ftc6h update-demo-nautilus-v9tpx "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/17/23 08:07:40.431
Aug 17 08:07:45.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 08:07:45.519: INFO: stderr: ""
Aug 17 08:07:45.519: INFO: stdout: "update-demo-nautilus-v9tpx "
Aug 17 08:07:45.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 08:07:45.599: INFO: stderr: ""
Aug 17 08:07:45.599: INFO: stdout: "true"
Aug 17 08:07:45.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 08:07:45.675: INFO: stderr: ""
Aug 17 08:07:45.675: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 08:07:45.675: INFO: validating pod update-demo-nautilus-v9tpx
Aug 17 08:07:45.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 08:07:45.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 08:07:45.678: INFO: update-demo-nautilus-v9tpx is verified up and running
STEP: scaling up the replication controller 08/17/23 08:07:45.678
Aug 17 08:07:45.679: INFO: scanned /root for discovery docs: <nil>
Aug 17 08:07:45.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 17 08:07:46.775: INFO: stderr: ""
Aug 17 08:07:46.776: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 08:07:46.776
Aug 17 08:07:46.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 08:07:46.858: INFO: stderr: ""
Aug 17 08:07:46.858: INFO: stdout: "update-demo-nautilus-hw5r8 update-demo-nautilus-v9tpx "
Aug 17 08:07:46.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-hw5r8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 08:07:46.937: INFO: stderr: ""
Aug 17 08:07:46.937: INFO: stdout: ""
Aug 17 08:07:46.937: INFO: update-demo-nautilus-hw5r8 is created but not running
Aug 17 08:07:51.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 08:07:52.022: INFO: stderr: ""
Aug 17 08:07:52.022: INFO: stdout: "update-demo-nautilus-hw5r8 update-demo-nautilus-v9tpx "
Aug 17 08:07:52.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-hw5r8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 08:07:52.103: INFO: stderr: ""
Aug 17 08:07:52.103: INFO: stdout: "true"
Aug 17 08:07:52.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-hw5r8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 08:07:52.180: INFO: stderr: ""
Aug 17 08:07:52.180: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 08:07:52.180: INFO: validating pod update-demo-nautilus-hw5r8
Aug 17 08:07:52.183: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 08:07:52.183: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 08:07:52.183: INFO: update-demo-nautilus-hw5r8 is verified up and running
Aug 17 08:07:52.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 08:07:52.262: INFO: stderr: ""
Aug 17 08:07:52.262: INFO: stdout: "true"
Aug 17 08:07:52.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 08:07:52.346: INFO: stderr: ""
Aug 17 08:07:52.346: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 08:07:52.346: INFO: validating pod update-demo-nautilus-v9tpx
Aug 17 08:07:52.348: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 08:07:52.348: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 08:07:52.348: INFO: update-demo-nautilus-v9tpx is verified up and running
STEP: using delete to clean up resources 08/17/23 08:07:52.348
Aug 17 08:07:52.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 delete --grace-period=0 --force -f -'
Aug 17 08:07:52.426: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 08:07:52.426: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 17 08:07:52.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get rc,svc -l name=update-demo --no-headers'
Aug 17 08:07:52.578: INFO: stderr: "No resources found in kubectl-8777 namespace.\n"
Aug 17 08:07:52.578: INFO: stdout: ""
Aug 17 08:07:52.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 08:07:52.670: INFO: stderr: ""
Aug 17 08:07:52.670: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:52.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8777" for this suite. 08/17/23 08:07:52.673
------------------------------
â€¢ [SLOW TEST] [20.123 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:32.554
    Aug 17 08:07:32.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 08:07:32.554
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:32.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:32.565
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/17/23 08:07:32.571
    Aug 17 08:07:32.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 create -f -'
    Aug 17 08:07:33.651: INFO: stderr: ""
    Aug 17 08:07:33.651: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 08:07:33.651
    Aug 17 08:07:33.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 08:07:33.732: INFO: stderr: ""
    Aug 17 08:07:33.732: INFO: stdout: "update-demo-nautilus-ftc6h update-demo-nautilus-v9tpx "
    Aug 17 08:07:33.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-ftc6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 08:07:33.817: INFO: stderr: ""
    Aug 17 08:07:33.817: INFO: stdout: ""
    Aug 17 08:07:33.817: INFO: update-demo-nautilus-ftc6h is created but not running
    Aug 17 08:07:38.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 08:07:38.908: INFO: stderr: ""
    Aug 17 08:07:38.908: INFO: stdout: "update-demo-nautilus-ftc6h update-demo-nautilus-v9tpx "
    Aug 17 08:07:38.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-ftc6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 08:07:38.993: INFO: stderr: ""
    Aug 17 08:07:38.993: INFO: stdout: "true"
    Aug 17 08:07:38.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-ftc6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 08:07:39.077: INFO: stderr: ""
    Aug 17 08:07:39.077: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 08:07:39.077: INFO: validating pod update-demo-nautilus-ftc6h
    Aug 17 08:07:39.080: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 08:07:39.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 08:07:39.080: INFO: update-demo-nautilus-ftc6h is verified up and running
    Aug 17 08:07:39.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 08:07:39.161: INFO: stderr: ""
    Aug 17 08:07:39.161: INFO: stdout: "true"
    Aug 17 08:07:39.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 08:07:39.243: INFO: stderr: ""
    Aug 17 08:07:39.243: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 08:07:39.243: INFO: validating pod update-demo-nautilus-v9tpx
    Aug 17 08:07:39.246: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 08:07:39.246: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 08:07:39.246: INFO: update-demo-nautilus-v9tpx is verified up and running
    STEP: scaling down the replication controller 08/17/23 08:07:39.246
    Aug 17 08:07:39.248: INFO: scanned /root for discovery docs: <nil>
    Aug 17 08:07:39.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 17 08:07:40.347: INFO: stderr: ""
    Aug 17 08:07:40.347: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 08:07:40.347
    Aug 17 08:07:40.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 08:07:40.431: INFO: stderr: ""
    Aug 17 08:07:40.431: INFO: stdout: "update-demo-nautilus-ftc6h update-demo-nautilus-v9tpx "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/17/23 08:07:40.431
    Aug 17 08:07:45.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 08:07:45.519: INFO: stderr: ""
    Aug 17 08:07:45.519: INFO: stdout: "update-demo-nautilus-v9tpx "
    Aug 17 08:07:45.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 08:07:45.599: INFO: stderr: ""
    Aug 17 08:07:45.599: INFO: stdout: "true"
    Aug 17 08:07:45.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 08:07:45.675: INFO: stderr: ""
    Aug 17 08:07:45.675: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 08:07:45.675: INFO: validating pod update-demo-nautilus-v9tpx
    Aug 17 08:07:45.678: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 08:07:45.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 08:07:45.678: INFO: update-demo-nautilus-v9tpx is verified up and running
    STEP: scaling up the replication controller 08/17/23 08:07:45.678
    Aug 17 08:07:45.679: INFO: scanned /root for discovery docs: <nil>
    Aug 17 08:07:45.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 17 08:07:46.775: INFO: stderr: ""
    Aug 17 08:07:46.776: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 08:07:46.776
    Aug 17 08:07:46.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 08:07:46.858: INFO: stderr: ""
    Aug 17 08:07:46.858: INFO: stdout: "update-demo-nautilus-hw5r8 update-demo-nautilus-v9tpx "
    Aug 17 08:07:46.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-hw5r8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 08:07:46.937: INFO: stderr: ""
    Aug 17 08:07:46.937: INFO: stdout: ""
    Aug 17 08:07:46.937: INFO: update-demo-nautilus-hw5r8 is created but not running
    Aug 17 08:07:51.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 08:07:52.022: INFO: stderr: ""
    Aug 17 08:07:52.022: INFO: stdout: "update-demo-nautilus-hw5r8 update-demo-nautilus-v9tpx "
    Aug 17 08:07:52.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-hw5r8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 08:07:52.103: INFO: stderr: ""
    Aug 17 08:07:52.103: INFO: stdout: "true"
    Aug 17 08:07:52.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-hw5r8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 08:07:52.180: INFO: stderr: ""
    Aug 17 08:07:52.180: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 08:07:52.180: INFO: validating pod update-demo-nautilus-hw5r8
    Aug 17 08:07:52.183: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 08:07:52.183: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 08:07:52.183: INFO: update-demo-nautilus-hw5r8 is verified up and running
    Aug 17 08:07:52.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 08:07:52.262: INFO: stderr: ""
    Aug 17 08:07:52.262: INFO: stdout: "true"
    Aug 17 08:07:52.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods update-demo-nautilus-v9tpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 08:07:52.346: INFO: stderr: ""
    Aug 17 08:07:52.346: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 08:07:52.346: INFO: validating pod update-demo-nautilus-v9tpx
    Aug 17 08:07:52.348: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 08:07:52.348: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 08:07:52.348: INFO: update-demo-nautilus-v9tpx is verified up and running
    STEP: using delete to clean up resources 08/17/23 08:07:52.348
    Aug 17 08:07:52.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 delete --grace-period=0 --force -f -'
    Aug 17 08:07:52.426: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 08:07:52.426: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 17 08:07:52.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get rc,svc -l name=update-demo --no-headers'
    Aug 17 08:07:52.578: INFO: stderr: "No resources found in kubectl-8777 namespace.\n"
    Aug 17 08:07:52.578: INFO: stdout: ""
    Aug 17 08:07:52.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-8777 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 17 08:07:52.670: INFO: stderr: ""
    Aug 17 08:07:52.670: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:52.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8777" for this suite. 08/17/23 08:07:52.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:52.677
Aug 17 08:07:52.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 08:07:52.678
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:52.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:52.703
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-7664/configmap-test-5e8ff709-e854-4090-8421-f36eba32c04d 08/17/23 08:07:52.706
STEP: Creating a pod to test consume configMaps 08/17/23 08:07:52.714
Aug 17 08:07:52.720: INFO: Waiting up to 5m0s for pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f" in namespace "configmap-7664" to be "Succeeded or Failed"
Aug 17 08:07:52.723: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351256ms
Aug 17 08:07:54.729: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008418331s
Aug 17 08:07:56.725: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Running", Reason="", readiness=false. Elapsed: 4.004934054s
Aug 17 08:07:58.725: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004637435s
STEP: Saw pod success 08/17/23 08:07:58.725
Aug 17 08:07:58.725: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f" satisfied condition "Succeeded or Failed"
Aug 17 08:07:58.728: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f container env-test: <nil>
STEP: delete the pod 08/17/23 08:07:58.731
Aug 17 08:07:58.736: INFO: Waiting for pod pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f to disappear
Aug 17 08:07:58.738: INFO: Pod pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:58.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7664" for this suite. 08/17/23 08:07:58.741
------------------------------
â€¢ [SLOW TEST] [6.069 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:52.677
    Aug 17 08:07:52.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 08:07:52.678
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:52.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:52.703
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-7664/configmap-test-5e8ff709-e854-4090-8421-f36eba32c04d 08/17/23 08:07:52.706
    STEP: Creating a pod to test consume configMaps 08/17/23 08:07:52.714
    Aug 17 08:07:52.720: INFO: Waiting up to 5m0s for pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f" in namespace "configmap-7664" to be "Succeeded or Failed"
    Aug 17 08:07:52.723: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351256ms
    Aug 17 08:07:54.729: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008418331s
    Aug 17 08:07:56.725: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Running", Reason="", readiness=false. Elapsed: 4.004934054s
    Aug 17 08:07:58.725: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004637435s
    STEP: Saw pod success 08/17/23 08:07:58.725
    Aug 17 08:07:58.725: INFO: Pod "pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f" satisfied condition "Succeeded or Failed"
    Aug 17 08:07:58.728: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f container env-test: <nil>
    STEP: delete the pod 08/17/23 08:07:58.731
    Aug 17 08:07:58.736: INFO: Waiting for pod pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f to disappear
    Aug 17 08:07:58.738: INFO: Pod pod-configmaps-317ff382-3099-468f-8b57-3c55cdd3071f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:58.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7664" for this suite. 08/17/23 08:07:58.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:58.746
Aug 17 08:07:58.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 08:07:58.747
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:58.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:58.756
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/17/23 08:07:58.758
Aug 17 08:07:58.758: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9082 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/17/23 08:07:58.807
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 08:07:58.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9082" for this suite. 08/17/23 08:07:58.815
------------------------------
â€¢ [0.071 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:58.746
    Aug 17 08:07:58.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 08:07:58.747
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:58.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:58.756
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/17/23 08:07:58.758
    Aug 17 08:07:58.758: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-9082 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/17/23 08:07:58.807
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:07:58.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9082" for this suite. 08/17/23 08:07:58.815
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:07:58.818
Aug 17 08:07:58.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename configmap 08/17/23 08:07:58.818
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:58.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:58.841
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-e621f007-f172-4330-99dd-38e83812ac93 08/17/23 08:07:58.843
STEP: Creating a pod to test consume configMaps 08/17/23 08:07:58.851
Aug 17 08:07:58.859: INFO: Waiting up to 5m0s for pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a" in namespace "configmap-1460" to be "Succeeded or Failed"
Aug 17 08:07:58.860: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564836ms
Aug 17 08:08:00.863: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a": Phase="Running", Reason="", readiness=false. Elapsed: 2.003650777s
Aug 17 08:08:02.863: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004218481s
STEP: Saw pod success 08/17/23 08:08:02.863
Aug 17 08:08:02.863: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a" satisfied condition "Succeeded or Failed"
Aug 17 08:08:02.865: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a container agnhost-container: <nil>
STEP: delete the pod 08/17/23 08:08:02.868
Aug 17 08:08:02.875: INFO: Waiting for pod pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a to disappear
Aug 17 08:08:02.876: INFO: Pod pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:02.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1460" for this suite. 08/17/23 08:08:02.879
------------------------------
â€¢ [4.065 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:07:58.818
    Aug 17 08:07:58.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename configmap 08/17/23 08:07:58.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:07:58.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:07:58.841
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-e621f007-f172-4330-99dd-38e83812ac93 08/17/23 08:07:58.843
    STEP: Creating a pod to test consume configMaps 08/17/23 08:07:58.851
    Aug 17 08:07:58.859: INFO: Waiting up to 5m0s for pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a" in namespace "configmap-1460" to be "Succeeded or Failed"
    Aug 17 08:07:58.860: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564836ms
    Aug 17 08:08:00.863: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a": Phase="Running", Reason="", readiness=false. Elapsed: 2.003650777s
    Aug 17 08:08:02.863: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004218481s
    STEP: Saw pod success 08/17/23 08:08:02.863
    Aug 17 08:08:02.863: INFO: Pod "pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a" satisfied condition "Succeeded or Failed"
    Aug 17 08:08:02.865: INFO: Trying to get logs from node yst-node2 pod pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 08:08:02.868
    Aug 17 08:08:02.875: INFO: Waiting for pod pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a to disappear
    Aug 17 08:08:02.876: INFO: Pod pod-configmaps-17424d5d-9477-4b56-8ade-1b9783a49a3a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:02.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1460" for this suite. 08/17/23 08:08:02.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:02.883
Aug 17 08:08:02.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename statefulset 08/17/23 08:08:02.884
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:02.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:02.9
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9530 08/17/23 08:08:02.904
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-9530 08/17/23 08:08:02.927
Aug 17 08:08:02.954: INFO: Found 0 stateful pods, waiting for 1
Aug 17 08:08:12.957: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/17/23 08:08:12.96
STEP: Getting /status 08/17/23 08:08:12.964
Aug 17 08:08:12.967: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/17/23 08:08:12.967
Aug 17 08:08:12.972: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/17/23 08:08:12.972
Aug 17 08:08:12.973: INFO: Observed &StatefulSet event: ADDED
Aug 17 08:08:12.973: INFO: Found Statefulset ss in namespace statefulset-9530 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 08:08:12.973: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/17/23 08:08:12.973
Aug 17 08:08:12.973: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 17 08:08:12.977: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/17/23 08:08:12.977
Aug 17 08:08:12.978: INFO: Observed &StatefulSet event: ADDED
Aug 17 08:08:12.978: INFO: Observed Statefulset ss in namespace statefulset-9530 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 08:08:12.979: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 08:08:12.979: INFO: Deleting all statefulset in ns statefulset-9530
Aug 17 08:08:12.980: INFO: Scaling statefulset ss to 0
Aug 17 08:08:22.990: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 08:08:22.991: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:22.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9530" for this suite. 08/17/23 08:08:23.001
------------------------------
â€¢ [SLOW TEST] [20.122 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:02.883
    Aug 17 08:08:02.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename statefulset 08/17/23 08:08:02.884
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:02.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:02.9
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9530 08/17/23 08:08:02.904
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-9530 08/17/23 08:08:02.927
    Aug 17 08:08:02.954: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 08:08:12.957: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/17/23 08:08:12.96
    STEP: Getting /status 08/17/23 08:08:12.964
    Aug 17 08:08:12.967: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/17/23 08:08:12.967
    Aug 17 08:08:12.972: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/17/23 08:08:12.972
    Aug 17 08:08:12.973: INFO: Observed &StatefulSet event: ADDED
    Aug 17 08:08:12.973: INFO: Found Statefulset ss in namespace statefulset-9530 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 08:08:12.973: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/17/23 08:08:12.973
    Aug 17 08:08:12.973: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 17 08:08:12.977: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/17/23 08:08:12.977
    Aug 17 08:08:12.978: INFO: Observed &StatefulSet event: ADDED
    Aug 17 08:08:12.978: INFO: Observed Statefulset ss in namespace statefulset-9530 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 08:08:12.979: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 08:08:12.979: INFO: Deleting all statefulset in ns statefulset-9530
    Aug 17 08:08:12.980: INFO: Scaling statefulset ss to 0
    Aug 17 08:08:22.990: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 08:08:22.991: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:22.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9530" for this suite. 08/17/23 08:08:23.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:23.005
Aug 17 08:08:23.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 08:08:23.006
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:23.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:23.016
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 17 08:08:23.038: INFO: Waiting up to 2m0s for pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" in namespace "var-expansion-1310" to be "container 0 failed with reason CreateContainerConfigError"
Aug 17 08:08:23.044: INFO: Pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334": Phase="Pending", Reason="", readiness=false. Elapsed: 5.608908ms
Aug 17 08:08:25.047: INFO: Pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008835241s
Aug 17 08:08:25.047: INFO: Pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 17 08:08:25.047: INFO: Deleting pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" in namespace "var-expansion-1310"
Aug 17 08:08:25.051: INFO: Wait up to 5m0s for pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:27.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1310" for this suite. 08/17/23 08:08:27.058
------------------------------
â€¢ [4.056 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:23.005
    Aug 17 08:08:23.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 08:08:23.006
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:23.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:23.016
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 17 08:08:23.038: INFO: Waiting up to 2m0s for pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" in namespace "var-expansion-1310" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 17 08:08:23.044: INFO: Pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334": Phase="Pending", Reason="", readiness=false. Elapsed: 5.608908ms
    Aug 17 08:08:25.047: INFO: Pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008835241s
    Aug 17 08:08:25.047: INFO: Pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 17 08:08:25.047: INFO: Deleting pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" in namespace "var-expansion-1310"
    Aug 17 08:08:25.051: INFO: Wait up to 5m0s for pod "var-expansion-36143c3d-d041-44bc-a88a-8bc5ddbfc334" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:27.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1310" for this suite. 08/17/23 08:08:27.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:27.062
Aug 17 08:08:27.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 08:08:27.063
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:27.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:27.072
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-75df702c-0e7c-4ff2-a3bf-b25939e7f43f 08/17/23 08:08:27.074
STEP: Creating a pod to test consume secrets 08/17/23 08:08:27.089
Aug 17 08:08:27.103: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a" in namespace "projected-7328" to be "Succeeded or Failed"
Aug 17 08:08:27.105: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287037ms
Aug 17 08:08:29.107: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a": Phase="Running", Reason="", readiness=false. Elapsed: 2.004548294s
Aug 17 08:08:31.108: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005447157s
STEP: Saw pod success 08/17/23 08:08:31.108
Aug 17 08:08:31.108: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a" satisfied condition "Succeeded or Failed"
Aug 17 08:08:31.110: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 08:08:31.113
Aug 17 08:08:31.119: INFO: Waiting for pod pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a to disappear
Aug 17 08:08:31.120: INFO: Pod pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:31.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7328" for this suite. 08/17/23 08:08:31.123
------------------------------
â€¢ [4.063 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:27.062
    Aug 17 08:08:27.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 08:08:27.063
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:27.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:27.072
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-75df702c-0e7c-4ff2-a3bf-b25939e7f43f 08/17/23 08:08:27.074
    STEP: Creating a pod to test consume secrets 08/17/23 08:08:27.089
    Aug 17 08:08:27.103: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a" in namespace "projected-7328" to be "Succeeded or Failed"
    Aug 17 08:08:27.105: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287037ms
    Aug 17 08:08:29.107: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a": Phase="Running", Reason="", readiness=false. Elapsed: 2.004548294s
    Aug 17 08:08:31.108: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005447157s
    STEP: Saw pod success 08/17/23 08:08:31.108
    Aug 17 08:08:31.108: INFO: Pod "pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a" satisfied condition "Succeeded or Failed"
    Aug 17 08:08:31.110: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 08:08:31.113
    Aug 17 08:08:31.119: INFO: Waiting for pod pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a to disappear
    Aug 17 08:08:31.120: INFO: Pod pod-projected-secrets-1ddac99a-a1d2-4f68-b444-f3a892237d0a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:31.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7328" for this suite. 08/17/23 08:08:31.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:31.126
Aug 17 08:08:31.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename runtimeclass 08/17/23 08:08:31.127
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:31.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:31.142
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:31.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3899" for this suite. 08/17/23 08:08:31.16
------------------------------
â€¢ [0.036 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:31.126
    Aug 17 08:08:31.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 08:08:31.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:31.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:31.142
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:31.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3899" for this suite. 08/17/23 08:08:31.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:31.163
Aug 17 08:08:31.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename init-container 08/17/23 08:08:31.164
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:31.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:31.173
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/17/23 08:08:31.181
Aug 17 08:08:31.181: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:35.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5905" for this suite. 08/17/23 08:08:35.948
------------------------------
â€¢ [4.789 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:31.163
    Aug 17 08:08:31.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename init-container 08/17/23 08:08:31.164
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:31.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:31.173
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/17/23 08:08:31.181
    Aug 17 08:08:31.181: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:35.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5905" for this suite. 08/17/23 08:08:35.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:35.953
Aug 17 08:08:35.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replication-controller 08/17/23 08:08:35.953
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:35.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:35.964
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/17/23 08:08:35.965
STEP: When the matched label of one of its pods change 08/17/23 08:08:35.971
Aug 17 08:08:35.977: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 17 08:08:40.980: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/17/23 08:08:40.986
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:41.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3253" for this suite. 08/17/23 08:08:41.993
------------------------------
â€¢ [SLOW TEST] [6.046 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:35.953
    Aug 17 08:08:35.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replication-controller 08/17/23 08:08:35.953
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:35.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:35.964
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/17/23 08:08:35.965
    STEP: When the matched label of one of its pods change 08/17/23 08:08:35.971
    Aug 17 08:08:35.977: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 17 08:08:40.980: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/17/23 08:08:40.986
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:41.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3253" for this suite. 08/17/23 08:08:41.993
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:41.999
Aug 17 08:08:41.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 08:08:42
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:42.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:42.015
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9499 08/17/23 08:08:42.017
STEP: creating service affinity-nodeport-transition in namespace services-9499 08/17/23 08:08:42.017
STEP: creating replication controller affinity-nodeport-transition in namespace services-9499 08/17/23 08:08:42.049
I0817 08:08:42.054827      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9499, replica count: 3
I0817 08:08:45.105347      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 08:08:45.112: INFO: Creating new exec pod
Aug 17 08:08:45.114: INFO: Waiting up to 5m0s for pod "execpod-affinity8g57b" in namespace "services-9499" to be "running"
Aug 17 08:08:45.116: INFO: Pod "execpod-affinity8g57b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572447ms
Aug 17 08:08:47.118: INFO: Pod "execpod-affinity8g57b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003980839s
Aug 17 08:08:47.118: INFO: Pod "execpod-affinity8g57b" satisfied condition "running"
Aug 17 08:08:48.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 17 08:08:48.286: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 17 08:08:48.286: INFO: stdout: ""
Aug 17 08:08:48.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 10.98.135.160 80'
Aug 17 08:08:48.419: INFO: stderr: "+ nc -v -z -w 2 10.98.135.160 80\nConnection to 10.98.135.160 80 port [tcp/http] succeeded!\n"
Aug 17 08:08:48.419: INFO: stdout: ""
Aug 17 08:08:48.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 10.60.200.177 32357'
Aug 17 08:08:48.537: INFO: stderr: "+ nc -v -z -w 2 10.60.200.177 32357\nConnection to 10.60.200.177 32357 port [tcp/*] succeeded!\n"
Aug 17 08:08:48.537: INFO: stdout: ""
Aug 17 08:08:48.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 10.60.200.175 32357'
Aug 17 08:08:48.663: INFO: stderr: "+ nc -v -z -w 2 10.60.200.175 32357\nConnection to 10.60.200.175 32357 port [tcp/*] succeeded!\n"
Aug 17 08:08:48.663: INFO: stdout: ""
Aug 17 08:08:48.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.60.200.175:32357/ ; done'
Aug 17 08:08:48.860: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n"
Aug 17 08:08:48.861: INFO: stdout: "\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-md2w2"
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:48.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.60.200.175:32357/ ; done'
Aug 17 08:08:49.062: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n"
Aug 17 08:08:49.062: INFO: stdout: "\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2"
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
Aug 17 08:08:49.062: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9499, will wait for the garbage collector to delete the pods 08/17/23 08:08:49.07
Aug 17 08:08:49.126: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.442695ms
Aug 17 08:08:49.227: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.865137ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:52.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9499" for this suite. 08/17/23 08:08:52.642
------------------------------
â€¢ [SLOW TEST] [10.645 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:41.999
    Aug 17 08:08:41.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 08:08:42
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:42.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:42.015
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9499 08/17/23 08:08:42.017
    STEP: creating service affinity-nodeport-transition in namespace services-9499 08/17/23 08:08:42.017
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9499 08/17/23 08:08:42.049
    I0817 08:08:42.054827      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9499, replica count: 3
    I0817 08:08:45.105347      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 08:08:45.112: INFO: Creating new exec pod
    Aug 17 08:08:45.114: INFO: Waiting up to 5m0s for pod "execpod-affinity8g57b" in namespace "services-9499" to be "running"
    Aug 17 08:08:45.116: INFO: Pod "execpod-affinity8g57b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572447ms
    Aug 17 08:08:47.118: INFO: Pod "execpod-affinity8g57b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003980839s
    Aug 17 08:08:47.118: INFO: Pod "execpod-affinity8g57b" satisfied condition "running"
    Aug 17 08:08:48.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 17 08:08:48.286: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 17 08:08:48.286: INFO: stdout: ""
    Aug 17 08:08:48.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 10.98.135.160 80'
    Aug 17 08:08:48.419: INFO: stderr: "+ nc -v -z -w 2 10.98.135.160 80\nConnection to 10.98.135.160 80 port [tcp/http] succeeded!\n"
    Aug 17 08:08:48.419: INFO: stdout: ""
    Aug 17 08:08:48.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 10.60.200.177 32357'
    Aug 17 08:08:48.537: INFO: stderr: "+ nc -v -z -w 2 10.60.200.177 32357\nConnection to 10.60.200.177 32357 port [tcp/*] succeeded!\n"
    Aug 17 08:08:48.537: INFO: stdout: ""
    Aug 17 08:08:48.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c nc -v -z -w 2 10.60.200.175 32357'
    Aug 17 08:08:48.663: INFO: stderr: "+ nc -v -z -w 2 10.60.200.175 32357\nConnection to 10.60.200.175 32357 port [tcp/*] succeeded!\n"
    Aug 17 08:08:48.663: INFO: stdout: ""
    Aug 17 08:08:48.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.60.200.175:32357/ ; done'
    Aug 17 08:08:48.860: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n"
    Aug 17 08:08:48.861: INFO: stdout: "\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-mlsdv\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-vfvnw\naffinity-nodeport-transition-md2w2"
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-mlsdv
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-vfvnw
    Aug 17 08:08:48.861: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:48.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-9499 exec execpod-affinity8g57b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.60.200.175:32357/ ; done'
    Aug 17 08:08:49.062: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.60.200.175:32357/\n"
    Aug 17 08:08:49.062: INFO: stdout: "\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2\naffinity-nodeport-transition-md2w2"
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Received response from host: affinity-nodeport-transition-md2w2
    Aug 17 08:08:49.062: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9499, will wait for the garbage collector to delete the pods 08/17/23 08:08:49.07
    Aug 17 08:08:49.126: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.442695ms
    Aug 17 08:08:49.227: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.865137ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:52.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9499" for this suite. 08/17/23 08:08:52.642
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:52.644
Aug 17 08:08:52.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename disruption 08/17/23 08:08:52.645
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:52.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:52.656
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/17/23 08:08:52.658
STEP: Waiting for the pdb to be processed 08/17/23 08:08:52.661
STEP: updating the pdb 08/17/23 08:08:52.669
STEP: Waiting for the pdb to be processed 08/17/23 08:08:52.677
STEP: patching the pdb 08/17/23 08:08:52.684
STEP: Waiting for the pdb to be processed 08/17/23 08:08:52.69
STEP: Waiting for the pdb to be deleted 08/17/23 08:08:54.697
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:54.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9599" for this suite. 08/17/23 08:08:54.701
------------------------------
â€¢ [2.059 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:52.644
    Aug 17 08:08:52.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename disruption 08/17/23 08:08:52.645
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:52.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:52.656
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/17/23 08:08:52.658
    STEP: Waiting for the pdb to be processed 08/17/23 08:08:52.661
    STEP: updating the pdb 08/17/23 08:08:52.669
    STEP: Waiting for the pdb to be processed 08/17/23 08:08:52.677
    STEP: patching the pdb 08/17/23 08:08:52.684
    STEP: Waiting for the pdb to be processed 08/17/23 08:08:52.69
    STEP: Waiting for the pdb to be deleted 08/17/23 08:08:54.697
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:54.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9599" for this suite. 08/17/23 08:08:54.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:54.705
Aug 17 08:08:54.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-runtime 08/17/23 08:08:54.705
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:54.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:54.767
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/17/23 08:08:54.769
STEP: wait for the container to reach Succeeded 08/17/23 08:08:54.774
STEP: get the container status 08/17/23 08:08:57.783
STEP: the container should be terminated 08/17/23 08:08:57.785
STEP: the termination message should be set 08/17/23 08:08:57.785
Aug 17 08:08:57.785: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/17/23 08:08:57.785
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 08:08:57.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3382" for this suite. 08/17/23 08:08:57.795
------------------------------
â€¢ [3.094 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:54.705
    Aug 17 08:08:54.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-runtime 08/17/23 08:08:54.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:54.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:54.767
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/17/23 08:08:54.769
    STEP: wait for the container to reach Succeeded 08/17/23 08:08:54.774
    STEP: get the container status 08/17/23 08:08:57.783
    STEP: the container should be terminated 08/17/23 08:08:57.785
    STEP: the termination message should be set 08/17/23 08:08:57.785
    Aug 17 08:08:57.785: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/17/23 08:08:57.785
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:08:57.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3382" for this suite. 08/17/23 08:08:57.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:08:57.799
Aug 17 08:08:57.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 08:08:57.8
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:57.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:57.809
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 08:08:57.833
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 08:08:58.152
STEP: Deploying the webhook pod 08/17/23 08:08:58.156
STEP: Wait for the deployment to be ready 08/17/23 08:08:58.162
Aug 17 08:08:58.165: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 08:09:00.171
STEP: Verifying the service has paired with the endpoint 08/17/23 08:09:00.178
Aug 17 08:09:01.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/17/23 08:09:01.208
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:09:01.233
STEP: Deleting the collection of validation webhooks 08/17/23 08:09:01.254
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:09:01.271
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:01.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1062" for this suite. 08/17/23 08:09:01.297
STEP: Destroying namespace "webhook-1062-markers" for this suite. 08/17/23 08:09:01.3
------------------------------
â€¢ [3.505 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:08:57.799
    Aug 17 08:08:57.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 08:08:57.8
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:08:57.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:08:57.809
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 08:08:57.833
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 08:08:58.152
    STEP: Deploying the webhook pod 08/17/23 08:08:58.156
    STEP: Wait for the deployment to be ready 08/17/23 08:08:58.162
    Aug 17 08:08:58.165: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 08:09:00.171
    STEP: Verifying the service has paired with the endpoint 08/17/23 08:09:00.178
    Aug 17 08:09:01.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/17/23 08:09:01.208
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:09:01.233
    STEP: Deleting the collection of validation webhooks 08/17/23 08:09:01.254
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 08:09:01.271
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:01.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1062" for this suite. 08/17/23 08:09:01.297
    STEP: Destroying namespace "webhook-1062-markers" for this suite. 08/17/23 08:09:01.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:01.305
Aug 17 08:09:01.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename containers 08/17/23 08:09:01.305
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:01.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:01.316
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/17/23 08:09:01.318
Aug 17 08:09:01.334: INFO: Waiting up to 5m0s for pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131" in namespace "containers-3325" to be "Succeeded or Failed"
Aug 17 08:09:01.338: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131": Phase="Pending", Reason="", readiness=false. Elapsed: 4.166275ms
Aug 17 08:09:03.341: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007364117s
Aug 17 08:09:05.342: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007549313s
STEP: Saw pod success 08/17/23 08:09:05.342
Aug 17 08:09:05.342: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131" satisfied condition "Succeeded or Failed"
Aug 17 08:09:05.344: INFO: Trying to get logs from node yst-node2 pod client-containers-523c3f9d-2662-4562-9a34-2e3236412131 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 08:09:05.347
Aug 17 08:09:05.353: INFO: Waiting for pod client-containers-523c3f9d-2662-4562-9a34-2e3236412131 to disappear
Aug 17 08:09:05.354: INFO: Pod client-containers-523c3f9d-2662-4562-9a34-2e3236412131 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:05.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3325" for this suite. 08/17/23 08:09:05.356
------------------------------
â€¢ [4.057 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:01.305
    Aug 17 08:09:01.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename containers 08/17/23 08:09:01.305
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:01.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:01.316
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/17/23 08:09:01.318
    Aug 17 08:09:01.334: INFO: Waiting up to 5m0s for pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131" in namespace "containers-3325" to be "Succeeded or Failed"
    Aug 17 08:09:01.338: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131": Phase="Pending", Reason="", readiness=false. Elapsed: 4.166275ms
    Aug 17 08:09:03.341: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007364117s
    Aug 17 08:09:05.342: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007549313s
    STEP: Saw pod success 08/17/23 08:09:05.342
    Aug 17 08:09:05.342: INFO: Pod "client-containers-523c3f9d-2662-4562-9a34-2e3236412131" satisfied condition "Succeeded or Failed"
    Aug 17 08:09:05.344: INFO: Trying to get logs from node yst-node2 pod client-containers-523c3f9d-2662-4562-9a34-2e3236412131 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 08:09:05.347
    Aug 17 08:09:05.353: INFO: Waiting for pod client-containers-523c3f9d-2662-4562-9a34-2e3236412131 to disappear
    Aug 17 08:09:05.354: INFO: Pod client-containers-523c3f9d-2662-4562-9a34-2e3236412131 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:05.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3325" for this suite. 08/17/23 08:09:05.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:05.362
Aug 17 08:09:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 08:09:05.363
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:05.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:05.373
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/17/23 08:09:05.376
Aug 17 08:09:05.396: INFO: Waiting up to 5m0s for pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab" in namespace "projected-9511" to be "running and ready"
Aug 17 08:09:05.398: INFO: Pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744587ms
Aug 17 08:09:05.398: INFO: The phase of Pod annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab is Pending, waiting for it to be Running (with Ready = true)
Aug 17 08:09:07.401: INFO: Pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.00560892s
Aug 17 08:09:07.401: INFO: The phase of Pod annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab is Running (Ready = true)
Aug 17 08:09:07.401: INFO: Pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab" satisfied condition "running and ready"
Aug 17 08:09:07.914: INFO: Successfully updated pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:11.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9511" for this suite. 08/17/23 08:09:11.93
------------------------------
â€¢ [SLOW TEST] [6.570 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:05.362
    Aug 17 08:09:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 08:09:05.363
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:05.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:05.373
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/17/23 08:09:05.376
    Aug 17 08:09:05.396: INFO: Waiting up to 5m0s for pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab" in namespace "projected-9511" to be "running and ready"
    Aug 17 08:09:05.398: INFO: Pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744587ms
    Aug 17 08:09:05.398: INFO: The phase of Pod annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 08:09:07.401: INFO: Pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.00560892s
    Aug 17 08:09:07.401: INFO: The phase of Pod annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab is Running (Ready = true)
    Aug 17 08:09:07.401: INFO: Pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab" satisfied condition "running and ready"
    Aug 17 08:09:07.914: INFO: Successfully updated pod "annotationupdateed341cc2-c917-45e7-9ce2-e623758752ab"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:11.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9511" for this suite. 08/17/23 08:09:11.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:11.934
Aug 17 08:09:11.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename gc 08/17/23 08:09:11.934
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:11.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:11.961
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/17/23 08:09:11.964
STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 08:09:11.972
STEP: delete the deployment 08/17/23 08:09:12.479
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/17/23 08:09:12.483
STEP: Gathering metrics 08/17/23 08:09:12.993
Aug 17 08:09:13.010: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
Aug 17 08:09:13.012: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.742276ms
Aug 17 08:09:13.012: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
Aug 17 08:09:13.012: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
Aug 17 08:09:13.086: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:13.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5915" for this suite. 08/17/23 08:09:13.09
------------------------------
â€¢ [1.161 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:11.934
    Aug 17 08:09:11.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename gc 08/17/23 08:09:11.934
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:11.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:11.961
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/17/23 08:09:11.964
    STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 08:09:11.972
    STEP: delete the deployment 08/17/23 08:09:12.479
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/17/23 08:09:12.483
    STEP: Gathering metrics 08/17/23 08:09:12.993
    Aug 17 08:09:13.010: INFO: Waiting up to 5m0s for pod "kube-controller-manager-yst-master" in namespace "kube-system" to be "running and ready"
    Aug 17 08:09:13.012: INFO: Pod "kube-controller-manager-yst-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.742276ms
    Aug 17 08:09:13.012: INFO: The phase of Pod kube-controller-manager-yst-master is Running (Ready = true)
    Aug 17 08:09:13.012: INFO: Pod "kube-controller-manager-yst-master" satisfied condition "running and ready"
    Aug 17 08:09:13.086: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:13.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5915" for this suite. 08/17/23 08:09:13.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:13.095
Aug 17 08:09:13.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename var-expansion 08/17/23 08:09:13.095
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:13.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:13.107
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/17/23 08:09:13.111
Aug 17 08:09:13.127: INFO: Waiting up to 5m0s for pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e" in namespace "var-expansion-1566" to be "Succeeded or Failed"
Aug 17 08:09:13.134: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.043675ms
Aug 17 08:09:15.136: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009737697s
Aug 17 08:09:17.136: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009077615s
STEP: Saw pod success 08/17/23 08:09:17.136
Aug 17 08:09:17.136: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e" satisfied condition "Succeeded or Failed"
Aug 17 08:09:17.137: INFO: Trying to get logs from node yst-node2 pod var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e container dapi-container: <nil>
STEP: delete the pod 08/17/23 08:09:17.141
Aug 17 08:09:17.147: INFO: Waiting for pod var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e to disappear
Aug 17 08:09:17.149: INFO: Pod var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:17.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1566" for this suite. 08/17/23 08:09:17.152
------------------------------
â€¢ [4.062 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:13.095
    Aug 17 08:09:13.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename var-expansion 08/17/23 08:09:13.095
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:13.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:13.107
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/17/23 08:09:13.111
    Aug 17 08:09:13.127: INFO: Waiting up to 5m0s for pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e" in namespace "var-expansion-1566" to be "Succeeded or Failed"
    Aug 17 08:09:13.134: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.043675ms
    Aug 17 08:09:15.136: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009737697s
    Aug 17 08:09:17.136: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009077615s
    STEP: Saw pod success 08/17/23 08:09:17.136
    Aug 17 08:09:17.136: INFO: Pod "var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e" satisfied condition "Succeeded or Failed"
    Aug 17 08:09:17.137: INFO: Trying to get logs from node yst-node2 pod var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e container dapi-container: <nil>
    STEP: delete the pod 08/17/23 08:09:17.141
    Aug 17 08:09:17.147: INFO: Waiting for pod var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e to disappear
    Aug 17 08:09:17.149: INFO: Pod var-expansion-5f6fd6b1-6a39-485b-ad53-14edc7353d1e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:17.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1566" for this suite. 08/17/23 08:09:17.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:17.158
Aug 17 08:09:17.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 08:09:17.159
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:17.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:17.17
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/17/23 08:09:17.172
Aug 17 08:09:17.192: INFO: Waiting up to 5m0s for pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846" in namespace "emptydir-1786" to be "Succeeded or Failed"
Aug 17 08:09:17.195: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111436ms
Aug 17 08:09:19.198: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006045444s
Aug 17 08:09:21.199: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006752184s
STEP: Saw pod success 08/17/23 08:09:21.199
Aug 17 08:09:21.199: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846" satisfied condition "Succeeded or Failed"
Aug 17 08:09:21.201: INFO: Trying to get logs from node yst-node2 pod pod-32b847cd-6668-45c2-a573-3ace9ebb9846 container test-container: <nil>
STEP: delete the pod 08/17/23 08:09:21.204
Aug 17 08:09:21.211: INFO: Waiting for pod pod-32b847cd-6668-45c2-a573-3ace9ebb9846 to disappear
Aug 17 08:09:21.212: INFO: Pod pod-32b847cd-6668-45c2-a573-3ace9ebb9846 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:21.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1786" for this suite. 08/17/23 08:09:21.215
------------------------------
â€¢ [4.059 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:17.158
    Aug 17 08:09:17.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 08:09:17.159
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:17.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:17.17
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/17/23 08:09:17.172
    Aug 17 08:09:17.192: INFO: Waiting up to 5m0s for pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846" in namespace "emptydir-1786" to be "Succeeded or Failed"
    Aug 17 08:09:17.195: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111436ms
    Aug 17 08:09:19.198: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006045444s
    Aug 17 08:09:21.199: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006752184s
    STEP: Saw pod success 08/17/23 08:09:21.199
    Aug 17 08:09:21.199: INFO: Pod "pod-32b847cd-6668-45c2-a573-3ace9ebb9846" satisfied condition "Succeeded or Failed"
    Aug 17 08:09:21.201: INFO: Trying to get logs from node yst-node2 pod pod-32b847cd-6668-45c2-a573-3ace9ebb9846 container test-container: <nil>
    STEP: delete the pod 08/17/23 08:09:21.204
    Aug 17 08:09:21.211: INFO: Waiting for pod pod-32b847cd-6668-45c2-a573-3ace9ebb9846 to disappear
    Aug 17 08:09:21.212: INFO: Pod pod-32b847cd-6668-45c2-a573-3ace9ebb9846 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:21.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1786" for this suite. 08/17/23 08:09:21.215
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:21.217
Aug 17 08:09:21.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 08:09:21.218
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:21.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:21.227
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 08:09:21.234
Aug 17 08:09:21.282: INFO: Waiting up to 5m0s for pod "pod-77617d00-481c-45f0-97eb-e458797ef33f" in namespace "emptydir-7587" to be "Succeeded or Failed"
Aug 17 08:09:21.313: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.849769ms
Aug 17 08:09:23.316: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03315328s
Aug 17 08:09:25.316: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033319709s
STEP: Saw pod success 08/17/23 08:09:25.316
Aug 17 08:09:25.316: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f" satisfied condition "Succeeded or Failed"
Aug 17 08:09:25.317: INFO: Trying to get logs from node yst-node2 pod pod-77617d00-481c-45f0-97eb-e458797ef33f container test-container: <nil>
STEP: delete the pod 08/17/23 08:09:25.321
Aug 17 08:09:25.326: INFO: Waiting for pod pod-77617d00-481c-45f0-97eb-e458797ef33f to disappear
Aug 17 08:09:25.327: INFO: Pod pod-77617d00-481c-45f0-97eb-e458797ef33f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:25.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7587" for this suite. 08/17/23 08:09:25.33
------------------------------
â€¢ [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:21.217
    Aug 17 08:09:21.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 08:09:21.218
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:21.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:21.227
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 08:09:21.234
    Aug 17 08:09:21.282: INFO: Waiting up to 5m0s for pod "pod-77617d00-481c-45f0-97eb-e458797ef33f" in namespace "emptydir-7587" to be "Succeeded or Failed"
    Aug 17 08:09:21.313: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.849769ms
    Aug 17 08:09:23.316: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03315328s
    Aug 17 08:09:25.316: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033319709s
    STEP: Saw pod success 08/17/23 08:09:25.316
    Aug 17 08:09:25.316: INFO: Pod "pod-77617d00-481c-45f0-97eb-e458797ef33f" satisfied condition "Succeeded or Failed"
    Aug 17 08:09:25.317: INFO: Trying to get logs from node yst-node2 pod pod-77617d00-481c-45f0-97eb-e458797ef33f container test-container: <nil>
    STEP: delete the pod 08/17/23 08:09:25.321
    Aug 17 08:09:25.326: INFO: Waiting for pod pod-77617d00-481c-45f0-97eb-e458797ef33f to disappear
    Aug 17 08:09:25.327: INFO: Pod pod-77617d00-481c-45f0-97eb-e458797ef33f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:25.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7587" for this suite. 08/17/23 08:09:25.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:25.333
Aug 17 08:09:25.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename pod-network-test 08/17/23 08:09:25.334
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:25.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:25.36
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3490 08/17/23 08:09:25.363
STEP: creating a selector 08/17/23 08:09:25.363
STEP: Creating the service pods in kubernetes 08/17/23 08:09:25.363
Aug 17 08:09:25.363: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 08:09:25.396: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3490" to be "running and ready"
Aug 17 08:09:25.400: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12703ms
Aug 17 08:09:25.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 08:09:27.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006953161s
Aug 17 08:09:27.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:29.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006992237s
Aug 17 08:09:29.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:31.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006580822s
Aug 17 08:09:31.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:33.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006871428s
Aug 17 08:09:33.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:35.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00735958s
Aug 17 08:09:35.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:37.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.0065745s
Aug 17 08:09:37.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:39.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006243338s
Aug 17 08:09:39.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:41.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00681387s
Aug 17 08:09:41.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:43.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007118637s
Aug 17 08:09:43.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:45.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006269448s
Aug 17 08:09:45.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 08:09:47.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006479528s
Aug 17 08:09:47.403: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 08:09:47.403: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 08:09:47.404: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3490" to be "running and ready"
Aug 17 08:09:47.406: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.728561ms
Aug 17 08:09:47.406: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 08:09:47.406: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 17 08:09:47.407: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3490" to be "running and ready"
Aug 17 08:09:47.409: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.291749ms
Aug 17 08:09:47.409: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 17 08:09:47.409: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 08:09:47.41
Aug 17 08:09:47.413: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3490" to be "running"
Aug 17 08:09:47.415: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.87275ms
Aug 17 08:09:49.418: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005204327s
Aug 17 08:09:49.418: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 08:09:49.420: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 17 08:09:49.420: INFO: Breadth first check of 172.32.3.25 on host 10.60.200.175...
Aug 17 08:09:49.421: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.105:9080/dial?request=hostname&protocol=udp&host=172.32.3.25&port=8081&tries=1'] Namespace:pod-network-test-3490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 08:09:49.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 08:09:49.422: INFO: ExecWithOptions: Clientset creation
Aug 17 08:09:49.422: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.32.3.25%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 08:09:49.472: INFO: Waiting for responses: map[]
Aug 17 08:09:49.472: INFO: reached 172.32.3.25 after 0/1 tries
Aug 17 08:09:49.472: INFO: Breadth first check of 172.32.123.171 on host 10.60.200.176...
Aug 17 08:09:49.474: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.105:9080/dial?request=hostname&protocol=udp&host=172.32.123.171&port=8081&tries=1'] Namespace:pod-network-test-3490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 08:09:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 08:09:49.474: INFO: ExecWithOptions: Clientset creation
Aug 17 08:09:49.474: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.32.123.171%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 08:09:49.531: INFO: Waiting for responses: map[]
Aug 17 08:09:49.531: INFO: reached 172.32.123.171 after 0/1 tries
Aug 17 08:09:49.531: INFO: Breadth first check of 172.32.238.126 on host 10.60.200.177...
Aug 17 08:09:49.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.105:9080/dial?request=hostname&protocol=udp&host=172.32.238.126&port=8081&tries=1'] Namespace:pod-network-test-3490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 08:09:49.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
Aug 17 08:09:49.533: INFO: ExecWithOptions: Clientset creation
Aug 17 08:09:49.533: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.32.238.126%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 08:09:49.582: INFO: Waiting for responses: map[]
Aug 17 08:09:49.582: INFO: reached 172.32.238.126 after 0/1 tries
Aug 17 08:09:49.582: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 08:09:49.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3490" for this suite. 08/17/23 08:09:49.585
------------------------------
â€¢ [SLOW TEST] [24.257 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:25.333
    Aug 17 08:09:25.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 08:09:25.334
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:25.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:25.36
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3490 08/17/23 08:09:25.363
    STEP: creating a selector 08/17/23 08:09:25.363
    STEP: Creating the service pods in kubernetes 08/17/23 08:09:25.363
    Aug 17 08:09:25.363: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 08:09:25.396: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3490" to be "running and ready"
    Aug 17 08:09:25.400: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12703ms
    Aug 17 08:09:25.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 08:09:27.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006953161s
    Aug 17 08:09:27.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:29.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006992237s
    Aug 17 08:09:29.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:31.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006580822s
    Aug 17 08:09:31.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:33.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006871428s
    Aug 17 08:09:33.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:35.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00735958s
    Aug 17 08:09:35.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:37.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.0065745s
    Aug 17 08:09:37.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:39.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006243338s
    Aug 17 08:09:39.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:41.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00681387s
    Aug 17 08:09:41.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:43.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007118637s
    Aug 17 08:09:43.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:45.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006269448s
    Aug 17 08:09:45.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 08:09:47.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006479528s
    Aug 17 08:09:47.403: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 08:09:47.403: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 08:09:47.404: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3490" to be "running and ready"
    Aug 17 08:09:47.406: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.728561ms
    Aug 17 08:09:47.406: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 08:09:47.406: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 17 08:09:47.407: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3490" to be "running and ready"
    Aug 17 08:09:47.409: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.291749ms
    Aug 17 08:09:47.409: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 17 08:09:47.409: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 08:09:47.41
    Aug 17 08:09:47.413: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3490" to be "running"
    Aug 17 08:09:47.415: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.87275ms
    Aug 17 08:09:49.418: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005204327s
    Aug 17 08:09:49.418: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 08:09:49.420: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 17 08:09:49.420: INFO: Breadth first check of 172.32.3.25 on host 10.60.200.175...
    Aug 17 08:09:49.421: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.105:9080/dial?request=hostname&protocol=udp&host=172.32.3.25&port=8081&tries=1'] Namespace:pod-network-test-3490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 08:09:49.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 08:09:49.422: INFO: ExecWithOptions: Clientset creation
    Aug 17 08:09:49.422: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.32.3.25%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 08:09:49.472: INFO: Waiting for responses: map[]
    Aug 17 08:09:49.472: INFO: reached 172.32.3.25 after 0/1 tries
    Aug 17 08:09:49.472: INFO: Breadth first check of 172.32.123.171 on host 10.60.200.176...
    Aug 17 08:09:49.474: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.105:9080/dial?request=hostname&protocol=udp&host=172.32.123.171&port=8081&tries=1'] Namespace:pod-network-test-3490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 08:09:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 08:09:49.474: INFO: ExecWithOptions: Clientset creation
    Aug 17 08:09:49.474: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.32.123.171%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 08:09:49.531: INFO: Waiting for responses: map[]
    Aug 17 08:09:49.531: INFO: reached 172.32.123.171 after 0/1 tries
    Aug 17 08:09:49.531: INFO: Breadth first check of 172.32.238.126 on host 10.60.200.177...
    Aug 17 08:09:49.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.32.238.105:9080/dial?request=hostname&protocol=udp&host=172.32.238.126&port=8081&tries=1'] Namespace:pod-network-test-3490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 08:09:49.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    Aug 17 08:09:49.533: INFO: ExecWithOptions: Clientset creation
    Aug 17 08:09:49.533: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.32.238.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.32.238.126%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 08:09:49.582: INFO: Waiting for responses: map[]
    Aug 17 08:09:49.582: INFO: reached 172.32.238.126 after 0/1 tries
    Aug 17 08:09:49.582: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:09:49.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3490" for this suite. 08/17/23 08:09:49.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:09:49.59
Aug 17 08:09:49.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename cronjob 08/17/23 08:09:49.591
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:49.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:49.66
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/17/23 08:09:49.662
STEP: Ensuring a job is scheduled 08/17/23 08:09:49.667
STEP: Ensuring exactly one is scheduled 08/17/23 08:10:01.67
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 08:10:01.672
STEP: Ensuring no more jobs are scheduled 08/17/23 08:10:01.673
STEP: Removing cronjob 08/17/23 08:15:01.676
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:01.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4933" for this suite. 08/17/23 08:15:01.682
------------------------------
â€¢ [SLOW TEST] [312.095 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:09:49.59
    Aug 17 08:09:49.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename cronjob 08/17/23 08:09:49.591
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:09:49.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:09:49.66
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/17/23 08:09:49.662
    STEP: Ensuring a job is scheduled 08/17/23 08:09:49.667
    STEP: Ensuring exactly one is scheduled 08/17/23 08:10:01.67
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 08:10:01.672
    STEP: Ensuring no more jobs are scheduled 08/17/23 08:10:01.673
    STEP: Removing cronjob 08/17/23 08:15:01.676
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:01.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4933" for this suite. 08/17/23 08:15:01.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:01.686
Aug 17 08:15:01.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename container-runtime 08/17/23 08:15:01.687
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:01.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:01.702
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/17/23 08:15:01.704
STEP: wait for the container to reach Succeeded 08/17/23 08:15:01.726
STEP: get the container status 08/17/23 08:15:04.745
STEP: the container should be terminated 08/17/23 08:15:04.747
STEP: the termination message should be set 08/17/23 08:15:04.747
Aug 17 08:15:04.747: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/17/23 08:15:04.747
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:04.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8074" for this suite. 08/17/23 08:15:04.758
------------------------------
â€¢ [3.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:01.686
    Aug 17 08:15:01.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename container-runtime 08/17/23 08:15:01.687
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:01.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:01.702
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/17/23 08:15:01.704
    STEP: wait for the container to reach Succeeded 08/17/23 08:15:01.726
    STEP: get the container status 08/17/23 08:15:04.745
    STEP: the container should be terminated 08/17/23 08:15:04.747
    STEP: the termination message should be set 08/17/23 08:15:04.747
    Aug 17 08:15:04.747: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/17/23 08:15:04.747
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:04.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8074" for this suite. 08/17/23 08:15:04.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:04.761
Aug 17 08:15:04.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename tables 08/17/23 08:15:04.761
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:04.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:04.774
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:04.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2513" for this suite. 08/17/23 08:15:04.805
------------------------------
â€¢ [0.049 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:04.761
    Aug 17 08:15:04.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename tables 08/17/23 08:15:04.761
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:04.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:04.774
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:04.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2513" for this suite. 08/17/23 08:15:04.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:04.81
Aug 17 08:15:04.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 08:15:04.811
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:04.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:04.821
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-7cb320f7-4cd0-4b00-9e4e-e4b83a0c6583 08/17/23 08:15:04.823
STEP: Creating a pod to test consume secrets 08/17/23 08:15:04.837
Aug 17 08:15:04.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12" in namespace "projected-1516" to be "Succeeded or Failed"
Aug 17 08:15:04.847: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64396ms
Aug 17 08:15:06.849: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003980411s
Aug 17 08:15:08.850: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005241677s
STEP: Saw pod success 08/17/23 08:15:08.85
Aug 17 08:15:08.850: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12" satisfied condition "Succeeded or Failed"
Aug 17 08:15:08.852: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 08:15:08.862
Aug 17 08:15:08.867: INFO: Waiting for pod pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12 to disappear
Aug 17 08:15:08.870: INFO: Pod pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:08.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1516" for this suite. 08/17/23 08:15:08.873
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:04.81
    Aug 17 08:15:04.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 08:15:04.811
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:04.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:04.821
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-7cb320f7-4cd0-4b00-9e4e-e4b83a0c6583 08/17/23 08:15:04.823
    STEP: Creating a pod to test consume secrets 08/17/23 08:15:04.837
    Aug 17 08:15:04.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12" in namespace "projected-1516" to be "Succeeded or Failed"
    Aug 17 08:15:04.847: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64396ms
    Aug 17 08:15:06.849: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003980411s
    Aug 17 08:15:08.850: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005241677s
    STEP: Saw pod success 08/17/23 08:15:08.85
    Aug 17 08:15:08.850: INFO: Pod "pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12" satisfied condition "Succeeded or Failed"
    Aug 17 08:15:08.852: INFO: Trying to get logs from node yst-node2 pod pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 08:15:08.862
    Aug 17 08:15:08.867: INFO: Waiting for pod pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12 to disappear
    Aug 17 08:15:08.870: INFO: Pod pod-projected-secrets-66738ee1-ec3e-4265-9b48-1270419ccd12 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:08.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1516" for this suite. 08/17/23 08:15:08.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:08.876
Aug 17 08:15:08.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename lease-test 08/17/23 08:15:08.877
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:08.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:08.886
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:08.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-333" for this suite. 08/17/23 08:15:08.929
------------------------------
â€¢ [0.057 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:08.876
    Aug 17 08:15:08.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename lease-test 08/17/23 08:15:08.877
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:08.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:08.886
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:08.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-333" for this suite. 08/17/23 08:15:08.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:08.934
Aug 17 08:15:08.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename dns 08/17/23 08:15:08.934
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:08.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:08.947
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/17/23 08:15:08.949
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1173.svc.cluster.local;sleep 1; done
 08/17/23 08:15:08.958
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1173.svc.cluster.local;sleep 1; done
 08/17/23 08:15:08.958
STEP: creating a pod to probe DNS 08/17/23 08:15:08.959
STEP: submitting the pod to kubernetes 08/17/23 08:15:08.959
Aug 17 08:15:08.964: INFO: Waiting up to 15m0s for pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978" in namespace "dns-1173" to be "running"
Aug 17 08:15:08.967: INFO: Pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790608ms
Aug 17 08:15:10.970: INFO: Pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978": Phase="Running", Reason="", readiness=true. Elapsed: 2.005226501s
Aug 17 08:15:10.970: INFO: Pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978" satisfied condition "running"
STEP: retrieving the pod 08/17/23 08:15:10.97
STEP: looking for the results for each expected name from probers 08/17/23 08:15:10.971
Aug 17 08:15:10.974: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:10.975: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:10.978: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:10.980: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:10.982: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:10.986: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:10.986: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1173.svc.cluster.local]

Aug 17 08:15:15.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:15.990: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:15.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:15.997: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:16.001: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

Aug 17 08:15:20.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:20.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:20.995: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:20.997: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:21.000: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

Aug 17 08:15:25.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:25.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:25.995: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:25.996: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:25.999: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

Aug 17 08:15:30.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:30.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:30.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:30.998: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:31.001: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

Aug 17 08:15:35.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:35.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:35.997: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:35.999: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
Aug 17 08:15:36.002: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

Aug 17 08:15:41.003: INFO: DNS probes using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 succeeded

STEP: deleting the pod 08/17/23 08:15:41.003
STEP: deleting the test headless service 08/17/23 08:15:41.011
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:41.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1173" for this suite. 08/17/23 08:15:41.025
------------------------------
â€¢ [SLOW TEST] [32.096 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:08.934
    Aug 17 08:15:08.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename dns 08/17/23 08:15:08.934
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:08.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:08.947
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/17/23 08:15:08.949
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1173.svc.cluster.local;sleep 1; done
     08/17/23 08:15:08.958
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1173.svc.cluster.local;sleep 1; done
     08/17/23 08:15:08.958
    STEP: creating a pod to probe DNS 08/17/23 08:15:08.959
    STEP: submitting the pod to kubernetes 08/17/23 08:15:08.959
    Aug 17 08:15:08.964: INFO: Waiting up to 15m0s for pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978" in namespace "dns-1173" to be "running"
    Aug 17 08:15:08.967: INFO: Pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790608ms
    Aug 17 08:15:10.970: INFO: Pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978": Phase="Running", Reason="", readiness=true. Elapsed: 2.005226501s
    Aug 17 08:15:10.970: INFO: Pod "dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 08:15:10.97
    STEP: looking for the results for each expected name from probers 08/17/23 08:15:10.971
    Aug 17 08:15:10.974: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:10.975: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:10.978: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:10.980: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:10.982: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:10.986: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:10.986: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1173.svc.cluster.local]

    Aug 17 08:15:15.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:15.990: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:15.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:15.997: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:16.001: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

    Aug 17 08:15:20.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:20.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:20.995: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:20.997: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:21.000: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

    Aug 17 08:15:25.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:25.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:25.995: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:25.996: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:25.999: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

    Aug 17 08:15:30.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:30.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:30.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:30.998: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:31.001: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

    Aug 17 08:15:35.989: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:35.991: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:35.997: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:35.999: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local from pod dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978: the server could not find the requested resource (get pods dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978)
    Aug 17 08:15:36.002: INFO: Lookups using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1173.svc.cluster.local]

    Aug 17 08:15:41.003: INFO: DNS probes using dns-1173/dns-test-83a0f3fc-3c62-418f-874d-c51c87c77978 succeeded

    STEP: deleting the pod 08/17/23 08:15:41.003
    STEP: deleting the test headless service 08/17/23 08:15:41.011
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:41.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1173" for this suite. 08/17/23 08:15:41.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:41.031
Aug 17 08:15:41.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename webhook 08/17/23 08:15:41.032
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:41.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:41.053
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 08:15:41.07
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 08:15:41.307
STEP: Deploying the webhook pod 08/17/23 08:15:41.31
STEP: Wait for the deployment to be ready 08/17/23 08:15:41.375
Aug 17 08:15:41.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 08:15:43.387
STEP: Verifying the service has paired with the endpoint 08/17/23 08:15:43.394
Aug 17 08:15:44.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 17 08:15:44.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2247-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 08:15:44.904
STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 08:15:44.914
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:47.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4140" for this suite. 08/17/23 08:15:47.483
STEP: Destroying namespace "webhook-4140-markers" for this suite. 08/17/23 08:15:47.488
------------------------------
â€¢ [SLOW TEST] [6.470 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:41.031
    Aug 17 08:15:41.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename webhook 08/17/23 08:15:41.032
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:41.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:41.053
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 08:15:41.07
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 08:15:41.307
    STEP: Deploying the webhook pod 08/17/23 08:15:41.31
    STEP: Wait for the deployment to be ready 08/17/23 08:15:41.375
    Aug 17 08:15:41.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 08:15:43.387
    STEP: Verifying the service has paired with the endpoint 08/17/23 08:15:43.394
    Aug 17 08:15:44.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 17 08:15:44.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2247-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 08:15:44.904
    STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 08:15:44.914
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:47.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4140" for this suite. 08/17/23 08:15:47.483
    STEP: Destroying namespace "webhook-4140-markers" for this suite. 08/17/23 08:15:47.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:47.503
Aug 17 08:15:47.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename services 08/17/23 08:15:47.504
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:47.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:47.532
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2585 08/17/23 08:15:47.536
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 08:15:47.559
STEP: creating service externalsvc in namespace services-2585 08/17/23 08:15:47.559
STEP: creating replication controller externalsvc in namespace services-2585 08/17/23 08:15:47.576
I0817 08:15:47.580211      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2585, replica count: 2
I0817 08:15:50.631593      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/17/23 08:15:50.633
Aug 17 08:15:50.653: INFO: Creating new exec pod
Aug 17 08:15:50.664: INFO: Waiting up to 5m0s for pod "execpodh67g5" in namespace "services-2585" to be "running"
Aug 17 08:15:50.666: INFO: Pod "execpodh67g5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040686ms
Aug 17 08:15:52.670: INFO: Pod "execpodh67g5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005354174s
Aug 17 08:15:52.670: INFO: Pod "execpodh67g5" satisfied condition "running"
Aug 17 08:15:52.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-2585 exec execpodh67g5 -- /bin/sh -x -c nslookup nodeport-service.services-2585.svc.cluster.local'
Aug 17 08:15:52.826: INFO: stderr: "+ nslookup nodeport-service.services-2585.svc.cluster.local\n"
Aug 17 08:15:52.826: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2585.svc.cluster.local\tcanonical name = externalsvc.services-2585.svc.cluster.local.\nName:\texternalsvc.services-2585.svc.cluster.local\nAddress: 10.111.162.96\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2585, will wait for the garbage collector to delete the pods 08/17/23 08:15:52.826
Aug 17 08:15:52.963: INFO: Deleting ReplicationController externalsvc took: 69.935965ms
Aug 17 08:15:53.065: INFO: Terminating ReplicationController externalsvc pods took: 101.487082ms
Aug 17 08:15:56.178: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:56.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2585" for this suite. 08/17/23 08:15:56.191
------------------------------
â€¢ [SLOW TEST] [8.692 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:47.503
    Aug 17 08:15:47.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename services 08/17/23 08:15:47.504
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:47.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:47.532
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-2585 08/17/23 08:15:47.536
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 08:15:47.559
    STEP: creating service externalsvc in namespace services-2585 08/17/23 08:15:47.559
    STEP: creating replication controller externalsvc in namespace services-2585 08/17/23 08:15:47.576
    I0817 08:15:47.580211      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2585, replica count: 2
    I0817 08:15:50.631593      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/17/23 08:15:50.633
    Aug 17 08:15:50.653: INFO: Creating new exec pod
    Aug 17 08:15:50.664: INFO: Waiting up to 5m0s for pod "execpodh67g5" in namespace "services-2585" to be "running"
    Aug 17 08:15:50.666: INFO: Pod "execpodh67g5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040686ms
    Aug 17 08:15:52.670: INFO: Pod "execpodh67g5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005354174s
    Aug 17 08:15:52.670: INFO: Pod "execpodh67g5" satisfied condition "running"
    Aug 17 08:15:52.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=services-2585 exec execpodh67g5 -- /bin/sh -x -c nslookup nodeport-service.services-2585.svc.cluster.local'
    Aug 17 08:15:52.826: INFO: stderr: "+ nslookup nodeport-service.services-2585.svc.cluster.local\n"
    Aug 17 08:15:52.826: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2585.svc.cluster.local\tcanonical name = externalsvc.services-2585.svc.cluster.local.\nName:\texternalsvc.services-2585.svc.cluster.local\nAddress: 10.111.162.96\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2585, will wait for the garbage collector to delete the pods 08/17/23 08:15:52.826
    Aug 17 08:15:52.963: INFO: Deleting ReplicationController externalsvc took: 69.935965ms
    Aug 17 08:15:53.065: INFO: Terminating ReplicationController externalsvc pods took: 101.487082ms
    Aug 17 08:15:56.178: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:56.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2585" for this suite. 08/17/23 08:15:56.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:56.199
Aug 17 08:15:56.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename replication-controller 08/17/23 08:15:56.2
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:56.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:56.211
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 17 08:15:56.214: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/17/23 08:15:56.239
STEP: Checking rc "condition-test" has the desired failure condition set 08/17/23 08:15:56.248
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/17/23 08:15:57.252
Aug 17 08:15:57.257: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/17/23 08:15:57.257
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:58.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9845" for this suite. 08/17/23 08:15:58.266
------------------------------
â€¢ [2.073 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:56.199
    Aug 17 08:15:56.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename replication-controller 08/17/23 08:15:56.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:56.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:56.211
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 17 08:15:56.214: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/17/23 08:15:56.239
    STEP: Checking rc "condition-test" has the desired failure condition set 08/17/23 08:15:56.248
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/17/23 08:15:57.252
    Aug 17 08:15:57.257: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/17/23 08:15:57.257
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:58.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9845" for this suite. 08/17/23 08:15:58.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:58.272
Aug 17 08:15:58.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename svcaccounts 08/17/23 08:15:58.273
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:58.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:58.293
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 17 08:15:58.311: INFO: created pod pod-service-account-defaultsa
Aug 17 08:15:58.311: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 17 08:15:58.314: INFO: created pod pod-service-account-mountsa
Aug 17 08:15:58.314: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 17 08:15:58.318: INFO: created pod pod-service-account-nomountsa
Aug 17 08:15:58.318: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 17 08:15:58.324: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 17 08:15:58.324: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 17 08:15:58.328: INFO: created pod pod-service-account-mountsa-mountspec
Aug 17 08:15:58.328: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 17 08:15:58.332: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 17 08:15:58.332: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 17 08:15:58.336: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 17 08:15:58.336: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 17 08:15:58.342: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 17 08:15:58.342: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 17 08:15:58.345: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 17 08:15:58.345: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 08:15:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5652" for this suite. 08/17/23 08:15:58.349
------------------------------
â€¢ [0.082 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:58.272
    Aug 17 08:15:58.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 08:15:58.273
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:58.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:58.293
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 17 08:15:58.311: INFO: created pod pod-service-account-defaultsa
    Aug 17 08:15:58.311: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 17 08:15:58.314: INFO: created pod pod-service-account-mountsa
    Aug 17 08:15:58.314: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 17 08:15:58.318: INFO: created pod pod-service-account-nomountsa
    Aug 17 08:15:58.318: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 17 08:15:58.324: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 17 08:15:58.324: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 17 08:15:58.328: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 17 08:15:58.328: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 17 08:15:58.332: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 17 08:15:58.332: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 17 08:15:58.336: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 17 08:15:58.336: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 17 08:15:58.342: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 17 08:15:58.342: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 17 08:15:58.345: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 17 08:15:58.345: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:15:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5652" for this suite. 08/17/23 08:15:58.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:15:58.354
Aug 17 08:15:58.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename namespaces 08/17/23 08:15:58.355
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:58.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:58.378
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/17/23 08:15:58.38
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:58.402
STEP: Creating a pod in the namespace 08/17/23 08:15:58.406
STEP: Waiting for the pod to have running status 08/17/23 08:15:58.418
Aug 17 08:15:58.418: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2622" to be "running"
Aug 17 08:15:58.424: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.798394ms
Aug 17 08:16:00.426: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007964296s
Aug 17 08:16:02.428: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00982388s
Aug 17 08:16:04.427: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009366915s
Aug 17 08:16:06.426: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008459808s
Aug 17 08:16:08.426: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.008449492s
Aug 17 08:16:08.426: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/17/23 08:16:08.426
STEP: Waiting for the namespace to be removed. 08/17/23 08:16:08.464
STEP: Recreating the namespace 08/17/23 08:16:19.466
STEP: Verifying there are no pods in the namespace 08/17/23 08:16:19.478
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:16:19.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6387" for this suite. 08/17/23 08:16:19.522
STEP: Destroying namespace "nsdeletetest-2622" for this suite. 08/17/23 08:16:19.561
Aug 17 08:16:19.564: INFO: Namespace nsdeletetest-2622 was already deleted
STEP: Destroying namespace "nsdeletetest-8858" for this suite. 08/17/23 08:16:19.564
------------------------------
â€¢ [SLOW TEST] [21.222 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:15:58.354
    Aug 17 08:15:58.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename namespaces 08/17/23 08:15:58.355
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:58.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:15:58.378
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/17/23 08:15:58.38
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:15:58.402
    STEP: Creating a pod in the namespace 08/17/23 08:15:58.406
    STEP: Waiting for the pod to have running status 08/17/23 08:15:58.418
    Aug 17 08:15:58.418: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2622" to be "running"
    Aug 17 08:15:58.424: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.798394ms
    Aug 17 08:16:00.426: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007964296s
    Aug 17 08:16:02.428: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00982388s
    Aug 17 08:16:04.427: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009366915s
    Aug 17 08:16:06.426: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008459808s
    Aug 17 08:16:08.426: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.008449492s
    Aug 17 08:16:08.426: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/17/23 08:16:08.426
    STEP: Waiting for the namespace to be removed. 08/17/23 08:16:08.464
    STEP: Recreating the namespace 08/17/23 08:16:19.466
    STEP: Verifying there are no pods in the namespace 08/17/23 08:16:19.478
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:16:19.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6387" for this suite. 08/17/23 08:16:19.522
    STEP: Destroying namespace "nsdeletetest-2622" for this suite. 08/17/23 08:16:19.561
    Aug 17 08:16:19.564: INFO: Namespace nsdeletetest-2622 was already deleted
    STEP: Destroying namespace "nsdeletetest-8858" for this suite. 08/17/23 08:16:19.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:16:19.578
Aug 17 08:16:19.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename containers 08/17/23 08:16:19.579
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:19.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:19.61
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/17/23 08:16:19.614
Aug 17 08:16:19.620: INFO: Waiting up to 5m0s for pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3" in namespace "containers-9834" to be "Succeeded or Failed"
Aug 17 08:16:19.623: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.270849ms
Aug 17 08:16:21.627: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006819215s
Aug 17 08:16:23.627: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007244759s
STEP: Saw pod success 08/17/23 08:16:23.627
Aug 17 08:16:23.627: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3" satisfied condition "Succeeded or Failed"
Aug 17 08:16:23.629: INFO: Trying to get logs from node yst-node2 pod client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 08:16:23.633
Aug 17 08:16:23.638: INFO: Waiting for pod client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3 to disappear
Aug 17 08:16:23.640: INFO: Pod client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 08:16:23.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9834" for this suite. 08/17/23 08:16:23.643
------------------------------
â€¢ [4.069 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:16:19.578
    Aug 17 08:16:19.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename containers 08/17/23 08:16:19.579
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:19.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:19.61
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/17/23 08:16:19.614
    Aug 17 08:16:19.620: INFO: Waiting up to 5m0s for pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3" in namespace "containers-9834" to be "Succeeded or Failed"
    Aug 17 08:16:19.623: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.270849ms
    Aug 17 08:16:21.627: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006819215s
    Aug 17 08:16:23.627: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007244759s
    STEP: Saw pod success 08/17/23 08:16:23.627
    Aug 17 08:16:23.627: INFO: Pod "client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3" satisfied condition "Succeeded or Failed"
    Aug 17 08:16:23.629: INFO: Trying to get logs from node yst-node2 pod client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 08:16:23.633
    Aug 17 08:16:23.638: INFO: Waiting for pod client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3 to disappear
    Aug 17 08:16:23.640: INFO: Pod client-containers-b0f2ba98-aa53-4854-ae08-10d2e62569b3 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:16:23.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9834" for this suite. 08/17/23 08:16:23.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:16:23.647
Aug 17 08:16:23.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 08:16:23.648
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:23.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:23.658
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 17 08:16:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:16:30.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3589" for this suite. 08/17/23 08:16:30.467
------------------------------
â€¢ [SLOW TEST] [6.826 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:16:23.647
    Aug 17 08:16:23.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 08:16:23.648
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:23.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:23.658
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 17 08:16:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:16:30.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3589" for this suite. 08/17/23 08:16:30.467
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:16:30.473
Aug 17 08:16:30.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 08:16:30.474
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:30.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:30.489
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/17/23 08:16:30.494
Aug 17 08:16:30.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3642 create -f -'
Aug 17 08:16:32.473: INFO: stderr: ""
Aug 17 08:16:32.473: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/17/23 08:16:32.473
Aug 17 08:16:33.476: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 08:16:33.476: INFO: Found 1 / 1
Aug 17 08:16:33.476: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/17/23 08:16:33.476
Aug 17 08:16:33.479: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 08:16:33.479: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 08:16:33.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3642 patch pod agnhost-primary-x5qqn -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 17 08:16:33.560: INFO: stderr: ""
Aug 17 08:16:33.560: INFO: stdout: "pod/agnhost-primary-x5qqn patched\n"
STEP: checking annotations 08/17/23 08:16:33.56
Aug 17 08:16:33.563: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 08:16:33.563: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 08:16:33.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3642" for this suite. 08/17/23 08:16:33.566
------------------------------
â€¢ [3.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:16:30.473
    Aug 17 08:16:30.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 08:16:30.474
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:30.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:30.489
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/17/23 08:16:30.494
    Aug 17 08:16:30.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3642 create -f -'
    Aug 17 08:16:32.473: INFO: stderr: ""
    Aug 17 08:16:32.473: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/17/23 08:16:32.473
    Aug 17 08:16:33.476: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 08:16:33.476: INFO: Found 1 / 1
    Aug 17 08:16:33.476: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/17/23 08:16:33.476
    Aug 17 08:16:33.479: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 08:16:33.479: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 17 08:16:33.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-3642 patch pod agnhost-primary-x5qqn -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 17 08:16:33.560: INFO: stderr: ""
    Aug 17 08:16:33.560: INFO: stdout: "pod/agnhost-primary-x5qqn patched\n"
    STEP: checking annotations 08/17/23 08:16:33.56
    Aug 17 08:16:33.563: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 08:16:33.563: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:16:33.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3642" for this suite. 08/17/23 08:16:33.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:16:33.569
Aug 17 08:16:33.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 08:16:33.57
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:33.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:33.58
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/17/23 08:16:33.583
Aug 17 08:16:33.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: rename a version 08/17/23 08:16:39.433
STEP: check the new version name is served 08/17/23 08:16:39.443
STEP: check the old version name is removed 08/17/23 08:16:41.97
STEP: check the other version is not changed 08/17/23 08:16:42.885
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:16:47.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2919" for this suite. 08/17/23 08:16:47.319
------------------------------
â€¢ [SLOW TEST] [13.753 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:16:33.569
    Aug 17 08:16:33.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 08:16:33.57
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:33.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:33.58
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/17/23 08:16:33.583
    Aug 17 08:16:33.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: rename a version 08/17/23 08:16:39.433
    STEP: check the new version name is served 08/17/23 08:16:39.443
    STEP: check the old version name is removed 08/17/23 08:16:41.97
    STEP: check the other version is not changed 08/17/23 08:16:42.885
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:16:47.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2919" for this suite. 08/17/23 08:16:47.319
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:16:47.323
Aug 17 08:16:47.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename kubectl 08/17/23 08:16:47.323
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:47.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:47.34
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 17 08:16:47.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 create -f -'
Aug 17 08:16:48.586: INFO: stderr: ""
Aug 17 08:16:48.586: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 17 08:16:48.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 create -f -'
Aug 17 08:16:48.928: INFO: stderr: ""
Aug 17 08:16:48.928: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/17/23 08:16:48.928
Aug 17 08:16:49.932: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 08:16:49.933: INFO: Found 1 / 1
Aug 17 08:16:49.933: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 17 08:16:49.935: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 08:16:49.935: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 08:16:49.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe pod agnhost-primary-b85kp'
Aug 17 08:16:50.033: INFO: stderr: ""
Aug 17 08:16:50.033: INFO: stdout: "Name:             agnhost-primary-b85kp\nNamespace:        kubectl-488\nPriority:         0\nService Account:  default\nNode:             yst-node2/10.60.200.177\nStart Time:       Thu, 17 Aug 2023 08:16:48 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: e707495fe8dc3d696faa12b1a43bc4bbde8ca7d0ffeb21879e0a3e59e9846a34\n                  cni.projectcalico.org/podIP: 172.32.238.79/32\n                  cni.projectcalico.org/podIPs: 172.32.238.79/32\nStatus:           Running\nIP:               172.32.238.79\nIPs:\n  IP:           172.32.238.79\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://69b10662a2335425f5536fc269d31e7613347adf6ed8dff69d4d2a446b0f9ef7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 17 Aug 2023 08:16:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mw58m (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-mw58m:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-488/agnhost-primary-b85kp to yst-node2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Aug 17 08:16:50.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe rc agnhost-primary'
Aug 17 08:16:50.135: INFO: stderr: ""
Aug 17 08:16:50.135: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-488\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-b85kp\n"
Aug 17 08:16:50.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe service agnhost-primary'
Aug 17 08:16:50.231: INFO: stderr: ""
Aug 17 08:16:50.231: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-488\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.110.128.192\nIPs:               10.110.128.192\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.32.238.79:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 17 08:16:50.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe node yst-master'
Aug 17 08:16:50.367: INFO: stderr: ""
Aug 17 08:16:50.367: INFO: stdout: "Name:               yst-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=yst-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.60.200.175/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.32.3.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 04 Jul 2023 01:59:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  yst-master\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 17 Aug 2023 08:16:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 17 Aug 2023 06:42:07 +0000   Thu, 17 Aug 2023 06:42:07 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 17 Aug 2023 08:16:43 +0000   Tue, 04 Jul 2023 01:59:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 17 Aug 2023 08:16:43 +0000   Tue, 04 Jul 2023 01:59:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 17 Aug 2023 08:16:43 +0000   Tue, 04 Jul 2023 01:59:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 17 Aug 2023 08:16:43 +0000   Thu, 17 Aug 2023 06:15:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.60.200.175\n  Hostname:    yst-master\nCapacity:\n  cpu:                8\n  ephemeral-storage:  200712444Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24688092Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  184976588085\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24585692Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 5f66c08a185a41e096164b24138d790e\n  System UUID:                F3141442-1231-0491-C5EB-80FA429079A8\n  Boot ID:                    6492b0bc-055f-40e6-9a93-a37c3a96cbb3\n  Kernel Version:             3.10.0-1160.92.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.26.6\n  Kube-Proxy Version:         v1.26.6\nPodCIDR:                      172.32.0.0/24\nPodCIDRs:                     172.32.0.0/24\nNon-terminated Pods:          (37 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  acc-global                  alert-apiserver-67bffd5d79-4jkdw                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-global                  chartmuseum-chartmuseum-fd9b485cc-98wvk                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  cluster-server-9775c76cc-9g6hh                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  gateway-proxy-v8vpk                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  keycloak-64b4b5569c-x5j9n                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  keycloak-db-5cc8f7f7d5-x6xbq                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  keycloak-observer-86b5cc8d77-qs2lj                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  manual-webserver-66754cc96-5rf5n                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  acc-kube-state-metrics-86c68f6799-bpbfb                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  acc-node-exporter-4nxg2                                    112m (1%)     270m (3%)   200Mi (0%)       220Mi (0%)     44d\n  acc-system                  accordion-data-provisioner-868fbcb9c9-94xx4                0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  cicd-apiserver-59d9f5977b-qj6tl                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  cicd-trigger-manager-7f78f48b64-9t4dx                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  filebeat-filebeat-wt7kd                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-core-c69dc8568-dxzjg                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-database-0                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-jobservice-66b66b4b9-f22t7                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-nginx-7bb6fcf984-xgzmf                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-notary-signer-cbbf7c95d-nlmpk                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  kiali-6d6fd7b96c-4qtgj                                     10m (0%)      0 (0%)      64Mi (0%)        1Gi (4%)       8d\n  acc-system                  kube-event-exporter-57bb5b5d55-gnbnd                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  minio-0                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  scouter-manager-6bd5945b66-zmwv2                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  thanos-compactor-0                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  user-ingress-controller-ltkfd                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  kube-system                 calico-kube-controllers-59f6c5b776-ld46r                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  kube-system                 calico-node-rvq9v                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         44d\n  kube-system                 coredns-69754fcccf-r7xb2                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     121m\n  kube-system                 etcd-yst-master                                            100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         120m\n  kube-system                 kube-apiserver-yst-master                                  250m (3%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-controller-manager-yst-master                         200m (2%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-proxy-mtcmj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-scheduler-yst-master                                  100m (1%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 metrics-server-d88c6d6d7-6n64h                             100m (1%)     0 (0%)      200Mi (0%)       0 (0%)         44d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  test                        acc-tomcat-ddb885997-2w79z                                 0 (0%)        0 (0%)      1536Mi (6%)      1536Mi (6%)    8d\n  test                        scouter-server-64f47bc954-r47gx                            0 (0%)        0 (0%)      1536Mi (6%)      1536Mi (6%)    29d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1222m (15%)   270m (3%)\n  memory             3706Mi (15%)  4486Mi (18%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
Aug 17 08:16:50.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe namespace kubectl-488'
Aug 17 08:16:50.471: INFO: stderr: ""
Aug 17 08:16:50.471: INFO: stdout: "Name:         kubectl-488\nLabels:       e2e-framework=kubectl\n              e2e-run=cc861678-4c64-437f-95a3-e3501ec4e99d\n              kubernetes.io/metadata.name=kubectl-488\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 08:16:50.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-488" for this suite. 08/17/23 08:16:50.476
------------------------------
â€¢ [3.158 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:16:47.323
    Aug 17 08:16:47.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename kubectl 08/17/23 08:16:47.323
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:47.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:47.34
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 17 08:16:47.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 create -f -'
    Aug 17 08:16:48.586: INFO: stderr: ""
    Aug 17 08:16:48.586: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 17 08:16:48.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 create -f -'
    Aug 17 08:16:48.928: INFO: stderr: ""
    Aug 17 08:16:48.928: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/17/23 08:16:48.928
    Aug 17 08:16:49.932: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 08:16:49.933: INFO: Found 1 / 1
    Aug 17 08:16:49.933: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 17 08:16:49.935: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 08:16:49.935: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 17 08:16:49.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe pod agnhost-primary-b85kp'
    Aug 17 08:16:50.033: INFO: stderr: ""
    Aug 17 08:16:50.033: INFO: stdout: "Name:             agnhost-primary-b85kp\nNamespace:        kubectl-488\nPriority:         0\nService Account:  default\nNode:             yst-node2/10.60.200.177\nStart Time:       Thu, 17 Aug 2023 08:16:48 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: e707495fe8dc3d696faa12b1a43bc4bbde8ca7d0ffeb21879e0a3e59e9846a34\n                  cni.projectcalico.org/podIP: 172.32.238.79/32\n                  cni.projectcalico.org/podIPs: 172.32.238.79/32\nStatus:           Running\nIP:               172.32.238.79\nIPs:\n  IP:           172.32.238.79\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://69b10662a2335425f5536fc269d31e7613347adf6ed8dff69d4d2a446b0f9ef7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 17 Aug 2023 08:16:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mw58m (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-mw58m:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-488/agnhost-primary-b85kp to yst-node2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Aug 17 08:16:50.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe rc agnhost-primary'
    Aug 17 08:16:50.135: INFO: stderr: ""
    Aug 17 08:16:50.135: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-488\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-b85kp\n"
    Aug 17 08:16:50.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe service agnhost-primary'
    Aug 17 08:16:50.231: INFO: stderr: ""
    Aug 17 08:16:50.231: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-488\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.110.128.192\nIPs:               10.110.128.192\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.32.238.79:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 17 08:16:50.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe node yst-master'
    Aug 17 08:16:50.367: INFO: stderr: ""
    Aug 17 08:16:50.367: INFO: stdout: "Name:               yst-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=yst-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.60.200.175/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.32.3.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 04 Jul 2023 01:59:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  yst-master\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 17 Aug 2023 08:16:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 17 Aug 2023 06:42:07 +0000   Thu, 17 Aug 2023 06:42:07 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 17 Aug 2023 08:16:43 +0000   Tue, 04 Jul 2023 01:59:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 17 Aug 2023 08:16:43 +0000   Tue, 04 Jul 2023 01:59:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 17 Aug 2023 08:16:43 +0000   Tue, 04 Jul 2023 01:59:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 17 Aug 2023 08:16:43 +0000   Thu, 17 Aug 2023 06:15:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.60.200.175\n  Hostname:    yst-master\nCapacity:\n  cpu:                8\n  ephemeral-storage:  200712444Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24688092Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  184976588085\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24585692Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 5f66c08a185a41e096164b24138d790e\n  System UUID:                F3141442-1231-0491-C5EB-80FA429079A8\n  Boot ID:                    6492b0bc-055f-40e6-9a93-a37c3a96cbb3\n  Kernel Version:             3.10.0-1160.92.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.26.6\n  Kube-Proxy Version:         v1.26.6\nPodCIDR:                      172.32.0.0/24\nPodCIDRs:                     172.32.0.0/24\nNon-terminated Pods:          (37 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  acc-global                  alert-apiserver-67bffd5d79-4jkdw                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-global                  chartmuseum-chartmuseum-fd9b485cc-98wvk                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  cluster-server-9775c76cc-9g6hh                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  gateway-proxy-v8vpk                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  keycloak-64b4b5569c-x5j9n                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  keycloak-db-5cc8f7f7d5-x6xbq                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  keycloak-observer-86b5cc8d77-qs2lj                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-global                  manual-webserver-66754cc96-5rf5n                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  acc-kube-state-metrics-86c68f6799-bpbfb                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  acc-node-exporter-4nxg2                                    112m (1%)     270m (3%)   200Mi (0%)       220Mi (0%)     44d\n  acc-system                  accordion-data-provisioner-868fbcb9c9-94xx4                0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  cicd-apiserver-59d9f5977b-qj6tl                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  cicd-trigger-manager-7f78f48b64-9t4dx                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  filebeat-filebeat-wt7kd                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-core-c69dc8568-dxzjg                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-database-0                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-jobservice-66b66b4b9-f22t7                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-nginx-7bb6fcf984-xgzmf                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  harbor-notary-signer-cbbf7c95d-nlmpk                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  kiali-6d6fd7b96c-4qtgj                                     10m (0%)      0 (0%)      64Mi (0%)        1Gi (4%)       8d\n  acc-system                  kube-event-exporter-57bb5b5d55-gnbnd                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  minio-0                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  scouter-manager-6bd5945b66-zmwv2                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  acc-system                  thanos-compactor-0                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  acc-system                  user-ingress-controller-ltkfd                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  kube-system                 calico-kube-controllers-59f6c5b776-ld46r                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         44d\n  kube-system                 calico-node-rvq9v                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         44d\n  kube-system                 coredns-69754fcccf-r7xb2                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     121m\n  kube-system                 etcd-yst-master                                            100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         120m\n  kube-system                 kube-apiserver-yst-master                                  250m (3%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-controller-manager-yst-master                         200m (2%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-proxy-mtcmj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-scheduler-yst-master                                  100m (1%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 metrics-server-d88c6d6d7-6n64h                             100m (1%)     0 (0%)      200Mi (0%)       0 (0%)         44d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-530fb14be5494c4e-fggsq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\n  test                        acc-tomcat-ddb885997-2w79z                                 0 (0%)        0 (0%)      1536Mi (6%)      1536Mi (6%)    8d\n  test                        scouter-server-64f47bc954-r47gx                            0 (0%)        0 (0%)      1536Mi (6%)      1536Mi (6%)    29d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1222m (15%)   270m (3%)\n  memory             3706Mi (15%)  4486Mi (18%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
    Aug 17 08:16:50.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=kubectl-488 describe namespace kubectl-488'
    Aug 17 08:16:50.471: INFO: stderr: ""
    Aug 17 08:16:50.471: INFO: stdout: "Name:         kubectl-488\nLabels:       e2e-framework=kubectl\n              e2e-run=cc861678-4c64-437f-95a3-e3501ec4e99d\n              kubernetes.io/metadata.name=kubectl-488\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:16:50.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-488" for this suite. 08/17/23 08:16:50.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:16:50.481
Aug 17 08:16:50.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename emptydir 08/17/23 08:16:50.482
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:50.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:50.499
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 08:16:50.503
Aug 17 08:16:50.514: INFO: Waiting up to 5m0s for pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611" in namespace "emptydir-7035" to be "Succeeded or Failed"
Aug 17 08:16:50.520: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611": Phase="Pending", Reason="", readiness=false. Elapsed: 5.202544ms
Aug 17 08:16:52.523: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008844169s
Aug 17 08:16:54.526: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011714896s
STEP: Saw pod success 08/17/23 08:16:54.526
Aug 17 08:16:54.526: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611" satisfied condition "Succeeded or Failed"
Aug 17 08:16:54.528: INFO: Trying to get logs from node yst-node2 pod pod-fd598069-bf58-42d8-b179-ff2f6dc55611 container test-container: <nil>
STEP: delete the pod 08/17/23 08:16:54.564
Aug 17 08:16:54.620: INFO: Waiting for pod pod-fd598069-bf58-42d8-b179-ff2f6dc55611 to disappear
Aug 17 08:16:54.622: INFO: Pod pod-fd598069-bf58-42d8-b179-ff2f6dc55611 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 08:16:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7035" for this suite. 08/17/23 08:16:54.625
------------------------------
â€¢ [4.175 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:16:50.481
    Aug 17 08:16:50.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename emptydir 08/17/23 08:16:50.482
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:50.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:50.499
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 08:16:50.503
    Aug 17 08:16:50.514: INFO: Waiting up to 5m0s for pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611" in namespace "emptydir-7035" to be "Succeeded or Failed"
    Aug 17 08:16:50.520: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611": Phase="Pending", Reason="", readiness=false. Elapsed: 5.202544ms
    Aug 17 08:16:52.523: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008844169s
    Aug 17 08:16:54.526: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011714896s
    STEP: Saw pod success 08/17/23 08:16:54.526
    Aug 17 08:16:54.526: INFO: Pod "pod-fd598069-bf58-42d8-b179-ff2f6dc55611" satisfied condition "Succeeded or Failed"
    Aug 17 08:16:54.528: INFO: Trying to get logs from node yst-node2 pod pod-fd598069-bf58-42d8-b179-ff2f6dc55611 container test-container: <nil>
    STEP: delete the pod 08/17/23 08:16:54.564
    Aug 17 08:16:54.620: INFO: Waiting for pod pod-fd598069-bf58-42d8-b179-ff2f6dc55611 to disappear
    Aug 17 08:16:54.622: INFO: Pod pod-fd598069-bf58-42d8-b179-ff2f6dc55611 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:16:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7035" for this suite. 08/17/23 08:16:54.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:16:54.658
Aug 17 08:16:54.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename cronjob 08/17/23 08:16:54.659
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:54.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:54.755
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/17/23 08:16:54.757
STEP: Ensuring more than one job is running at a time 08/17/23 08:16:54.785
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/17/23 08:18:00.79
STEP: Removing cronjob 08/17/23 08:18:00.793
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 08:18:00.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2905" for this suite. 08/17/23 08:18:00.799
------------------------------
â€¢ [SLOW TEST] [66.145 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:16:54.658
    Aug 17 08:16:54.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename cronjob 08/17/23 08:16:54.659
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:16:54.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:16:54.755
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/17/23 08:16:54.757
    STEP: Ensuring more than one job is running at a time 08/17/23 08:16:54.785
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/17/23 08:18:00.79
    STEP: Removing cronjob 08/17/23 08:18:00.793
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:18:00.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2905" for this suite. 08/17/23 08:18:00.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:18:00.803
Aug 17 08:18:00.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename secrets 08/17/23 08:18:00.804
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:00.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:00.817
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-53c5f164-52d0-4f8e-89a7-87503ee31474 08/17/23 08:18:00.825
STEP: Creating a pod to test consume secrets 08/17/23 08:18:00.832
Aug 17 08:18:00.843: INFO: Waiting up to 5m0s for pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad" in namespace "secrets-5955" to be "Succeeded or Failed"
Aug 17 08:18:00.848: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635251ms
Aug 17 08:18:02.851: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007993946s
Aug 17 08:18:04.851: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007889295s
Aug 17 08:18:06.850: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007003781s
STEP: Saw pod success 08/17/23 08:18:06.85
Aug 17 08:18:06.850: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad" satisfied condition "Succeeded or Failed"
Aug 17 08:18:06.852: INFO: Trying to get logs from node yst-node2 pod pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 08:18:06.855
Aug 17 08:18:06.861: INFO: Waiting for pod pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad to disappear
Aug 17 08:18:06.862: INFO: Pod pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 08:18:06.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5955" for this suite. 08/17/23 08:18:06.864
------------------------------
â€¢ [SLOW TEST] [6.063 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:18:00.803
    Aug 17 08:18:00.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename secrets 08/17/23 08:18:00.804
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:00.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:00.817
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-53c5f164-52d0-4f8e-89a7-87503ee31474 08/17/23 08:18:00.825
    STEP: Creating a pod to test consume secrets 08/17/23 08:18:00.832
    Aug 17 08:18:00.843: INFO: Waiting up to 5m0s for pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad" in namespace "secrets-5955" to be "Succeeded or Failed"
    Aug 17 08:18:00.848: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635251ms
    Aug 17 08:18:02.851: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007993946s
    Aug 17 08:18:04.851: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007889295s
    Aug 17 08:18:06.850: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007003781s
    STEP: Saw pod success 08/17/23 08:18:06.85
    Aug 17 08:18:06.850: INFO: Pod "pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad" satisfied condition "Succeeded or Failed"
    Aug 17 08:18:06.852: INFO: Trying to get logs from node yst-node2 pod pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 08:18:06.855
    Aug 17 08:18:06.861: INFO: Waiting for pod pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad to disappear
    Aug 17 08:18:06.862: INFO: Pod pod-secrets-0a1894ac-9e80-4fea-b590-3a0e267287ad no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:18:06.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5955" for this suite. 08/17/23 08:18:06.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:18:06.883
Aug 17 08:18:06.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename init-container 08/17/23 08:18:06.884
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:06.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:06.899
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/17/23 08:18:06.901
Aug 17 08:18:06.901: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:18:13.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7778" for this suite. 08/17/23 08:18:13.226
------------------------------
â€¢ [SLOW TEST] [6.346 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:18:06.883
    Aug 17 08:18:06.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename init-container 08/17/23 08:18:06.884
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:06.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:06.899
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/17/23 08:18:06.901
    Aug 17 08:18:06.901: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:18:13.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7778" for this suite. 08/17/23 08:18:13.226
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:18:13.23
Aug 17 08:18:13.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 08:18:13.231
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:13.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:13.242
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 17 08:18:13.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 08:18:15.241
Aug 17 08:18:15.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 create -f -'
Aug 17 08:18:16.429: INFO: stderr: ""
Aug 17 08:18:16.429: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 17 08:18:16.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 delete e2e-test-crd-publish-openapi-7697-crds test-cr'
Aug 17 08:18:16.516: INFO: stderr: ""
Aug 17 08:18:16.516: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 17 08:18:16.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 apply -f -'
Aug 17 08:18:16.836: INFO: stderr: ""
Aug 17 08:18:16.836: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 17 08:18:16.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 delete e2e-test-crd-publish-openapi-7697-crds test-cr'
Aug 17 08:18:16.919: INFO: stderr: ""
Aug 17 08:18:16.919: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/17/23 08:18:16.919
Aug 17 08:18:16.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 explain e2e-test-crd-publish-openapi-7697-crds'
Aug 17 08:18:17.229: INFO: stderr: ""
Aug 17 08:18:17.229: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7697-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 08:18:19.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2497" for this suite. 08/17/23 08:18:19.726
------------------------------
â€¢ [SLOW TEST] [6.500 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:18:13.23
    Aug 17 08:18:13.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 08:18:13.231
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:13.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:13.242
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 17 08:18:13.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 08:18:15.241
    Aug 17 08:18:15.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 create -f -'
    Aug 17 08:18:16.429: INFO: stderr: ""
    Aug 17 08:18:16.429: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 17 08:18:16.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 delete e2e-test-crd-publish-openapi-7697-crds test-cr'
    Aug 17 08:18:16.516: INFO: stderr: ""
    Aug 17 08:18:16.516: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 17 08:18:16.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 apply -f -'
    Aug 17 08:18:16.836: INFO: stderr: ""
    Aug 17 08:18:16.836: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 17 08:18:16.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 --namespace=crd-publish-openapi-2497 delete e2e-test-crd-publish-openapi-7697-crds test-cr'
    Aug 17 08:18:16.919: INFO: stderr: ""
    Aug 17 08:18:16.919: INFO: stdout: "e2e-test-crd-publish-openapi-7697-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/17/23 08:18:16.919
    Aug 17 08:18:16.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3068892012 --namespace=crd-publish-openapi-2497 explain e2e-test-crd-publish-openapi-7697-crds'
    Aug 17 08:18:17.229: INFO: stderr: ""
    Aug 17 08:18:17.229: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7697-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:18:19.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2497" for this suite. 08/17/23 08:18:19.726
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:18:19.73
Aug 17 08:18:19.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename projected 08/17/23 08:18:19.731
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:19.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:19.75
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/17/23 08:18:19.754
Aug 17 08:18:19.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e" in namespace "projected-4736" to be "Succeeded or Failed"
Aug 17 08:18:19.804: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003697ms
Aug 17 08:18:21.807: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005019367s
Aug 17 08:18:23.806: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004918498s
STEP: Saw pod success 08/17/23 08:18:23.807
Aug 17 08:18:23.807: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e" satisfied condition "Succeeded or Failed"
Aug 17 08:18:23.808: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e container client-container: <nil>
STEP: delete the pod 08/17/23 08:18:23.811
Aug 17 08:18:23.816: INFO: Waiting for pod downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e to disappear
Aug 17 08:18:23.818: INFO: Pod downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 08:18:23.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4736" for this suite. 08/17/23 08:18:23.82
------------------------------
â€¢ [4.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:18:19.73
    Aug 17 08:18:19.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename projected 08/17/23 08:18:19.731
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:19.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:19.75
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/17/23 08:18:19.754
    Aug 17 08:18:19.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e" in namespace "projected-4736" to be "Succeeded or Failed"
    Aug 17 08:18:19.804: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003697ms
    Aug 17 08:18:21.807: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005019367s
    Aug 17 08:18:23.806: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004918498s
    STEP: Saw pod success 08/17/23 08:18:23.807
    Aug 17 08:18:23.807: INFO: Pod "downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e" satisfied condition "Succeeded or Failed"
    Aug 17 08:18:23.808: INFO: Trying to get logs from node yst-node2 pod downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e container client-container: <nil>
    STEP: delete the pod 08/17/23 08:18:23.811
    Aug 17 08:18:23.816: INFO: Waiting for pod downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e to disappear
    Aug 17 08:18:23.818: INFO: Pod downwardapi-volume-44ea01eb-3bd7-473c-a3eb-68e8d251d04e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:18:23.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4736" for this suite. 08/17/23 08:18:23.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 08:18:23.824
Aug 17 08:18:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
STEP: Building a namespace api object, basename resourcequota 08/17/23 08:18:23.825
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:23.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:23.833
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/17/23 08:18:23.836
STEP: Counting existing ResourceQuota 08/17/23 08:18:29.839
STEP: Creating a ResourceQuota 08/17/23 08:18:34.841
STEP: Ensuring resource quota status is calculated 08/17/23 08:18:34.844
STEP: Creating a Secret 08/17/23 08:18:36.846
STEP: Ensuring resource quota status captures secret creation 08/17/23 08:18:36.853
STEP: Deleting a secret 08/17/23 08:18:38.855
STEP: Ensuring resource quota status released usage 08/17/23 08:18:38.859
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 08:18:40.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1023" for this suite. 08/17/23 08:18:40.865
------------------------------
â€¢ [SLOW TEST] [17.044 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 08:18:23.824
    Aug 17 08:18:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3068892012
    STEP: Building a namespace api object, basename resourcequota 08/17/23 08:18:23.825
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 08:18:23.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 08:18:23.833
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/17/23 08:18:23.836
    STEP: Counting existing ResourceQuota 08/17/23 08:18:29.839
    STEP: Creating a ResourceQuota 08/17/23 08:18:34.841
    STEP: Ensuring resource quota status is calculated 08/17/23 08:18:34.844
    STEP: Creating a Secret 08/17/23 08:18:36.846
    STEP: Ensuring resource quota status captures secret creation 08/17/23 08:18:36.853
    STEP: Deleting a secret 08/17/23 08:18:38.855
    STEP: Ensuring resource quota status released usage 08/17/23 08:18:38.859
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 08:18:40.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1023" for this suite. 08/17/23 08:18:40.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 17 08:18:40.869: INFO: Running AfterSuite actions on node 1
Aug 17 08:18:40.869: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 17 08:18:40.869: INFO: Running AfterSuite actions on node 1
    Aug 17 08:18:40.869: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.075 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5614.810 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h33m35.100817493s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

