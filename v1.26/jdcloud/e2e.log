I0517 07:37:42.255219      23 e2e.go:126] Starting e2e run "ae2668ca-7e49-4bda-8445-8d9b7c33c609" on Ginkgo node 1
May 17 07:37:42.270: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1684309062 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May 17 07:37:42.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 07:37:42.367: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 17 07:37:42.378: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 17 07:37:42.394: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 17 07:37:42.394: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 17 07:37:42.394: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 17 07:37:42.398: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May 17 07:37:42.398: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 17 07:37:42.398: INFO: e2e test version: v1.26.4
May 17 07:37:42.398: INFO: kube-apiserver version: v1.26.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May 17 07:37:42.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 07:37:42.401: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.036 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May 17 07:37:42.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 07:37:42.367: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    May 17 07:37:42.378: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May 17 07:37:42.394: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May 17 07:37:42.394: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    May 17 07:37:42.394: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May 17 07:37:42.398: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    May 17 07:37:42.398: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    May 17 07:37:42.398: INFO: e2e test version: v1.26.4
    May 17 07:37:42.398: INFO: kube-apiserver version: v1.26.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May 17 07:37:42.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 07:37:42.401: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:37:42.424
May 17 07:37:42.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 07:37:42.425
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:37:42.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:37:42.435
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8514 05/17/23 07:37:42.437
STEP: changing the ExternalName service to type=ClusterIP 05/17/23 07:37:42.439
STEP: creating replication controller externalname-service in namespace services-8514 05/17/23 07:37:42.447
I0517 07:37:42.451240      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8514, replica count: 2
I0517 07:37:45.503268      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 07:37:45.503: INFO: Creating new exec pod
May 17 07:37:45.507: INFO: Waiting up to 5m0s for pod "execpod4v8fn" in namespace "services-8514" to be "running"
May 17 07:37:45.509: INFO: Pod "execpod4v8fn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.54861ms
May 17 07:37:47.512: INFO: Pod "execpod4v8fn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004531022s
May 17 07:37:47.512: INFO: Pod "execpod4v8fn" satisfied condition "running"
May 17 07:37:48.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8514 exec execpod4v8fn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May 17 07:37:48.632: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 17 07:37:48.632: INFO: stdout: ""
May 17 07:37:48.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8514 exec execpod4v8fn -- /bin/sh -x -c nc -v -z -w 2 10.97.53.84 80'
May 17 07:37:48.737: INFO: stderr: "+ nc -v -z -w 2 10.97.53.84 80\nConnection to 10.97.53.84 80 port [tcp/http] succeeded!\n"
May 17 07:37:48.737: INFO: stdout: ""
May 17 07:37:48.737: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 07:37:48.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8514" for this suite. 05/17/23 07:37:48.75
------------------------------
â€¢ [SLOW TEST] [6.329 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:37:42.424
    May 17 07:37:42.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 07:37:42.425
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:37:42.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:37:42.435
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8514 05/17/23 07:37:42.437
    STEP: changing the ExternalName service to type=ClusterIP 05/17/23 07:37:42.439
    STEP: creating replication controller externalname-service in namespace services-8514 05/17/23 07:37:42.447
    I0517 07:37:42.451240      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8514, replica count: 2
    I0517 07:37:45.503268      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 07:37:45.503: INFO: Creating new exec pod
    May 17 07:37:45.507: INFO: Waiting up to 5m0s for pod "execpod4v8fn" in namespace "services-8514" to be "running"
    May 17 07:37:45.509: INFO: Pod "execpod4v8fn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.54861ms
    May 17 07:37:47.512: INFO: Pod "execpod4v8fn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004531022s
    May 17 07:37:47.512: INFO: Pod "execpod4v8fn" satisfied condition "running"
    May 17 07:37:48.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8514 exec execpod4v8fn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May 17 07:37:48.632: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May 17 07:37:48.632: INFO: stdout: ""
    May 17 07:37:48.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8514 exec execpod4v8fn -- /bin/sh -x -c nc -v -z -w 2 10.97.53.84 80'
    May 17 07:37:48.737: INFO: stderr: "+ nc -v -z -w 2 10.97.53.84 80\nConnection to 10.97.53.84 80 port [tcp/http] succeeded!\n"
    May 17 07:37:48.737: INFO: stdout: ""
    May 17 07:37:48.737: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 07:37:48.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8514" for this suite. 05/17/23 07:37:48.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:37:48.754
May 17 07:37:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svcaccounts 05/17/23 07:37:48.755
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:37:48.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:37:48.762
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
May 17 07:37:48.770: INFO: created pod
May 17 07:37:48.770: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8324" to be "Succeeded or Failed"
May 17 07:37:48.772: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.341306ms
May 17 07:37:50.775: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00503998s
May 17 07:37:52.775: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004642589s
STEP: Saw pod success 05/17/23 07:37:52.775
May 17 07:37:52.775: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 17 07:38:22.775: INFO: polling logs
May 17 07:38:22.787: INFO: Pod logs: 
I0517 07:37:49.348679       1 log.go:198] OK: Got token
I0517 07:37:49.348712       1 log.go:198] validating with in-cluster discovery
I0517 07:37:49.348936       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0517 07:37:49.348962       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8324:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684309669, NotBefore:1684309069, IssuedAt:1684309069, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8324", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5a74cad9-63ed-47c5-b0a6-17c18050ccf5"}}}
I0517 07:37:49.356035       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0517 07:37:49.361673       1 log.go:198] OK: Validated signature on JWT
I0517 07:37:49.362004       1 log.go:198] OK: Got valid claims from token!
I0517 07:37:49.362036       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8324:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684309669, NotBefore:1684309069, IssuedAt:1684309069, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8324", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5a74cad9-63ed-47c5-b0a6-17c18050ccf5"}}}

May 17 07:38:22.787: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 17 07:38:22.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8324" for this suite. 05/17/23 07:38:22.793
------------------------------
â€¢ [SLOW TEST] [34.044 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:37:48.754
    May 17 07:37:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 07:37:48.755
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:37:48.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:37:48.762
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    May 17 07:37:48.770: INFO: created pod
    May 17 07:37:48.770: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8324" to be "Succeeded or Failed"
    May 17 07:37:48.772: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.341306ms
    May 17 07:37:50.775: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00503998s
    May 17 07:37:52.775: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004642589s
    STEP: Saw pod success 05/17/23 07:37:52.775
    May 17 07:37:52.775: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May 17 07:38:22.775: INFO: polling logs
    May 17 07:38:22.787: INFO: Pod logs: 
    I0517 07:37:49.348679       1 log.go:198] OK: Got token
    I0517 07:37:49.348712       1 log.go:198] validating with in-cluster discovery
    I0517 07:37:49.348936       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0517 07:37:49.348962       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8324:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684309669, NotBefore:1684309069, IssuedAt:1684309069, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8324", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5a74cad9-63ed-47c5-b0a6-17c18050ccf5"}}}
    I0517 07:37:49.356035       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0517 07:37:49.361673       1 log.go:198] OK: Validated signature on JWT
    I0517 07:37:49.362004       1 log.go:198] OK: Got valid claims from token!
    I0517 07:37:49.362036       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8324:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684309669, NotBefore:1684309069, IssuedAt:1684309069, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8324", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5a74cad9-63ed-47c5-b0a6-17c18050ccf5"}}}

    May 17 07:38:22.787: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 17 07:38:22.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8324" for this suite. 05/17/23 07:38:22.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:38:22.798
May 17 07:38:22.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename taint-single-pod 05/17/23 07:38:22.799
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:38:22.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:38:22.807
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
May 17 07:38:22.809: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 07:39:22.822: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
May 17 07:39:22.823: INFO: Starting informer...
STEP: Starting pod... 05/17/23 07:39:22.823
May 17 07:39:23.032: INFO: Pod is running on k8s-node1. Tainting Node
STEP: Trying to apply a taint on the Node 05/17/23 07:39:23.032
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 07:39:23.041
STEP: Waiting short time to make sure Pod is queued for deletion 05/17/23 07:39:23.043
May 17 07:39:23.043: INFO: Pod wasn't evicted. Proceeding
May 17 07:39:23.043: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 07:39:23.051
STEP: Waiting some time to make sure that toleration time passed. 05/17/23 07:39:23.052
May 17 07:40:38.053: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:40:38.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-4739" for this suite. 05/17/23 07:40:38.056
------------------------------
â€¢ [SLOW TEST] [135.261 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:38:22.798
    May 17 07:38:22.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename taint-single-pod 05/17/23 07:38:22.799
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:38:22.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:38:22.807
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    May 17 07:38:22.809: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 07:39:22.822: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    May 17 07:39:22.823: INFO: Starting informer...
    STEP: Starting pod... 05/17/23 07:39:22.823
    May 17 07:39:23.032: INFO: Pod is running on k8s-node1. Tainting Node
    STEP: Trying to apply a taint on the Node 05/17/23 07:39:23.032
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 07:39:23.041
    STEP: Waiting short time to make sure Pod is queued for deletion 05/17/23 07:39:23.043
    May 17 07:39:23.043: INFO: Pod wasn't evicted. Proceeding
    May 17 07:39:23.043: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 07:39:23.051
    STEP: Waiting some time to make sure that toleration time passed. 05/17/23 07:39:23.052
    May 17 07:40:38.053: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:38.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-4739" for this suite. 05/17/23 07:40:38.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:38.06
May 17 07:40:38.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 07:40:38.061
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:38.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:38.068
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 05/17/23 07:40:38.07
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 07:40:38.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7659" for this suite. 05/17/23 07:40:38.073
------------------------------
â€¢ [0.015 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:38.06
    May 17 07:40:38.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 07:40:38.061
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:38.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:38.068
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 05/17/23 07:40:38.07
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:38.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7659" for this suite. 05/17/23 07:40:38.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:38.076
May 17 07:40:38.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-pred 05/17/23 07:40:38.077
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:38.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:38.085
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 17 07:40:38.087: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 07:40:38.090: INFO: Waiting for terminating namespaces to be deleted...
May 17 07:40:38.092: INFO: 
Logging pods the apiserver thinks is on node k8s-node1 before test
May 17 07:40:38.094: INFO: calico-kube-controllers-57b57c56f-fwfmb from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.094: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 07:40:38.094: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.094: INFO: 	Container calico-node ready: true, restart count 0
May 17 07:40:38.094: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.094: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:40:38.094: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:40:38.094: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:40:38.094: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 07:40:38.094: INFO: taint-eviction-4 from taint-single-pod-4739 started at 2023-05-17 07:39:22 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.094: INFO: 	Container pause ready: true, restart count 0
May 17 07:40:38.094: INFO: 
Logging pods the apiserver thinks is on node k8s-node2 before test
May 17 07:40:38.097: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.097: INFO: 	Container calico-node ready: true, restart count 0
May 17 07:40:38.097: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.097: INFO: 	Container coredns ready: true, restart count 0
May 17 07:40:38.097: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.097: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:40:38.097: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
May 17 07:40:38.097: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 07:40:38.097: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:40:38.097: INFO: 	Container e2e ready: true, restart count 0
May 17 07:40:38.097: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:40:38.097: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:40:38.097: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:40:38.097: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/17/23 07:40:38.097
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175fde540547de0a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 05/17/23 07:40:38.111
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:40:39.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4295" for this suite. 05/17/23 07:40:39.111
------------------------------
â€¢ [1.053 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:38.076
    May 17 07:40:38.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-pred 05/17/23 07:40:38.077
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:38.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:38.085
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 17 07:40:38.087: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 07:40:38.090: INFO: Waiting for terminating namespaces to be deleted...
    May 17 07:40:38.092: INFO: 
    Logging pods the apiserver thinks is on node k8s-node1 before test
    May 17 07:40:38.094: INFO: calico-kube-controllers-57b57c56f-fwfmb from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.094: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May 17 07:40:38.094: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.094: INFO: 	Container calico-node ready: true, restart count 0
    May 17 07:40:38.094: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.094: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:40:38.094: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:40:38.094: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:40:38.094: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 07:40:38.094: INFO: taint-eviction-4 from taint-single-pod-4739 started at 2023-05-17 07:39:22 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.094: INFO: 	Container pause ready: true, restart count 0
    May 17 07:40:38.094: INFO: 
    Logging pods the apiserver thinks is on node k8s-node2 before test
    May 17 07:40:38.097: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.097: INFO: 	Container calico-node ready: true, restart count 0
    May 17 07:40:38.097: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.097: INFO: 	Container coredns ready: true, restart count 0
    May 17 07:40:38.097: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.097: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:40:38.097: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
    May 17 07:40:38.097: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 07:40:38.097: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:40:38.097: INFO: 	Container e2e ready: true, restart count 0
    May 17 07:40:38.097: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:40:38.097: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:40:38.097: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:40:38.097: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/17/23 07:40:38.097
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175fde540547de0a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 05/17/23 07:40:38.111
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:39.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4295" for this suite. 05/17/23 07:40:39.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:39.131
May 17 07:40:39.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename namespaces 05/17/23 07:40:39.131
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:39.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:39.139
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 05/17/23 07:40:39.141
May 17 07:40:39.142: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/17/23 07:40:39.142
May 17 07:40:39.145: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/17/23 07:40:39.145
May 17 07:40:39.149: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:40:39.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-672" for this suite. 05/17/23 07:40:39.151
------------------------------
â€¢ [0.023 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:39.131
    May 17 07:40:39.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename namespaces 05/17/23 07:40:39.131
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:39.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:39.139
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 05/17/23 07:40:39.141
    May 17 07:40:39.142: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/17/23 07:40:39.142
    May 17 07:40:39.145: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/17/23 07:40:39.145
    May 17 07:40:39.149: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:39.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-672" for this suite. 05/17/23 07:40:39.151
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:39.154
May 17 07:40:39.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 07:40:39.155
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:39.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:39.163
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 05/17/23 07:40:39.165
STEP: setting up watch 05/17/23 07:40:39.165
STEP: submitting the pod to kubernetes 05/17/23 07:40:39.267
STEP: verifying the pod is in kubernetes 05/17/23 07:40:39.272
STEP: verifying pod creation was observed 05/17/23 07:40:39.274
May 17 07:40:39.274: INFO: Waiting up to 5m0s for pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390" in namespace "pods-5937" to be "running"
May 17 07:40:39.275: INFO: Pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693621ms
May 17 07:40:41.279: INFO: Pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390": Phase="Running", Reason="", readiness=true. Elapsed: 2.00496611s
May 17 07:40:41.279: INFO: Pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390" satisfied condition "running"
STEP: deleting the pod gracefully 05/17/23 07:40:41.28
STEP: verifying pod deletion was observed 05/17/23 07:40:41.284
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 07:40:43.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5937" for this suite. 05/17/23 07:40:43.787
------------------------------
â€¢ [4.635 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:39.154
    May 17 07:40:39.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 07:40:39.155
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:39.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:39.163
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 05/17/23 07:40:39.165
    STEP: setting up watch 05/17/23 07:40:39.165
    STEP: submitting the pod to kubernetes 05/17/23 07:40:39.267
    STEP: verifying the pod is in kubernetes 05/17/23 07:40:39.272
    STEP: verifying pod creation was observed 05/17/23 07:40:39.274
    May 17 07:40:39.274: INFO: Waiting up to 5m0s for pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390" in namespace "pods-5937" to be "running"
    May 17 07:40:39.275: INFO: Pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693621ms
    May 17 07:40:41.279: INFO: Pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390": Phase="Running", Reason="", readiness=true. Elapsed: 2.00496611s
    May 17 07:40:41.279: INFO: Pod "pod-submit-remove-06e8cb8a-310f-417a-9018-b5e44cc52390" satisfied condition "running"
    STEP: deleting the pod gracefully 05/17/23 07:40:41.28
    STEP: verifying pod deletion was observed 05/17/23 07:40:41.284
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:43.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5937" for this suite. 05/17/23 07:40:43.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:43.79
May 17 07:40:43.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 07:40:43.791
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:43.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:43.798
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-232aee13-53d2-4465-9ce5-4a83e4ba91cb 05/17/23 07:40:43.8
STEP: Creating a pod to test consume secrets 05/17/23 07:40:43.802
May 17 07:40:43.806: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97" in namespace "projected-1614" to be "Succeeded or Failed"
May 17 07:40:43.807: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.167601ms
May 17 07:40:45.810: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003681665s
May 17 07:40:47.811: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00427961s
STEP: Saw pod success 05/17/23 07:40:47.811
May 17 07:40:47.811: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97" satisfied condition "Succeeded or Failed"
May 17 07:40:47.812: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 07:40:47.821
May 17 07:40:47.828: INFO: Waiting for pod pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97 to disappear
May 17 07:40:47.830: INFO: Pod pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 17 07:40:47.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1614" for this suite. 05/17/23 07:40:47.832
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:43.79
    May 17 07:40:43.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 07:40:43.791
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:43.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:43.798
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-232aee13-53d2-4465-9ce5-4a83e4ba91cb 05/17/23 07:40:43.8
    STEP: Creating a pod to test consume secrets 05/17/23 07:40:43.802
    May 17 07:40:43.806: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97" in namespace "projected-1614" to be "Succeeded or Failed"
    May 17 07:40:43.807: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.167601ms
    May 17 07:40:45.810: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003681665s
    May 17 07:40:47.811: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00427961s
    STEP: Saw pod success 05/17/23 07:40:47.811
    May 17 07:40:47.811: INFO: Pod "pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97" satisfied condition "Succeeded or Failed"
    May 17 07:40:47.812: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 07:40:47.821
    May 17 07:40:47.828: INFO: Waiting for pod pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97 to disappear
    May 17 07:40:47.830: INFO: Pod pod-projected-secrets-20527125-ba07-4357-b829-152d3f6c5b97 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:47.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1614" for this suite. 05/17/23 07:40:47.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:47.836
May 17 07:40:47.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 07:40:47.837
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:47.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:47.844
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 05/17/23 07:40:47.846
May 17 07:40:47.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May 17 07:40:47.905: INFO: stderr: ""
May 17 07:40:47.905: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 05/17/23 07:40:47.905
May 17 07:40:47.905: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 17 07:40:47.905: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5562" to be "running and ready, or succeeded"
May 17 07:40:47.907: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.862775ms
May 17 07:40:47.907: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-node1' to be 'Running' but was 'Pending'
May 17 07:40:49.910: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004852612s
May 17 07:40:49.910: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 17 07:40:49.910: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/17/23 07:40:49.91
May 17 07:40:49.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator'
May 17 07:40:49.968: INFO: stderr: ""
May 17 07:40:49.968: INFO: stdout: "I0517 07:40:48.472221       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/wd4 354\nI0517 07:40:48.672591       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/w5r7 252\nI0517 07:40:48.872906       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/7pr 508\nI0517 07:40:49.073227       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/6mb 225\nI0517 07:40:49.272558       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/xw9l 345\nI0517 07:40:49.472868       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/6hl4 578\nI0517 07:40:49.673193       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2p7t 333\nI0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\n"
STEP: limiting log lines 05/17/23 07:40:49.968
May 17 07:40:49.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --tail=1'
May 17 07:40:50.023: INFO: stderr: ""
May 17 07:40:50.023: INFO: stdout: "I0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\n"
May 17 07:40:50.023: INFO: got output "I0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\n"
STEP: limiting log bytes 05/17/23 07:40:50.023
May 17 07:40:50.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --limit-bytes=1'
May 17 07:40:50.084: INFO: stderr: ""
May 17 07:40:50.084: INFO: stdout: "I"
May 17 07:40:50.084: INFO: got output "I"
STEP: exposing timestamps 05/17/23 07:40:50.084
May 17 07:40:50.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --tail=1 --timestamps'
May 17 07:40:50.141: INFO: stderr: ""
May 17 07:40:50.141: INFO: stdout: "2023-05-17T15:40:50.072945624+08:00 I0517 07:40:50.072807       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9bcl 295\n"
May 17 07:40:50.141: INFO: got output "2023-05-17T15:40:50.072945624+08:00 I0517 07:40:50.072807       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9bcl 295\n"
STEP: restricting to a time range 05/17/23 07:40:50.141
May 17 07:40:52.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --since=1s'
May 17 07:40:52.702: INFO: stderr: ""
May 17 07:40:52.702: INFO: stdout: "I0517 07:40:51.872666       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/mqs 384\nI0517 07:40:52.072998       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/4nfr 523\nI0517 07:40:52.272316       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lp8 556\nI0517 07:40:52.472655       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/5lds 368\nI0517 07:40:52.673016       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/v56 438\n"
May 17 07:40:52.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --since=24h'
May 17 07:40:52.761: INFO: stderr: ""
May 17 07:40:52.761: INFO: stdout: "I0517 07:40:48.472221       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/wd4 354\nI0517 07:40:48.672591       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/w5r7 252\nI0517 07:40:48.872906       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/7pr 508\nI0517 07:40:49.073227       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/6mb 225\nI0517 07:40:49.272558       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/xw9l 345\nI0517 07:40:49.472868       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/6hl4 578\nI0517 07:40:49.673193       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2p7t 333\nI0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\nI0517 07:40:50.072807       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9bcl 295\nI0517 07:40:50.273135       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/mtx 458\nI0517 07:40:50.472399       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/sfn2 528\nI0517 07:40:50.672706       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/ldq 257\nI0517 07:40:50.873063       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/4s7 219\nI0517 07:40:51.072338       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/sqvn 281\nI0517 07:40:51.272680       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/qnt5 526\nI0517 07:40:51.473000       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/ddr 569\nI0517 07:40:51.672289       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/5r2v 419\nI0517 07:40:51.872666       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/mqs 384\nI0517 07:40:52.072998       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/4nfr 523\nI0517 07:40:52.272316       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lp8 556\nI0517 07:40:52.472655       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/5lds 368\nI0517 07:40:52.673016       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/v56 438\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
May 17 07:40:52.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 delete pod logs-generator'
May 17 07:40:53.799: INFO: stderr: ""
May 17 07:40:53.799: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 07:40:53.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5562" for this suite. 05/17/23 07:40:53.802
------------------------------
â€¢ [SLOW TEST] [5.968 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:47.836
    May 17 07:40:47.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 07:40:47.837
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:47.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:47.844
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 05/17/23 07:40:47.846
    May 17 07:40:47.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May 17 07:40:47.905: INFO: stderr: ""
    May 17 07:40:47.905: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 05/17/23 07:40:47.905
    May 17 07:40:47.905: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May 17 07:40:47.905: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5562" to be "running and ready, or succeeded"
    May 17 07:40:47.907: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.862775ms
    May 17 07:40:47.907: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-node1' to be 'Running' but was 'Pending'
    May 17 07:40:49.910: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004852612s
    May 17 07:40:49.910: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May 17 07:40:49.910: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/17/23 07:40:49.91
    May 17 07:40:49.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator'
    May 17 07:40:49.968: INFO: stderr: ""
    May 17 07:40:49.968: INFO: stdout: "I0517 07:40:48.472221       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/wd4 354\nI0517 07:40:48.672591       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/w5r7 252\nI0517 07:40:48.872906       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/7pr 508\nI0517 07:40:49.073227       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/6mb 225\nI0517 07:40:49.272558       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/xw9l 345\nI0517 07:40:49.472868       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/6hl4 578\nI0517 07:40:49.673193       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2p7t 333\nI0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\n"
    STEP: limiting log lines 05/17/23 07:40:49.968
    May 17 07:40:49.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --tail=1'
    May 17 07:40:50.023: INFO: stderr: ""
    May 17 07:40:50.023: INFO: stdout: "I0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\n"
    May 17 07:40:50.023: INFO: got output "I0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\n"
    STEP: limiting log bytes 05/17/23 07:40:50.023
    May 17 07:40:50.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --limit-bytes=1'
    May 17 07:40:50.084: INFO: stderr: ""
    May 17 07:40:50.084: INFO: stdout: "I"
    May 17 07:40:50.084: INFO: got output "I"
    STEP: exposing timestamps 05/17/23 07:40:50.084
    May 17 07:40:50.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --tail=1 --timestamps'
    May 17 07:40:50.141: INFO: stderr: ""
    May 17 07:40:50.141: INFO: stdout: "2023-05-17T15:40:50.072945624+08:00 I0517 07:40:50.072807       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9bcl 295\n"
    May 17 07:40:50.141: INFO: got output "2023-05-17T15:40:50.072945624+08:00 I0517 07:40:50.072807       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9bcl 295\n"
    STEP: restricting to a time range 05/17/23 07:40:50.141
    May 17 07:40:52.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --since=1s'
    May 17 07:40:52.702: INFO: stderr: ""
    May 17 07:40:52.702: INFO: stdout: "I0517 07:40:51.872666       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/mqs 384\nI0517 07:40:52.072998       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/4nfr 523\nI0517 07:40:52.272316       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lp8 556\nI0517 07:40:52.472655       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/5lds 368\nI0517 07:40:52.673016       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/v56 438\n"
    May 17 07:40:52.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 logs logs-generator logs-generator --since=24h'
    May 17 07:40:52.761: INFO: stderr: ""
    May 17 07:40:52.761: INFO: stdout: "I0517 07:40:48.472221       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/wd4 354\nI0517 07:40:48.672591       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/w5r7 252\nI0517 07:40:48.872906       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/7pr 508\nI0517 07:40:49.073227       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/6mb 225\nI0517 07:40:49.272558       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/xw9l 345\nI0517 07:40:49.472868       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/6hl4 578\nI0517 07:40:49.673193       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2p7t 333\nI0517 07:40:49.872449       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9rh 511\nI0517 07:40:50.072807       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/9bcl 295\nI0517 07:40:50.273135       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/mtx 458\nI0517 07:40:50.472399       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/sfn2 528\nI0517 07:40:50.672706       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/ldq 257\nI0517 07:40:50.873063       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/4s7 219\nI0517 07:40:51.072338       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/sqvn 281\nI0517 07:40:51.272680       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/qnt5 526\nI0517 07:40:51.473000       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/ddr 569\nI0517 07:40:51.672289       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/5r2v 419\nI0517 07:40:51.872666       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/mqs 384\nI0517 07:40:52.072998       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/4nfr 523\nI0517 07:40:52.272316       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lp8 556\nI0517 07:40:52.472655       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/5lds 368\nI0517 07:40:52.673016       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/v56 438\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    May 17 07:40:52.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5562 delete pod logs-generator'
    May 17 07:40:53.799: INFO: stderr: ""
    May 17 07:40:53.799: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:53.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5562" for this suite. 05/17/23 07:40:53.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:53.805
May 17 07:40:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 07:40:53.806
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:53.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:53.816
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 07:40:53.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5806" for this suite. 05/17/23 07:40:53.837
------------------------------
â€¢ [0.035 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:53.805
    May 17 07:40:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 07:40:53.806
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:53.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:53.816
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:53.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5806" for this suite. 05/17/23 07:40:53.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:53.84
May 17 07:40:53.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 07:40:53.841
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:53.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:53.848
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
May 17 07:40:53.854: INFO: Waiting up to 2m0s for pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" in namespace "var-expansion-2578" to be "container 0 failed with reason CreateContainerConfigError"
May 17 07:40:53.856: INFO: Pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.285927ms
May 17 07:40:55.858: INFO: Pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00360992s
May 17 07:40:55.858: INFO: Pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May 17 07:40:55.858: INFO: Deleting pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" in namespace "var-expansion-2578"
May 17 07:40:55.861: INFO: Wait up to 5m0s for pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 07:40:57.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2578" for this suite. 05/17/23 07:40:57.868
------------------------------
â€¢ [4.030 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:53.84
    May 17 07:40:53.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 07:40:53.841
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:53.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:53.848
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    May 17 07:40:53.854: INFO: Waiting up to 2m0s for pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" in namespace "var-expansion-2578" to be "container 0 failed with reason CreateContainerConfigError"
    May 17 07:40:53.856: INFO: Pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.285927ms
    May 17 07:40:55.858: INFO: Pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00360992s
    May 17 07:40:55.858: INFO: Pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May 17 07:40:55.858: INFO: Deleting pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" in namespace "var-expansion-2578"
    May 17 07:40:55.861: INFO: Wait up to 5m0s for pod "var-expansion-73ea207e-a3c2-4c5c-a103-45fdbe3371d7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 07:40:57.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2578" for this suite. 05/17/23 07:40:57.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:40:57.871
May 17 07:40:57.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 07:40:57.872
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:57.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:57.88
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/17/23 07:40:57.884
May 17 07:40:57.888: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4969" to be "running and ready"
May 17 07:40:57.889: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363376ms
May 17 07:40:57.889: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 07:40:59.892: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003732575s
May 17 07:40:59.892: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 07:40:59.892: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 05/17/23 07:40:59.893
May 17 07:40:59.896: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4969" to be "running and ready"
May 17 07:40:59.897: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.148147ms
May 17 07:40:59.897: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 07:41:01.900: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003837455s
May 17 07:41:01.900: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May 17 07:41:01.900: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/17/23 07:41:01.902
May 17 07:41:01.906: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 07:41:01.907: INFO: Pod pod-with-prestop-http-hook still exists
May 17 07:41:03.908: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 07:41:03.911: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/17/23 07:41:03.911
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 17 07:41:03.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4969" for this suite. 05/17/23 07:41:03.917
------------------------------
â€¢ [SLOW TEST] [6.049 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:40:57.871
    May 17 07:40:57.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 07:40:57.872
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:40:57.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:40:57.88
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 07:40:57.884
    May 17 07:40:57.888: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4969" to be "running and ready"
    May 17 07:40:57.889: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363376ms
    May 17 07:40:57.889: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:40:59.892: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003732575s
    May 17 07:40:59.892: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 07:40:59.892: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 05/17/23 07:40:59.893
    May 17 07:40:59.896: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4969" to be "running and ready"
    May 17 07:40:59.897: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.148147ms
    May 17 07:40:59.897: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:41:01.900: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003837455s
    May 17 07:41:01.900: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May 17 07:41:01.900: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/17/23 07:41:01.902
    May 17 07:41:01.906: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 17 07:41:01.907: INFO: Pod pod-with-prestop-http-hook still exists
    May 17 07:41:03.908: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 17 07:41:03.911: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/17/23 07:41:03.911
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 17 07:41:03.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4969" for this suite. 05/17/23 07:41:03.917
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:41:03.92
May 17 07:41:03.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 07:41:03.921
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:41:03.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:41:03.93
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 07:41:03.937
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:41:04.195
STEP: Deploying the webhook pod 05/17/23 07:41:04.199
STEP: Wait for the deployment to be ready 05/17/23 07:41:04.204
May 17 07:41:04.220: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 07:41:06.225
STEP: Verifying the service has paired with the endpoint 05/17/23 07:41:06.232
May 17 07:41:07.233: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 07:41:07.235
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 07:41:07.245
STEP: Creating a dummy validating-webhook-configuration object 05/17/23 07:41:07.253
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/17/23 07:41:07.257
STEP: Creating a dummy mutating-webhook-configuration object 05/17/23 07:41:07.259
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/17/23 07:41:07.264
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:41:07.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1980" for this suite. 05/17/23 07:41:07.288
STEP: Destroying namespace "webhook-1980-markers" for this suite. 05/17/23 07:41:07.292
------------------------------
â€¢ [3.375 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:41:03.92
    May 17 07:41:03.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 07:41:03.921
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:41:03.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:41:03.93
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 07:41:03.937
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:41:04.195
    STEP: Deploying the webhook pod 05/17/23 07:41:04.199
    STEP: Wait for the deployment to be ready 05/17/23 07:41:04.204
    May 17 07:41:04.220: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 07:41:06.225
    STEP: Verifying the service has paired with the endpoint 05/17/23 07:41:06.232
    May 17 07:41:07.233: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 07:41:07.235
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 07:41:07.245
    STEP: Creating a dummy validating-webhook-configuration object 05/17/23 07:41:07.253
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/17/23 07:41:07.257
    STEP: Creating a dummy mutating-webhook-configuration object 05/17/23 07:41:07.259
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/17/23 07:41:07.264
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:41:07.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1980" for this suite. 05/17/23 07:41:07.288
    STEP: Destroying namespace "webhook-1980-markers" for this suite. 05/17/23 07:41:07.292
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:41:07.295
May 17 07:41:07.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-preemption 05/17/23 07:41:07.296
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:41:07.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:41:07.303
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 17 07:41:07.310: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 07:42:07.322: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:07.324
May 17 07:42:07.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 07:42:07.324
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:07.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:07.331
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 05/17/23 07:42:07.333
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 07:42:07.333
May 17 07:42:07.336: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7033" to be "running"
May 17 07:42:07.337: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.151306ms
May 17 07:42:09.340: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003626698s
May 17 07:42:09.340: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 07:42:09.341
May 17 07:42:09.347: INFO: found a healthy node: k8s-node1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
May 17 07:42:15.382: INFO: pods created so far: [1 1 1]
May 17 07:42:15.382: INFO: length of pods created so far: 3
May 17 07:42:17.388: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
May 17 07:42:24.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:42:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7033" for this suite. 05/17/23 07:42:24.426
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8132" for this suite. 05/17/23 07:42:24.428
------------------------------
â€¢ [SLOW TEST] [77.135 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:41:07.295
    May 17 07:41:07.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 07:41:07.296
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:41:07.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:41:07.303
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 17 07:41:07.310: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 07:42:07.322: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:07.324
    May 17 07:42:07.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 07:42:07.324
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:07.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:07.331
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 05/17/23 07:42:07.333
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 07:42:07.333
    May 17 07:42:07.336: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7033" to be "running"
    May 17 07:42:07.337: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.151306ms
    May 17 07:42:09.340: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003626698s
    May 17 07:42:09.340: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 07:42:09.341
    May 17 07:42:09.347: INFO: found a healthy node: k8s-node1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    May 17 07:42:15.382: INFO: pods created so far: [1 1 1]
    May 17 07:42:15.382: INFO: length of pods created so far: 3
    May 17 07:42:17.388: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:24.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7033" for this suite. 05/17/23 07:42:24.426
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8132" for this suite. 05/17/23 07:42:24.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:24.431
May 17 07:42:24.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 07:42:24.432
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:24.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:24.44
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-51b3af58-acbb-440c-8dda-6051a3f64cee 05/17/23 07:42:24.441
STEP: Creating secret with name secret-projected-all-test-volume-df152fdc-dd5e-4319-b02b-778996f5d5c5 05/17/23 07:42:24.443
STEP: Creating a pod to test Check all projections for projected volume plugin 05/17/23 07:42:24.445
May 17 07:42:24.449: INFO: Waiting up to 5m0s for pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28" in namespace "projected-1838" to be "Succeeded or Failed"
May 17 07:42:24.450: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28": Phase="Pending", Reason="", readiness=false. Elapsed: 1.184767ms
May 17 07:42:26.453: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003925746s
May 17 07:42:28.453: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00371805s
STEP: Saw pod success 05/17/23 07:42:28.453
May 17 07:42:28.453: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28" satisfied condition "Succeeded or Failed"
May 17 07:42:28.454: INFO: Trying to get logs from node k8s-node1 pod projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28 container projected-all-volume-test: <nil>
STEP: delete the pod 05/17/23 07:42:28.457
May 17 07:42:28.464: INFO: Waiting for pod projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28 to disappear
May 17 07:42:28.465: INFO: Pod projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
May 17 07:42:28.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1838" for this suite. 05/17/23 07:42:28.467
------------------------------
â€¢ [4.038 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:24.431
    May 17 07:42:24.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 07:42:24.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:24.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:24.44
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-51b3af58-acbb-440c-8dda-6051a3f64cee 05/17/23 07:42:24.441
    STEP: Creating secret with name secret-projected-all-test-volume-df152fdc-dd5e-4319-b02b-778996f5d5c5 05/17/23 07:42:24.443
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/17/23 07:42:24.445
    May 17 07:42:24.449: INFO: Waiting up to 5m0s for pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28" in namespace "projected-1838" to be "Succeeded or Failed"
    May 17 07:42:24.450: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28": Phase="Pending", Reason="", readiness=false. Elapsed: 1.184767ms
    May 17 07:42:26.453: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003925746s
    May 17 07:42:28.453: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00371805s
    STEP: Saw pod success 05/17/23 07:42:28.453
    May 17 07:42:28.453: INFO: Pod "projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28" satisfied condition "Succeeded or Failed"
    May 17 07:42:28.454: INFO: Trying to get logs from node k8s-node1 pod projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28 container projected-all-volume-test: <nil>
    STEP: delete the pod 05/17/23 07:42:28.457
    May 17 07:42:28.464: INFO: Waiting for pod projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28 to disappear
    May 17 07:42:28.465: INFO: Pod projected-volume-f3434d81-dd3c-4d2d-a86f-cc6721ed4e28 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:28.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1838" for this suite. 05/17/23 07:42:28.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:28.469
May 17 07:42:28.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 07:42:28.47
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:28.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:28.478
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
May 17 07:42:28.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: creating the pod 05/17/23 07:42:28.48
STEP: submitting the pod to kubernetes 05/17/23 07:42:28.48
May 17 07:42:28.483: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0" in namespace "pods-5881" to be "running and ready"
May 17 07:42:28.485: INFO: Pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.17156ms
May 17 07:42:28.485: INFO: The phase of Pod pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:42:30.487: INFO: Pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00343834s
May 17 07:42:30.487: INFO: The phase of Pod pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0 is Running (Ready = true)
May 17 07:42:30.487: INFO: Pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 07:42:30.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5881" for this suite. 05/17/23 07:42:30.497
------------------------------
â€¢ [2.030 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:28.469
    May 17 07:42:28.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 07:42:28.47
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:28.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:28.478
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    May 17 07:42:28.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: creating the pod 05/17/23 07:42:28.48
    STEP: submitting the pod to kubernetes 05/17/23 07:42:28.48
    May 17 07:42:28.483: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0" in namespace "pods-5881" to be "running and ready"
    May 17 07:42:28.485: INFO: Pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.17156ms
    May 17 07:42:28.485: INFO: The phase of Pod pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:42:30.487: INFO: Pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00343834s
    May 17 07:42:30.487: INFO: The phase of Pod pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0 is Running (Ready = true)
    May 17 07:42:30.487: INFO: Pod "pod-logs-websocket-5d35dafc-d63e-43bd-b725-6a1a52c727b0" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:30.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5881" for this suite. 05/17/23 07:42:30.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:30.5
May 17 07:42:30.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 07:42:30.501
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:30.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:30.507
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 05/17/23 07:42:30.509
May 17 07:42:30.512: INFO: Waiting up to 5m0s for pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90" in namespace "downward-api-3412" to be "Succeeded or Failed"
May 17 07:42:30.513: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35824ms
May 17 07:42:32.517: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004612548s
May 17 07:42:34.517: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004592724s
STEP: Saw pod success 05/17/23 07:42:34.517
May 17 07:42:34.517: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90" satisfied condition "Succeeded or Failed"
May 17 07:42:34.518: INFO: Trying to get logs from node k8s-node1 pod downward-api-e62d9535-007f-4b38-9b76-130aa717ea90 container dapi-container: <nil>
STEP: delete the pod 05/17/23 07:42:34.521
May 17 07:42:34.527: INFO: Waiting for pod downward-api-e62d9535-007f-4b38-9b76-130aa717ea90 to disappear
May 17 07:42:34.529: INFO: Pod downward-api-e62d9535-007f-4b38-9b76-130aa717ea90 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 17 07:42:34.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3412" for this suite. 05/17/23 07:42:34.53
------------------------------
â€¢ [4.033 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:30.5
    May 17 07:42:30.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:42:30.501
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:30.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:30.507
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 05/17/23 07:42:30.509
    May 17 07:42:30.512: INFO: Waiting up to 5m0s for pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90" in namespace "downward-api-3412" to be "Succeeded or Failed"
    May 17 07:42:30.513: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35824ms
    May 17 07:42:32.517: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004612548s
    May 17 07:42:34.517: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004592724s
    STEP: Saw pod success 05/17/23 07:42:34.517
    May 17 07:42:34.517: INFO: Pod "downward-api-e62d9535-007f-4b38-9b76-130aa717ea90" satisfied condition "Succeeded or Failed"
    May 17 07:42:34.518: INFO: Trying to get logs from node k8s-node1 pod downward-api-e62d9535-007f-4b38-9b76-130aa717ea90 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 07:42:34.521
    May 17 07:42:34.527: INFO: Waiting for pod downward-api-e62d9535-007f-4b38-9b76-130aa717ea90 to disappear
    May 17 07:42:34.529: INFO: Pod downward-api-e62d9535-007f-4b38-9b76-130aa717ea90 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:34.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3412" for this suite. 05/17/23 07:42:34.53
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:34.533
May 17 07:42:34.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename ingressclass 05/17/23 07:42:34.534
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:34.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:34.541
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/17/23 07:42:34.543
STEP: getting /apis/networking.k8s.io 05/17/23 07:42:34.545
STEP: getting /apis/networking.k8s.iov1 05/17/23 07:42:34.545
STEP: creating 05/17/23 07:42:34.546
STEP: getting 05/17/23 07:42:34.552
STEP: listing 05/17/23 07:42:34.553
STEP: watching 05/17/23 07:42:34.555
May 17 07:42:34.555: INFO: starting watch
STEP: patching 05/17/23 07:42:34.555
STEP: updating 05/17/23 07:42:34.557
May 17 07:42:34.560: INFO: waiting for watch events with expected annotations
May 17 07:42:34.560: INFO: saw patched and updated annotations
STEP: deleting 05/17/23 07:42:34.56
STEP: deleting a collection 05/17/23 07:42:34.564
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
May 17 07:42:34.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-4073" for this suite. 05/17/23 07:42:34.572
------------------------------
â€¢ [0.041 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:34.533
    May 17 07:42:34.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename ingressclass 05/17/23 07:42:34.534
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:34.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:34.541
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/17/23 07:42:34.543
    STEP: getting /apis/networking.k8s.io 05/17/23 07:42:34.545
    STEP: getting /apis/networking.k8s.iov1 05/17/23 07:42:34.545
    STEP: creating 05/17/23 07:42:34.546
    STEP: getting 05/17/23 07:42:34.552
    STEP: listing 05/17/23 07:42:34.553
    STEP: watching 05/17/23 07:42:34.555
    May 17 07:42:34.555: INFO: starting watch
    STEP: patching 05/17/23 07:42:34.555
    STEP: updating 05/17/23 07:42:34.557
    May 17 07:42:34.560: INFO: waiting for watch events with expected annotations
    May 17 07:42:34.560: INFO: saw patched and updated annotations
    STEP: deleting 05/17/23 07:42:34.56
    STEP: deleting a collection 05/17/23 07:42:34.564
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:34.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-4073" for this suite. 05/17/23 07:42:34.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:34.575
May 17 07:42:34.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 07:42:34.576
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:34.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:34.584
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-014aed07-1935-44f2-b458-6a1f21643100 05/17/23 07:42:34.585
STEP: Creating a pod to test consume configMaps 05/17/23 07:42:34.587
May 17 07:42:34.592: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44" in namespace "projected-5010" to be "Succeeded or Failed"
May 17 07:42:34.593: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44": Phase="Pending", Reason="", readiness=false. Elapsed: 1.16122ms
May 17 07:42:36.596: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004434571s
May 17 07:42:38.595: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003664126s
STEP: Saw pod success 05/17/23 07:42:38.595
May 17 07:42:38.595: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44" satisfied condition "Succeeded or Failed"
May 17 07:42:38.597: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:42:38.599
May 17 07:42:38.605: INFO: Waiting for pod pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44 to disappear
May 17 07:42:38.607: INFO: Pod pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 07:42:38.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5010" for this suite. 05/17/23 07:42:38.608
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:34.575
    May 17 07:42:34.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 07:42:34.576
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:34.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:34.584
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-014aed07-1935-44f2-b458-6a1f21643100 05/17/23 07:42:34.585
    STEP: Creating a pod to test consume configMaps 05/17/23 07:42:34.587
    May 17 07:42:34.592: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44" in namespace "projected-5010" to be "Succeeded or Failed"
    May 17 07:42:34.593: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44": Phase="Pending", Reason="", readiness=false. Elapsed: 1.16122ms
    May 17 07:42:36.596: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004434571s
    May 17 07:42:38.595: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003664126s
    STEP: Saw pod success 05/17/23 07:42:38.595
    May 17 07:42:38.595: INFO: Pod "pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44" satisfied condition "Succeeded or Failed"
    May 17 07:42:38.597: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:42:38.599
    May 17 07:42:38.605: INFO: Waiting for pod pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44 to disappear
    May 17 07:42:38.607: INFO: Pod pod-projected-configmaps-2aca1413-b31a-40fe-9f09-b79c0680db44 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:38.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5010" for this suite. 05/17/23 07:42:38.608
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:38.611
May 17 07:42:38.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:42:38.612
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:38.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:38.62
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May 17 07:42:38.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:42:44.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7720" for this suite. 05/17/23 07:42:44.758
------------------------------
â€¢ [SLOW TEST] [6.150 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:38.611
    May 17 07:42:38.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:42:38.612
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:38.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:38.62
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May 17 07:42:38.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:44.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7720" for this suite. 05/17/23 07:42:44.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:44.762
May 17 07:42:44.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:42:44.763
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:44.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:44.779
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/17/23 07:42:44.78
STEP: getting /apis/node.k8s.io 05/17/23 07:42:44.781
STEP: getting /apis/node.k8s.io/v1 05/17/23 07:42:44.782
STEP: creating 05/17/23 07:42:44.782
STEP: watching 05/17/23 07:42:44.79
May 17 07:42:44.790: INFO: starting watch
STEP: getting 05/17/23 07:42:44.792
STEP: listing 05/17/23 07:42:44.794
STEP: patching 05/17/23 07:42:44.795
STEP: updating 05/17/23 07:42:44.797
May 17 07:42:44.800: INFO: waiting for watch events with expected annotations
STEP: deleting 05/17/23 07:42:44.8
STEP: deleting a collection 05/17/23 07:42:44.804
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 17 07:42:44.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5927" for this suite. 05/17/23 07:42:44.811
------------------------------
â€¢ [0.052 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:44.762
    May 17 07:42:44.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:42:44.763
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:44.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:44.779
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/17/23 07:42:44.78
    STEP: getting /apis/node.k8s.io 05/17/23 07:42:44.781
    STEP: getting /apis/node.k8s.io/v1 05/17/23 07:42:44.782
    STEP: creating 05/17/23 07:42:44.782
    STEP: watching 05/17/23 07:42:44.79
    May 17 07:42:44.790: INFO: starting watch
    STEP: getting 05/17/23 07:42:44.792
    STEP: listing 05/17/23 07:42:44.794
    STEP: patching 05/17/23 07:42:44.795
    STEP: updating 05/17/23 07:42:44.797
    May 17 07:42:44.800: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/17/23 07:42:44.8
    STEP: deleting a collection 05/17/23 07:42:44.804
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:44.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5927" for this suite. 05/17/23 07:42:44.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:44.815
May 17 07:42:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename endpointslice 05/17/23 07:42:44.815
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:44.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:44.823
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 17 07:42:46.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7784" for this suite. 05/17/23 07:42:46.847
------------------------------
â€¢ [2.035 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:44.815
    May 17 07:42:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename endpointslice 05/17/23 07:42:44.815
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:44.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:44.823
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:46.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7784" for this suite. 05/17/23 07:42:46.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:46.85
May 17 07:42:46.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 07:42:46.85
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:46.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:46.858
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-f3bee62f-2799-49ce-a265-0a1651488e67 05/17/23 07:42:46.859
STEP: Creating a pod to test consume secrets 05/17/23 07:42:46.861
May 17 07:42:46.865: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80" in namespace "projected-4747" to be "Succeeded or Failed"
May 17 07:42:46.867: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80": Phase="Pending", Reason="", readiness=false. Elapsed: 1.133924ms
May 17 07:42:48.869: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003264344s
May 17 07:42:50.870: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004154294s
STEP: Saw pod success 05/17/23 07:42:50.87
May 17 07:42:50.870: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80" satisfied condition "Succeeded or Failed"
May 17 07:42:50.871: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 07:42:50.874
May 17 07:42:50.881: INFO: Waiting for pod pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80 to disappear
May 17 07:42:50.882: INFO: Pod pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 17 07:42:50.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4747" for this suite. 05/17/23 07:42:50.883
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:46.85
    May 17 07:42:46.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 07:42:46.85
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:46.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:46.858
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-f3bee62f-2799-49ce-a265-0a1651488e67 05/17/23 07:42:46.859
    STEP: Creating a pod to test consume secrets 05/17/23 07:42:46.861
    May 17 07:42:46.865: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80" in namespace "projected-4747" to be "Succeeded or Failed"
    May 17 07:42:46.867: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80": Phase="Pending", Reason="", readiness=false. Elapsed: 1.133924ms
    May 17 07:42:48.869: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003264344s
    May 17 07:42:50.870: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004154294s
    STEP: Saw pod success 05/17/23 07:42:50.87
    May 17 07:42:50.870: INFO: Pod "pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80" satisfied condition "Succeeded or Failed"
    May 17 07:42:50.871: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 07:42:50.874
    May 17 07:42:50.881: INFO: Waiting for pod pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80 to disappear
    May 17 07:42:50.882: INFO: Pod pod-projected-secrets-c7cd16d6-6dae-4d31-80f6-cdd42d46cb80 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 17 07:42:50.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4747" for this suite. 05/17/23 07:42:50.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:42:50.886
May 17 07:42:50.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 07:42:50.887
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:50.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:50.893
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1683 05/17/23 07:42:50.895
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 05/17/23 07:42:50.897
May 17 07:42:50.901: INFO: Found 0 stateful pods, waiting for 3
May 17 07:43:00.905: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:43:00.905: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:43:00.905: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:43:00.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 07:43:01.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 07:43:01.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 07:43:01.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/17/23 07:43:11.012
May 17 07:43:11.028: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/17/23 07:43:11.028
STEP: Updating Pods in reverse ordinal order 05/17/23 07:43:21.037
May 17 07:43:21.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 07:43:21.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 07:43:21.135: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 07:43:21.135: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 05/17/23 07:43:31.146
May 17 07:43:31.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 07:43:31.240: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 07:43:31.240: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 07:43:31.240: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 07:43:41.263: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/17/23 07:43:51.274
May 17 07:43:51.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 07:43:51.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 07:43:51.365: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 07:43:51.365: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 07:44:01.379: INFO: Deleting all statefulset in ns statefulset-1683
May 17 07:44:01.380: INFO: Scaling statefulset ss2 to 0
May 17 07:44:11.391: INFO: Waiting for statefulset status.replicas updated to 0
May 17 07:44:11.393: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 07:44:11.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1683" for this suite. 05/17/23 07:44:11.399
------------------------------
â€¢ [SLOW TEST] [80.515 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:42:50.886
    May 17 07:42:50.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 07:42:50.887
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:42:50.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:42:50.893
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1683 05/17/23 07:42:50.895
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 05/17/23 07:42:50.897
    May 17 07:42:50.901: INFO: Found 0 stateful pods, waiting for 3
    May 17 07:43:00.905: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:43:00.905: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:43:00.905: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:43:00.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 07:43:01.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 07:43:01.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 07:43:01.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/17/23 07:43:11.012
    May 17 07:43:11.028: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/17/23 07:43:11.028
    STEP: Updating Pods in reverse ordinal order 05/17/23 07:43:21.037
    May 17 07:43:21.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 07:43:21.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 07:43:21.135: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 07:43:21.135: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 05/17/23 07:43:31.146
    May 17 07:43:31.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 07:43:31.240: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 07:43:31.240: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 07:43:31.240: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 07:43:41.263: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/17/23 07:43:51.274
    May 17 07:43:51.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-1683 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 07:43:51.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 07:43:51.365: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 07:43:51.365: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 07:44:01.379: INFO: Deleting all statefulset in ns statefulset-1683
    May 17 07:44:01.380: INFO: Scaling statefulset ss2 to 0
    May 17 07:44:11.391: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 07:44:11.393: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:11.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1683" for this suite. 05/17/23 07:44:11.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:11.402
May 17 07:44:11.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename job 05/17/23 07:44:11.403
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:11.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:11.409
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 05/17/23 07:44:11.41
STEP: Ensuring job reaches completions 05/17/23 07:44:11.413
STEP: Ensuring pods with index for job exist 05/17/23 07:44:19.416
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 17 07:44:19.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4848" for this suite. 05/17/23 07:44:19.42
------------------------------
â€¢ [SLOW TEST] [8.020 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:11.402
    May 17 07:44:11.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename job 05/17/23 07:44:11.403
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:11.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:11.409
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 05/17/23 07:44:11.41
    STEP: Ensuring job reaches completions 05/17/23 07:44:11.413
    STEP: Ensuring pods with index for job exist 05/17/23 07:44:19.416
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:19.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4848" for this suite. 05/17/23 07:44:19.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:19.424
May 17 07:44:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 07:44:19.424
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:19.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:19.432
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-bd900343-cbdd-45f5-bcc6-be687f30e447 05/17/23 07:44:19.434
STEP: Creating a pod to test consume configMaps 05/17/23 07:44:19.436
May 17 07:44:19.439: INFO: Waiting up to 5m0s for pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc" in namespace "configmap-9990" to be "Succeeded or Failed"
May 17 07:44:19.441: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.203179ms
May 17 07:44:21.442: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003061906s
May 17 07:44:23.443: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003550446s
STEP: Saw pod success 05/17/23 07:44:23.443
May 17 07:44:23.443: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc" satisfied condition "Succeeded or Failed"
May 17 07:44:23.444: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:44:23.454
May 17 07:44:23.461: INFO: Waiting for pod pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc to disappear
May 17 07:44:23.462: INFO: Pod pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 07:44:23.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9990" for this suite. 05/17/23 07:44:23.464
------------------------------
â€¢ [4.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:19.424
    May 17 07:44:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 07:44:19.424
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:19.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:19.432
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-bd900343-cbdd-45f5-bcc6-be687f30e447 05/17/23 07:44:19.434
    STEP: Creating a pod to test consume configMaps 05/17/23 07:44:19.436
    May 17 07:44:19.439: INFO: Waiting up to 5m0s for pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc" in namespace "configmap-9990" to be "Succeeded or Failed"
    May 17 07:44:19.441: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.203179ms
    May 17 07:44:21.442: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003061906s
    May 17 07:44:23.443: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003550446s
    STEP: Saw pod success 05/17/23 07:44:23.443
    May 17 07:44:23.443: INFO: Pod "pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc" satisfied condition "Succeeded or Failed"
    May 17 07:44:23.444: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:44:23.454
    May 17 07:44:23.461: INFO: Waiting for pod pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc to disappear
    May 17 07:44:23.462: INFO: Pod pod-configmaps-c613a754-511a-4bc9-8c8d-65c2e4a9ebcc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:23.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9990" for this suite. 05/17/23 07:44:23.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:23.467
May 17 07:44:23.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:44:23.468
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:23.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:23.477
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
May 17 07:44:23.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/17/23 07:44:24.799
May 17 07:44:24.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
May 17 07:44:25.222: INFO: stderr: ""
May 17 07:44:25.222: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 17 07:44:25.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 delete e2e-test-crd-publish-openapi-9924-crds test-foo'
May 17 07:44:25.273: INFO: stderr: ""
May 17 07:44:25.273: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 17 07:44:25.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 apply -f -'
May 17 07:44:25.419: INFO: stderr: ""
May 17 07:44:25.420: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 17 07:44:25.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 delete e2e-test-crd-publish-openapi-9924-crds test-foo'
May 17 07:44:25.470: INFO: stderr: ""
May 17 07:44:25.470: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/17/23 07:44:25.47
May 17 07:44:25.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
May 17 07:44:25.610: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/17/23 07:44:25.61
May 17 07:44:25.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
May 17 07:44:25.759: INFO: rc: 1
May 17 07:44:25.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 apply -f -'
May 17 07:44:25.905: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/17/23 07:44:25.905
May 17 07:44:25.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
May 17 07:44:26.055: INFO: rc: 1
May 17 07:44:26.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 apply -f -'
May 17 07:44:26.207: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/17/23 07:44:26.207
May 17 07:44:26.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds'
May 17 07:44:26.348: INFO: stderr: ""
May 17 07:44:26.348: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/17/23 07:44:26.348
May 17 07:44:26.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.metadata'
May 17 07:44:26.488: INFO: stderr: ""
May 17 07:44:26.488: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 17 07:44:26.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.spec'
May 17 07:44:26.625: INFO: stderr: ""
May 17 07:44:26.625: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 17 07:44:26.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.spec.bars'
May 17 07:44:26.770: INFO: stderr: ""
May 17 07:44:26.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/17/23 07:44:26.77
May 17 07:44:26.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.spec.bars2'
May 17 07:44:26.910: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:44:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2858" for this suite. 05/17/23 07:44:28.235
------------------------------
â€¢ [4.771 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:23.467
    May 17 07:44:23.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:44:23.468
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:23.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:23.477
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    May 17 07:44:23.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/17/23 07:44:24.799
    May 17 07:44:24.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
    May 17 07:44:25.222: INFO: stderr: ""
    May 17 07:44:25.222: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May 17 07:44:25.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 delete e2e-test-crd-publish-openapi-9924-crds test-foo'
    May 17 07:44:25.273: INFO: stderr: ""
    May 17 07:44:25.273: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May 17 07:44:25.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 apply -f -'
    May 17 07:44:25.419: INFO: stderr: ""
    May 17 07:44:25.420: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May 17 07:44:25.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 delete e2e-test-crd-publish-openapi-9924-crds test-foo'
    May 17 07:44:25.470: INFO: stderr: ""
    May 17 07:44:25.470: INFO: stdout: "e2e-test-crd-publish-openapi-9924-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/17/23 07:44:25.47
    May 17 07:44:25.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
    May 17 07:44:25.610: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/17/23 07:44:25.61
    May 17 07:44:25.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
    May 17 07:44:25.759: INFO: rc: 1
    May 17 07:44:25.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 apply -f -'
    May 17 07:44:25.905: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/17/23 07:44:25.905
    May 17 07:44:25.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 create -f -'
    May 17 07:44:26.055: INFO: rc: 1
    May 17 07:44:26.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 --namespace=crd-publish-openapi-2858 apply -f -'
    May 17 07:44:26.207: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/17/23 07:44:26.207
    May 17 07:44:26.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds'
    May 17 07:44:26.348: INFO: stderr: ""
    May 17 07:44:26.348: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/17/23 07:44:26.348
    May 17 07:44:26.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.metadata'
    May 17 07:44:26.488: INFO: stderr: ""
    May 17 07:44:26.488: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May 17 07:44:26.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.spec'
    May 17 07:44:26.625: INFO: stderr: ""
    May 17 07:44:26.625: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May 17 07:44:26.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.spec.bars'
    May 17 07:44:26.770: INFO: stderr: ""
    May 17 07:44:26.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9924-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/17/23 07:44:26.77
    May 17 07:44:26.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-2858 explain e2e-test-crd-publish-openapi-9924-crds.spec.bars2'
    May 17 07:44:26.910: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2858" for this suite. 05/17/23 07:44:28.235
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:28.239
May 17 07:44:28.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:44:28.239
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:28.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:28.246
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May 17 07:44:28.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:44:28.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3950" for this suite. 05/17/23 07:44:28.77
------------------------------
â€¢ [0.534 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:28.239
    May 17 07:44:28.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:44:28.239
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:28.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:28.246
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May 17 07:44:28.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:28.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3950" for this suite. 05/17/23 07:44:28.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:28.773
May 17 07:44:28.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 07:44:28.773
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:28.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:28.781
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 07:44:28.787
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:44:29.221
STEP: Deploying the webhook pod 05/17/23 07:44:29.225
STEP: Wait for the deployment to be ready 05/17/23 07:44:29.232
May 17 07:44:29.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 07:44:31.24
STEP: Verifying the service has paired with the endpoint 05/17/23 07:44:31.245
May 17 07:44:32.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/17/23 07:44:32.248
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:32.248
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/17/23 07:44:32.258
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/17/23 07:44:33.262
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:33.262
STEP: Having no error when timeout is longer than webhook latency 05/17/23 07:44:34.275
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:34.276
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/17/23 07:44:39.294
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:39.294
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:44:44.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2689" for this suite. 05/17/23 07:44:44.324
STEP: Destroying namespace "webhook-2689-markers" for this suite. 05/17/23 07:44:44.327
------------------------------
â€¢ [SLOW TEST] [15.557 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:28.773
    May 17 07:44:28.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 07:44:28.773
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:28.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:28.781
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 07:44:28.787
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:44:29.221
    STEP: Deploying the webhook pod 05/17/23 07:44:29.225
    STEP: Wait for the deployment to be ready 05/17/23 07:44:29.232
    May 17 07:44:29.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 07:44:31.24
    STEP: Verifying the service has paired with the endpoint 05/17/23 07:44:31.245
    May 17 07:44:32.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/17/23 07:44:32.248
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:32.248
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/17/23 07:44:32.258
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/17/23 07:44:33.262
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:33.262
    STEP: Having no error when timeout is longer than webhook latency 05/17/23 07:44:34.275
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:34.276
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/17/23 07:44:39.294
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 07:44:39.294
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:44.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2689" for this suite. 05/17/23 07:44:44.324
    STEP: Destroying namespace "webhook-2689-markers" for this suite. 05/17/23 07:44:44.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:44.33
May 17 07:44:44.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 07:44:44.331
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:44.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:44.338
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 07:44:44.345
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:44:44.687
STEP: Deploying the webhook pod 05/17/23 07:44:44.69
STEP: Wait for the deployment to be ready 05/17/23 07:44:44.696
May 17 07:44:44.700: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 07:44:46.706
STEP: Verifying the service has paired with the endpoint 05/17/23 07:44:46.712
May 17 07:44:47.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 05/17/23 07:44:47.714
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/17/23 07:44:47.724
STEP: Creating a configMap that should not be mutated 05/17/23 07:44:47.728
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/17/23 07:44:47.733
STEP: Creating a configMap that should be mutated 05/17/23 07:44:47.737
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:44:47.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7351" for this suite. 05/17/23 07:44:47.765
STEP: Destroying namespace "webhook-7351-markers" for this suite. 05/17/23 07:44:47.769
------------------------------
â€¢ [3.441 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:44.33
    May 17 07:44:44.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 07:44:44.331
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:44.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:44.338
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 07:44:44.345
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:44:44.687
    STEP: Deploying the webhook pod 05/17/23 07:44:44.69
    STEP: Wait for the deployment to be ready 05/17/23 07:44:44.696
    May 17 07:44:44.700: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 07:44:46.706
    STEP: Verifying the service has paired with the endpoint 05/17/23 07:44:46.712
    May 17 07:44:47.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 05/17/23 07:44:47.714
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/17/23 07:44:47.724
    STEP: Creating a configMap that should not be mutated 05/17/23 07:44:47.728
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/17/23 07:44:47.733
    STEP: Creating a configMap that should be mutated 05/17/23 07:44:47.737
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:47.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7351" for this suite. 05/17/23 07:44:47.765
    STEP: Destroying namespace "webhook-7351-markers" for this suite. 05/17/23 07:44:47.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:47.772
May 17 07:44:47.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 07:44:47.772
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:47.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:47.78
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:44:47.781
May 17 07:44:47.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461" in namespace "projected-1115" to be "Succeeded or Failed"
May 17 07:44:47.786: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461": Phase="Pending", Reason="", readiness=false. Elapsed: 1.15834ms
May 17 07:44:49.789: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004103465s
May 17 07:44:51.790: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00477238s
STEP: Saw pod success 05/17/23 07:44:51.79
May 17 07:44:51.790: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461" satisfied condition "Succeeded or Failed"
May 17 07:44:51.791: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461 container client-container: <nil>
STEP: delete the pod 05/17/23 07:44:51.794
May 17 07:44:51.799: INFO: Waiting for pod downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461 to disappear
May 17 07:44:51.801: INFO: Pod downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 07:44:51.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1115" for this suite. 05/17/23 07:44:51.802
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:47.772
    May 17 07:44:47.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 07:44:47.772
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:47.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:47.78
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:44:47.781
    May 17 07:44:47.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461" in namespace "projected-1115" to be "Succeeded or Failed"
    May 17 07:44:47.786: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461": Phase="Pending", Reason="", readiness=false. Elapsed: 1.15834ms
    May 17 07:44:49.789: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004103465s
    May 17 07:44:51.790: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00477238s
    STEP: Saw pod success 05/17/23 07:44:51.79
    May 17 07:44:51.790: INFO: Pod "downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461" satisfied condition "Succeeded or Failed"
    May 17 07:44:51.791: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461 container client-container: <nil>
    STEP: delete the pod 05/17/23 07:44:51.794
    May 17 07:44:51.799: INFO: Waiting for pod downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461 to disappear
    May 17 07:44:51.801: INFO: Pod downwardapi-volume-1cb882db-8b6d-49e6-803d-40839d57e461 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 07:44:51.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1115" for this suite. 05/17/23 07:44:51.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:44:51.806
May 17 07:44:51.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 07:44:51.806
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:51.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:51.813
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/17/23 07:44:51.814
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
 05/17/23 07:44:51.817
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
 05/17/23 07:44:51.817
STEP: creating a pod to probe DNS 05/17/23 07:44:51.817
STEP: submitting the pod to kubernetes 05/17/23 07:44:51.817
May 17 07:44:51.821: INFO: Waiting up to 15m0s for pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729" in namespace "dns-3541" to be "running"
May 17 07:44:51.823: INFO: Pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288704ms
May 17 07:44:53.826: INFO: Pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729": Phase="Running", Reason="", readiness=true. Elapsed: 2.004779309s
May 17 07:44:53.826: INFO: Pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729" satisfied condition "running"
STEP: retrieving the pod 05/17/23 07:44:53.826
STEP: looking for the results for each expected name from probers 05/17/23 07:44:53.828
May 17 07:44:53.832: INFO: DNS probes using dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729 succeeded

STEP: deleting the pod 05/17/23 07:44:53.832
STEP: changing the externalName to bar.example.com 05/17/23 07:44:53.838
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
 05/17/23 07:44:53.841
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
 05/17/23 07:44:53.841
STEP: creating a second pod to probe DNS 05/17/23 07:44:53.842
STEP: submitting the pod to kubernetes 05/17/23 07:44:53.842
May 17 07:44:53.845: INFO: Waiting up to 15m0s for pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb" in namespace "dns-3541" to be "running"
May 17 07:44:53.846: INFO: Pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.221326ms
May 17 07:44:55.849: INFO: Pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004395675s
May 17 07:44:55.849: INFO: Pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb" satisfied condition "running"
STEP: retrieving the pod 05/17/23 07:44:55.849
STEP: looking for the results for each expected name from probers 05/17/23 07:44:55.851
May 17 07:44:55.853: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:44:55.855: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:44:55.855: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

May 17 07:45:00.857: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:00.859: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:00.859: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

May 17 07:45:05.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:05.862: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:05.862: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

May 17 07:45:10.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:10.861: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:10.861: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

May 17 07:45:15.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:15.862: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:15.862: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

May 17 07:45:20.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:20.862: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 07:45:20.862: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

May 17 07:45:25.859: INFO: DNS probes using dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb succeeded

STEP: deleting the pod 05/17/23 07:45:25.859
STEP: changing the service to type=ClusterIP 05/17/23 07:45:25.865
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
 05/17/23 07:45:25.876
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
 05/17/23 07:45:25.876
STEP: creating a third pod to probe DNS 05/17/23 07:45:25.876
STEP: submitting the pod to kubernetes 05/17/23 07:45:25.877
May 17 07:45:25.881: INFO: Waiting up to 15m0s for pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7" in namespace "dns-3541" to be "running"
May 17 07:45:25.882: INFO: Pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1885ms
May 17 07:45:27.885: INFO: Pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7": Phase="Running", Reason="", readiness=true. Elapsed: 2.003475045s
May 17 07:45:27.885: INFO: Pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7" satisfied condition "running"
STEP: retrieving the pod 05/17/23 07:45:27.885
STEP: looking for the results for each expected name from probers 05/17/23 07:45:27.886
May 17 07:45:27.890: INFO: DNS probes using dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7 succeeded

STEP: deleting the pod 05/17/23 07:45:27.89
STEP: deleting the test externalName service 05/17/23 07:45:27.897
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 07:45:27.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3541" for this suite. 05/17/23 07:45:27.905
------------------------------
â€¢ [SLOW TEST] [36.102 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:44:51.806
    May 17 07:44:51.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 07:44:51.806
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:44:51.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:44:51.813
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/17/23 07:44:51.814
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
     05/17/23 07:44:51.817
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
     05/17/23 07:44:51.817
    STEP: creating a pod to probe DNS 05/17/23 07:44:51.817
    STEP: submitting the pod to kubernetes 05/17/23 07:44:51.817
    May 17 07:44:51.821: INFO: Waiting up to 15m0s for pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729" in namespace "dns-3541" to be "running"
    May 17 07:44:51.823: INFO: Pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288704ms
    May 17 07:44:53.826: INFO: Pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729": Phase="Running", Reason="", readiness=true. Elapsed: 2.004779309s
    May 17 07:44:53.826: INFO: Pod "dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 07:44:53.826
    STEP: looking for the results for each expected name from probers 05/17/23 07:44:53.828
    May 17 07:44:53.832: INFO: DNS probes using dns-test-335aaa6d-607a-4d0a-b4ec-3343232c1729 succeeded

    STEP: deleting the pod 05/17/23 07:44:53.832
    STEP: changing the externalName to bar.example.com 05/17/23 07:44:53.838
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
     05/17/23 07:44:53.841
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
     05/17/23 07:44:53.841
    STEP: creating a second pod to probe DNS 05/17/23 07:44:53.842
    STEP: submitting the pod to kubernetes 05/17/23 07:44:53.842
    May 17 07:44:53.845: INFO: Waiting up to 15m0s for pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb" in namespace "dns-3541" to be "running"
    May 17 07:44:53.846: INFO: Pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.221326ms
    May 17 07:44:55.849: INFO: Pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004395675s
    May 17 07:44:55.849: INFO: Pod "dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 07:44:55.849
    STEP: looking for the results for each expected name from probers 05/17/23 07:44:55.851
    May 17 07:44:55.853: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:44:55.855: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:44:55.855: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

    May 17 07:45:00.857: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:00.859: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:00.859: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

    May 17 07:45:05.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:05.862: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:05.862: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

    May 17 07:45:10.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:10.861: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:10.861: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

    May 17 07:45:15.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:15.862: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:15.862: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

    May 17 07:45:20.860: INFO: File wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:20.862: INFO: File jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local from pod  dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 07:45:20.862: INFO: Lookups using dns-3541/dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb failed for: [wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local]

    May 17 07:45:25.859: INFO: DNS probes using dns-test-7034ad83-48fb-422a-bc8d-b9a8e6b6c9fb succeeded

    STEP: deleting the pod 05/17/23 07:45:25.859
    STEP: changing the service to type=ClusterIP 05/17/23 07:45:25.865
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
     05/17/23 07:45:25.876
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3541.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3541.svc.cluster.local; sleep 1; done
     05/17/23 07:45:25.876
    STEP: creating a third pod to probe DNS 05/17/23 07:45:25.876
    STEP: submitting the pod to kubernetes 05/17/23 07:45:25.877
    May 17 07:45:25.881: INFO: Waiting up to 15m0s for pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7" in namespace "dns-3541" to be "running"
    May 17 07:45:25.882: INFO: Pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1885ms
    May 17 07:45:27.885: INFO: Pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7": Phase="Running", Reason="", readiness=true. Elapsed: 2.003475045s
    May 17 07:45:27.885: INFO: Pod "dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 07:45:27.885
    STEP: looking for the results for each expected name from probers 05/17/23 07:45:27.886
    May 17 07:45:27.890: INFO: DNS probes using dns-test-dfac7301-86d2-4a8d-b644-c077af2bcff7 succeeded

    STEP: deleting the pod 05/17/23 07:45:27.89
    STEP: deleting the test externalName service 05/17/23 07:45:27.897
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 07:45:27.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3541" for this suite. 05/17/23 07:45:27.905
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:45:27.908
May 17 07:45:27.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replication-controller 05/17/23 07:45:27.909
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:27.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:27.917
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 05/17/23 07:45:27.919
May 17 07:45:27.922: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7625" to be "running and ready"
May 17 07:45:27.924: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.207125ms
May 17 07:45:27.924: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 17 07:45:29.926: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.003433239s
May 17 07:45:29.926: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May 17 07:45:29.926: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/17/23 07:45:29.927
STEP: Then the orphan pod is adopted 05/17/23 07:45:29.93
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 17 07:45:30.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7625" for this suite. 05/17/23 07:45:30.936
------------------------------
â€¢ [3.032 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:45:27.908
    May 17 07:45:27.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replication-controller 05/17/23 07:45:27.909
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:27.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:27.917
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/17/23 07:45:27.919
    May 17 07:45:27.922: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7625" to be "running and ready"
    May 17 07:45:27.924: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.207125ms
    May 17 07:45:27.924: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:45:29.926: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.003433239s
    May 17 07:45:29.926: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May 17 07:45:29.926: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/17/23 07:45:29.927
    STEP: Then the orphan pod is adopted 05/17/23 07:45:29.93
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 17 07:45:30.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7625" for this suite. 05/17/23 07:45:30.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:45:30.94
May 17 07:45:30.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename namespaces 05/17/23 07:45:30.941
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:30.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:30.948
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-923" 05/17/23 07:45:30.949
May 17 07:45:30.953: INFO: Namespace "namespaces-923" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ae2668ca-7e49-4bda-8445-8d9b7c33c609", "kubernetes.io/metadata.name":"namespaces-923", "namespaces-923":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:45:30.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-923" for this suite. 05/17/23 07:45:30.954
------------------------------
â€¢ [0.018 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:45:30.94
    May 17 07:45:30.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename namespaces 05/17/23 07:45:30.941
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:30.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:30.948
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-923" 05/17/23 07:45:30.949
    May 17 07:45:30.953: INFO: Namespace "namespaces-923" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ae2668ca-7e49-4bda-8445-8d9b7c33c609", "kubernetes.io/metadata.name":"namespaces-923", "namespaces-923":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:45:30.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-923" for this suite. 05/17/23 07:45:30.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:45:30.958
May 17 07:45:30.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 07:45:30.959
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:30.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:30.965
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:45:30.967
May 17 07:45:30.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad" in namespace "downward-api-8469" to be "Succeeded or Failed"
May 17 07:45:30.973: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.158593ms
May 17 07:45:32.975: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003337595s
May 17 07:45:34.975: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003621687s
STEP: Saw pod success 05/17/23 07:45:34.975
May 17 07:45:34.975: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad" satisfied condition "Succeeded or Failed"
May 17 07:45:34.977: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad container client-container: <nil>
STEP: delete the pod 05/17/23 07:45:34.98
May 17 07:45:34.989: INFO: Waiting for pod downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad to disappear
May 17 07:45:34.990: INFO: Pod downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 07:45:34.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8469" for this suite. 05/17/23 07:45:34.992
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:45:30.958
    May 17 07:45:30.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:45:30.959
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:30.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:30.965
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:45:30.967
    May 17 07:45:30.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad" in namespace "downward-api-8469" to be "Succeeded or Failed"
    May 17 07:45:30.973: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.158593ms
    May 17 07:45:32.975: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003337595s
    May 17 07:45:34.975: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003621687s
    STEP: Saw pod success 05/17/23 07:45:34.975
    May 17 07:45:34.975: INFO: Pod "downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad" satisfied condition "Succeeded or Failed"
    May 17 07:45:34.977: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad container client-container: <nil>
    STEP: delete the pod 05/17/23 07:45:34.98
    May 17 07:45:34.989: INFO: Waiting for pod downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad to disappear
    May 17 07:45:34.990: INFO: Pod downwardapi-volume-d4eeefd9-3cf8-4fa3-a7f3-9f44051b4aad no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 07:45:34.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8469" for this suite. 05/17/23 07:45:34.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:45:34.995
May 17 07:45:34.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 07:45:34.996
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:35.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:35.003
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-6f27751e-ebd4-4bf6-95e2-c7fabdf94573 05/17/23 07:45:35.005
STEP: Creating a pod to test consume configMaps 05/17/23 07:45:35.008
May 17 07:45:35.012: INFO: Waiting up to 5m0s for pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3" in namespace "configmap-9223" to be "Succeeded or Failed"
May 17 07:45:35.013: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268727ms
May 17 07:45:37.016: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004649s
May 17 07:45:39.015: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003863522s
STEP: Saw pod success 05/17/23 07:45:39.015
May 17 07:45:39.016: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3" satisfied condition "Succeeded or Failed"
May 17 07:45:39.017: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:45:39.02
May 17 07:45:39.026: INFO: Waiting for pod pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3 to disappear
May 17 07:45:39.027: INFO: Pod pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 07:45:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9223" for this suite. 05/17/23 07:45:39.029
------------------------------
â€¢ [4.036 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:45:34.995
    May 17 07:45:34.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 07:45:34.996
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:35.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:35.003
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-6f27751e-ebd4-4bf6-95e2-c7fabdf94573 05/17/23 07:45:35.005
    STEP: Creating a pod to test consume configMaps 05/17/23 07:45:35.008
    May 17 07:45:35.012: INFO: Waiting up to 5m0s for pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3" in namespace "configmap-9223" to be "Succeeded or Failed"
    May 17 07:45:35.013: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268727ms
    May 17 07:45:37.016: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004649s
    May 17 07:45:39.015: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003863522s
    STEP: Saw pod success 05/17/23 07:45:39.015
    May 17 07:45:39.016: INFO: Pod "pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3" satisfied condition "Succeeded or Failed"
    May 17 07:45:39.017: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:45:39.02
    May 17 07:45:39.026: INFO: Waiting for pod pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3 to disappear
    May 17 07:45:39.027: INFO: Pod pod-configmaps-15e2a7c2-4d66-48c1-89b0-53ad476b00a3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 07:45:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9223" for this suite. 05/17/23 07:45:39.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:45:39.032
May 17 07:45:39.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-preemption 05/17/23 07:45:39.032
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:39.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:39.039
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 17 07:45:39.046: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 07:46:39.059: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:46:39.061
May 17 07:46:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 07:46:39.062
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:46:39.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:46:39.069
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
May 17 07:46:39.076: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May 17 07:46:39.077: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
May 17 07:46:39.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:46:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2736" for this suite. 05/17/23 07:46:39.107
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4029" for this suite. 05/17/23 07:46:39.11
------------------------------
â€¢ [SLOW TEST] [60.081 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:45:39.032
    May 17 07:45:39.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 07:45:39.032
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:45:39.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:45:39.039
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 17 07:45:39.046: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 07:46:39.059: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:46:39.061
    May 17 07:46:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 07:46:39.062
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:46:39.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:46:39.069
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    May 17 07:46:39.076: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May 17 07:46:39.077: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    May 17 07:46:39.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:46:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2736" for this suite. 05/17/23 07:46:39.107
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4029" for this suite. 05/17/23 07:46:39.11
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:46:39.112
May 17 07:46:39.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename cronjob 05/17/23 07:46:39.113
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:46:39.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:46:39.119
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/17/23 07:46:39.12
STEP: Ensuring a job is scheduled 05/17/23 07:46:39.123
STEP: Ensuring exactly one is scheduled 05/17/23 07:47:01.126
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 07:47:01.127
STEP: Ensuring no more jobs are scheduled 05/17/23 07:47:01.129
STEP: Removing cronjob 05/17/23 07:52:01.133
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 17 07:52:01.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1477" for this suite. 05/17/23 07:52:01.137
------------------------------
â€¢ [SLOW TEST] [322.028 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:46:39.112
    May 17 07:46:39.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename cronjob 05/17/23 07:46:39.113
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:46:39.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:46:39.119
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/17/23 07:46:39.12
    STEP: Ensuring a job is scheduled 05/17/23 07:46:39.123
    STEP: Ensuring exactly one is scheduled 05/17/23 07:47:01.126
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 07:47:01.127
    STEP: Ensuring no more jobs are scheduled 05/17/23 07:47:01.129
    STEP: Removing cronjob 05/17/23 07:52:01.133
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:01.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1477" for this suite. 05/17/23 07:52:01.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:01.141
May 17 07:52:01.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 07:52:01.141
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:01.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:01.151
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:52:01.153
May 17 07:52:01.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9" in namespace "downward-api-3387" to be "Succeeded or Failed"
May 17 07:52:01.158: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.249818ms
May 17 07:52:03.160: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003624068s
May 17 07:52:05.161: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004602956s
STEP: Saw pod success 05/17/23 07:52:05.161
May 17 07:52:05.161: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9" satisfied condition "Succeeded or Failed"
May 17 07:52:05.163: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9 container client-container: <nil>
STEP: delete the pod 05/17/23 07:52:05.172
May 17 07:52:05.177: INFO: Waiting for pod downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9 to disappear
May 17 07:52:05.178: INFO: Pod downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 07:52:05.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3387" for this suite. 05/17/23 07:52:05.18
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:01.141
    May 17 07:52:01.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:52:01.141
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:01.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:01.151
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:52:01.153
    May 17 07:52:01.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9" in namespace "downward-api-3387" to be "Succeeded or Failed"
    May 17 07:52:01.158: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.249818ms
    May 17 07:52:03.160: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003624068s
    May 17 07:52:05.161: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004602956s
    STEP: Saw pod success 05/17/23 07:52:05.161
    May 17 07:52:05.161: INFO: Pod "downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9" satisfied condition "Succeeded or Failed"
    May 17 07:52:05.163: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9 container client-container: <nil>
    STEP: delete the pod 05/17/23 07:52:05.172
    May 17 07:52:05.177: INFO: Waiting for pod downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9 to disappear
    May 17 07:52:05.178: INFO: Pod downwardapi-volume-79fcf640-2020-4183-9b26-0c4dbcdb54b9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:05.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3387" for this suite. 05/17/23 07:52:05.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:05.184
May 17 07:52:05.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename subpath 05/17/23 07:52:05.185
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:05.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:05.193
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 07:52:05.194
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-sm4s 05/17/23 07:52:05.198
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 07:52:05.198
May 17 07:52:05.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-sm4s" in namespace "subpath-9725" to be "Succeeded or Failed"
May 17 07:52:05.204: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Pending", Reason="", readiness=false. Elapsed: 1.270993ms
May 17 07:52:07.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.004405545s
May 17 07:52:09.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 4.004519749s
May 17 07:52:11.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 6.003676646s
May 17 07:52:13.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 8.004103252s
May 17 07:52:15.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 10.005125582s
May 17 07:52:17.208: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.005585152s
May 17 07:52:19.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 14.005128241s
May 17 07:52:21.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 16.005052498s
May 17 07:52:23.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 18.003795081s
May 17 07:52:25.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 20.004124296s
May 17 07:52:27.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=false. Elapsed: 22.004888057s
May 17 07:52:29.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005020429s
STEP: Saw pod success 05/17/23 07:52:29.207
May 17 07:52:29.208: INFO: Pod "pod-subpath-test-projected-sm4s" satisfied condition "Succeeded or Failed"
May 17 07:52:29.209: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-projected-sm4s container test-container-subpath-projected-sm4s: <nil>
STEP: delete the pod 05/17/23 07:52:29.212
May 17 07:52:29.218: INFO: Waiting for pod pod-subpath-test-projected-sm4s to disappear
May 17 07:52:29.220: INFO: Pod pod-subpath-test-projected-sm4s no longer exists
STEP: Deleting pod pod-subpath-test-projected-sm4s 05/17/23 07:52:29.22
May 17 07:52:29.220: INFO: Deleting pod "pod-subpath-test-projected-sm4s" in namespace "subpath-9725"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 17 07:52:29.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9725" for this suite. 05/17/23 07:52:29.223
------------------------------
â€¢ [SLOW TEST] [24.041 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:05.184
    May 17 07:52:05.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename subpath 05/17/23 07:52:05.185
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:05.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:05.193
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 07:52:05.194
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-sm4s 05/17/23 07:52:05.198
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 07:52:05.198
    May 17 07:52:05.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-sm4s" in namespace "subpath-9725" to be "Succeeded or Failed"
    May 17 07:52:05.204: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Pending", Reason="", readiness=false. Elapsed: 1.270993ms
    May 17 07:52:07.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.004405545s
    May 17 07:52:09.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 4.004519749s
    May 17 07:52:11.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 6.003676646s
    May 17 07:52:13.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 8.004103252s
    May 17 07:52:15.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 10.005125582s
    May 17 07:52:17.208: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.005585152s
    May 17 07:52:19.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 14.005128241s
    May 17 07:52:21.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 16.005052498s
    May 17 07:52:23.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 18.003795081s
    May 17 07:52:25.206: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=true. Elapsed: 20.004124296s
    May 17 07:52:27.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Running", Reason="", readiness=false. Elapsed: 22.004888057s
    May 17 07:52:29.207: INFO: Pod "pod-subpath-test-projected-sm4s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005020429s
    STEP: Saw pod success 05/17/23 07:52:29.207
    May 17 07:52:29.208: INFO: Pod "pod-subpath-test-projected-sm4s" satisfied condition "Succeeded or Failed"
    May 17 07:52:29.209: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-projected-sm4s container test-container-subpath-projected-sm4s: <nil>
    STEP: delete the pod 05/17/23 07:52:29.212
    May 17 07:52:29.218: INFO: Waiting for pod pod-subpath-test-projected-sm4s to disappear
    May 17 07:52:29.220: INFO: Pod pod-subpath-test-projected-sm4s no longer exists
    STEP: Deleting pod pod-subpath-test-projected-sm4s 05/17/23 07:52:29.22
    May 17 07:52:29.220: INFO: Deleting pod "pod-subpath-test-projected-sm4s" in namespace "subpath-9725"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:29.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9725" for this suite. 05/17/23 07:52:29.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:29.226
May 17 07:52:29.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename containers 05/17/23 07:52:29.227
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:29.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:29.234
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 05/17/23 07:52:29.235
May 17 07:52:29.239: INFO: Waiting up to 5m0s for pod "client-containers-59e65767-0be8-4417-9739-dc682924af89" in namespace "containers-5360" to be "Succeeded or Failed"
May 17 07:52:29.240: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.15472ms
May 17 07:52:31.243: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004559712s
May 17 07:52:33.243: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003994633s
STEP: Saw pod success 05/17/23 07:52:33.243
May 17 07:52:33.243: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89" satisfied condition "Succeeded or Failed"
May 17 07:52:33.244: INFO: Trying to get logs from node k8s-node1 pod client-containers-59e65767-0be8-4417-9739-dc682924af89 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:52:33.247
May 17 07:52:33.253: INFO: Waiting for pod client-containers-59e65767-0be8-4417-9739-dc682924af89 to disappear
May 17 07:52:33.254: INFO: Pod client-containers-59e65767-0be8-4417-9739-dc682924af89 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 17 07:52:33.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5360" for this suite. 05/17/23 07:52:33.256
------------------------------
â€¢ [4.033 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:29.226
    May 17 07:52:29.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename containers 05/17/23 07:52:29.227
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:29.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:29.234
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 05/17/23 07:52:29.235
    May 17 07:52:29.239: INFO: Waiting up to 5m0s for pod "client-containers-59e65767-0be8-4417-9739-dc682924af89" in namespace "containers-5360" to be "Succeeded or Failed"
    May 17 07:52:29.240: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.15472ms
    May 17 07:52:31.243: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004559712s
    May 17 07:52:33.243: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003994633s
    STEP: Saw pod success 05/17/23 07:52:33.243
    May 17 07:52:33.243: INFO: Pod "client-containers-59e65767-0be8-4417-9739-dc682924af89" satisfied condition "Succeeded or Failed"
    May 17 07:52:33.244: INFO: Trying to get logs from node k8s-node1 pod client-containers-59e65767-0be8-4417-9739-dc682924af89 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:52:33.247
    May 17 07:52:33.253: INFO: Waiting for pod client-containers-59e65767-0be8-4417-9739-dc682924af89 to disappear
    May 17 07:52:33.254: INFO: Pod client-containers-59e65767-0be8-4417-9739-dc682924af89 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:33.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5360" for this suite. 05/17/23 07:52:33.256
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:33.259
May 17 07:52:33.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 07:52:33.26
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:33.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:33.267
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 07:52:33.273
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:52:33.586
STEP: Deploying the webhook pod 05/17/23 07:52:33.59
STEP: Wait for the deployment to be ready 05/17/23 07:52:33.596
May 17 07:52:33.600: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 07:52:35.605
STEP: Verifying the service has paired with the endpoint 05/17/23 07:52:35.612
May 17 07:52:36.612: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
May 17 07:52:36.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3671-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 07:52:37.12
STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 07:52:37.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:52:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3516" for this suite. 05/17/23 07:52:39.687
STEP: Destroying namespace "webhook-3516-markers" for this suite. 05/17/23 07:52:39.689
------------------------------
â€¢ [SLOW TEST] [6.433 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:33.259
    May 17 07:52:33.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 07:52:33.26
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:33.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:33.267
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 07:52:33.273
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:52:33.586
    STEP: Deploying the webhook pod 05/17/23 07:52:33.59
    STEP: Wait for the deployment to be ready 05/17/23 07:52:33.596
    May 17 07:52:33.600: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 07:52:35.605
    STEP: Verifying the service has paired with the endpoint 05/17/23 07:52:35.612
    May 17 07:52:36.612: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    May 17 07:52:36.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3671-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 07:52:37.12
    STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 07:52:37.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3516" for this suite. 05/17/23 07:52:39.687
    STEP: Destroying namespace "webhook-3516-markers" for this suite. 05/17/23 07:52:39.689
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:39.692
May 17 07:52:39.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 07:52:39.693
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:39.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:39.702
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 07:52:39.712
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:52:39.962
STEP: Deploying the webhook pod 05/17/23 07:52:39.966
STEP: Wait for the deployment to be ready 05/17/23 07:52:39.972
May 17 07:52:39.976: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 07:52:41.981
STEP: Verifying the service has paired with the endpoint 05/17/23 07:52:41.988
May 17 07:52:42.989: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 07:52:42.991
STEP: create a pod 05/17/23 07:52:43.002
May 17 07:52:43.005: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2423" to be "running"
May 17 07:52:43.006: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265607ms
May 17 07:52:45.008: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003104603s
May 17 07:52:45.008: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/17/23 07:52:45.008
May 17 07:52:45.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=webhook-2423 attach --namespace=webhook-2423 to-be-attached-pod -i -c=container1'
May 17 07:52:45.068: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:52:45.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2423" for this suite. 05/17/23 07:52:45.088
STEP: Destroying namespace "webhook-2423-markers" for this suite. 05/17/23 07:52:45.091
------------------------------
â€¢ [SLOW TEST] [5.402 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:39.692
    May 17 07:52:39.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 07:52:39.693
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:39.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:39.702
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 07:52:39.712
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:52:39.962
    STEP: Deploying the webhook pod 05/17/23 07:52:39.966
    STEP: Wait for the deployment to be ready 05/17/23 07:52:39.972
    May 17 07:52:39.976: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 07:52:41.981
    STEP: Verifying the service has paired with the endpoint 05/17/23 07:52:41.988
    May 17 07:52:42.989: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 07:52:42.991
    STEP: create a pod 05/17/23 07:52:43.002
    May 17 07:52:43.005: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2423" to be "running"
    May 17 07:52:43.006: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265607ms
    May 17 07:52:45.008: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003104603s
    May 17 07:52:45.008: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/17/23 07:52:45.008
    May 17 07:52:45.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=webhook-2423 attach --namespace=webhook-2423 to-be-attached-pod -i -c=container1'
    May 17 07:52:45.068: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:45.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2423" for this suite. 05/17/23 07:52:45.088
    STEP: Destroying namespace "webhook-2423-markers" for this suite. 05/17/23 07:52:45.091
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:45.095
May 17 07:52:45.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename containers 05/17/23 07:52:45.095
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:45.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:45.102
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 05/17/23 07:52:45.103
May 17 07:52:45.107: INFO: Waiting up to 5m0s for pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88" in namespace "containers-4752" to be "Succeeded or Failed"
May 17 07:52:45.109: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.240688ms
May 17 07:52:47.110: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003076886s
May 17 07:52:49.111: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003730987s
STEP: Saw pod success 05/17/23 07:52:49.111
May 17 07:52:49.111: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88" satisfied condition "Succeeded or Failed"
May 17 07:52:49.113: INFO: Trying to get logs from node k8s-node1 pod client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:52:49.116
May 17 07:52:49.121: INFO: Waiting for pod client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88 to disappear
May 17 07:52:49.122: INFO: Pod client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 17 07:52:49.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4752" for this suite. 05/17/23 07:52:49.124
------------------------------
â€¢ [4.032 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:45.095
    May 17 07:52:45.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename containers 05/17/23 07:52:45.095
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:45.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:45.102
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 05/17/23 07:52:45.103
    May 17 07:52:45.107: INFO: Waiting up to 5m0s for pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88" in namespace "containers-4752" to be "Succeeded or Failed"
    May 17 07:52:45.109: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.240688ms
    May 17 07:52:47.110: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003076886s
    May 17 07:52:49.111: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003730987s
    STEP: Saw pod success 05/17/23 07:52:49.111
    May 17 07:52:49.111: INFO: Pod "client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88" satisfied condition "Succeeded or Failed"
    May 17 07:52:49.113: INFO: Trying to get logs from node k8s-node1 pod client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:52:49.116
    May 17 07:52:49.121: INFO: Waiting for pod client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88 to disappear
    May 17 07:52:49.122: INFO: Pod client-containers-3edd1b72-d08a-4686-b4f6-367180c15b88 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:49.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4752" for this suite. 05/17/23 07:52:49.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:49.128
May 17 07:52:49.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 07:52:49.129
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:49.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:49.141
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-d50354ed-f218-4fce-bb4c-5443bfe25323 05/17/23 07:52:49.143
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 07:52:49.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8169" for this suite. 05/17/23 07:52:49.146
------------------------------
â€¢ [0.020 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:49.128
    May 17 07:52:49.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 07:52:49.129
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:49.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:49.141
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-d50354ed-f218-4fce-bb4c-5443bfe25323 05/17/23 07:52:49.143
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:49.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8169" for this suite. 05/17/23 07:52:49.146
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:49.148
May 17 07:52:49.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 07:52:49.149
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:49.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:49.155
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 07:52:49.157
May 17 07:52:49.160: INFO: Waiting up to 5m0s for pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad" in namespace "emptydir-7144" to be "Succeeded or Failed"
May 17 07:52:49.161: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.474333ms
May 17 07:52:51.163: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003314772s
May 17 07:52:53.163: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003661843s
STEP: Saw pod success 05/17/23 07:52:53.163
May 17 07:52:53.164: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad" satisfied condition "Succeeded or Failed"
May 17 07:52:53.165: INFO: Trying to get logs from node k8s-node1 pod pod-33046a0d-3b45-4207-b1b2-d7fe311294ad container test-container: <nil>
STEP: delete the pod 05/17/23 07:52:53.168
May 17 07:52:53.174: INFO: Waiting for pod pod-33046a0d-3b45-4207-b1b2-d7fe311294ad to disappear
May 17 07:52:53.175: INFO: Pod pod-33046a0d-3b45-4207-b1b2-d7fe311294ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 07:52:53.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7144" for this suite. 05/17/23 07:52:53.177
------------------------------
â€¢ [4.031 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:49.148
    May 17 07:52:49.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 07:52:49.149
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:49.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:49.155
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 07:52:49.157
    May 17 07:52:49.160: INFO: Waiting up to 5m0s for pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad" in namespace "emptydir-7144" to be "Succeeded or Failed"
    May 17 07:52:49.161: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.474333ms
    May 17 07:52:51.163: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003314772s
    May 17 07:52:53.163: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003661843s
    STEP: Saw pod success 05/17/23 07:52:53.163
    May 17 07:52:53.164: INFO: Pod "pod-33046a0d-3b45-4207-b1b2-d7fe311294ad" satisfied condition "Succeeded or Failed"
    May 17 07:52:53.165: INFO: Trying to get logs from node k8s-node1 pod pod-33046a0d-3b45-4207-b1b2-d7fe311294ad container test-container: <nil>
    STEP: delete the pod 05/17/23 07:52:53.168
    May 17 07:52:53.174: INFO: Waiting for pod pod-33046a0d-3b45-4207-b1b2-d7fe311294ad to disappear
    May 17 07:52:53.175: INFO: Pod pod-33046a0d-3b45-4207-b1b2-d7fe311294ad no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 07:52:53.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7144" for this suite. 05/17/23 07:52:53.177
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:52:53.179
May 17 07:52:53.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 07:52:53.18
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:53.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:53.188
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8480 05/17/23 07:52:53.19
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 07:52:53.198
STEP: creating service externalsvc in namespace services-8480 05/17/23 07:52:53.198
STEP: creating replication controller externalsvc in namespace services-8480 05/17/23 07:52:53.206
I0517 07:52:53.208998      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8480, replica count: 2
I0517 07:52:56.260289      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/17/23 07:52:56.262
May 17 07:52:56.270: INFO: Creating new exec pod
May 17 07:52:56.273: INFO: Waiting up to 5m0s for pod "execpod6jkl9" in namespace "services-8480" to be "running"
May 17 07:52:56.275: INFO: Pod "execpod6jkl9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.212613ms
May 17 07:52:58.276: INFO: Pod "execpod6jkl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.002833732s
May 17 07:52:58.276: INFO: Pod "execpod6jkl9" satisfied condition "running"
May 17 07:52:58.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8480 exec execpod6jkl9 -- /bin/sh -x -c nslookup nodeport-service.services-8480.svc.cluster.local'
May 17 07:52:58.392: INFO: stderr: "+ nslookup nodeport-service.services-8480.svc.cluster.local\n"
May 17 07:52:58.392: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8480.svc.cluster.local\tcanonical name = externalsvc.services-8480.svc.cluster.local.\nName:\texternalsvc.services-8480.svc.cluster.local\nAddress: 10.96.91.149\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8480, will wait for the garbage collector to delete the pods 05/17/23 07:52:58.392
May 17 07:52:58.448: INFO: Deleting ReplicationController externalsvc took: 3.99291ms
May 17 07:52:58.549: INFO: Terminating ReplicationController externalsvc pods took: 100.475774ms
May 17 07:53:00.157: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 07:53:00.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8480" for this suite. 05/17/23 07:53:00.164
------------------------------
â€¢ [SLOW TEST] [6.987 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:52:53.179
    May 17 07:52:53.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 07:52:53.18
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:52:53.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:52:53.188
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8480 05/17/23 07:52:53.19
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 07:52:53.198
    STEP: creating service externalsvc in namespace services-8480 05/17/23 07:52:53.198
    STEP: creating replication controller externalsvc in namespace services-8480 05/17/23 07:52:53.206
    I0517 07:52:53.208998      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8480, replica count: 2
    I0517 07:52:56.260289      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/17/23 07:52:56.262
    May 17 07:52:56.270: INFO: Creating new exec pod
    May 17 07:52:56.273: INFO: Waiting up to 5m0s for pod "execpod6jkl9" in namespace "services-8480" to be "running"
    May 17 07:52:56.275: INFO: Pod "execpod6jkl9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.212613ms
    May 17 07:52:58.276: INFO: Pod "execpod6jkl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.002833732s
    May 17 07:52:58.276: INFO: Pod "execpod6jkl9" satisfied condition "running"
    May 17 07:52:58.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8480 exec execpod6jkl9 -- /bin/sh -x -c nslookup nodeport-service.services-8480.svc.cluster.local'
    May 17 07:52:58.392: INFO: stderr: "+ nslookup nodeport-service.services-8480.svc.cluster.local\n"
    May 17 07:52:58.392: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8480.svc.cluster.local\tcanonical name = externalsvc.services-8480.svc.cluster.local.\nName:\texternalsvc.services-8480.svc.cluster.local\nAddress: 10.96.91.149\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8480, will wait for the garbage collector to delete the pods 05/17/23 07:52:58.392
    May 17 07:52:58.448: INFO: Deleting ReplicationController externalsvc took: 3.99291ms
    May 17 07:52:58.549: INFO: Terminating ReplicationController externalsvc pods took: 100.475774ms
    May 17 07:53:00.157: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 07:53:00.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8480" for this suite. 05/17/23 07:53:00.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:53:00.168
May 17 07:53:00.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:53:00.169
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:00.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:00.176
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/17/23 07:53:00.178
May 17 07:53:00.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 07:53:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:53:06.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2702" for this suite. 05/17/23 07:53:06.83
------------------------------
â€¢ [SLOW TEST] [6.665 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:53:00.168
    May 17 07:53:00.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:53:00.169
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:00.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:00.176
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/17/23 07:53:00.178
    May 17 07:53:00.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 07:53:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:53:06.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2702" for this suite. 05/17/23 07:53:06.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:53:06.834
May 17 07:53:06.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pod-network-test 05/17/23 07:53:06.835
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:06.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:06.843
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6503 05/17/23 07:53:06.845
STEP: creating a selector 05/17/23 07:53:06.845
STEP: Creating the service pods in kubernetes 05/17/23 07:53:06.845
May 17 07:53:06.845: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 07:53:06.855: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6503" to be "running and ready"
May 17 07:53:06.857: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.877527ms
May 17 07:53:06.857: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:53:08.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004391011s
May 17 07:53:08.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 07:53:10.860: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004731599s
May 17 07:53:10.860: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 07:53:12.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004072069s
May 17 07:53:12.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 07:53:14.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004254382s
May 17 07:53:14.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 07:53:16.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00443304s
May 17 07:53:16.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 07:53:18.861: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005603005s
May 17 07:53:18.861: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 07:53:18.861: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 07:53:18.862: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6503" to be "running and ready"
May 17 07:53:18.863: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.303384ms
May 17 07:53:18.863: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 07:53:18.863: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 07:53:18.864
May 17 07:53:18.868: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6503" to be "running"
May 17 07:53:18.870: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146635ms
May 17 07:53:20.872: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004313131s
May 17 07:53:20.872: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 07:53:20.874: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 17 07:53:20.874: INFO: Breadth first check of 192.168.36.84 on host 10.0.79.210...
May 17 07:53:20.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.126:9080/dial?request=hostname&protocol=udp&host=192.168.36.84&port=8081&tries=1'] Namespace:pod-network-test-6503 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 07:53:20.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 07:53:20.875: INFO: ExecWithOptions: Clientset creation
May 17 07:53:20.875: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6503/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.36.84%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 07:53:20.924: INFO: Waiting for responses: map[]
May 17 07:53:20.924: INFO: reached 192.168.36.84 after 0/1 tries
May 17 07:53:20.924: INFO: Breadth first check of 192.168.169.188 on host 10.0.79.211...
May 17 07:53:20.925: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.126:9080/dial?request=hostname&protocol=udp&host=192.168.169.188&port=8081&tries=1'] Namespace:pod-network-test-6503 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 07:53:20.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 07:53:20.926: INFO: ExecWithOptions: Clientset creation
May 17 07:53:20.926: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6503/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.169.188%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 07:53:20.966: INFO: Waiting for responses: map[]
May 17 07:53:20.966: INFO: reached 192.168.169.188 after 0/1 tries
May 17 07:53:20.966: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 17 07:53:20.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6503" for this suite. 05/17/23 07:53:20.968
------------------------------
â€¢ [SLOW TEST] [14.137 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:53:06.834
    May 17 07:53:06.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 07:53:06.835
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:06.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:06.843
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6503 05/17/23 07:53:06.845
    STEP: creating a selector 05/17/23 07:53:06.845
    STEP: Creating the service pods in kubernetes 05/17/23 07:53:06.845
    May 17 07:53:06.845: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 07:53:06.855: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6503" to be "running and ready"
    May 17 07:53:06.857: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.877527ms
    May 17 07:53:06.857: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:53:08.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004391011s
    May 17 07:53:08.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 07:53:10.860: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004731599s
    May 17 07:53:10.860: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 07:53:12.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004072069s
    May 17 07:53:12.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 07:53:14.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004254382s
    May 17 07:53:14.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 07:53:16.859: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00443304s
    May 17 07:53:16.859: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 07:53:18.861: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005603005s
    May 17 07:53:18.861: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 07:53:18.861: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 07:53:18.862: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6503" to be "running and ready"
    May 17 07:53:18.863: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.303384ms
    May 17 07:53:18.863: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 07:53:18.863: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 07:53:18.864
    May 17 07:53:18.868: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6503" to be "running"
    May 17 07:53:18.870: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146635ms
    May 17 07:53:20.872: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004313131s
    May 17 07:53:20.872: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 07:53:20.874: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    May 17 07:53:20.874: INFO: Breadth first check of 192.168.36.84 on host 10.0.79.210...
    May 17 07:53:20.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.126:9080/dial?request=hostname&protocol=udp&host=192.168.36.84&port=8081&tries=1'] Namespace:pod-network-test-6503 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 07:53:20.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 07:53:20.875: INFO: ExecWithOptions: Clientset creation
    May 17 07:53:20.875: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6503/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.36.84%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 07:53:20.924: INFO: Waiting for responses: map[]
    May 17 07:53:20.924: INFO: reached 192.168.36.84 after 0/1 tries
    May 17 07:53:20.924: INFO: Breadth first check of 192.168.169.188 on host 10.0.79.211...
    May 17 07:53:20.925: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.126:9080/dial?request=hostname&protocol=udp&host=192.168.169.188&port=8081&tries=1'] Namespace:pod-network-test-6503 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 07:53:20.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 07:53:20.926: INFO: ExecWithOptions: Clientset creation
    May 17 07:53:20.926: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6503/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.169.188%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 07:53:20.966: INFO: Waiting for responses: map[]
    May 17 07:53:20.966: INFO: reached 192.168.169.188 after 0/1 tries
    May 17 07:53:20.966: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 17 07:53:20.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6503" for this suite. 05/17/23 07:53:20.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:53:20.972
May 17 07:53:20.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 07:53:20.972
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:20.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:20.98
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
May 17 07:53:20.985: INFO: Waiting up to 2m0s for pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" in namespace "var-expansion-3727" to be "container 0 failed with reason CreateContainerConfigError"
May 17 07:53:20.987: INFO: Pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335175ms
May 17 07:53:22.990: INFO: Pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004102775s
May 17 07:53:22.990: INFO: Pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May 17 07:53:22.990: INFO: Deleting pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" in namespace "var-expansion-3727"
May 17 07:53:22.993: INFO: Wait up to 5m0s for pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 07:53:26.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3727" for this suite. 05/17/23 07:53:27
------------------------------
â€¢ [SLOW TEST] [6.031 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:53:20.972
    May 17 07:53:20.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 07:53:20.972
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:20.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:20.98
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    May 17 07:53:20.985: INFO: Waiting up to 2m0s for pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" in namespace "var-expansion-3727" to be "container 0 failed with reason CreateContainerConfigError"
    May 17 07:53:20.987: INFO: Pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335175ms
    May 17 07:53:22.990: INFO: Pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004102775s
    May 17 07:53:22.990: INFO: Pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May 17 07:53:22.990: INFO: Deleting pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" in namespace "var-expansion-3727"
    May 17 07:53:22.993: INFO: Wait up to 5m0s for pod "var-expansion-21fb4c17-3d72-4c68-9ea1-48d33a8c34d4" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 07:53:26.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3727" for this suite. 05/17/23 07:53:27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:53:27.003
May 17 07:53:27.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 07:53:27.004
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:27.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:27.011
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-9da74c50-437c-4e7a-a45d-00e88113d4be 05/17/23 07:53:27.013
STEP: Creating a pod to test consume configMaps 05/17/23 07:53:27.016
May 17 07:53:27.020: INFO: Waiting up to 5m0s for pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263" in namespace "configmap-697" to be "Succeeded or Failed"
May 17 07:53:27.021: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265946ms
May 17 07:53:29.024: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004477804s
May 17 07:53:31.024: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004156369s
STEP: Saw pod success 05/17/23 07:53:31.024
May 17 07:53:31.024: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263" satisfied condition "Succeeded or Failed"
May 17 07:53:31.026: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263 container configmap-volume-test: <nil>
STEP: delete the pod 05/17/23 07:53:31.029
May 17 07:53:31.035: INFO: Waiting for pod pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263 to disappear
May 17 07:53:31.037: INFO: Pod pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 07:53:31.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-697" for this suite. 05/17/23 07:53:31.039
------------------------------
â€¢ [4.038 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:53:27.003
    May 17 07:53:27.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 07:53:27.004
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:27.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:27.011
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-9da74c50-437c-4e7a-a45d-00e88113d4be 05/17/23 07:53:27.013
    STEP: Creating a pod to test consume configMaps 05/17/23 07:53:27.016
    May 17 07:53:27.020: INFO: Waiting up to 5m0s for pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263" in namespace "configmap-697" to be "Succeeded or Failed"
    May 17 07:53:27.021: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265946ms
    May 17 07:53:29.024: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004477804s
    May 17 07:53:31.024: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004156369s
    STEP: Saw pod success 05/17/23 07:53:31.024
    May 17 07:53:31.024: INFO: Pod "pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263" satisfied condition "Succeeded or Failed"
    May 17 07:53:31.026: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263 container configmap-volume-test: <nil>
    STEP: delete the pod 05/17/23 07:53:31.029
    May 17 07:53:31.035: INFO: Waiting for pod pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263 to disappear
    May 17 07:53:31.037: INFO: Pod pod-configmaps-e25543d4-618e-4d54-ae30-6906e7097263 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 07:53:31.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-697" for this suite. 05/17/23 07:53:31.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:53:31.042
May 17 07:53:31.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 07:53:31.042
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:31.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:31.051
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5363 05/17/23 07:53:31.052
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-5363 05/17/23 07:53:31.055
May 17 07:53:31.061: INFO: Found 0 stateful pods, waiting for 1
May 17 07:53:41.064: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/17/23 07:53:41.067
STEP: updating a scale subresource 05/17/23 07:53:41.068
STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 07:53:41.072
STEP: Patch a scale subresource 05/17/23 07:53:41.073
STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 07:53:41.077
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 07:53:41.079: INFO: Deleting all statefulset in ns statefulset-5363
May 17 07:53:41.080: INFO: Scaling statefulset ss to 0
May 17 07:53:51.089: INFO: Waiting for statefulset status.replicas updated to 0
May 17 07:53:51.091: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 07:53:51.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5363" for this suite. 05/17/23 07:53:51.099
------------------------------
â€¢ [SLOW TEST] [20.060 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:53:31.042
    May 17 07:53:31.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 07:53:31.042
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:31.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:31.051
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5363 05/17/23 07:53:31.052
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-5363 05/17/23 07:53:31.055
    May 17 07:53:31.061: INFO: Found 0 stateful pods, waiting for 1
    May 17 07:53:41.064: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/17/23 07:53:41.067
    STEP: updating a scale subresource 05/17/23 07:53:41.068
    STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 07:53:41.072
    STEP: Patch a scale subresource 05/17/23 07:53:41.073
    STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 07:53:41.077
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 07:53:41.079: INFO: Deleting all statefulset in ns statefulset-5363
    May 17 07:53:41.080: INFO: Scaling statefulset ss to 0
    May 17 07:53:51.089: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 07:53:51.091: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 07:53:51.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5363" for this suite. 05/17/23 07:53:51.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:53:51.102
May 17 07:53:51.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svcaccounts 05/17/23 07:53:51.103
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:51.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:51.111
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  05/17/23 07:53:51.113
May 17 07:53:51.116: INFO: Waiting up to 5m0s for pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3" in namespace "svcaccounts-5620" to be "Succeeded or Failed"
May 17 07:53:51.118: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.222566ms
May 17 07:53:53.121: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00410294s
May 17 07:53:55.122: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005820726s
STEP: Saw pod success 05/17/23 07:53:55.122
May 17 07:53:55.122: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3" satisfied condition "Succeeded or Failed"
May 17 07:53:55.124: INFO: Trying to get logs from node k8s-node1 pod test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:53:55.127
May 17 07:53:55.134: INFO: Waiting for pod test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3 to disappear
May 17 07:53:55.136: INFO: Pod test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 17 07:53:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5620" for this suite. 05/17/23 07:53:55.138
------------------------------
â€¢ [4.039 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:53:51.102
    May 17 07:53:51.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 07:53:51.103
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:51.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:51.111
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  05/17/23 07:53:51.113
    May 17 07:53:51.116: INFO: Waiting up to 5m0s for pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3" in namespace "svcaccounts-5620" to be "Succeeded or Failed"
    May 17 07:53:51.118: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.222566ms
    May 17 07:53:53.121: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00410294s
    May 17 07:53:55.122: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005820726s
    STEP: Saw pod success 05/17/23 07:53:55.122
    May 17 07:53:55.122: INFO: Pod "test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3" satisfied condition "Succeeded or Failed"
    May 17 07:53:55.124: INFO: Trying to get logs from node k8s-node1 pod test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:53:55.127
    May 17 07:53:55.134: INFO: Waiting for pod test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3 to disappear
    May 17 07:53:55.136: INFO: Pod test-pod-f7ab4610-611a-47a4-95bf-25553ba66de3 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 17 07:53:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5620" for this suite. 05/17/23 07:53:55.138
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:53:55.141
May 17 07:53:55.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 07:53:55.142
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:55.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:55.149
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
May 17 07:53:55.154: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 17 07:54:00.159: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 07:54:00.159
May 17 07:54:00.159: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 17 07:54:02.162: INFO: Creating deployment "test-rollover-deployment"
May 17 07:54:02.167: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 17 07:54:04.171: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 17 07:54:04.174: INFO: Ensure that both replica sets have 1 created replica
May 17 07:54:04.176: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 17 07:54:04.181: INFO: Updating deployment test-rollover-deployment
May 17 07:54:04.181: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 17 07:54:06.187: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 17 07:54:06.190: INFO: Make sure deployment "test-rollover-deployment" is complete
May 17 07:54:06.192: INFO: all replica sets need to contain the pod-template-hash label
May 17 07:54:06.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:54:08.197: INFO: all replica sets need to contain the pod-template-hash label
May 17 07:54:08.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:54:10.198: INFO: all replica sets need to contain the pod-template-hash label
May 17 07:54:10.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:54:12.199: INFO: all replica sets need to contain the pod-template-hash label
May 17 07:54:12.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:54:14.198: INFO: all replica sets need to contain the pod-template-hash label
May 17 07:54:14.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:54:16.198: INFO: 
May 17 07:54:16.199: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 07:54:16.203: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6315  36654955-67f8-4e55-8cf6-bfc8fb364b88 1189166 2 2023-05-17 07:54:02 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00007e558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 07:54:02 +0000 UTC,LastTransitionTime:2023-05-17 07:54:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-17 07:54:15 +0000 UTC,LastTransitionTime:2023-05-17 07:54:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 07:54:16.204: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6315  97733659-60cd-420b-9700-b74a045703c4 1189156 2 2023-05-17 07:54:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 36654955-67f8-4e55-8cf6-bfc8fb364b88 0xc004e88367 0xc004e88368}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36654955-67f8-4e55-8cf6-bfc8fb364b88\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e88418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 07:54:16.204: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 17 07:54:16.204: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6315  3051ea7d-1740-46f6-89f5-f0ac9e3a6723 1189165 2 2023-05-17 07:53:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 36654955-67f8-4e55-8cf6-bfc8fb364b88 0xc004e881d7 0xc004e881d8}] [] [{e2e.test Update apps/v1 2023-05-17 07:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36654955-67f8-4e55-8cf6-bfc8fb364b88\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e882c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 07:54:16.204: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6315  d029e41a-7b22-4473-8f22-7b95a9d72216 1189117 2 2023-05-17 07:54:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 36654955-67f8-4e55-8cf6-bfc8fb364b88 0xc004e88487 0xc004e88488}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36654955-67f8-4e55-8cf6-bfc8fb364b88\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e88538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 07:54:16.206: INFO: Pod "test-rollover-deployment-6c6df9974f-lf6r5" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-lf6r5 test-rollover-deployment-6c6df9974f- deployment-6315  abb216ae-f811-4d81-bf9f-81a469744422 1189133 0 2023-05-17 07:54:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:80ce3c7244157b1a8d7d2b2532d2cdeb0776de0fb647349ffbe16e547450f09b cni.projectcalico.org/podIP:192.168.36.111/32 cni.projectcalico.org/podIPs:192.168.36.111/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 97733659-60cd-420b-9700-b74a045703c4 0xc004e3dcd7 0xc004e3dcd8}] [] [{calico Update v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97733659-60cd-420b-9700-b74a045703c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:54:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zts2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zts2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.111,StartTime:2023-05-17 07:54:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:54:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://89b9dde7e3d964284a579e08406971f146bdd9dda64e012190fb6f1413acd9e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 07:54:16.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6315" for this suite. 05/17/23 07:54:16.208
------------------------------
â€¢ [SLOW TEST] [21.071 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:53:55.141
    May 17 07:53:55.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 07:53:55.142
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:53:55.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:53:55.149
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    May 17 07:53:55.154: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May 17 07:54:00.159: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 07:54:00.159
    May 17 07:54:00.159: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May 17 07:54:02.162: INFO: Creating deployment "test-rollover-deployment"
    May 17 07:54:02.167: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May 17 07:54:04.171: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May 17 07:54:04.174: INFO: Ensure that both replica sets have 1 created replica
    May 17 07:54:04.176: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May 17 07:54:04.181: INFO: Updating deployment test-rollover-deployment
    May 17 07:54:04.181: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May 17 07:54:06.187: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May 17 07:54:06.190: INFO: Make sure deployment "test-rollover-deployment" is complete
    May 17 07:54:06.192: INFO: all replica sets need to contain the pod-template-hash label
    May 17 07:54:06.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:54:08.197: INFO: all replica sets need to contain the pod-template-hash label
    May 17 07:54:08.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:54:10.198: INFO: all replica sets need to contain the pod-template-hash label
    May 17 07:54:10.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:54:12.199: INFO: all replica sets need to contain the pod-template-hash label
    May 17 07:54:12.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:54:14.198: INFO: all replica sets need to contain the pod-template-hash label
    May 17 07:54:14.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:54:16.198: INFO: 
    May 17 07:54:16.199: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 07:54:16.203: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6315  36654955-67f8-4e55-8cf6-bfc8fb364b88 1189166 2 2023-05-17 07:54:02 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00007e558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 07:54:02 +0000 UTC,LastTransitionTime:2023-05-17 07:54:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-17 07:54:15 +0000 UTC,LastTransitionTime:2023-05-17 07:54:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 07:54:16.204: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6315  97733659-60cd-420b-9700-b74a045703c4 1189156 2 2023-05-17 07:54:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 36654955-67f8-4e55-8cf6-bfc8fb364b88 0xc004e88367 0xc004e88368}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36654955-67f8-4e55-8cf6-bfc8fb364b88\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e88418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 07:54:16.204: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May 17 07:54:16.204: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6315  3051ea7d-1740-46f6-89f5-f0ac9e3a6723 1189165 2 2023-05-17 07:53:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 36654955-67f8-4e55-8cf6-bfc8fb364b88 0xc004e881d7 0xc004e881d8}] [] [{e2e.test Update apps/v1 2023-05-17 07:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36654955-67f8-4e55-8cf6-bfc8fb364b88\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e882c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 07:54:16.204: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6315  d029e41a-7b22-4473-8f22-7b95a9d72216 1189117 2 2023-05-17 07:54:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 36654955-67f8-4e55-8cf6-bfc8fb364b88 0xc004e88487 0xc004e88488}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36654955-67f8-4e55-8cf6-bfc8fb364b88\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e88538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 07:54:16.206: INFO: Pod "test-rollover-deployment-6c6df9974f-lf6r5" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-lf6r5 test-rollover-deployment-6c6df9974f- deployment-6315  abb216ae-f811-4d81-bf9f-81a469744422 1189133 0 2023-05-17 07:54:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:80ce3c7244157b1a8d7d2b2532d2cdeb0776de0fb647349ffbe16e547450f09b cni.projectcalico.org/podIP:192.168.36.111/32 cni.projectcalico.org/podIPs:192.168.36.111/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 97733659-60cd-420b-9700-b74a045703c4 0xc004e3dcd7 0xc004e3dcd8}] [] [{calico Update v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97733659-60cd-420b-9700-b74a045703c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:54:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zts2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zts2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:54:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.111,StartTime:2023-05-17 07:54:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:54:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://89b9dde7e3d964284a579e08406971f146bdd9dda64e012190fb6f1413acd9e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:16.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6315" for this suite. 05/17/23 07:54:16.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:16.212
May 17 07:54:16.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replicaset 05/17/23 07:54:16.213
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:16.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:16.221
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May 17 07:54:16.228: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 07:54:21.232: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 07:54:21.232
STEP: Scaling up "test-rs" replicaset  05/17/23 07:54:21.233
May 17 07:54:21.237: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/17/23 07:54:21.237
W0517 07:54:21.241492      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 17 07:54:21.242: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:54:21.251: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:54:21.257: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:54:21.260: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:54:22.002: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 2, AvailableReplicas 2
May 17 07:54:22.252: INFO: observed Replicaset test-rs in namespace replicaset-693 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 17 07:54:22.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-693" for this suite. 05/17/23 07:54:22.254
------------------------------
â€¢ [SLOW TEST] [6.045 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:16.212
    May 17 07:54:16.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replicaset 05/17/23 07:54:16.213
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:16.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:16.221
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May 17 07:54:16.228: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 07:54:21.232: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 07:54:21.232
    STEP: Scaling up "test-rs" replicaset  05/17/23 07:54:21.233
    May 17 07:54:21.237: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/17/23 07:54:21.237
    W0517 07:54:21.241492      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 17 07:54:21.242: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:54:21.251: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:54:21.257: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:54:21.260: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:54:22.002: INFO: observed ReplicaSet test-rs in namespace replicaset-693 with ReadyReplicas 2, AvailableReplicas 2
    May 17 07:54:22.252: INFO: observed Replicaset test-rs in namespace replicaset-693 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:22.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-693" for this suite. 05/17/23 07:54:22.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:22.258
May 17 07:54:22.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubelet-test 05/17/23 07:54:22.258
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:22.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:22.267
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 17 07:54:26.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9846" for this suite. 05/17/23 07:54:26.278
------------------------------
â€¢ [4.025 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:22.258
    May 17 07:54:22.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 07:54:22.258
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:22.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:22.267
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:26.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9846" for this suite. 05/17/23 07:54:26.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:26.283
May 17 07:54:26.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename csiinlinevolumes 05/17/23 07:54:26.284
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:26.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:26.291
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 05/17/23 07:54:26.293
STEP: getting 05/17/23 07:54:26.301
STEP: listing in namespace 05/17/23 07:54:26.302
STEP: patching 05/17/23 07:54:26.303
STEP: deleting 05/17/23 07:54:26.307
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May 17 07:54:26.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9520" for this suite. 05/17/23 07:54:26.313
------------------------------
â€¢ [0.033 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:26.283
    May 17 07:54:26.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename csiinlinevolumes 05/17/23 07:54:26.284
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:26.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:26.291
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 05/17/23 07:54:26.293
    STEP: getting 05/17/23 07:54:26.301
    STEP: listing in namespace 05/17/23 07:54:26.302
    STEP: patching 05/17/23 07:54:26.303
    STEP: deleting 05/17/23 07:54:26.307
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:26.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9520" for this suite. 05/17/23 07:54:26.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:26.317
May 17 07:54:26.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 07:54:26.318
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:26.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:26.324
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-d5e9ddb6-b000-4d16-aeaa-e5e12a1389c1 05/17/23 07:54:26.326
STEP: Creating a pod to test consume configMaps 05/17/23 07:54:26.328
May 17 07:54:26.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8" in namespace "configmap-4929" to be "Succeeded or Failed"
May 17 07:54:26.333: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.256086ms
May 17 07:54:28.335: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003187338s
May 17 07:54:30.337: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00536015s
STEP: Saw pod success 05/17/23 07:54:30.337
May 17 07:54:30.338: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8" satisfied condition "Succeeded or Failed"
May 17 07:54:30.339: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:54:30.348
May 17 07:54:30.354: INFO: Waiting for pod pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8 to disappear
May 17 07:54:30.356: INFO: Pod pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 07:54:30.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4929" for this suite. 05/17/23 07:54:30.357
------------------------------
â€¢ [4.043 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:26.317
    May 17 07:54:26.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 07:54:26.318
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:26.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:26.324
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-d5e9ddb6-b000-4d16-aeaa-e5e12a1389c1 05/17/23 07:54:26.326
    STEP: Creating a pod to test consume configMaps 05/17/23 07:54:26.328
    May 17 07:54:26.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8" in namespace "configmap-4929" to be "Succeeded or Failed"
    May 17 07:54:26.333: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.256086ms
    May 17 07:54:28.335: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003187338s
    May 17 07:54:30.337: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00536015s
    STEP: Saw pod success 05/17/23 07:54:30.337
    May 17 07:54:30.338: INFO: Pod "pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8" satisfied condition "Succeeded or Failed"
    May 17 07:54:30.339: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:54:30.348
    May 17 07:54:30.354: INFO: Waiting for pod pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8 to disappear
    May 17 07:54:30.356: INFO: Pod pod-configmaps-e44b1f66-7f0d-4e2b-a560-c8ba1961c1c8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:30.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4929" for this suite. 05/17/23 07:54:30.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:30.361
May 17 07:54:30.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 07:54:30.362
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:30.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:30.368
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/17/23 07:54:30.37
STEP: submitting the pod to kubernetes 05/17/23 07:54:30.37
STEP: verifying QOS class is set on the pod 05/17/23 07:54:30.374
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
May 17 07:54:30.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1406" for this suite. 05/17/23 07:54:30.377
------------------------------
â€¢ [0.019 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:30.361
    May 17 07:54:30.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 07:54:30.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:30.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:30.368
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/17/23 07:54:30.37
    STEP: submitting the pod to kubernetes 05/17/23 07:54:30.37
    STEP: verifying QOS class is set on the pod 05/17/23 07:54:30.374
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:30.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1406" for this suite. 05/17/23 07:54:30.377
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:30.38
May 17 07:54:30.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubelet-test 05/17/23 07:54:30.381
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:30.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:30.388
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 17 07:54:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3641" for this suite. 05/17/23 07:54:30.401
------------------------------
â€¢ [0.023 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:30.38
    May 17 07:54:30.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 07:54:30.381
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:30.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:30.388
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3641" for this suite. 05/17/23 07:54:30.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:30.404
May 17 07:54:30.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename gc 05/17/23 07:54:30.405
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:30.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:30.414
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/17/23 07:54:30.417
STEP: delete the rc 05/17/23 07:54:35.423
STEP: wait for the rc to be deleted 05/17/23 07:54:35.426
May 17 07:54:36.434: INFO: 80 pods remaining
May 17 07:54:36.434: INFO: 80 pods has nil DeletionTimestamp
May 17 07:54:36.434: INFO: 
May 17 07:54:37.432: INFO: 71 pods remaining
May 17 07:54:37.432: INFO: 70 pods has nil DeletionTimestamp
May 17 07:54:37.432: INFO: 
May 17 07:54:38.432: INFO: 60 pods remaining
May 17 07:54:38.432: INFO: 60 pods has nil DeletionTimestamp
May 17 07:54:38.432: INFO: 
May 17 07:54:39.432: INFO: 40 pods remaining
May 17 07:54:39.432: INFO: 40 pods has nil DeletionTimestamp
May 17 07:54:39.432: INFO: 
May 17 07:54:40.433: INFO: 31 pods remaining
May 17 07:54:40.433: INFO: 31 pods has nil DeletionTimestamp
May 17 07:54:40.433: INFO: 
May 17 07:54:41.431: INFO: 20 pods remaining
May 17 07:54:41.431: INFO: 20 pods has nil DeletionTimestamp
May 17 07:54:41.431: INFO: 
STEP: Gathering metrics 05/17/23 07:54:42.43
May 17 07:54:42.443: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
May 17 07:54:42.445: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.971448ms
May 17 07:54:42.445: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
May 17 07:54:42.445: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
May 17 07:54:42.490: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 17 07:54:42.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1458" for this suite. 05/17/23 07:54:42.493
------------------------------
â€¢ [SLOW TEST] [12.092 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:30.404
    May 17 07:54:30.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename gc 05/17/23 07:54:30.405
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:30.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:30.414
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/17/23 07:54:30.417
    STEP: delete the rc 05/17/23 07:54:35.423
    STEP: wait for the rc to be deleted 05/17/23 07:54:35.426
    May 17 07:54:36.434: INFO: 80 pods remaining
    May 17 07:54:36.434: INFO: 80 pods has nil DeletionTimestamp
    May 17 07:54:36.434: INFO: 
    May 17 07:54:37.432: INFO: 71 pods remaining
    May 17 07:54:37.432: INFO: 70 pods has nil DeletionTimestamp
    May 17 07:54:37.432: INFO: 
    May 17 07:54:38.432: INFO: 60 pods remaining
    May 17 07:54:38.432: INFO: 60 pods has nil DeletionTimestamp
    May 17 07:54:38.432: INFO: 
    May 17 07:54:39.432: INFO: 40 pods remaining
    May 17 07:54:39.432: INFO: 40 pods has nil DeletionTimestamp
    May 17 07:54:39.432: INFO: 
    May 17 07:54:40.433: INFO: 31 pods remaining
    May 17 07:54:40.433: INFO: 31 pods has nil DeletionTimestamp
    May 17 07:54:40.433: INFO: 
    May 17 07:54:41.431: INFO: 20 pods remaining
    May 17 07:54:41.431: INFO: 20 pods has nil DeletionTimestamp
    May 17 07:54:41.431: INFO: 
    STEP: Gathering metrics 05/17/23 07:54:42.43
    May 17 07:54:42.443: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
    May 17 07:54:42.445: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.971448ms
    May 17 07:54:42.445: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
    May 17 07:54:42.445: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
    May 17 07:54:42.490: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:42.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1458" for this suite. 05/17/23 07:54:42.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:42.497
May 17 07:54:42.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename ingress 05/17/23 07:54:42.497
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:42.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:42.505
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/17/23 07:54:42.507
STEP: getting /apis/networking.k8s.io 05/17/23 07:54:42.508
STEP: getting /apis/networking.k8s.iov1 05/17/23 07:54:42.509
STEP: creating 05/17/23 07:54:42.509
STEP: getting 05/17/23 07:54:42.517
STEP: listing 05/17/23 07:54:42.518
STEP: watching 05/17/23 07:54:42.519
May 17 07:54:42.519: INFO: starting watch
STEP: cluster-wide listing 05/17/23 07:54:42.52
STEP: cluster-wide watching 05/17/23 07:54:42.521
May 17 07:54:42.521: INFO: starting watch
STEP: patching 05/17/23 07:54:42.522
STEP: updating 05/17/23 07:54:42.526
May 17 07:54:42.530: INFO: waiting for watch events with expected annotations
May 17 07:54:42.530: INFO: saw patched and updated annotations
STEP: patching /status 05/17/23 07:54:42.53
STEP: updating /status 05/17/23 07:54:42.533
STEP: get /status 05/17/23 07:54:42.537
STEP: deleting 05/17/23 07:54:42.538
STEP: deleting a collection 05/17/23 07:54:42.543
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
May 17 07:54:42.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-8277" for this suite. 05/17/23 07:54:42.55
------------------------------
â€¢ [0.056 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:42.497
    May 17 07:54:42.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename ingress 05/17/23 07:54:42.497
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:42.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:42.505
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/17/23 07:54:42.507
    STEP: getting /apis/networking.k8s.io 05/17/23 07:54:42.508
    STEP: getting /apis/networking.k8s.iov1 05/17/23 07:54:42.509
    STEP: creating 05/17/23 07:54:42.509
    STEP: getting 05/17/23 07:54:42.517
    STEP: listing 05/17/23 07:54:42.518
    STEP: watching 05/17/23 07:54:42.519
    May 17 07:54:42.519: INFO: starting watch
    STEP: cluster-wide listing 05/17/23 07:54:42.52
    STEP: cluster-wide watching 05/17/23 07:54:42.521
    May 17 07:54:42.521: INFO: starting watch
    STEP: patching 05/17/23 07:54:42.522
    STEP: updating 05/17/23 07:54:42.526
    May 17 07:54:42.530: INFO: waiting for watch events with expected annotations
    May 17 07:54:42.530: INFO: saw patched and updated annotations
    STEP: patching /status 05/17/23 07:54:42.53
    STEP: updating /status 05/17/23 07:54:42.533
    STEP: get /status 05/17/23 07:54:42.537
    STEP: deleting 05/17/23 07:54:42.538
    STEP: deleting a collection 05/17/23 07:54:42.543
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:42.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-8277" for this suite. 05/17/23 07:54:42.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:42.553
May 17 07:54:42.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 07:54:42.553
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:42.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:42.561
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 05/17/23 07:54:42.562
STEP: Creating a ResourceQuota 05/17/23 07:54:47.564
STEP: Ensuring resource quota status is calculated 05/17/23 07:54:47.567
STEP: Creating a Pod that fits quota 05/17/23 07:54:49.571
STEP: Ensuring ResourceQuota status captures the pod usage 05/17/23 07:54:49.58
STEP: Not allowing a pod to be created that exceeds remaining quota 05/17/23 07:54:51.582
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/17/23 07:54:51.584
STEP: Ensuring a pod cannot update its resource requirements 05/17/23 07:54:51.585
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/17/23 07:54:51.587
STEP: Deleting the pod 05/17/23 07:54:53.591
STEP: Ensuring resource quota status released the pod usage 05/17/23 07:54:53.598
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 07:54:55.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7856" for this suite. 05/17/23 07:54:55.602
------------------------------
â€¢ [SLOW TEST] [13.052 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:42.553
    May 17 07:54:42.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 07:54:42.553
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:42.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:42.561
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 05/17/23 07:54:42.562
    STEP: Creating a ResourceQuota 05/17/23 07:54:47.564
    STEP: Ensuring resource quota status is calculated 05/17/23 07:54:47.567
    STEP: Creating a Pod that fits quota 05/17/23 07:54:49.571
    STEP: Ensuring ResourceQuota status captures the pod usage 05/17/23 07:54:49.58
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/17/23 07:54:51.582
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/17/23 07:54:51.584
    STEP: Ensuring a pod cannot update its resource requirements 05/17/23 07:54:51.585
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/17/23 07:54:51.587
    STEP: Deleting the pod 05/17/23 07:54:53.591
    STEP: Ensuring resource quota status released the pod usage 05/17/23 07:54:53.598
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 07:54:55.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7856" for this suite. 05/17/23 07:54:55.602
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:54:55.605
May 17 07:54:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 07:54:55.606
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:55.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:55.616
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 07:54:55.624
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:54:55.898
STEP: Deploying the webhook pod 05/17/23 07:54:55.903
STEP: Wait for the deployment to be ready 05/17/23 07:54:55.911
May 17 07:54:55.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 07:54:57.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/17/23 07:54:59.923
STEP: Verifying the service has paired with the endpoint 05/17/23 07:54:59.931
May 17 07:55:00.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/17/23 07:55:00.934
STEP: create a pod that should be updated by the webhook 05/17/23 07:55:00.944
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:55:00.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-23" for this suite. 05/17/23 07:55:00.977
STEP: Destroying namespace "webhook-23-markers" for this suite. 05/17/23 07:55:00.981
------------------------------
â€¢ [SLOW TEST] [5.379 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:54:55.605
    May 17 07:54:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 07:54:55.606
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:54:55.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:54:55.616
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 07:54:55.624
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 07:54:55.898
    STEP: Deploying the webhook pod 05/17/23 07:54:55.903
    STEP: Wait for the deployment to be ready 05/17/23 07:54:55.911
    May 17 07:54:55.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May 17 07:54:57.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 54, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/17/23 07:54:59.923
    STEP: Verifying the service has paired with the endpoint 05/17/23 07:54:59.931
    May 17 07:55:00.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/17/23 07:55:00.934
    STEP: create a pod that should be updated by the webhook 05/17/23 07:55:00.944
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:55:00.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-23" for this suite. 05/17/23 07:55:00.977
    STEP: Destroying namespace "webhook-23-markers" for this suite. 05/17/23 07:55:00.981
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:55:00.984
May 17 07:55:00.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename aggregator 05/17/23 07:55:00.985
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:55:00.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:55:00.994
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May 17 07:55:00.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/17/23 07:55:00.997
May 17 07:55:02.013: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 17 07:55:04.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:06.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:08.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:10.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:12.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:14.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:16.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:18.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:20.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:22.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:24.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:55:26.156: INFO: Waited 113.815824ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 05/17/23 07:55:26.183
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/17/23 07:55:26.185
STEP: List APIServices 05/17/23 07:55:26.189
May 17 07:55:26.194: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
May 17 07:55:26.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-8291" for this suite. 05/17/23 07:55:26.351
------------------------------
â€¢ [SLOW TEST] [25.415 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:55:00.984
    May 17 07:55:00.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename aggregator 05/17/23 07:55:00.985
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:55:00.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:55:00.994
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May 17 07:55:00.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/17/23 07:55:00.997
    May 17 07:55:02.013: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May 17 07:55:04.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:06.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:08.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:10.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:12.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:14.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:16.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:18.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:20.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:22.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:24.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 55, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:55:26.156: INFO: Waited 113.815824ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 05/17/23 07:55:26.183
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/17/23 07:55:26.185
    STEP: List APIServices 05/17/23 07:55:26.189
    May 17 07:55:26.194: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    May 17 07:55:26.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-8291" for this suite. 05/17/23 07:55:26.351
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:55:26.4
May 17 07:55:26.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 07:55:26.401
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:55:26.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:55:26.408
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4820 05/17/23 07:55:26.41
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 05/17/23 07:55:26.413
May 17 07:55:26.418: INFO: Found 0 stateful pods, waiting for 3
May 17 07:55:36.421: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:55:36.421: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:55:36.421: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/17/23 07:55:36.426
May 17 07:55:36.442: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/17/23 07:55:36.442
STEP: Not applying an update when the partition is greater than the number of replicas 05/17/23 07:55:46.453
STEP: Performing a canary update 05/17/23 07:55:46.453
May 17 07:55:46.469: INFO: Updating stateful set ss2
May 17 07:55:46.472: INFO: Waiting for Pod statefulset-4820/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 05/17/23 07:55:56.477
May 17 07:55:56.494: INFO: Found 2 stateful pods, waiting for 3
May 17 07:56:06.500: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:56:06.500: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:56:06.500: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/17/23 07:56:06.504
May 17 07:56:06.519: INFO: Updating stateful set ss2
May 17 07:56:06.522: INFO: Waiting for Pod statefulset-4820/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
May 17 07:56:16.545: INFO: Updating stateful set ss2
May 17 07:56:16.547: INFO: Waiting for StatefulSet statefulset-4820/ss2 to complete update
May 17 07:56:16.547: INFO: Waiting for Pod statefulset-4820/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 07:56:26.554: INFO: Deleting all statefulset in ns statefulset-4820
May 17 07:56:26.555: INFO: Scaling statefulset ss2 to 0
May 17 07:56:36.565: INFO: Waiting for statefulset status.replicas updated to 0
May 17 07:56:36.566: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 07:56:36.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4820" for this suite. 05/17/23 07:56:36.574
------------------------------
â€¢ [SLOW TEST] [70.177 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:55:26.4
    May 17 07:55:26.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 07:55:26.401
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:55:26.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:55:26.408
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4820 05/17/23 07:55:26.41
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 05/17/23 07:55:26.413
    May 17 07:55:26.418: INFO: Found 0 stateful pods, waiting for 3
    May 17 07:55:36.421: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:55:36.421: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:55:36.421: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/17/23 07:55:36.426
    May 17 07:55:36.442: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/17/23 07:55:36.442
    STEP: Not applying an update when the partition is greater than the number of replicas 05/17/23 07:55:46.453
    STEP: Performing a canary update 05/17/23 07:55:46.453
    May 17 07:55:46.469: INFO: Updating stateful set ss2
    May 17 07:55:46.472: INFO: Waiting for Pod statefulset-4820/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 05/17/23 07:55:56.477
    May 17 07:55:56.494: INFO: Found 2 stateful pods, waiting for 3
    May 17 07:56:06.500: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:56:06.500: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:56:06.500: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/17/23 07:56:06.504
    May 17 07:56:06.519: INFO: Updating stateful set ss2
    May 17 07:56:06.522: INFO: Waiting for Pod statefulset-4820/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    May 17 07:56:16.545: INFO: Updating stateful set ss2
    May 17 07:56:16.547: INFO: Waiting for StatefulSet statefulset-4820/ss2 to complete update
    May 17 07:56:16.547: INFO: Waiting for Pod statefulset-4820/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 07:56:26.554: INFO: Deleting all statefulset in ns statefulset-4820
    May 17 07:56:26.555: INFO: Scaling statefulset ss2 to 0
    May 17 07:56:36.565: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 07:56:36.566: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 07:56:36.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4820" for this suite. 05/17/23 07:56:36.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:56:36.577
May 17 07:56:36.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 07:56:36.578
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:36.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:36.585
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 05/17/23 07:56:36.587
STEP: Creating a ResourceQuota 05/17/23 07:56:41.589
STEP: Ensuring resource quota status is calculated 05/17/23 07:56:41.592
STEP: Creating a ReplicaSet 05/17/23 07:56:43.594
STEP: Ensuring resource quota status captures replicaset creation 05/17/23 07:56:43.602
STEP: Deleting a ReplicaSet 05/17/23 07:56:45.606
STEP: Ensuring resource quota status released usage 05/17/23 07:56:45.61
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 07:56:47.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2246" for this suite. 05/17/23 07:56:47.614
------------------------------
â€¢ [SLOW TEST] [11.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:56:36.577
    May 17 07:56:36.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 07:56:36.578
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:36.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:36.585
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 05/17/23 07:56:36.587
    STEP: Creating a ResourceQuota 05/17/23 07:56:41.589
    STEP: Ensuring resource quota status is calculated 05/17/23 07:56:41.592
    STEP: Creating a ReplicaSet 05/17/23 07:56:43.594
    STEP: Ensuring resource quota status captures replicaset creation 05/17/23 07:56:43.602
    STEP: Deleting a ReplicaSet 05/17/23 07:56:45.606
    STEP: Ensuring resource quota status released usage 05/17/23 07:56:45.61
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 07:56:47.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2246" for this suite. 05/17/23 07:56:47.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:56:47.618
May 17 07:56:47.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:56:47.618
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:47.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:47.627
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May 17 07:56:47.634: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7129 to be scheduled
May 17 07:56:47.635: INFO: 1 pods are not scheduled: [runtimeclass-7129/test-runtimeclass-runtimeclass-7129-preconfigured-handler-gw6zq(14fbef40-0a13-41d2-ac6f-17fa33a5f8be)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 17 07:56:49.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7129" for this suite. 05/17/23 07:56:49.642
------------------------------
â€¢ [2.027 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:56:47.618
    May 17 07:56:47.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:56:47.618
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:47.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:47.627
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May 17 07:56:47.634: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7129 to be scheduled
    May 17 07:56:47.635: INFO: 1 pods are not scheduled: [runtimeclass-7129/test-runtimeclass-runtimeclass-7129-preconfigured-handler-gw6zq(14fbef40-0a13-41d2-ac6f-17fa33a5f8be)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 17 07:56:49.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7129" for this suite. 05/17/23 07:56:49.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:56:49.647
May 17 07:56:49.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-pred 05/17/23 07:56:49.647
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:49.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:49.654
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 17 07:56:49.656: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 07:56:49.659: INFO: Waiting for terminating namespaces to be deleted...
May 17 07:56:49.660: INFO: 
Logging pods the apiserver thinks is on node k8s-node1 before test
May 17 07:56:49.663: INFO: calico-kube-controllers-57b57c56f-fwfmb from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.663: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 07:56:49.663: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.663: INFO: 	Container calico-node ready: true, restart count 0
May 17 07:56:49.663: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.663: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:56:49.663: INFO: test-runtimeclass-runtimeclass-7129-preconfigured-handler-gw6zq from runtimeclass-7129 started at 2023-05-17 07:56:47 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.663: INFO: 	Container test ready: false, restart count 0
May 17 07:56:49.663: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:56:49.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:56:49.663: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 07:56:49.663: INFO: webhook-to-be-mutated from webhook-23 started at 2023-05-17 07:55:00 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.663: INFO: 	Container example ready: false, restart count 0
May 17 07:56:49.663: INFO: 
Logging pods the apiserver thinks is on node k8s-node2 before test
May 17 07:56:49.666: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.666: INFO: 	Container calico-node ready: true, restart count 0
May 17 07:56:49.666: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.666: INFO: 	Container coredns ready: true, restart count 0
May 17 07:56:49.666: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.666: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:56:49.666: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
May 17 07:56:49.666: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 07:56:49.666: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:56:49.666: INFO: 	Container e2e ready: true, restart count 0
May 17 07:56:49.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:56:49.666: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:56:49.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:56:49.666: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 07:56:49.666
May 17 07:56:49.670: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5902" to be "running"
May 17 07:56:49.671: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.228415ms
May 17 07:56:51.675: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004518359s
May 17 07:56:51.675: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 07:56:51.676
STEP: Trying to apply a random label on the found node. 05/17/23 07:56:51.681
STEP: verifying the node has the label kubernetes.io/e2e-efd9c1f0-6632-4324-a022-296fb5ddfa6a 42 05/17/23 07:56:51.687
STEP: Trying to relaunch the pod, now with labels. 05/17/23 07:56:51.689
May 17 07:56:51.691: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5902" to be "not pending"
May 17 07:56:51.693: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.130031ms
May 17 07:56:53.696: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.00410376s
May 17 07:56:53.696: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-efd9c1f0-6632-4324-a022-296fb5ddfa6a off the node k8s-node1 05/17/23 07:56:53.697
STEP: verifying the node doesn't have the label kubernetes.io/e2e-efd9c1f0-6632-4324-a022-296fb5ddfa6a 05/17/23 07:56:53.704
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:56:53.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5902" for this suite. 05/17/23 07:56:53.707
------------------------------
â€¢ [4.063 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:56:49.647
    May 17 07:56:49.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-pred 05/17/23 07:56:49.647
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:49.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:49.654
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 17 07:56:49.656: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 07:56:49.659: INFO: Waiting for terminating namespaces to be deleted...
    May 17 07:56:49.660: INFO: 
    Logging pods the apiserver thinks is on node k8s-node1 before test
    May 17 07:56:49.663: INFO: calico-kube-controllers-57b57c56f-fwfmb from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.663: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May 17 07:56:49.663: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.663: INFO: 	Container calico-node ready: true, restart count 0
    May 17 07:56:49.663: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.663: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:56:49.663: INFO: test-runtimeclass-runtimeclass-7129-preconfigured-handler-gw6zq from runtimeclass-7129 started at 2023-05-17 07:56:47 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.663: INFO: 	Container test ready: false, restart count 0
    May 17 07:56:49.663: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:56:49.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:56:49.663: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 07:56:49.663: INFO: webhook-to-be-mutated from webhook-23 started at 2023-05-17 07:55:00 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.663: INFO: 	Container example ready: false, restart count 0
    May 17 07:56:49.663: INFO: 
    Logging pods the apiserver thinks is on node k8s-node2 before test
    May 17 07:56:49.666: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.666: INFO: 	Container calico-node ready: true, restart count 0
    May 17 07:56:49.666: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.666: INFO: 	Container coredns ready: true, restart count 0
    May 17 07:56:49.666: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.666: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:56:49.666: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
    May 17 07:56:49.666: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 07:56:49.666: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:56:49.666: INFO: 	Container e2e ready: true, restart count 0
    May 17 07:56:49.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:56:49.666: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:56:49.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:56:49.666: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 07:56:49.666
    May 17 07:56:49.670: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5902" to be "running"
    May 17 07:56:49.671: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.228415ms
    May 17 07:56:51.675: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004518359s
    May 17 07:56:51.675: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 07:56:51.676
    STEP: Trying to apply a random label on the found node. 05/17/23 07:56:51.681
    STEP: verifying the node has the label kubernetes.io/e2e-efd9c1f0-6632-4324-a022-296fb5ddfa6a 42 05/17/23 07:56:51.687
    STEP: Trying to relaunch the pod, now with labels. 05/17/23 07:56:51.689
    May 17 07:56:51.691: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5902" to be "not pending"
    May 17 07:56:51.693: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.130031ms
    May 17 07:56:53.696: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.00410376s
    May 17 07:56:53.696: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-efd9c1f0-6632-4324-a022-296fb5ddfa6a off the node k8s-node1 05/17/23 07:56:53.697
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-efd9c1f0-6632-4324-a022-296fb5ddfa6a 05/17/23 07:56:53.704
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:56:53.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5902" for this suite. 05/17/23 07:56:53.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:56:53.71
May 17 07:56:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 07:56:53.71
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:53.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:53.717
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-29ea212e-8e88-4158-9abc-fcf7d85f0311 05/17/23 07:56:53.719
STEP: Creating a pod to test consume configMaps 05/17/23 07:56:53.721
May 17 07:56:53.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a" in namespace "projected-1847" to be "Succeeded or Failed"
May 17 07:56:53.725: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.20375ms
May 17 07:56:55.728: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00394298s
May 17 07:56:57.728: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004395772s
STEP: Saw pod success 05/17/23 07:56:57.728
May 17 07:56:57.728: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a" satisfied condition "Succeeded or Failed"
May 17 07:56:57.730: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:56:57.738
May 17 07:56:57.743: INFO: Waiting for pod pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a to disappear
May 17 07:56:57.744: INFO: Pod pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 07:56:57.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1847" for this suite. 05/17/23 07:56:57.746
------------------------------
â€¢ [4.039 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:56:53.71
    May 17 07:56:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 07:56:53.71
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:53.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:53.717
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-29ea212e-8e88-4158-9abc-fcf7d85f0311 05/17/23 07:56:53.719
    STEP: Creating a pod to test consume configMaps 05/17/23 07:56:53.721
    May 17 07:56:53.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a" in namespace "projected-1847" to be "Succeeded or Failed"
    May 17 07:56:53.725: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.20375ms
    May 17 07:56:55.728: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00394298s
    May 17 07:56:57.728: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004395772s
    STEP: Saw pod success 05/17/23 07:56:57.728
    May 17 07:56:57.728: INFO: Pod "pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a" satisfied condition "Succeeded or Failed"
    May 17 07:56:57.730: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:56:57.738
    May 17 07:56:57.743: INFO: Waiting for pod pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a to disappear
    May 17 07:56:57.744: INFO: Pod pod-projected-configmaps-622bb097-16e5-4648-ad5d-7d25f9046a3a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 07:56:57.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1847" for this suite. 05/17/23 07:56:57.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:56:57.75
May 17 07:56:57.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-pred 05/17/23 07:56:57.751
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:57.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:57.758
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 17 07:56:57.760: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 07:56:57.763: INFO: Waiting for terminating namespaces to be deleted...
May 17 07:56:57.765: INFO: 
Logging pods the apiserver thinks is on node k8s-node1 before test
May 17 07:56:57.768: INFO: calico-kube-controllers-57b57c56f-fwfmb from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.768: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 07:56:57.768: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.768: INFO: 	Container calico-node ready: true, restart count 0
May 17 07:56:57.768: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.768: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:56:57.768: INFO: with-labels from sched-pred-5902 started at 2023-05-17 07:56:51 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.768: INFO: 	Container with-labels ready: true, restart count 0
May 17 07:56:57.768: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:56:57.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:56:57.768: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 07:56:57.768: INFO: webhook-to-be-mutated from webhook-23 started at 2023-05-17 07:55:00 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.768: INFO: 	Container example ready: false, restart count 0
May 17 07:56:57.768: INFO: 
Logging pods the apiserver thinks is on node k8s-node2 before test
May 17 07:56:57.771: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.771: INFO: 	Container calico-node ready: true, restart count 0
May 17 07:56:57.771: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.771: INFO: 	Container coredns ready: true, restart count 0
May 17 07:56:57.771: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.771: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:56:57.771: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
May 17 07:56:57.771: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 07:56:57.771: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:56:57.771: INFO: 	Container e2e ready: true, restart count 0
May 17 07:56:57.771: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:56:57.771: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 07:56:57.771: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:56:57.771: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node k8s-node1 05/17/23 07:56:57.782
STEP: verifying the node has the label node k8s-node2 05/17/23 07:56:57.79
May 17 07:56:57.797: INFO: Pod calico-kube-controllers-57b57c56f-fwfmb requesting resource cpu=0m on Node k8s-node1
May 17 07:56:57.797: INFO: Pod calico-node-cwcgg requesting resource cpu=250m on Node k8s-node2
May 17 07:56:57.797: INFO: Pod calico-node-vdnbq requesting resource cpu=250m on Node k8s-node1
May 17 07:56:57.797: INFO: Pod coredns-5bbd96d687-7h7tp requesting resource cpu=100m on Node k8s-node2
May 17 07:56:57.797: INFO: Pod kube-proxy-stkj5 requesting resource cpu=0m on Node k8s-node2
May 17 07:56:57.797: INFO: Pod kube-proxy-t87gs requesting resource cpu=0m on Node k8s-node1
May 17 07:56:57.797: INFO: Pod with-labels requesting resource cpu=0m on Node k8s-node1
May 17 07:56:57.797: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-node2
May 17 07:56:57.797: INFO: Pod sonobuoy-e2e-job-874306d8c9804b9b requesting resource cpu=0m on Node k8s-node2
May 17 07:56:57.797: INFO: Pod sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 requesting resource cpu=0m on Node k8s-node2
May 17 07:56:57.797: INFO: Pod sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d requesting resource cpu=0m on Node k8s-node1
May 17 07:56:57.797: INFO: Pod webhook-to-be-mutated requesting resource cpu=0m on Node k8s-node1
STEP: Starting Pods to consume most of the cluster CPU. 05/17/23 07:56:57.797
May 17 07:56:57.797: INFO: Creating a pod which consumes cpu=5425m on Node k8s-node1
May 17 07:56:57.801: INFO: Creating a pod which consumes cpu=5355m on Node k8s-node2
May 17 07:56:57.803: INFO: Waiting up to 5m0s for pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87" in namespace "sched-pred-2061" to be "running"
May 17 07:56:57.804: INFO: Pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.084843ms
May 17 07:56:59.807: INFO: Pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87": Phase="Running", Reason="", readiness=true. Elapsed: 2.003395671s
May 17 07:56:59.807: INFO: Pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87" satisfied condition "running"
May 17 07:56:59.807: INFO: Waiting up to 5m0s for pod "filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd" in namespace "sched-pred-2061" to be "running"
May 17 07:56:59.808: INFO: Pod "filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd": Phase="Running", Reason="", readiness=true. Elapsed: 1.232681ms
May 17 07:56:59.808: INFO: Pod "filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/17/23 07:56:59.808
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf381fd3c9c8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2061/filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87 to k8s-node1] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf383f0b94c6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf383fe4681c], Reason = [Created], Message = [Created container filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf38440246ab], Reason = [Started], Message = [Started container filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf381fef17b1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2061/filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd to k8s-node2] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf383dfff997], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf383f5bed50], Reason = [Created], Message = [Created container filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf384355489e], Reason = [Started], Message = [Started container filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd] 05/17/23 07:56:59.81
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175fdf38979ad6e6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 05/17/23 07:56:59.817
STEP: removing the label node off the node k8s-node1 05/17/23 07:57:00.817
STEP: verifying the node doesn't have the label node 05/17/23 07:57:00.824
STEP: removing the label node off the node k8s-node2 05/17/23 07:57:00.825
STEP: verifying the node doesn't have the label node 05/17/23 07:57:00.832
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:57:00.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2061" for this suite. 05/17/23 07:57:00.835
------------------------------
â€¢ [3.087 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:56:57.75
    May 17 07:56:57.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-pred 05/17/23 07:56:57.751
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:56:57.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:56:57.758
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 17 07:56:57.760: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 07:56:57.763: INFO: Waiting for terminating namespaces to be deleted...
    May 17 07:56:57.765: INFO: 
    Logging pods the apiserver thinks is on node k8s-node1 before test
    May 17 07:56:57.768: INFO: calico-kube-controllers-57b57c56f-fwfmb from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.768: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May 17 07:56:57.768: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.768: INFO: 	Container calico-node ready: true, restart count 0
    May 17 07:56:57.768: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.768: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:56:57.768: INFO: with-labels from sched-pred-5902 started at 2023-05-17 07:56:51 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.768: INFO: 	Container with-labels ready: true, restart count 0
    May 17 07:56:57.768: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:56:57.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:56:57.768: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 07:56:57.768: INFO: webhook-to-be-mutated from webhook-23 started at 2023-05-17 07:55:00 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.768: INFO: 	Container example ready: false, restart count 0
    May 17 07:56:57.768: INFO: 
    Logging pods the apiserver thinks is on node k8s-node2 before test
    May 17 07:56:57.771: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.771: INFO: 	Container calico-node ready: true, restart count 0
    May 17 07:56:57.771: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.771: INFO: 	Container coredns ready: true, restart count 0
    May 17 07:56:57.771: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.771: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:56:57.771: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
    May 17 07:56:57.771: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 07:56:57.771: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:56:57.771: INFO: 	Container e2e ready: true, restart count 0
    May 17 07:56:57.771: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:56:57.771: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 07:56:57.771: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:56:57.771: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node k8s-node1 05/17/23 07:56:57.782
    STEP: verifying the node has the label node k8s-node2 05/17/23 07:56:57.79
    May 17 07:56:57.797: INFO: Pod calico-kube-controllers-57b57c56f-fwfmb requesting resource cpu=0m on Node k8s-node1
    May 17 07:56:57.797: INFO: Pod calico-node-cwcgg requesting resource cpu=250m on Node k8s-node2
    May 17 07:56:57.797: INFO: Pod calico-node-vdnbq requesting resource cpu=250m on Node k8s-node1
    May 17 07:56:57.797: INFO: Pod coredns-5bbd96d687-7h7tp requesting resource cpu=100m on Node k8s-node2
    May 17 07:56:57.797: INFO: Pod kube-proxy-stkj5 requesting resource cpu=0m on Node k8s-node2
    May 17 07:56:57.797: INFO: Pod kube-proxy-t87gs requesting resource cpu=0m on Node k8s-node1
    May 17 07:56:57.797: INFO: Pod with-labels requesting resource cpu=0m on Node k8s-node1
    May 17 07:56:57.797: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-node2
    May 17 07:56:57.797: INFO: Pod sonobuoy-e2e-job-874306d8c9804b9b requesting resource cpu=0m on Node k8s-node2
    May 17 07:56:57.797: INFO: Pod sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 requesting resource cpu=0m on Node k8s-node2
    May 17 07:56:57.797: INFO: Pod sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d requesting resource cpu=0m on Node k8s-node1
    May 17 07:56:57.797: INFO: Pod webhook-to-be-mutated requesting resource cpu=0m on Node k8s-node1
    STEP: Starting Pods to consume most of the cluster CPU. 05/17/23 07:56:57.797
    May 17 07:56:57.797: INFO: Creating a pod which consumes cpu=5425m on Node k8s-node1
    May 17 07:56:57.801: INFO: Creating a pod which consumes cpu=5355m on Node k8s-node2
    May 17 07:56:57.803: INFO: Waiting up to 5m0s for pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87" in namespace "sched-pred-2061" to be "running"
    May 17 07:56:57.804: INFO: Pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.084843ms
    May 17 07:56:59.807: INFO: Pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87": Phase="Running", Reason="", readiness=true. Elapsed: 2.003395671s
    May 17 07:56:59.807: INFO: Pod "filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87" satisfied condition "running"
    May 17 07:56:59.807: INFO: Waiting up to 5m0s for pod "filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd" in namespace "sched-pred-2061" to be "running"
    May 17 07:56:59.808: INFO: Pod "filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd": Phase="Running", Reason="", readiness=true. Elapsed: 1.232681ms
    May 17 07:56:59.808: INFO: Pod "filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/17/23 07:56:59.808
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf381fd3c9c8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2061/filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87 to k8s-node1] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf383f0b94c6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf383fe4681c], Reason = [Created], Message = [Created container filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87.175fdf38440246ab], Reason = [Started], Message = [Started container filler-pod-c11b4ba4-c6f5-4a68-baef-b16648140b87] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf381fef17b1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2061/filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd to k8s-node2] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf383dfff997], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf383f5bed50], Reason = [Created], Message = [Created container filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd.175fdf384355489e], Reason = [Started], Message = [Started container filler-pod-ef3f3a28-0095-4c7f-b916-4bc93f1370bd] 05/17/23 07:56:59.81
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175fdf38979ad6e6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 05/17/23 07:56:59.817
    STEP: removing the label node off the node k8s-node1 05/17/23 07:57:00.817
    STEP: verifying the node doesn't have the label node 05/17/23 07:57:00.824
    STEP: removing the label node off the node k8s-node2 05/17/23 07:57:00.825
    STEP: verifying the node doesn't have the label node 05/17/23 07:57:00.832
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:57:00.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2061" for this suite. 05/17/23 07:57:00.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:57:00.838
May 17 07:57:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 07:57:00.839
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:00.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:00.845
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-3486 05/17/23 07:57:00.847
STEP: creating replication controller nodeport-test in namespace services-3486 05/17/23 07:57:00.857
I0517 07:57:00.861100      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3486, replica count: 2
I0517 07:57:03.912764      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 07:57:03.912: INFO: Creating new exec pod
May 17 07:57:03.917: INFO: Waiting up to 5m0s for pod "execpodgwgsq" in namespace "services-3486" to be "running"
May 17 07:57:03.918: INFO: Pod "execpodgwgsq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48046ms
May 17 07:57:05.921: INFO: Pod "execpodgwgsq": Phase="Running", Reason="", readiness=true. Elapsed: 2.003831696s
May 17 07:57:05.921: INFO: Pod "execpodgwgsq" satisfied condition "running"
May 17 07:57:06.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
May 17 07:57:07.048: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 17 07:57:07.048: INFO: stdout: ""
May 17 07:57:07.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 10.108.90.127 80'
May 17 07:57:07.154: INFO: stderr: "+ nc -v -z -w 2 10.108.90.127 80\nConnection to 10.108.90.127 80 port [tcp/http] succeeded!\n"
May 17 07:57:07.154: INFO: stdout: ""
May 17 07:57:07.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 32052'
May 17 07:57:07.254: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 32052\nConnection to 10.0.79.210 32052 port [tcp/*] succeeded!\n"
May 17 07:57:07.254: INFO: stdout: ""
May 17 07:57:07.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 32052'
May 17 07:57:07.363: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 32052\nConnection to 10.0.79.211 32052 port [tcp/*] succeeded!\n"
May 17 07:57:07.363: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 07:57:07.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3486" for this suite. 05/17/23 07:57:07.365
------------------------------
â€¢ [SLOW TEST] [6.530 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:57:00.838
    May 17 07:57:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 07:57:00.839
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:00.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:00.845
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-3486 05/17/23 07:57:00.847
    STEP: creating replication controller nodeport-test in namespace services-3486 05/17/23 07:57:00.857
    I0517 07:57:00.861100      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3486, replica count: 2
    I0517 07:57:03.912764      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 07:57:03.912: INFO: Creating new exec pod
    May 17 07:57:03.917: INFO: Waiting up to 5m0s for pod "execpodgwgsq" in namespace "services-3486" to be "running"
    May 17 07:57:03.918: INFO: Pod "execpodgwgsq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48046ms
    May 17 07:57:05.921: INFO: Pod "execpodgwgsq": Phase="Running", Reason="", readiness=true. Elapsed: 2.003831696s
    May 17 07:57:05.921: INFO: Pod "execpodgwgsq" satisfied condition "running"
    May 17 07:57:06.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    May 17 07:57:07.048: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May 17 07:57:07.048: INFO: stdout: ""
    May 17 07:57:07.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 10.108.90.127 80'
    May 17 07:57:07.154: INFO: stderr: "+ nc -v -z -w 2 10.108.90.127 80\nConnection to 10.108.90.127 80 port [tcp/http] succeeded!\n"
    May 17 07:57:07.154: INFO: stdout: ""
    May 17 07:57:07.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 32052'
    May 17 07:57:07.254: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 32052\nConnection to 10.0.79.210 32052 port [tcp/*] succeeded!\n"
    May 17 07:57:07.254: INFO: stdout: ""
    May 17 07:57:07.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3486 exec execpodgwgsq -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 32052'
    May 17 07:57:07.363: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 32052\nConnection to 10.0.79.211 32052 port [tcp/*] succeeded!\n"
    May 17 07:57:07.363: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 07:57:07.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3486" for this suite. 05/17/23 07:57:07.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:57:07.368
May 17 07:57:07.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename security-context-test 05/17/23 07:57:07.369
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:07.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:07.377
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
May 17 07:57:07.383: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453" in namespace "security-context-test-9915" to be "Succeeded or Failed"
May 17 07:57:07.384: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453": Phase="Pending", Reason="", readiness=false. Elapsed: 1.219389ms
May 17 07:57:09.387: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004170035s
May 17 07:57:11.387: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004562221s
May 17 07:57:11.387: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 17 07:57:11.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9915" for this suite. 05/17/23 07:57:11.394
------------------------------
â€¢ [4.029 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:57:07.368
    May 17 07:57:07.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename security-context-test 05/17/23 07:57:07.369
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:07.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:07.377
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    May 17 07:57:07.383: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453" in namespace "security-context-test-9915" to be "Succeeded or Failed"
    May 17 07:57:07.384: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453": Phase="Pending", Reason="", readiness=false. Elapsed: 1.219389ms
    May 17 07:57:09.387: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004170035s
    May 17 07:57:11.387: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004562221s
    May 17 07:57:11.387: INFO: Pod "alpine-nnp-false-dfe9cb94-b1d6-4141-977f-8f9661096453" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 17 07:57:11.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9915" for this suite. 05/17/23 07:57:11.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:57:11.398
May 17 07:57:11.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 07:57:11.399
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:11.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:11.406
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 05/17/23 07:57:11.411
STEP: watching for Pod to be ready 05/17/23 07:57:11.415
May 17 07:57:11.415: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 17 07:57:11.417: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
May 17 07:57:11.423: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
May 17 07:57:11.856: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
May 17 07:57:12.740: INFO: Found Pod pod-test in namespace pods-4783 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/17/23 07:57:12.742
STEP: getting the Pod and ensuring that it's patched 05/17/23 07:57:12.748
STEP: replacing the Pod's status Ready condition to False 05/17/23 07:57:12.75
STEP: check the Pod again to ensure its Ready conditions are False 05/17/23 07:57:12.756
STEP: deleting the Pod via a Collection with a LabelSelector 05/17/23 07:57:12.756
STEP: watching for the Pod to be deleted 05/17/23 07:57:12.76
May 17 07:57:12.761: INFO: observed event type MODIFIED
May 17 07:57:13.861: INFO: observed event type MODIFIED
May 17 07:57:14.862: INFO: observed event type MODIFIED
May 17 07:57:15.752: INFO: observed event type MODIFIED
May 17 07:57:15.755: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 07:57:15.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4783" for this suite. 05/17/23 07:57:15.76
------------------------------
â€¢ [4.365 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:57:11.398
    May 17 07:57:11.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 07:57:11.399
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:11.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:11.406
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 05/17/23 07:57:11.411
    STEP: watching for Pod to be ready 05/17/23 07:57:11.415
    May 17 07:57:11.415: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May 17 07:57:11.417: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
    May 17 07:57:11.423: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
    May 17 07:57:11.856: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
    May 17 07:57:12.740: INFO: Found Pod pod-test in namespace pods-4783 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 07:57:11 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/17/23 07:57:12.742
    STEP: getting the Pod and ensuring that it's patched 05/17/23 07:57:12.748
    STEP: replacing the Pod's status Ready condition to False 05/17/23 07:57:12.75
    STEP: check the Pod again to ensure its Ready conditions are False 05/17/23 07:57:12.756
    STEP: deleting the Pod via a Collection with a LabelSelector 05/17/23 07:57:12.756
    STEP: watching for the Pod to be deleted 05/17/23 07:57:12.76
    May 17 07:57:12.761: INFO: observed event type MODIFIED
    May 17 07:57:13.861: INFO: observed event type MODIFIED
    May 17 07:57:14.862: INFO: observed event type MODIFIED
    May 17 07:57:15.752: INFO: observed event type MODIFIED
    May 17 07:57:15.755: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 07:57:15.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4783" for this suite. 05/17/23 07:57:15.76
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:57:15.763
May 17 07:57:15.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:57:15.764
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:15.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:15.771
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
May 17 07:57:15.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 07:57:17.101
May 17 07:57:17.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 create -f -'
May 17 07:57:17.534: INFO: stderr: ""
May 17 07:57:17.534: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 17 07:57:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 delete e2e-test-crd-publish-openapi-2667-crds test-cr'
May 17 07:57:17.590: INFO: stderr: ""
May 17 07:57:17.590: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 17 07:57:17.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 apply -f -'
May 17 07:57:17.749: INFO: stderr: ""
May 17 07:57:17.749: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 17 07:57:17.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 delete e2e-test-crd-publish-openapi-2667-crds test-cr'
May 17 07:57:17.803: INFO: stderr: ""
May 17 07:57:17.803: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/17/23 07:57:17.803
May 17 07:57:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 explain e2e-test-crd-publish-openapi-2667-crds'
May 17 07:57:17.965: INFO: stderr: ""
May 17 07:57:17.965: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2667-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 07:57:19.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8616" for this suite. 05/17/23 07:57:19.306
------------------------------
â€¢ [3.547 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:57:15.763
    May 17 07:57:15.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:57:15.764
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:15.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:15.771
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    May 17 07:57:15.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 07:57:17.101
    May 17 07:57:17.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 create -f -'
    May 17 07:57:17.534: INFO: stderr: ""
    May 17 07:57:17.534: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May 17 07:57:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 delete e2e-test-crd-publish-openapi-2667-crds test-cr'
    May 17 07:57:17.590: INFO: stderr: ""
    May 17 07:57:17.590: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May 17 07:57:17.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 apply -f -'
    May 17 07:57:17.749: INFO: stderr: ""
    May 17 07:57:17.749: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May 17 07:57:17.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 --namespace=crd-publish-openapi-8616 delete e2e-test-crd-publish-openapi-2667-crds test-cr'
    May 17 07:57:17.803: INFO: stderr: ""
    May 17 07:57:17.803: INFO: stdout: "e2e-test-crd-publish-openapi-2667-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/17/23 07:57:17.803
    May 17 07:57:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-8616 explain e2e-test-crd-publish-openapi-2667-crds'
    May 17 07:57:17.965: INFO: stderr: ""
    May 17 07:57:17.965: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2667-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 07:57:19.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8616" for this suite. 05/17/23 07:57:19.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:57:19.311
May 17 07:57:19.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename job 05/17/23 07:57:19.312
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:19.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:19.321
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 05/17/23 07:57:19.324
STEP: Patching the Job 05/17/23 07:57:19.327
STEP: Watching for Job to be patched 05/17/23 07:57:19.331
May 17 07:57:19.332: INFO: Event ADDED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking:]
May 17 07:57:19.332: INFO: Event MODIFIED found for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/17/23 07:57:19.332
STEP: Watching for Job to be updated 05/17/23 07:57:19.354
May 17 07:57:19.355: INFO: Event MODIFIED found for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:57:19.355: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/17/23 07:57:19.355
May 17 07:57:19.356: INFO: Job: e2e-b5fhg as labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg]
STEP: Waiting for job to complete 05/17/23 07:57:19.356
STEP: Delete a job collection with a labelselector 05/17/23 07:57:27.359
STEP: Watching for Job to be deleted 05/17/23 07:57:27.362
May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:57:27.364: INFO: Event DELETED found for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/17/23 07:57:27.364
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 17 07:57:27.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8950" for this suite. 05/17/23 07:57:27.369
------------------------------
â€¢ [SLOW TEST] [8.061 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:57:19.311
    May 17 07:57:19.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename job 05/17/23 07:57:19.312
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:19.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:19.321
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 05/17/23 07:57:19.324
    STEP: Patching the Job 05/17/23 07:57:19.327
    STEP: Watching for Job to be patched 05/17/23 07:57:19.331
    May 17 07:57:19.332: INFO: Event ADDED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking:]
    May 17 07:57:19.332: INFO: Event MODIFIED found for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/17/23 07:57:19.332
    STEP: Watching for Job to be updated 05/17/23 07:57:19.354
    May 17 07:57:19.355: INFO: Event MODIFIED found for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:57:19.355: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/17/23 07:57:19.355
    May 17 07:57:19.356: INFO: Job: e2e-b5fhg as labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg]
    STEP: Waiting for job to complete 05/17/23 07:57:19.356
    STEP: Delete a job collection with a labelselector 05/17/23 07:57:27.359
    STEP: Watching for Job to be deleted 05/17/23 07:57:27.362
    May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:57:27.364: INFO: Event MODIFIED observed for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:57:27.364: INFO: Event DELETED found for Job e2e-b5fhg in namespace job-8950 with labels: map[e2e-b5fhg:patched e2e-job-label:e2e-b5fhg] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/17/23 07:57:27.364
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 17 07:57:27.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8950" for this suite. 05/17/23 07:57:27.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:57:27.372
May 17 07:57:27.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 07:57:27.373
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:27.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:27.383
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/17/23 07:57:27.385
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9807;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9807;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +notcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_udp@PTR;check="$$(dig +tcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_tcp@PTR;sleep 1; done
 05/17/23 07:57:27.395
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9807;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9807;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +notcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_udp@PTR;check="$$(dig +tcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_tcp@PTR;sleep 1; done
 05/17/23 07:57:27.395
STEP: creating a pod to probe DNS 05/17/23 07:57:27.395
STEP: submitting the pod to kubernetes 05/17/23 07:57:27.395
May 17 07:57:27.400: INFO: Waiting up to 15m0s for pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970" in namespace "dns-9807" to be "running"
May 17 07:57:27.401: INFO: Pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298393ms
May 17 07:57:29.403: INFO: Pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970": Phase="Running", Reason="", readiness=true. Elapsed: 2.003569123s
May 17 07:57:29.403: INFO: Pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970" satisfied condition "running"
STEP: retrieving the pod 05/17/23 07:57:29.403
STEP: looking for the results for each expected name from probers 05/17/23 07:57:29.405
May 17 07:57:29.407: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.408: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.410: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.411: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.413: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.414: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.416: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.417: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.425: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.426: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.427: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.430: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.431: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.433: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.434: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:29.439: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

May 17 07:57:34.443: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.445: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.449: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.452: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.454: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.461: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.462: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.464: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.465: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.467: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.468: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.469: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.471: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:34.476: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

May 17 07:57:39.443: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.445: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.446: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.449: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.452: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.453: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.460: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.461: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.462: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.464: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.465: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.468: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.469: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:39.475: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

May 17 07:57:44.444: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.454: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.461: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.462: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.463: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.465: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.466: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.469: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.470: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:44.476: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

May 17 07:57:49.444: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.452: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.462: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.464: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.465: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.468: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.471: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.472: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:49.478: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

May 17 07:57:54.444: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.451: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.452: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.454: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.463: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.464: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.466: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.469: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.472: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.473: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
May 17 07:57:54.480: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

May 17 07:57:59.476: INFO: DNS probes using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 succeeded

STEP: deleting the pod 05/17/23 07:57:59.476
STEP: deleting the test service 05/17/23 07:57:59.484
STEP: deleting the test headless service 05/17/23 07:57:59.497
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 07:57:59.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9807" for this suite. 05/17/23 07:57:59.504
------------------------------
â€¢ [SLOW TEST] [32.135 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:57:27.372
    May 17 07:57:27.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 07:57:27.373
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:27.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:27.383
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/17/23 07:57:27.385
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9807;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9807;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +notcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_udp@PTR;check="$$(dig +tcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_tcp@PTR;sleep 1; done
     05/17/23 07:57:27.395
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9807;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9807;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9807.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9807.svc;check="$$(dig +notcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_udp@PTR;check="$$(dig +tcp +noall +answer +search 35.116.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.116.35_tcp@PTR;sleep 1; done
     05/17/23 07:57:27.395
    STEP: creating a pod to probe DNS 05/17/23 07:57:27.395
    STEP: submitting the pod to kubernetes 05/17/23 07:57:27.395
    May 17 07:57:27.400: INFO: Waiting up to 15m0s for pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970" in namespace "dns-9807" to be "running"
    May 17 07:57:27.401: INFO: Pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298393ms
    May 17 07:57:29.403: INFO: Pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970": Phase="Running", Reason="", readiness=true. Elapsed: 2.003569123s
    May 17 07:57:29.403: INFO: Pod "dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 07:57:29.403
    STEP: looking for the results for each expected name from probers 05/17/23 07:57:29.405
    May 17 07:57:29.407: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.408: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.410: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.411: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.413: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.414: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.416: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.417: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.425: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.426: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.427: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.430: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.431: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.433: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.434: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:29.439: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

    May 17 07:57:34.443: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.445: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.449: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.452: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.454: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.461: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.462: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.464: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.465: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.467: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.468: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.469: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.471: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:34.476: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

    May 17 07:57:39.443: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.445: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.446: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.449: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.452: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.453: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.460: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.461: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.462: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.464: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.465: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.468: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.469: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:39.475: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

    May 17 07:57:44.444: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.454: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.461: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.462: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.463: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.465: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.466: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.469: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.470: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:44.476: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

    May 17 07:57:49.444: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.452: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.462: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.464: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.465: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.468: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.471: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.472: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:49.478: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

    May 17 07:57:54.444: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.451: INFO: Unable to read wheezy_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.452: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.454: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.463: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.464: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.466: INFO: Unable to read jessie_udp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807 from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.469: INFO: Unable to read jessie_udp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.472: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.473: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc from pod dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970: the server could not find the requested resource (get pods dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970)
    May 17 07:57:54.480: INFO: Lookups using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9807 wheezy_tcp@dns-test-service.dns-9807 wheezy_udp@dns-test-service.dns-9807.svc wheezy_tcp@dns-test-service.dns-9807.svc wheezy_udp@_http._tcp.dns-test-service.dns-9807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9807 jessie_tcp@dns-test-service.dns-9807 jessie_udp@dns-test-service.dns-9807.svc jessie_tcp@dns-test-service.dns-9807.svc jessie_udp@_http._tcp.dns-test-service.dns-9807.svc jessie_tcp@_http._tcp.dns-test-service.dns-9807.svc]

    May 17 07:57:59.476: INFO: DNS probes using dns-9807/dns-test-7444a835-a0a8-4ee1-a1ce-42501499e970 succeeded

    STEP: deleting the pod 05/17/23 07:57:59.476
    STEP: deleting the test service 05/17/23 07:57:59.484
    STEP: deleting the test headless service 05/17/23 07:57:59.497
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 07:57:59.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9807" for this suite. 05/17/23 07:57:59.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:57:59.508
May 17 07:57:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:57:59.509
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:59.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:59.517
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May 17 07:57:59.526: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2064 to be scheduled
May 17 07:57:59.528: INFO: 1 pods are not scheduled: [runtimeclass-2064/test-runtimeclass-runtimeclass-2064-preconfigured-handler-nblpw(14700400-3418-4913-98e8-2effce199380)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 17 07:58:01.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2064" for this suite. 05/17/23 07:58:01.536
------------------------------
â€¢ [2.031 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:57:59.508
    May 17 07:57:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:57:59.509
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:57:59.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:57:59.517
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May 17 07:57:59.526: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2064 to be scheduled
    May 17 07:57:59.528: INFO: 1 pods are not scheduled: [runtimeclass-2064/test-runtimeclass-runtimeclass-2064-preconfigured-handler-nblpw(14700400-3418-4913-98e8-2effce199380)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:01.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2064" for this suite. 05/17/23 07:58:01.536
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:01.539
May 17 07:58:01.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 07:58:01.54
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:01.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:01.548
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May 17 07:58:01.550: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 17 07:58:01.554: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 07:58:06.557: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 07:58:06.557
May 17 07:58:06.557: INFO: Creating deployment "test-rolling-update-deployment"
May 17 07:58:06.561: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 17 07:58:06.563: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 17 07:58:08.568: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 17 07:58:08.570: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 07:58:08.573: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1164  2ff0a014-2dd8-4035-bd4c-44c561debeca 1192423 1 2023-05-17 07:58:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-17 07:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029984d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 07:58:06 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-17 07:58:07 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 07:58:08.575: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1164  b87611d3-2e55-468c-b961-4dc7e45d721e 1192413 1 2023-05-17 07:58:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2ff0a014-2dd8-4035-bd4c-44c561debeca 0xc000d6cf17 0xc000d6cf18}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ff0a014-2dd8-4035-bd4c-44c561debeca\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d6d028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 07:58:08.575: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 17 07:58:08.575: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1164  a7e69f02-62e4-4eb9-87cf-9232d7413ed8 1192422 2 2023-05-17 07:58:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2ff0a014-2dd8-4035-bd4c-44c561debeca 0xc000d6c897 0xc000d6c898}] [] [{e2e.test Update apps/v1 2023-05-17 07:58:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ff0a014-2dd8-4035-bd4c-44c561debeca\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000d6cb48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 07:58:08.577: INFO: Pod "test-rolling-update-deployment-7549d9f46d-6htvh" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-6htvh test-rolling-update-deployment-7549d9f46d- deployment-1164  a3fc9d9e-7a6c-4352-9007-6bb335c13580 1192412 0 2023-05-17 07:58:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:658f160c9c37c0f176d08207a8881f39f77d3828f8d18ece759361d6945f035a cni.projectcalico.org/podIP:192.168.36.111/32 cni.projectcalico.org/podIPs:192.168.36.111/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d b87611d3-2e55-468c-b961-4dc7e45d721e 0xc004da9b77 0xc004da9b78}] [] [{kube-controller-manager Update v1 2023-05-17 07:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b87611d3-2e55-468c-b961-4dc7e45d721e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q42gz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q42gz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.111,StartTime:2023-05-17 07:58:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:58:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://4a753a99d72a1f53d2868028a012e1839a490993cb2421e692234522b069d919,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 07:58:08.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1164" for this suite. 05/17/23 07:58:08.579
------------------------------
â€¢ [SLOW TEST] [7.042 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:01.539
    May 17 07:58:01.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 07:58:01.54
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:01.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:01.548
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May 17 07:58:01.550: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    May 17 07:58:01.554: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 07:58:06.557: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 07:58:06.557
    May 17 07:58:06.557: INFO: Creating deployment "test-rolling-update-deployment"
    May 17 07:58:06.561: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May 17 07:58:06.563: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May 17 07:58:08.568: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May 17 07:58:08.570: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 07:58:08.573: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1164  2ff0a014-2dd8-4035-bd4c-44c561debeca 1192423 1 2023-05-17 07:58:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-17 07:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029984d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 07:58:06 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-17 07:58:07 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 07:58:08.575: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1164  b87611d3-2e55-468c-b961-4dc7e45d721e 1192413 1 2023-05-17 07:58:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2ff0a014-2dd8-4035-bd4c-44c561debeca 0xc000d6cf17 0xc000d6cf18}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ff0a014-2dd8-4035-bd4c-44c561debeca\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d6d028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 07:58:08.575: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May 17 07:58:08.575: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1164  a7e69f02-62e4-4eb9-87cf-9232d7413ed8 1192422 2 2023-05-17 07:58:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2ff0a014-2dd8-4035-bd4c-44c561debeca 0xc000d6c897 0xc000d6c898}] [] [{e2e.test Update apps/v1 2023-05-17 07:58:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ff0a014-2dd8-4035-bd4c-44c561debeca\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000d6cb48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 07:58:08.577: INFO: Pod "test-rolling-update-deployment-7549d9f46d-6htvh" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-6htvh test-rolling-update-deployment-7549d9f46d- deployment-1164  a3fc9d9e-7a6c-4352-9007-6bb335c13580 1192412 0 2023-05-17 07:58:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:658f160c9c37c0f176d08207a8881f39f77d3828f8d18ece759361d6945f035a cni.projectcalico.org/podIP:192.168.36.111/32 cni.projectcalico.org/podIPs:192.168.36.111/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d b87611d3-2e55-468c-b961-4dc7e45d721e 0xc004da9b77 0xc004da9b78}] [] [{kube-controller-manager Update v1 2023-05-17 07:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b87611d3-2e55-468c-b961-4dc7e45d721e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 07:58:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q42gz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q42gz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:58:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.111,StartTime:2023-05-17 07:58:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:58:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://4a753a99d72a1f53d2868028a012e1839a490993cb2421e692234522b069d919,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:08.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1164" for this suite. 05/17/23 07:58:08.579
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:08.582
May 17 07:58:08.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename podtemplate 05/17/23 07:58:08.582
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:08.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:08.591
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/17/23 07:58:08.592
STEP: Replace a pod template 05/17/23 07:58:08.595
May 17 07:58:08.599: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May 17 07:58:08.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8668" for this suite. 05/17/23 07:58:08.601
------------------------------
â€¢ [0.022 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:08.582
    May 17 07:58:08.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename podtemplate 05/17/23 07:58:08.582
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:08.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:08.591
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/17/23 07:58:08.592
    STEP: Replace a pod template 05/17/23 07:58:08.595
    May 17 07:58:08.599: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:08.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8668" for this suite. 05/17/23 07:58:08.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:08.603
May 17 07:58:08.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 07:58:08.604
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:08.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:08.611
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-97c95a2f-ef76-44e2-a7de-6b2add8b80f7 05/17/23 07:58:08.62
STEP: Creating a pod to test consume secrets 05/17/23 07:58:08.624
May 17 07:58:08.627: INFO: Waiting up to 5m0s for pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8" in namespace "secrets-6211" to be "Succeeded or Failed"
May 17 07:58:08.628: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200896ms
May 17 07:58:10.631: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00416266s
May 17 07:58:12.631: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00335981s
STEP: Saw pod success 05/17/23 07:58:12.631
May 17 07:58:12.631: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8" satisfied condition "Succeeded or Failed"
May 17 07:58:12.632: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 07:58:12.635
May 17 07:58:12.642: INFO: Waiting for pod pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8 to disappear
May 17 07:58:12.643: INFO: Pod pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 07:58:12.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6211" for this suite. 05/17/23 07:58:12.645
STEP: Destroying namespace "secret-namespace-2728" for this suite. 05/17/23 07:58:12.648
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:08.603
    May 17 07:58:08.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 07:58:08.604
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:08.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:08.611
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-97c95a2f-ef76-44e2-a7de-6b2add8b80f7 05/17/23 07:58:08.62
    STEP: Creating a pod to test consume secrets 05/17/23 07:58:08.624
    May 17 07:58:08.627: INFO: Waiting up to 5m0s for pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8" in namespace "secrets-6211" to be "Succeeded or Failed"
    May 17 07:58:08.628: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200896ms
    May 17 07:58:10.631: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00416266s
    May 17 07:58:12.631: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00335981s
    STEP: Saw pod success 05/17/23 07:58:12.631
    May 17 07:58:12.631: INFO: Pod "pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8" satisfied condition "Succeeded or Failed"
    May 17 07:58:12.632: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 07:58:12.635
    May 17 07:58:12.642: INFO: Waiting for pod pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8 to disappear
    May 17 07:58:12.643: INFO: Pod pod-secrets-b9df8910-ce9f-4f3c-b854-20a84876aec8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:12.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6211" for this suite. 05/17/23 07:58:12.645
    STEP: Destroying namespace "secret-namespace-2728" for this suite. 05/17/23 07:58:12.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:12.651
May 17 07:58:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename csistoragecapacity 05/17/23 07:58:12.651
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:12.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:12.661
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/17/23 07:58:12.662
STEP: getting /apis/storage.k8s.io 05/17/23 07:58:12.663
STEP: getting /apis/storage.k8s.io/v1 05/17/23 07:58:12.664
STEP: creating 05/17/23 07:58:12.665
STEP: watching 05/17/23 07:58:12.671
May 17 07:58:12.671: INFO: starting watch
STEP: getting 05/17/23 07:58:12.675
STEP: listing in namespace 05/17/23 07:58:12.676
STEP: listing across namespaces 05/17/23 07:58:12.677
STEP: patching 05/17/23 07:58:12.678
STEP: updating 05/17/23 07:58:12.68
May 17 07:58:12.683: INFO: waiting for watch events with expected annotations in namespace
May 17 07:58:12.683: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/17/23 07:58:12.683
STEP: deleting a collection 05/17/23 07:58:12.688
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
May 17 07:58:12.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-451" for this suite. 05/17/23 07:58:12.695
------------------------------
â€¢ [0.047 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:12.651
    May 17 07:58:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename csistoragecapacity 05/17/23 07:58:12.651
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:12.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:12.661
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/17/23 07:58:12.662
    STEP: getting /apis/storage.k8s.io 05/17/23 07:58:12.663
    STEP: getting /apis/storage.k8s.io/v1 05/17/23 07:58:12.664
    STEP: creating 05/17/23 07:58:12.665
    STEP: watching 05/17/23 07:58:12.671
    May 17 07:58:12.671: INFO: starting watch
    STEP: getting 05/17/23 07:58:12.675
    STEP: listing in namespace 05/17/23 07:58:12.676
    STEP: listing across namespaces 05/17/23 07:58:12.677
    STEP: patching 05/17/23 07:58:12.678
    STEP: updating 05/17/23 07:58:12.68
    May 17 07:58:12.683: INFO: waiting for watch events with expected annotations in namespace
    May 17 07:58:12.683: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/17/23 07:58:12.683
    STEP: deleting a collection 05/17/23 07:58:12.688
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:12.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-451" for this suite. 05/17/23 07:58:12.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:12.698
May 17 07:58:12.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename daemonsets 05/17/23 07:58:12.698
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:12.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:12.705
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 05/17/23 07:58:12.713
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 07:58:12.715
May 17 07:58:12.717: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:12.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 07:58:12.721: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 07:58:13.723: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:13.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:58:13.725: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 07:58:14.724: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:14.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 07:58:14.726: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/17/23 07:58:14.727
May 17 07:58:14.734: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:14.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:58:14.735: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 07:58:15.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:15.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:58:15.740: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 07:58:16.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:16.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:58:16.740: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 07:58:17.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:17.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:58:17.739: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 07:58:18.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 07:58:18.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 07:58:18.740: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/17/23 07:58:18.741
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5814, will wait for the garbage collector to delete the pods 05/17/23 07:58:18.741
May 17 07:58:18.796: INFO: Deleting DaemonSet.extensions daemon-set took: 3.01619ms
May 17 07:58:18.897: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.706357ms
May 17 07:58:20.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 07:58:20.899: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 07:58:20.900: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1192623"},"items":null}

May 17 07:58:20.902: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1192623"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 07:58:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5814" for this suite. 05/17/23 07:58:20.907
------------------------------
â€¢ [SLOW TEST] [8.213 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:12.698
    May 17 07:58:12.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename daemonsets 05/17/23 07:58:12.698
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:12.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:12.705
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 05/17/23 07:58:12.713
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 07:58:12.715
    May 17 07:58:12.717: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:12.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 07:58:12.721: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 07:58:13.723: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:13.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:58:13.725: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 07:58:14.724: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:14.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 07:58:14.726: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/17/23 07:58:14.727
    May 17 07:58:14.734: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:14.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:58:14.735: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 07:58:15.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:15.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:58:15.740: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 07:58:16.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:16.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:58:16.740: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 07:58:17.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:17.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:58:17.739: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 07:58:18.738: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 07:58:18.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 07:58:18.740: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 07:58:18.741
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5814, will wait for the garbage collector to delete the pods 05/17/23 07:58:18.741
    May 17 07:58:18.796: INFO: Deleting DaemonSet.extensions daemon-set took: 3.01619ms
    May 17 07:58:18.897: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.706357ms
    May 17 07:58:20.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 07:58:20.899: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 07:58:20.900: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1192623"},"items":null}

    May 17 07:58:20.902: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1192623"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5814" for this suite. 05/17/23 07:58:20.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:20.911
May 17 07:58:20.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename job 05/17/23 07:58:20.912
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:20.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:20.92
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 05/17/23 07:58:20.921
STEP: Ensure pods equal to parallelism count is attached to the job 05/17/23 07:58:20.924
STEP: patching /status 05/17/23 07:58:22.928
STEP: updating /status 05/17/23 07:58:22.933
STEP: get /status 05/17/23 07:58:22.954
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 17 07:58:22.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8263" for this suite. 05/17/23 07:58:22.957
------------------------------
â€¢ [2.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:20.911
    May 17 07:58:20.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename job 05/17/23 07:58:20.912
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:20.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:20.92
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 05/17/23 07:58:20.921
    STEP: Ensure pods equal to parallelism count is attached to the job 05/17/23 07:58:20.924
    STEP: patching /status 05/17/23 07:58:22.928
    STEP: updating /status 05/17/23 07:58:22.933
    STEP: get /status 05/17/23 07:58:22.954
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:22.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8263" for this suite. 05/17/23 07:58:22.957
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:22.96
May 17 07:58:22.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename endpointslice 05/17/23 07:58:22.961
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:22.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:22.971
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
May 17 07:58:22.976: INFO: Endpoints addresses: [10.0.79.209] , ports: [6443]
May 17 07:58:22.976: INFO: EndpointSlices addresses: [10.0.79.209] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 17 07:58:22.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6941" for this suite. 05/17/23 07:58:22.978
------------------------------
â€¢ [0.020 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:22.96
    May 17 07:58:22.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename endpointslice 05/17/23 07:58:22.961
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:22.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:22.971
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    May 17 07:58:22.976: INFO: Endpoints addresses: [10.0.79.209] , ports: [6443]
    May 17 07:58:22.976: INFO: EndpointSlices addresses: [10.0.79.209] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:22.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6941" for this suite. 05/17/23 07:58:22.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:22.981
May 17 07:58:22.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 07:58:22.982
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:22.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:22.99
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 05/17/23 07:58:22.992
STEP: Getting a ResourceQuota 05/17/23 07:58:22.995
STEP: Listing all ResourceQuotas with LabelSelector 05/17/23 07:58:22.996
STEP: Patching the ResourceQuota 05/17/23 07:58:22.998
STEP: Deleting a Collection of ResourceQuotas 05/17/23 07:58:23
STEP: Verifying the deleted ResourceQuota 05/17/23 07:58:23.006
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 07:58:23.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7283" for this suite. 05/17/23 07:58:23.009
------------------------------
â€¢ [0.031 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:22.981
    May 17 07:58:22.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 07:58:22.982
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:22.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:22.99
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 05/17/23 07:58:22.992
    STEP: Getting a ResourceQuota 05/17/23 07:58:22.995
    STEP: Listing all ResourceQuotas with LabelSelector 05/17/23 07:58:22.996
    STEP: Patching the ResourceQuota 05/17/23 07:58:22.998
    STEP: Deleting a Collection of ResourceQuotas 05/17/23 07:58:23
    STEP: Verifying the deleted ResourceQuota 05/17/23 07:58:23.006
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:23.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7283" for this suite. 05/17/23 07:58:23.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:23.012
May 17 07:58:23.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 07:58:23.013
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:23.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:23.021
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-3803 05/17/23 07:58:23.023
STEP: creating service affinity-clusterip in namespace services-3803 05/17/23 07:58:23.023
STEP: creating replication controller affinity-clusterip in namespace services-3803 05/17/23 07:58:23.031
I0517 07:58:23.034189      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3803, replica count: 3
I0517 07:58:26.085629      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 07:58:26.088: INFO: Creating new exec pod
May 17 07:58:26.092: INFO: Waiting up to 5m0s for pod "execpod-affinitywzmbb" in namespace "services-3803" to be "running"
May 17 07:58:26.094: INFO: Pod "execpod-affinitywzmbb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542074ms
May 17 07:58:28.095: INFO: Pod "execpod-affinitywzmbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003322266s
May 17 07:58:28.096: INFO: Pod "execpod-affinitywzmbb" satisfied condition "running"
May 17 07:58:29.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3803 exec execpod-affinitywzmbb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
May 17 07:58:29.204: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 17 07:58:29.204: INFO: stdout: ""
May 17 07:58:29.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3803 exec execpod-affinitywzmbb -- /bin/sh -x -c nc -v -z -w 2 10.109.109.178 80'
May 17 07:58:29.297: INFO: stderr: "+ nc -v -z -w 2 10.109.109.178 80\nConnection to 10.109.109.178 80 port [tcp/http] succeeded!\n"
May 17 07:58:29.297: INFO: stdout: ""
May 17 07:58:29.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3803 exec execpod-affinitywzmbb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.109.178:80/ ; done'
May 17 07:58:29.453: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n"
May 17 07:58:29.453: INFO: stdout: "\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t"
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
May 17 07:58:29.453: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3803, will wait for the garbage collector to delete the pods 05/17/23 07:58:29.461
May 17 07:58:29.516: INFO: Deleting ReplicationController affinity-clusterip took: 2.892834ms
May 17 07:58:29.617: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.765845ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 07:58:31.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3803" for this suite. 05/17/23 07:58:31.729
------------------------------
â€¢ [SLOW TEST] [8.720 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:23.012
    May 17 07:58:23.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 07:58:23.013
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:23.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:23.021
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-3803 05/17/23 07:58:23.023
    STEP: creating service affinity-clusterip in namespace services-3803 05/17/23 07:58:23.023
    STEP: creating replication controller affinity-clusterip in namespace services-3803 05/17/23 07:58:23.031
    I0517 07:58:23.034189      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3803, replica count: 3
    I0517 07:58:26.085629      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 07:58:26.088: INFO: Creating new exec pod
    May 17 07:58:26.092: INFO: Waiting up to 5m0s for pod "execpod-affinitywzmbb" in namespace "services-3803" to be "running"
    May 17 07:58:26.094: INFO: Pod "execpod-affinitywzmbb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542074ms
    May 17 07:58:28.095: INFO: Pod "execpod-affinitywzmbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003322266s
    May 17 07:58:28.096: INFO: Pod "execpod-affinitywzmbb" satisfied condition "running"
    May 17 07:58:29.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3803 exec execpod-affinitywzmbb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    May 17 07:58:29.204: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May 17 07:58:29.204: INFO: stdout: ""
    May 17 07:58:29.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3803 exec execpod-affinitywzmbb -- /bin/sh -x -c nc -v -z -w 2 10.109.109.178 80'
    May 17 07:58:29.297: INFO: stderr: "+ nc -v -z -w 2 10.109.109.178 80\nConnection to 10.109.109.178 80 port [tcp/http] succeeded!\n"
    May 17 07:58:29.297: INFO: stdout: ""
    May 17 07:58:29.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3803 exec execpod-affinitywzmbb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.109.178:80/ ; done'
    May 17 07:58:29.453: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.178:80/\n"
    May 17 07:58:29.453: INFO: stdout: "\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t\naffinity-clusterip-zzp5t"
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Received response from host: affinity-clusterip-zzp5t
    May 17 07:58:29.453: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3803, will wait for the garbage collector to delete the pods 05/17/23 07:58:29.461
    May 17 07:58:29.516: INFO: Deleting ReplicationController affinity-clusterip took: 2.892834ms
    May 17 07:58:29.617: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.765845ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:31.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3803" for this suite. 05/17/23 07:58:31.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:31.734
May 17 07:58:31.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 07:58:31.734
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:31.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:31.742
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 05/17/23 07:58:31.744
May 17 07:58:31.744: INFO: Creating e2e-svc-a-t6q7t
May 17 07:58:31.749: INFO: Creating e2e-svc-b-46s8k
May 17 07:58:31.757: INFO: Creating e2e-svc-c-dggqc
STEP: deleting service collection 05/17/23 07:58:31.765
May 17 07:58:31.785: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 07:58:31.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-268" for this suite. 05/17/23 07:58:31.787
------------------------------
â€¢ [0.056 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:31.734
    May 17 07:58:31.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 07:58:31.734
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:31.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:31.742
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 05/17/23 07:58:31.744
    May 17 07:58:31.744: INFO: Creating e2e-svc-a-t6q7t
    May 17 07:58:31.749: INFO: Creating e2e-svc-b-46s8k
    May 17 07:58:31.757: INFO: Creating e2e-svc-c-dggqc
    STEP: deleting service collection 05/17/23 07:58:31.765
    May 17 07:58:31.785: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:31.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-268" for this suite. 05/17/23 07:58:31.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:31.79
May 17 07:58:31.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 07:58:31.79
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:31.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:31.798
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 07:58:31.8
May 17 07:58:31.805: INFO: Waiting up to 5m0s for pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1" in namespace "emptydir-6417" to be "Succeeded or Failed"
May 17 07:58:31.806: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.366185ms
May 17 07:58:33.809: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00372102s
May 17 07:58:35.809: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004017286s
STEP: Saw pod success 05/17/23 07:58:35.809
May 17 07:58:35.809: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1" satisfied condition "Succeeded or Failed"
May 17 07:58:35.812: INFO: Trying to get logs from node k8s-node1 pod pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1 container test-container: <nil>
STEP: delete the pod 05/17/23 07:58:35.815
May 17 07:58:35.820: INFO: Waiting for pod pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1 to disappear
May 17 07:58:35.822: INFO: Pod pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 07:58:35.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6417" for this suite. 05/17/23 07:58:35.823
------------------------------
â€¢ [4.036 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:31.79
    May 17 07:58:31.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 07:58:31.79
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:31.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:31.798
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 07:58:31.8
    May 17 07:58:31.805: INFO: Waiting up to 5m0s for pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1" in namespace "emptydir-6417" to be "Succeeded or Failed"
    May 17 07:58:31.806: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.366185ms
    May 17 07:58:33.809: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00372102s
    May 17 07:58:35.809: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004017286s
    STEP: Saw pod success 05/17/23 07:58:35.809
    May 17 07:58:35.809: INFO: Pod "pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1" satisfied condition "Succeeded or Failed"
    May 17 07:58:35.812: INFO: Trying to get logs from node k8s-node1 pod pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1 container test-container: <nil>
    STEP: delete the pod 05/17/23 07:58:35.815
    May 17 07:58:35.820: INFO: Waiting for pod pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1 to disappear
    May 17 07:58:35.822: INFO: Pod pod-f991ad72-4847-4f9d-8f9e-4b2bfe075be1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 07:58:35.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6417" for this suite. 05/17/23 07:58:35.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:58:35.827
May 17 07:58:35.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename gc 05/17/23 07:58:35.827
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:35.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:35.835
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/17/23 07:58:35.838
STEP: delete the rc 05/17/23 07:58:40.845
STEP: wait for the rc to be deleted 05/17/23 07:58:40.848
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/17/23 07:58:45.85
STEP: Gathering metrics 05/17/23 07:59:15.859
May 17 07:59:15.870: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
May 17 07:59:15.871: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.29675ms
May 17 07:59:15.871: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
May 17 07:59:15.871: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
May 17 07:59:15.916: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 17 07:59:15.916: INFO: Deleting pod "simpletest.rc-22cvv" in namespace "gc-1955"
May 17 07:59:15.923: INFO: Deleting pod "simpletest.rc-2467r" in namespace "gc-1955"
May 17 07:59:15.930: INFO: Deleting pod "simpletest.rc-24fws" in namespace "gc-1955"
May 17 07:59:15.934: INFO: Deleting pod "simpletest.rc-275m5" in namespace "gc-1955"
May 17 07:59:15.939: INFO: Deleting pod "simpletest.rc-29lzr" in namespace "gc-1955"
May 17 07:59:15.946: INFO: Deleting pod "simpletest.rc-2cggz" in namespace "gc-1955"
May 17 07:59:15.951: INFO: Deleting pod "simpletest.rc-2pgrr" in namespace "gc-1955"
May 17 07:59:15.956: INFO: Deleting pod "simpletest.rc-2zvph" in namespace "gc-1955"
May 17 07:59:15.961: INFO: Deleting pod "simpletest.rc-4ct6p" in namespace "gc-1955"
May 17 07:59:15.965: INFO: Deleting pod "simpletest.rc-4p57q" in namespace "gc-1955"
May 17 07:59:15.970: INFO: Deleting pod "simpletest.rc-4wnld" in namespace "gc-1955"
May 17 07:59:15.977: INFO: Deleting pod "simpletest.rc-57fqq" in namespace "gc-1955"
May 17 07:59:15.984: INFO: Deleting pod "simpletest.rc-58clj" in namespace "gc-1955"
May 17 07:59:15.990: INFO: Deleting pod "simpletest.rc-5xnns" in namespace "gc-1955"
May 17 07:59:15.995: INFO: Deleting pod "simpletest.rc-67cw5" in namespace "gc-1955"
May 17 07:59:16.002: INFO: Deleting pod "simpletest.rc-6lfrq" in namespace "gc-1955"
May 17 07:59:16.009: INFO: Deleting pod "simpletest.rc-82kjm" in namespace "gc-1955"
May 17 07:59:16.014: INFO: Deleting pod "simpletest.rc-84rmf" in namespace "gc-1955"
May 17 07:59:16.020: INFO: Deleting pod "simpletest.rc-8q9dm" in namespace "gc-1955"
May 17 07:59:16.035: INFO: Deleting pod "simpletest.rc-8vjvv" in namespace "gc-1955"
May 17 07:59:16.041: INFO: Deleting pod "simpletest.rc-8wzlz" in namespace "gc-1955"
May 17 07:59:16.051: INFO: Deleting pod "simpletest.rc-9kq4b" in namespace "gc-1955"
May 17 07:59:16.061: INFO: Deleting pod "simpletest.rc-9mh78" in namespace "gc-1955"
May 17 07:59:16.066: INFO: Deleting pod "simpletest.rc-9qrrq" in namespace "gc-1955"
May 17 07:59:16.071: INFO: Deleting pod "simpletest.rc-9wgbn" in namespace "gc-1955"
May 17 07:59:16.075: INFO: Deleting pod "simpletest.rc-9xmwk" in namespace "gc-1955"
May 17 07:59:16.080: INFO: Deleting pod "simpletest.rc-b4fcg" in namespace "gc-1955"
May 17 07:59:16.087: INFO: Deleting pod "simpletest.rc-bfktq" in namespace "gc-1955"
May 17 07:59:16.093: INFO: Deleting pod "simpletest.rc-bnkjf" in namespace "gc-1955"
May 17 07:59:16.100: INFO: Deleting pod "simpletest.rc-bz9wm" in namespace "gc-1955"
May 17 07:59:16.104: INFO: Deleting pod "simpletest.rc-bzvkb" in namespace "gc-1955"
May 17 07:59:16.108: INFO: Deleting pod "simpletest.rc-cd6m4" in namespace "gc-1955"
May 17 07:59:16.116: INFO: Deleting pod "simpletest.rc-dbx4q" in namespace "gc-1955"
May 17 07:59:16.122: INFO: Deleting pod "simpletest.rc-djvtp" in namespace "gc-1955"
May 17 07:59:16.127: INFO: Deleting pod "simpletest.rc-f2pl2" in namespace "gc-1955"
May 17 07:59:16.131: INFO: Deleting pod "simpletest.rc-fc9n4" in namespace "gc-1955"
May 17 07:59:16.137: INFO: Deleting pod "simpletest.rc-fcgts" in namespace "gc-1955"
May 17 07:59:16.141: INFO: Deleting pod "simpletest.rc-fqt5k" in namespace "gc-1955"
May 17 07:59:16.148: INFO: Deleting pod "simpletest.rc-gsgz7" in namespace "gc-1955"
May 17 07:59:16.156: INFO: Deleting pod "simpletest.rc-gxfvn" in namespace "gc-1955"
May 17 07:59:16.161: INFO: Deleting pod "simpletest.rc-h4dwl" in namespace "gc-1955"
May 17 07:59:16.167: INFO: Deleting pod "simpletest.rc-h67pn" in namespace "gc-1955"
May 17 07:59:16.171: INFO: Deleting pod "simpletest.rc-hx98t" in namespace "gc-1955"
May 17 07:59:16.196: INFO: Deleting pod "simpletest.rc-hzwzb" in namespace "gc-1955"
May 17 07:59:16.207: INFO: Deleting pod "simpletest.rc-jc6lp" in namespace "gc-1955"
May 17 07:59:16.215: INFO: Deleting pod "simpletest.rc-jvttz" in namespace "gc-1955"
May 17 07:59:16.227: INFO: Deleting pod "simpletest.rc-k5fhc" in namespace "gc-1955"
May 17 07:59:16.234: INFO: Deleting pod "simpletest.rc-k6cnd" in namespace "gc-1955"
May 17 07:59:16.247: INFO: Deleting pod "simpletest.rc-kplc4" in namespace "gc-1955"
May 17 07:59:16.253: INFO: Deleting pod "simpletest.rc-kz6pz" in namespace "gc-1955"
May 17 07:59:16.258: INFO: Deleting pod "simpletest.rc-l2mzm" in namespace "gc-1955"
May 17 07:59:16.262: INFO: Deleting pod "simpletest.rc-l8rtd" in namespace "gc-1955"
May 17 07:59:16.267: INFO: Deleting pod "simpletest.rc-lww79" in namespace "gc-1955"
May 17 07:59:16.271: INFO: Deleting pod "simpletest.rc-mbftq" in namespace "gc-1955"
May 17 07:59:16.276: INFO: Deleting pod "simpletest.rc-mfxsk" in namespace "gc-1955"
May 17 07:59:16.313: INFO: Deleting pod "simpletest.rc-mgc8c" in namespace "gc-1955"
May 17 07:59:16.359: INFO: Deleting pod "simpletest.rc-mhmd9" in namespace "gc-1955"
May 17 07:59:16.408: INFO: Deleting pod "simpletest.rc-mr8kk" in namespace "gc-1955"
May 17 07:59:16.459: INFO: Deleting pod "simpletest.rc-ms67t" in namespace "gc-1955"
May 17 07:59:16.508: INFO: Deleting pod "simpletest.rc-mtw25" in namespace "gc-1955"
May 17 07:59:16.558: INFO: Deleting pod "simpletest.rc-n2j8l" in namespace "gc-1955"
May 17 07:59:16.606: INFO: Deleting pod "simpletest.rc-n2w89" in namespace "gc-1955"
May 17 07:59:16.656: INFO: Deleting pod "simpletest.rc-n4zh9" in namespace "gc-1955"
May 17 07:59:16.707: INFO: Deleting pod "simpletest.rc-n7hvf" in namespace "gc-1955"
May 17 07:59:16.756: INFO: Deleting pod "simpletest.rc-nhkfc" in namespace "gc-1955"
May 17 07:59:16.807: INFO: Deleting pod "simpletest.rc-nlxsb" in namespace "gc-1955"
May 17 07:59:16.876: INFO: Deleting pod "simpletest.rc-ntlk8" in namespace "gc-1955"
May 17 07:59:16.910: INFO: Deleting pod "simpletest.rc-nz5gb" in namespace "gc-1955"
May 17 07:59:16.959: INFO: Deleting pod "simpletest.rc-p7b46" in namespace "gc-1955"
May 17 07:59:17.012: INFO: Deleting pod "simpletest.rc-phl4h" in namespace "gc-1955"
May 17 07:59:17.058: INFO: Deleting pod "simpletest.rc-pmn5g" in namespace "gc-1955"
May 17 07:59:17.106: INFO: Deleting pod "simpletest.rc-pn4vv" in namespace "gc-1955"
May 17 07:59:17.158: INFO: Deleting pod "simpletest.rc-ppmqx" in namespace "gc-1955"
May 17 07:59:17.206: INFO: Deleting pod "simpletest.rc-px56b" in namespace "gc-1955"
May 17 07:59:17.257: INFO: Deleting pod "simpletest.rc-qg6vv" in namespace "gc-1955"
May 17 07:59:17.306: INFO: Deleting pod "simpletest.rc-r4xr9" in namespace "gc-1955"
May 17 07:59:17.357: INFO: Deleting pod "simpletest.rc-rqrfq" in namespace "gc-1955"
May 17 07:59:17.424: INFO: Deleting pod "simpletest.rc-s5wdx" in namespace "gc-1955"
May 17 07:59:17.457: INFO: Deleting pod "simpletest.rc-sb25h" in namespace "gc-1955"
May 17 07:59:17.507: INFO: Deleting pod "simpletest.rc-td892" in namespace "gc-1955"
May 17 07:59:17.556: INFO: Deleting pod "simpletest.rc-tqxcq" in namespace "gc-1955"
May 17 07:59:17.608: INFO: Deleting pod "simpletest.rc-tw5g7" in namespace "gc-1955"
May 17 07:59:17.658: INFO: Deleting pod "simpletest.rc-txrrz" in namespace "gc-1955"
May 17 07:59:17.708: INFO: Deleting pod "simpletest.rc-tzmxt" in namespace "gc-1955"
May 17 07:59:17.756: INFO: Deleting pod "simpletest.rc-v57lr" in namespace "gc-1955"
May 17 07:59:17.808: INFO: Deleting pod "simpletest.rc-vgvvz" in namespace "gc-1955"
May 17 07:59:17.859: INFO: Deleting pod "simpletest.rc-vt74l" in namespace "gc-1955"
May 17 07:59:17.912: INFO: Deleting pod "simpletest.rc-vtqlp" in namespace "gc-1955"
May 17 07:59:17.962: INFO: Deleting pod "simpletest.rc-vx2gl" in namespace "gc-1955"
May 17 07:59:18.025: INFO: Deleting pod "simpletest.rc-wss5g" in namespace "gc-1955"
May 17 07:59:18.057: INFO: Deleting pod "simpletest.rc-x5fpm" in namespace "gc-1955"
May 17 07:59:18.111: INFO: Deleting pod "simpletest.rc-x7dw6" in namespace "gc-1955"
May 17 07:59:18.158: INFO: Deleting pod "simpletest.rc-xb65f" in namespace "gc-1955"
May 17 07:59:18.207: INFO: Deleting pod "simpletest.rc-zg2t2" in namespace "gc-1955"
May 17 07:59:18.257: INFO: Deleting pod "simpletest.rc-zgqlq" in namespace "gc-1955"
May 17 07:59:18.306: INFO: Deleting pod "simpletest.rc-zqlxp" in namespace "gc-1955"
May 17 07:59:18.355: INFO: Deleting pod "simpletest.rc-zqnm5" in namespace "gc-1955"
May 17 07:59:18.409: INFO: Deleting pod "simpletest.rc-zvzzz" in namespace "gc-1955"
May 17 07:59:18.458: INFO: Deleting pod "simpletest.rc-zwjl4" in namespace "gc-1955"
May 17 07:59:18.507: INFO: Deleting pod "simpletest.rc-zwvzk" in namespace "gc-1955"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 17 07:59:18.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1955" for this suite. 05/17/23 07:59:18.603
------------------------------
â€¢ [SLOW TEST] [42.828 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:58:35.827
    May 17 07:58:35.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename gc 05/17/23 07:58:35.827
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:58:35.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:58:35.835
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/17/23 07:58:35.838
    STEP: delete the rc 05/17/23 07:58:40.845
    STEP: wait for the rc to be deleted 05/17/23 07:58:40.848
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/17/23 07:58:45.85
    STEP: Gathering metrics 05/17/23 07:59:15.859
    May 17 07:59:15.870: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
    May 17 07:59:15.871: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.29675ms
    May 17 07:59:15.871: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
    May 17 07:59:15.871: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
    May 17 07:59:15.916: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May 17 07:59:15.916: INFO: Deleting pod "simpletest.rc-22cvv" in namespace "gc-1955"
    May 17 07:59:15.923: INFO: Deleting pod "simpletest.rc-2467r" in namespace "gc-1955"
    May 17 07:59:15.930: INFO: Deleting pod "simpletest.rc-24fws" in namespace "gc-1955"
    May 17 07:59:15.934: INFO: Deleting pod "simpletest.rc-275m5" in namespace "gc-1955"
    May 17 07:59:15.939: INFO: Deleting pod "simpletest.rc-29lzr" in namespace "gc-1955"
    May 17 07:59:15.946: INFO: Deleting pod "simpletest.rc-2cggz" in namespace "gc-1955"
    May 17 07:59:15.951: INFO: Deleting pod "simpletest.rc-2pgrr" in namespace "gc-1955"
    May 17 07:59:15.956: INFO: Deleting pod "simpletest.rc-2zvph" in namespace "gc-1955"
    May 17 07:59:15.961: INFO: Deleting pod "simpletest.rc-4ct6p" in namespace "gc-1955"
    May 17 07:59:15.965: INFO: Deleting pod "simpletest.rc-4p57q" in namespace "gc-1955"
    May 17 07:59:15.970: INFO: Deleting pod "simpletest.rc-4wnld" in namespace "gc-1955"
    May 17 07:59:15.977: INFO: Deleting pod "simpletest.rc-57fqq" in namespace "gc-1955"
    May 17 07:59:15.984: INFO: Deleting pod "simpletest.rc-58clj" in namespace "gc-1955"
    May 17 07:59:15.990: INFO: Deleting pod "simpletest.rc-5xnns" in namespace "gc-1955"
    May 17 07:59:15.995: INFO: Deleting pod "simpletest.rc-67cw5" in namespace "gc-1955"
    May 17 07:59:16.002: INFO: Deleting pod "simpletest.rc-6lfrq" in namespace "gc-1955"
    May 17 07:59:16.009: INFO: Deleting pod "simpletest.rc-82kjm" in namespace "gc-1955"
    May 17 07:59:16.014: INFO: Deleting pod "simpletest.rc-84rmf" in namespace "gc-1955"
    May 17 07:59:16.020: INFO: Deleting pod "simpletest.rc-8q9dm" in namespace "gc-1955"
    May 17 07:59:16.035: INFO: Deleting pod "simpletest.rc-8vjvv" in namespace "gc-1955"
    May 17 07:59:16.041: INFO: Deleting pod "simpletest.rc-8wzlz" in namespace "gc-1955"
    May 17 07:59:16.051: INFO: Deleting pod "simpletest.rc-9kq4b" in namespace "gc-1955"
    May 17 07:59:16.061: INFO: Deleting pod "simpletest.rc-9mh78" in namespace "gc-1955"
    May 17 07:59:16.066: INFO: Deleting pod "simpletest.rc-9qrrq" in namespace "gc-1955"
    May 17 07:59:16.071: INFO: Deleting pod "simpletest.rc-9wgbn" in namespace "gc-1955"
    May 17 07:59:16.075: INFO: Deleting pod "simpletest.rc-9xmwk" in namespace "gc-1955"
    May 17 07:59:16.080: INFO: Deleting pod "simpletest.rc-b4fcg" in namespace "gc-1955"
    May 17 07:59:16.087: INFO: Deleting pod "simpletest.rc-bfktq" in namespace "gc-1955"
    May 17 07:59:16.093: INFO: Deleting pod "simpletest.rc-bnkjf" in namespace "gc-1955"
    May 17 07:59:16.100: INFO: Deleting pod "simpletest.rc-bz9wm" in namespace "gc-1955"
    May 17 07:59:16.104: INFO: Deleting pod "simpletest.rc-bzvkb" in namespace "gc-1955"
    May 17 07:59:16.108: INFO: Deleting pod "simpletest.rc-cd6m4" in namespace "gc-1955"
    May 17 07:59:16.116: INFO: Deleting pod "simpletest.rc-dbx4q" in namespace "gc-1955"
    May 17 07:59:16.122: INFO: Deleting pod "simpletest.rc-djvtp" in namespace "gc-1955"
    May 17 07:59:16.127: INFO: Deleting pod "simpletest.rc-f2pl2" in namespace "gc-1955"
    May 17 07:59:16.131: INFO: Deleting pod "simpletest.rc-fc9n4" in namespace "gc-1955"
    May 17 07:59:16.137: INFO: Deleting pod "simpletest.rc-fcgts" in namespace "gc-1955"
    May 17 07:59:16.141: INFO: Deleting pod "simpletest.rc-fqt5k" in namespace "gc-1955"
    May 17 07:59:16.148: INFO: Deleting pod "simpletest.rc-gsgz7" in namespace "gc-1955"
    May 17 07:59:16.156: INFO: Deleting pod "simpletest.rc-gxfvn" in namespace "gc-1955"
    May 17 07:59:16.161: INFO: Deleting pod "simpletest.rc-h4dwl" in namespace "gc-1955"
    May 17 07:59:16.167: INFO: Deleting pod "simpletest.rc-h67pn" in namespace "gc-1955"
    May 17 07:59:16.171: INFO: Deleting pod "simpletest.rc-hx98t" in namespace "gc-1955"
    May 17 07:59:16.196: INFO: Deleting pod "simpletest.rc-hzwzb" in namespace "gc-1955"
    May 17 07:59:16.207: INFO: Deleting pod "simpletest.rc-jc6lp" in namespace "gc-1955"
    May 17 07:59:16.215: INFO: Deleting pod "simpletest.rc-jvttz" in namespace "gc-1955"
    May 17 07:59:16.227: INFO: Deleting pod "simpletest.rc-k5fhc" in namespace "gc-1955"
    May 17 07:59:16.234: INFO: Deleting pod "simpletest.rc-k6cnd" in namespace "gc-1955"
    May 17 07:59:16.247: INFO: Deleting pod "simpletest.rc-kplc4" in namespace "gc-1955"
    May 17 07:59:16.253: INFO: Deleting pod "simpletest.rc-kz6pz" in namespace "gc-1955"
    May 17 07:59:16.258: INFO: Deleting pod "simpletest.rc-l2mzm" in namespace "gc-1955"
    May 17 07:59:16.262: INFO: Deleting pod "simpletest.rc-l8rtd" in namespace "gc-1955"
    May 17 07:59:16.267: INFO: Deleting pod "simpletest.rc-lww79" in namespace "gc-1955"
    May 17 07:59:16.271: INFO: Deleting pod "simpletest.rc-mbftq" in namespace "gc-1955"
    May 17 07:59:16.276: INFO: Deleting pod "simpletest.rc-mfxsk" in namespace "gc-1955"
    May 17 07:59:16.313: INFO: Deleting pod "simpletest.rc-mgc8c" in namespace "gc-1955"
    May 17 07:59:16.359: INFO: Deleting pod "simpletest.rc-mhmd9" in namespace "gc-1955"
    May 17 07:59:16.408: INFO: Deleting pod "simpletest.rc-mr8kk" in namespace "gc-1955"
    May 17 07:59:16.459: INFO: Deleting pod "simpletest.rc-ms67t" in namespace "gc-1955"
    May 17 07:59:16.508: INFO: Deleting pod "simpletest.rc-mtw25" in namespace "gc-1955"
    May 17 07:59:16.558: INFO: Deleting pod "simpletest.rc-n2j8l" in namespace "gc-1955"
    May 17 07:59:16.606: INFO: Deleting pod "simpletest.rc-n2w89" in namespace "gc-1955"
    May 17 07:59:16.656: INFO: Deleting pod "simpletest.rc-n4zh9" in namespace "gc-1955"
    May 17 07:59:16.707: INFO: Deleting pod "simpletest.rc-n7hvf" in namespace "gc-1955"
    May 17 07:59:16.756: INFO: Deleting pod "simpletest.rc-nhkfc" in namespace "gc-1955"
    May 17 07:59:16.807: INFO: Deleting pod "simpletest.rc-nlxsb" in namespace "gc-1955"
    May 17 07:59:16.876: INFO: Deleting pod "simpletest.rc-ntlk8" in namespace "gc-1955"
    May 17 07:59:16.910: INFO: Deleting pod "simpletest.rc-nz5gb" in namespace "gc-1955"
    May 17 07:59:16.959: INFO: Deleting pod "simpletest.rc-p7b46" in namespace "gc-1955"
    May 17 07:59:17.012: INFO: Deleting pod "simpletest.rc-phl4h" in namespace "gc-1955"
    May 17 07:59:17.058: INFO: Deleting pod "simpletest.rc-pmn5g" in namespace "gc-1955"
    May 17 07:59:17.106: INFO: Deleting pod "simpletest.rc-pn4vv" in namespace "gc-1955"
    May 17 07:59:17.158: INFO: Deleting pod "simpletest.rc-ppmqx" in namespace "gc-1955"
    May 17 07:59:17.206: INFO: Deleting pod "simpletest.rc-px56b" in namespace "gc-1955"
    May 17 07:59:17.257: INFO: Deleting pod "simpletest.rc-qg6vv" in namespace "gc-1955"
    May 17 07:59:17.306: INFO: Deleting pod "simpletest.rc-r4xr9" in namespace "gc-1955"
    May 17 07:59:17.357: INFO: Deleting pod "simpletest.rc-rqrfq" in namespace "gc-1955"
    May 17 07:59:17.424: INFO: Deleting pod "simpletest.rc-s5wdx" in namespace "gc-1955"
    May 17 07:59:17.457: INFO: Deleting pod "simpletest.rc-sb25h" in namespace "gc-1955"
    May 17 07:59:17.507: INFO: Deleting pod "simpletest.rc-td892" in namespace "gc-1955"
    May 17 07:59:17.556: INFO: Deleting pod "simpletest.rc-tqxcq" in namespace "gc-1955"
    May 17 07:59:17.608: INFO: Deleting pod "simpletest.rc-tw5g7" in namespace "gc-1955"
    May 17 07:59:17.658: INFO: Deleting pod "simpletest.rc-txrrz" in namespace "gc-1955"
    May 17 07:59:17.708: INFO: Deleting pod "simpletest.rc-tzmxt" in namespace "gc-1955"
    May 17 07:59:17.756: INFO: Deleting pod "simpletest.rc-v57lr" in namespace "gc-1955"
    May 17 07:59:17.808: INFO: Deleting pod "simpletest.rc-vgvvz" in namespace "gc-1955"
    May 17 07:59:17.859: INFO: Deleting pod "simpletest.rc-vt74l" in namespace "gc-1955"
    May 17 07:59:17.912: INFO: Deleting pod "simpletest.rc-vtqlp" in namespace "gc-1955"
    May 17 07:59:17.962: INFO: Deleting pod "simpletest.rc-vx2gl" in namespace "gc-1955"
    May 17 07:59:18.025: INFO: Deleting pod "simpletest.rc-wss5g" in namespace "gc-1955"
    May 17 07:59:18.057: INFO: Deleting pod "simpletest.rc-x5fpm" in namespace "gc-1955"
    May 17 07:59:18.111: INFO: Deleting pod "simpletest.rc-x7dw6" in namespace "gc-1955"
    May 17 07:59:18.158: INFO: Deleting pod "simpletest.rc-xb65f" in namespace "gc-1955"
    May 17 07:59:18.207: INFO: Deleting pod "simpletest.rc-zg2t2" in namespace "gc-1955"
    May 17 07:59:18.257: INFO: Deleting pod "simpletest.rc-zgqlq" in namespace "gc-1955"
    May 17 07:59:18.306: INFO: Deleting pod "simpletest.rc-zqlxp" in namespace "gc-1955"
    May 17 07:59:18.355: INFO: Deleting pod "simpletest.rc-zqnm5" in namespace "gc-1955"
    May 17 07:59:18.409: INFO: Deleting pod "simpletest.rc-zvzzz" in namespace "gc-1955"
    May 17 07:59:18.458: INFO: Deleting pod "simpletest.rc-zwjl4" in namespace "gc-1955"
    May 17 07:59:18.507: INFO: Deleting pod "simpletest.rc-zwvzk" in namespace "gc-1955"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 17 07:59:18.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1955" for this suite. 05/17/23 07:59:18.603
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:59:18.655
May 17 07:59:18.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 07:59:18.656
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:59:18.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:59:18.664
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-9147 05/17/23 07:59:18.665
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[] 05/17/23 07:59:18.672
May 17 07:59:18.673: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 17 07:59:19.677: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9147 05/17/23 07:59:19.677
May 17 07:59:19.682: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9147" to be "running and ready"
May 17 07:59:19.683: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.230315ms
May 17 07:59:19.683: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:59:21.686: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003965492s
May 17 07:59:21.686: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:59:23.686: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003958502s
May 17 07:59:23.686: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:59:25.686: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.004646579s
May 17 07:59:25.686: INFO: The phase of Pod pod1 is Running (Ready = true)
May 17 07:59:25.686: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[pod1:[100]] 05/17/23 07:59:25.688
May 17 07:59:25.692: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9147 05/17/23 07:59:25.692
May 17 07:59:25.695: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9147" to be "running and ready"
May 17 07:59:25.696: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.048282ms
May 17 07:59:25.696: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:59:27.700: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004334261s
May 17 07:59:27.700: INFO: The phase of Pod pod2 is Running (Ready = true)
May 17 07:59:27.700: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[pod1:[100] pod2:[101]] 05/17/23 07:59:27.701
May 17 07:59:27.707: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/17/23 07:59:27.707
May 17 07:59:27.707: INFO: Creating new exec pod
May 17 07:59:27.710: INFO: Waiting up to 5m0s for pod "execpodqlb8r" in namespace "services-9147" to be "running"
May 17 07:59:27.711: INFO: Pod "execpodqlb8r": Phase="Pending", Reason="", readiness=false. Elapsed: 1.177559ms
May 17 07:59:29.714: INFO: Pod "execpodqlb8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.004206198s
May 17 07:59:29.714: INFO: Pod "execpodqlb8r" satisfied condition "running"
May 17 07:59:30.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
May 17 07:59:30.817: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May 17 07:59:30.817: INFO: stdout: ""
May 17 07:59:30.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 10.105.1.198 80'
May 17 07:59:30.903: INFO: stderr: "+ nc -v -z -w 2 10.105.1.198 80\nConnection to 10.105.1.198 80 port [tcp/http] succeeded!\n"
May 17 07:59:30.903: INFO: stdout: ""
May 17 07:59:30.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
May 17 07:59:30.991: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May 17 07:59:30.991: INFO: stdout: ""
May 17 07:59:30.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 10.105.1.198 81'
May 17 07:59:31.081: INFO: stderr: "+ nc -v -z -w 2 10.105.1.198 81\nConnection to 10.105.1.198 81 port [tcp/*] succeeded!\n"
May 17 07:59:31.081: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9147 05/17/23 07:59:31.081
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[pod2:[101]] 05/17/23 07:59:31.087
May 17 07:59:32.096: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9147 05/17/23 07:59:32.096
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[] 05/17/23 07:59:32.102
May 17 07:59:32.106: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 07:59:32.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9147" for this suite. 05/17/23 07:59:32.116
------------------------------
â€¢ [SLOW TEST] [13.463 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:59:18.655
    May 17 07:59:18.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 07:59:18.656
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:59:18.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:59:18.664
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-9147 05/17/23 07:59:18.665
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[] 05/17/23 07:59:18.672
    May 17 07:59:18.673: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    May 17 07:59:19.677: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9147 05/17/23 07:59:19.677
    May 17 07:59:19.682: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9147" to be "running and ready"
    May 17 07:59:19.683: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.230315ms
    May 17 07:59:19.683: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:59:21.686: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003965492s
    May 17 07:59:21.686: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:59:23.686: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003958502s
    May 17 07:59:23.686: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:59:25.686: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.004646579s
    May 17 07:59:25.686: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 17 07:59:25.686: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[pod1:[100]] 05/17/23 07:59:25.688
    May 17 07:59:25.692: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9147 05/17/23 07:59:25.692
    May 17 07:59:25.695: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9147" to be "running and ready"
    May 17 07:59:25.696: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.048282ms
    May 17 07:59:25.696: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:59:27.700: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004334261s
    May 17 07:59:27.700: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 17 07:59:27.700: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[pod1:[100] pod2:[101]] 05/17/23 07:59:27.701
    May 17 07:59:27.707: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/17/23 07:59:27.707
    May 17 07:59:27.707: INFO: Creating new exec pod
    May 17 07:59:27.710: INFO: Waiting up to 5m0s for pod "execpodqlb8r" in namespace "services-9147" to be "running"
    May 17 07:59:27.711: INFO: Pod "execpodqlb8r": Phase="Pending", Reason="", readiness=false. Elapsed: 1.177559ms
    May 17 07:59:29.714: INFO: Pod "execpodqlb8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.004206198s
    May 17 07:59:29.714: INFO: Pod "execpodqlb8r" satisfied condition "running"
    May 17 07:59:30.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    May 17 07:59:30.817: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May 17 07:59:30.817: INFO: stdout: ""
    May 17 07:59:30.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 10.105.1.198 80'
    May 17 07:59:30.903: INFO: stderr: "+ nc -v -z -w 2 10.105.1.198 80\nConnection to 10.105.1.198 80 port [tcp/http] succeeded!\n"
    May 17 07:59:30.903: INFO: stdout: ""
    May 17 07:59:30.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    May 17 07:59:30.991: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May 17 07:59:30.991: INFO: stdout: ""
    May 17 07:59:30.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9147 exec execpodqlb8r -- /bin/sh -x -c nc -v -z -w 2 10.105.1.198 81'
    May 17 07:59:31.081: INFO: stderr: "+ nc -v -z -w 2 10.105.1.198 81\nConnection to 10.105.1.198 81 port [tcp/*] succeeded!\n"
    May 17 07:59:31.081: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9147 05/17/23 07:59:31.081
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[pod2:[101]] 05/17/23 07:59:31.087
    May 17 07:59:32.096: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9147 05/17/23 07:59:32.096
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9147 to expose endpoints map[] 05/17/23 07:59:32.102
    May 17 07:59:32.106: INFO: successfully validated that service multi-endpoint-test in namespace services-9147 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 07:59:32.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9147" for this suite. 05/17/23 07:59:32.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:59:32.12
May 17 07:59:32.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename job 05/17/23 07:59:32.121
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:59:32.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:59:32.127
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 05/17/23 07:59:32.129
STEP: Ensuring job reaches completions 05/17/23 07:59:32.132
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 17 07:59:44.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3759" for this suite. 05/17/23 07:59:44.136
------------------------------
â€¢ [SLOW TEST] [12.019 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:59:32.12
    May 17 07:59:32.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename job 05/17/23 07:59:32.121
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:59:32.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:59:32.127
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 05/17/23 07:59:32.129
    STEP: Ensuring job reaches completions 05/17/23 07:59:32.132
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 17 07:59:44.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3759" for this suite. 05/17/23 07:59:44.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 07:59:44.14
May 17 07:59:44.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename cronjob 05/17/23 07:59:44.14
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:59:44.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:59:44.148
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/17/23 07:59:44.149
STEP: Ensuring a job is scheduled 05/17/23 07:59:44.152
STEP: Ensuring exactly one is scheduled 05/17/23 08:00:00.154
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 08:00:00.155
STEP: Ensuring the job is replaced with a new one 05/17/23 08:00:00.156
STEP: Removing cronjob 05/17/23 08:01:00.159
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 17 08:01:00.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-243" for this suite. 05/17/23 08:01:00.166
------------------------------
â€¢ [SLOW TEST] [76.030 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 07:59:44.14
    May 17 07:59:44.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename cronjob 05/17/23 07:59:44.14
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:59:44.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:59:44.148
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/17/23 07:59:44.149
    STEP: Ensuring a job is scheduled 05/17/23 07:59:44.152
    STEP: Ensuring exactly one is scheduled 05/17/23 08:00:00.154
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 08:00:00.155
    STEP: Ensuring the job is replaced with a new one 05/17/23 08:00:00.156
    STEP: Removing cronjob 05/17/23 08:01:00.159
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:00.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-243" for this suite. 05/17/23 08:01:00.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:00.17
May 17 08:01:00.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 08:01:00.171
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:00.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:00.178
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 05/17/23 08:01:00.18
May 17 08:01:00.183: INFO: Waiting up to 5m0s for pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6" in namespace "pods-1964" to be "running and ready"
May 17 08:01:00.185: INFO: Pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.321972ms
May 17 08:01:00.185: INFO: The phase of Pod pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:01:02.188: INFO: Pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004260817s
May 17 08:01:02.188: INFO: The phase of Pod pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6 is Running (Ready = true)
May 17 08:01:02.188: INFO: Pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6" satisfied condition "running and ready"
May 17 08:01:02.191: INFO: Pod pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6 has hostIP: 10.0.79.210
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 08:01:02.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1964" for this suite. 05/17/23 08:01:02.193
------------------------------
â€¢ [2.026 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:00.17
    May 17 08:01:00.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 08:01:00.171
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:00.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:00.178
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 05/17/23 08:01:00.18
    May 17 08:01:00.183: INFO: Waiting up to 5m0s for pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6" in namespace "pods-1964" to be "running and ready"
    May 17 08:01:00.185: INFO: Pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.321972ms
    May 17 08:01:00.185: INFO: The phase of Pod pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:01:02.188: INFO: Pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004260817s
    May 17 08:01:02.188: INFO: The phase of Pod pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6 is Running (Ready = true)
    May 17 08:01:02.188: INFO: Pod "pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6" satisfied condition "running and ready"
    May 17 08:01:02.191: INFO: Pod pod-hostip-c6ef255b-fcde-4418-bcf6-6b38370827e6 has hostIP: 10.0.79.210
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:02.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1964" for this suite. 05/17/23 08:01:02.193
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:02.196
May 17 08:01:02.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:01:02.197
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:02.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:02.206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:01:02.207
May 17 08:01:02.212: INFO: Waiting up to 5m0s for pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61" in namespace "projected-306" to be "Succeeded or Failed"
May 17 08:01:02.213: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 1.425134ms
May 17 08:01:04.217: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005439941s
May 17 08:01:06.218: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006522085s
STEP: Saw pod success 05/17/23 08:01:06.218
May 17 08:01:06.218: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61" satisfied condition "Succeeded or Failed"
May 17 08:01:06.220: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61 container client-container: <nil>
STEP: delete the pod 05/17/23 08:01:06.229
May 17 08:01:06.236: INFO: Waiting for pod downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61 to disappear
May 17 08:01:06.237: INFO: Pod downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:01:06.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-306" for this suite. 05/17/23 08:01:06.239
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:02.196
    May 17 08:01:02.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:01:02.197
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:02.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:02.206
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:01:02.207
    May 17 08:01:02.212: INFO: Waiting up to 5m0s for pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61" in namespace "projected-306" to be "Succeeded or Failed"
    May 17 08:01:02.213: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 1.425134ms
    May 17 08:01:04.217: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005439941s
    May 17 08:01:06.218: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006522085s
    STEP: Saw pod success 05/17/23 08:01:06.218
    May 17 08:01:06.218: INFO: Pod "downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61" satisfied condition "Succeeded or Failed"
    May 17 08:01:06.220: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:01:06.229
    May 17 08:01:06.236: INFO: Waiting for pod downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61 to disappear
    May 17 08:01:06.237: INFO: Pod downwardapi-volume-520110c3-2674-40fe-b711-1869cea2fb61 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:06.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-306" for this suite. 05/17/23 08:01:06.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:06.242
May 17 08:01:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:01:06.242
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:06.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:06.25
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 05/17/23 08:01:06.252
May 17 08:01:06.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: mark a version not serverd 05/17/23 08:01:09.54
STEP: check the unserved version gets removed 05/17/23 08:01:09.552
STEP: check the other version is not changed 05/17/23 08:01:10.366
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:01:12.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3368" for this suite. 05/17/23 08:01:12.997
------------------------------
â€¢ [SLOW TEST] [6.759 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:06.242
    May 17 08:01:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:01:06.242
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:06.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:06.25
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 05/17/23 08:01:06.252
    May 17 08:01:06.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: mark a version not serverd 05/17/23 08:01:09.54
    STEP: check the unserved version gets removed 05/17/23 08:01:09.552
    STEP: check the other version is not changed 05/17/23 08:01:10.366
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:12.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3368" for this suite. 05/17/23 08:01:12.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:13.001
May 17 08:01:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename daemonsets 05/17/23 08:01:13.002
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:13.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:13.014
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
May 17 08:01:13.025: INFO: Create a RollingUpdate DaemonSet
May 17 08:01:13.028: INFO: Check that daemon pods launch on every node of the cluster
May 17 08:01:13.029: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:01:13.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:01:13.032: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:01:14.035: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:01:14.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:01:14.037: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:01:15.036: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:01:15.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 08:01:15.038: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
May 17 08:01:15.038: INFO: Update the DaemonSet to trigger a rollout
May 17 08:01:15.043: INFO: Updating DaemonSet daemon-set
May 17 08:01:18.051: INFO: Roll back the DaemonSet before rollout is complete
May 17 08:01:18.055: INFO: Updating DaemonSet daemon-set
May 17 08:01:18.055: INFO: Make sure DaemonSet rollback is complete
May 17 08:01:18.057: INFO: Wrong image for pod: daemon-set-w8n74. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
May 17 08:01:18.057: INFO: Pod daemon-set-w8n74 is not available
May 17 08:01:18.059: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:01:19.065: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:01:20.065: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:01:21.064: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:01:22.063: INFO: Pod daemon-set-bxhl4 is not available
May 17 08:01:22.065: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:01:22.067
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7343, will wait for the garbage collector to delete the pods 05/17/23 08:01:22.068
May 17 08:01:22.123: INFO: Deleting DaemonSet.extensions daemon-set took: 3.631222ms
May 17 08:01:22.224: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.837677ms
May 17 08:01:24.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:01:24.327: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 08:01:24.328: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1195923"},"items":null}

May 17 08:01:24.329: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1195923"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:01:24.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7343" for this suite. 05/17/23 08:01:24.335
------------------------------
â€¢ [SLOW TEST] [11.338 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:13.001
    May 17 08:01:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename daemonsets 05/17/23 08:01:13.002
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:13.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:13.014
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    May 17 08:01:13.025: INFO: Create a RollingUpdate DaemonSet
    May 17 08:01:13.028: INFO: Check that daemon pods launch on every node of the cluster
    May 17 08:01:13.029: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:01:13.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:01:13.032: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:01:14.035: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:01:14.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:01:14.037: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:01:15.036: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:01:15.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 08:01:15.038: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    May 17 08:01:15.038: INFO: Update the DaemonSet to trigger a rollout
    May 17 08:01:15.043: INFO: Updating DaemonSet daemon-set
    May 17 08:01:18.051: INFO: Roll back the DaemonSet before rollout is complete
    May 17 08:01:18.055: INFO: Updating DaemonSet daemon-set
    May 17 08:01:18.055: INFO: Make sure DaemonSet rollback is complete
    May 17 08:01:18.057: INFO: Wrong image for pod: daemon-set-w8n74. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    May 17 08:01:18.057: INFO: Pod daemon-set-w8n74 is not available
    May 17 08:01:18.059: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:01:19.065: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:01:20.065: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:01:21.064: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:01:22.063: INFO: Pod daemon-set-bxhl4 is not available
    May 17 08:01:22.065: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:01:22.067
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7343, will wait for the garbage collector to delete the pods 05/17/23 08:01:22.068
    May 17 08:01:22.123: INFO: Deleting DaemonSet.extensions daemon-set took: 3.631222ms
    May 17 08:01:22.224: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.837677ms
    May 17 08:01:24.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:01:24.327: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 08:01:24.328: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1195923"},"items":null}

    May 17 08:01:24.329: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1195923"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:24.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7343" for this suite. 05/17/23 08:01:24.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:24.34
May 17 08:01:24.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:01:24.34
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:24.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:24.35
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 05/17/23 08:01:24.352
May 17 08:01:24.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 create -f -'
May 17 08:01:24.796: INFO: stderr: ""
May 17 08:01:24.796: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:01:24.796
May 17 08:01:24.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 08:01:24.851: INFO: stderr: ""
May 17 08:01:24.851: INFO: stdout: "update-demo-nautilus-b6p95 update-demo-nautilus-hcwdk "
May 17 08:01:24.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-b6p95 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:01:24.902: INFO: stderr: ""
May 17 08:01:24.902: INFO: stdout: ""
May 17 08:01:24.902: INFO: update-demo-nautilus-b6p95 is created but not running
May 17 08:01:29.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 08:01:29.957: INFO: stderr: ""
May 17 08:01:29.957: INFO: stdout: "update-demo-nautilus-b6p95 update-demo-nautilus-hcwdk "
May 17 08:01:29.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-b6p95 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:01:30.007: INFO: stderr: ""
May 17 08:01:30.007: INFO: stdout: "true"
May 17 08:01:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-b6p95 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 08:01:30.058: INFO: stderr: ""
May 17 08:01:30.058: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 17 08:01:30.058: INFO: validating pod update-demo-nautilus-b6p95
May 17 08:01:30.060: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 08:01:30.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 08:01:30.060: INFO: update-demo-nautilus-b6p95 is verified up and running
May 17 08:01:30.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:01:30.117: INFO: stderr: ""
May 17 08:01:30.117: INFO: stdout: "true"
May 17 08:01:30.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 08:01:30.169: INFO: stderr: ""
May 17 08:01:30.169: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 17 08:01:30.169: INFO: validating pod update-demo-nautilus-hcwdk
May 17 08:01:30.172: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 08:01:30.172: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 08:01:30.172: INFO: update-demo-nautilus-hcwdk is verified up and running
STEP: scaling down the replication controller 05/17/23 08:01:30.172
May 17 08:01:30.173: INFO: scanned /root for discovery docs: <nil>
May 17 08:01:30.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 17 08:01:31.235: INFO: stderr: ""
May 17 08:01:31.235: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:01:31.235
May 17 08:01:31.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 08:01:31.287: INFO: stderr: ""
May 17 08:01:31.287: INFO: stdout: "update-demo-nautilus-hcwdk "
May 17 08:01:31.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:01:31.340: INFO: stderr: ""
May 17 08:01:31.340: INFO: stdout: "true"
May 17 08:01:31.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 08:01:31.393: INFO: stderr: ""
May 17 08:01:31.393: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 17 08:01:31.393: INFO: validating pod update-demo-nautilus-hcwdk
May 17 08:01:31.395: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 08:01:31.395: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 08:01:31.395: INFO: update-demo-nautilus-hcwdk is verified up and running
STEP: scaling up the replication controller 05/17/23 08:01:31.395
May 17 08:01:31.396: INFO: scanned /root for discovery docs: <nil>
May 17 08:01:31.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 17 08:01:32.459: INFO: stderr: ""
May 17 08:01:32.459: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:01:32.459
May 17 08:01:32.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 08:01:32.512: INFO: stderr: ""
May 17 08:01:32.512: INFO: stdout: "update-demo-nautilus-gpk5d update-demo-nautilus-hcwdk "
May 17 08:01:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-gpk5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:01:32.560: INFO: stderr: ""
May 17 08:01:32.560: INFO: stdout: "true"
May 17 08:01:32.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-gpk5d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 08:01:32.610: INFO: stderr: ""
May 17 08:01:32.610: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 17 08:01:32.610: INFO: validating pod update-demo-nautilus-gpk5d
May 17 08:01:32.612: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 08:01:32.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 08:01:32.612: INFO: update-demo-nautilus-gpk5d is verified up and running
May 17 08:01:32.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:01:32.662: INFO: stderr: ""
May 17 08:01:32.662: INFO: stdout: "true"
May 17 08:01:32.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 08:01:32.710: INFO: stderr: ""
May 17 08:01:32.710: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 17 08:01:32.710: INFO: validating pod update-demo-nautilus-hcwdk
May 17 08:01:32.712: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 08:01:32.712: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 08:01:32.712: INFO: update-demo-nautilus-hcwdk is verified up and running
STEP: using delete to clean up resources 05/17/23 08:01:32.712
May 17 08:01:32.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 delete --grace-period=0 --force -f -'
May 17 08:01:32.764: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:01:32.764: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 17 08:01:32.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get rc,svc -l name=update-demo --no-headers'
May 17 08:01:32.819: INFO: stderr: "No resources found in kubectl-566 namespace.\n"
May 17 08:01:32.819: INFO: stdout: ""
May 17 08:01:32.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 08:01:32.872: INFO: stderr: ""
May 17 08:01:32.872: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:01:32.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-566" for this suite. 05/17/23 08:01:32.875
------------------------------
â€¢ [SLOW TEST] [8.539 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:24.34
    May 17 08:01:24.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:01:24.34
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:24.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:24.35
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 05/17/23 08:01:24.352
    May 17 08:01:24.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 create -f -'
    May 17 08:01:24.796: INFO: stderr: ""
    May 17 08:01:24.796: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:01:24.796
    May 17 08:01:24.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 08:01:24.851: INFO: stderr: ""
    May 17 08:01:24.851: INFO: stdout: "update-demo-nautilus-b6p95 update-demo-nautilus-hcwdk "
    May 17 08:01:24.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-b6p95 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:01:24.902: INFO: stderr: ""
    May 17 08:01:24.902: INFO: stdout: ""
    May 17 08:01:24.902: INFO: update-demo-nautilus-b6p95 is created but not running
    May 17 08:01:29.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 08:01:29.957: INFO: stderr: ""
    May 17 08:01:29.957: INFO: stdout: "update-demo-nautilus-b6p95 update-demo-nautilus-hcwdk "
    May 17 08:01:29.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-b6p95 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:01:30.007: INFO: stderr: ""
    May 17 08:01:30.007: INFO: stdout: "true"
    May 17 08:01:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-b6p95 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 08:01:30.058: INFO: stderr: ""
    May 17 08:01:30.058: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 17 08:01:30.058: INFO: validating pod update-demo-nautilus-b6p95
    May 17 08:01:30.060: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 08:01:30.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 08:01:30.060: INFO: update-demo-nautilus-b6p95 is verified up and running
    May 17 08:01:30.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:01:30.117: INFO: stderr: ""
    May 17 08:01:30.117: INFO: stdout: "true"
    May 17 08:01:30.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 08:01:30.169: INFO: stderr: ""
    May 17 08:01:30.169: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 17 08:01:30.169: INFO: validating pod update-demo-nautilus-hcwdk
    May 17 08:01:30.172: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 08:01:30.172: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 08:01:30.172: INFO: update-demo-nautilus-hcwdk is verified up and running
    STEP: scaling down the replication controller 05/17/23 08:01:30.172
    May 17 08:01:30.173: INFO: scanned /root for discovery docs: <nil>
    May 17 08:01:30.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May 17 08:01:31.235: INFO: stderr: ""
    May 17 08:01:31.235: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:01:31.235
    May 17 08:01:31.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 08:01:31.287: INFO: stderr: ""
    May 17 08:01:31.287: INFO: stdout: "update-demo-nautilus-hcwdk "
    May 17 08:01:31.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:01:31.340: INFO: stderr: ""
    May 17 08:01:31.340: INFO: stdout: "true"
    May 17 08:01:31.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 08:01:31.393: INFO: stderr: ""
    May 17 08:01:31.393: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 17 08:01:31.393: INFO: validating pod update-demo-nautilus-hcwdk
    May 17 08:01:31.395: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 08:01:31.395: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 08:01:31.395: INFO: update-demo-nautilus-hcwdk is verified up and running
    STEP: scaling up the replication controller 05/17/23 08:01:31.395
    May 17 08:01:31.396: INFO: scanned /root for discovery docs: <nil>
    May 17 08:01:31.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May 17 08:01:32.459: INFO: stderr: ""
    May 17 08:01:32.459: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:01:32.459
    May 17 08:01:32.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 08:01:32.512: INFO: stderr: ""
    May 17 08:01:32.512: INFO: stdout: "update-demo-nautilus-gpk5d update-demo-nautilus-hcwdk "
    May 17 08:01:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-gpk5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:01:32.560: INFO: stderr: ""
    May 17 08:01:32.560: INFO: stdout: "true"
    May 17 08:01:32.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-gpk5d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 08:01:32.610: INFO: stderr: ""
    May 17 08:01:32.610: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 17 08:01:32.610: INFO: validating pod update-demo-nautilus-gpk5d
    May 17 08:01:32.612: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 08:01:32.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 08:01:32.612: INFO: update-demo-nautilus-gpk5d is verified up and running
    May 17 08:01:32.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:01:32.662: INFO: stderr: ""
    May 17 08:01:32.662: INFO: stdout: "true"
    May 17 08:01:32.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods update-demo-nautilus-hcwdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 08:01:32.710: INFO: stderr: ""
    May 17 08:01:32.710: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 17 08:01:32.710: INFO: validating pod update-demo-nautilus-hcwdk
    May 17 08:01:32.712: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 08:01:32.712: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 08:01:32.712: INFO: update-demo-nautilus-hcwdk is verified up and running
    STEP: using delete to clean up resources 05/17/23 08:01:32.712
    May 17 08:01:32.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 delete --grace-period=0 --force -f -'
    May 17 08:01:32.764: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:01:32.764: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May 17 08:01:32.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get rc,svc -l name=update-demo --no-headers'
    May 17 08:01:32.819: INFO: stderr: "No resources found in kubectl-566 namespace.\n"
    May 17 08:01:32.819: INFO: stdout: ""
    May 17 08:01:32.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-566 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 17 08:01:32.872: INFO: stderr: ""
    May 17 08:01:32.872: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:32.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-566" for this suite. 05/17/23 08:01:32.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:32.879
May 17 08:01:32.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:01:32.88
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:32.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:32.888
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-9b2140aa-7505-4c77-824d-e728effe71f9 05/17/23 08:01:32.892
STEP: Creating secret with name s-test-opt-upd-5541b90d-839d-4fc0-acfd-57777f80404d 05/17/23 08:01:32.894
STEP: Creating the pod 05/17/23 08:01:32.896
May 17 08:01:32.900: INFO: Waiting up to 5m0s for pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa" in namespace "secrets-5891" to be "running and ready"
May 17 08:01:32.902: INFO: Pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245491ms
May 17 08:01:32.902: INFO: The phase of Pod pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa is Pending, waiting for it to be Running (with Ready = true)
May 17 08:01:34.905: INFO: Pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004097565s
May 17 08:01:34.905: INFO: The phase of Pod pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa is Running (Ready = true)
May 17 08:01:34.905: INFO: Pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-9b2140aa-7505-4c77-824d-e728effe71f9 05/17/23 08:01:34.915
STEP: Updating secret s-test-opt-upd-5541b90d-839d-4fc0-acfd-57777f80404d 05/17/23 08:01:34.918
STEP: Creating secret with name s-test-opt-create-81cd6b87-5dc8-43d9-906f-092bca0b0541 05/17/23 08:01:34.92
STEP: waiting to observe update in volume 05/17/23 08:01:34.923
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:01:38.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5891" for this suite. 05/17/23 08:01:38.946
------------------------------
â€¢ [SLOW TEST] [6.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:32.879
    May 17 08:01:32.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:01:32.88
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:32.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:32.888
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-9b2140aa-7505-4c77-824d-e728effe71f9 05/17/23 08:01:32.892
    STEP: Creating secret with name s-test-opt-upd-5541b90d-839d-4fc0-acfd-57777f80404d 05/17/23 08:01:32.894
    STEP: Creating the pod 05/17/23 08:01:32.896
    May 17 08:01:32.900: INFO: Waiting up to 5m0s for pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa" in namespace "secrets-5891" to be "running and ready"
    May 17 08:01:32.902: INFO: Pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245491ms
    May 17 08:01:32.902: INFO: The phase of Pod pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:01:34.905: INFO: Pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004097565s
    May 17 08:01:34.905: INFO: The phase of Pod pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa is Running (Ready = true)
    May 17 08:01:34.905: INFO: Pod "pod-secrets-3d3576bc-cc24-451f-8dbe-98c51bc325aa" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-9b2140aa-7505-4c77-824d-e728effe71f9 05/17/23 08:01:34.915
    STEP: Updating secret s-test-opt-upd-5541b90d-839d-4fc0-acfd-57777f80404d 05/17/23 08:01:34.918
    STEP: Creating secret with name s-test-opt-create-81cd6b87-5dc8-43d9-906f-092bca0b0541 05/17/23 08:01:34.92
    STEP: waiting to observe update in volume 05/17/23 08:01:34.923
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:38.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5891" for this suite. 05/17/23 08:01:38.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:38.951
May 17 08:01:38.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:01:38.951
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:38.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:38.96
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-6947d44e-9bf3-4f01-bbd8-032cc897bd58 05/17/23 08:01:38.961
STEP: Creating a pod to test consume secrets 05/17/23 08:01:38.964
May 17 08:01:38.968: INFO: Waiting up to 5m0s for pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215" in namespace "secrets-9478" to be "Succeeded or Failed"
May 17 08:01:38.970: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215": Phase="Pending", Reason="", readiness=false. Elapsed: 1.313941ms
May 17 08:01:40.973: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004647436s
May 17 08:01:42.973: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004607773s
STEP: Saw pod success 05/17/23 08:01:42.973
May 17 08:01:42.973: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215" satisfied condition "Succeeded or Failed"
May 17 08:01:42.975: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:01:42.986
May 17 08:01:42.992: INFO: Waiting for pod pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215 to disappear
May 17 08:01:42.993: INFO: Pod pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:01:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9478" for this suite. 05/17/23 08:01:42.995
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:38.951
    May 17 08:01:38.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:01:38.951
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:38.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:38.96
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-6947d44e-9bf3-4f01-bbd8-032cc897bd58 05/17/23 08:01:38.961
    STEP: Creating a pod to test consume secrets 05/17/23 08:01:38.964
    May 17 08:01:38.968: INFO: Waiting up to 5m0s for pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215" in namespace "secrets-9478" to be "Succeeded or Failed"
    May 17 08:01:38.970: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215": Phase="Pending", Reason="", readiness=false. Elapsed: 1.313941ms
    May 17 08:01:40.973: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004647436s
    May 17 08:01:42.973: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004607773s
    STEP: Saw pod success 05/17/23 08:01:42.973
    May 17 08:01:42.973: INFO: Pod "pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215" satisfied condition "Succeeded or Failed"
    May 17 08:01:42.975: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:01:42.986
    May 17 08:01:42.992: INFO: Waiting for pod pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215 to disappear
    May 17 08:01:42.993: INFO: Pod pod-secrets-15088e40-97c1-4e60-b0f0-043bc8b99215 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9478" for this suite. 05/17/23 08:01:42.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:42.998
May 17 08:01:42.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 08:01:42.999
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:43.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:43.006
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May 17 08:01:43.016: INFO: Waiting up to 5m0s for pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398" in namespace "emptydir-wrapper-6748" to be "running and ready"
May 17 08:01:43.017: INFO: Pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398": Phase="Pending", Reason="", readiness=false. Elapsed: 1.061939ms
May 17 08:01:43.017: INFO: The phase of Pod pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:01:45.020: INFO: Pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398": Phase="Running", Reason="", readiness=true. Elapsed: 2.004673817s
May 17 08:01:45.020: INFO: The phase of Pod pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398 is Running (Ready = true)
May 17 08:01:45.020: INFO: Pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/17/23 08:01:45.022
STEP: Cleaning up the configmap 05/17/23 08:01:45.025
STEP: Cleaning up the pod 05/17/23 08:01:45.029
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:01:45.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-6748" for this suite. 05/17/23 08:01:45.036
------------------------------
â€¢ [2.041 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:42.998
    May 17 08:01:42.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 08:01:42.999
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:43.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:43.006
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May 17 08:01:43.016: INFO: Waiting up to 5m0s for pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398" in namespace "emptydir-wrapper-6748" to be "running and ready"
    May 17 08:01:43.017: INFO: Pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398": Phase="Pending", Reason="", readiness=false. Elapsed: 1.061939ms
    May 17 08:01:43.017: INFO: The phase of Pod pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:01:45.020: INFO: Pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398": Phase="Running", Reason="", readiness=true. Elapsed: 2.004673817s
    May 17 08:01:45.020: INFO: The phase of Pod pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398 is Running (Ready = true)
    May 17 08:01:45.020: INFO: Pod "pod-secrets-32c3d08f-6ce4-4c65-81d3-0fd804e2a398" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/17/23 08:01:45.022
    STEP: Cleaning up the configmap 05/17/23 08:01:45.025
    STEP: Cleaning up the pod 05/17/23 08:01:45.029
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:45.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-6748" for this suite. 05/17/23 08:01:45.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:45.04
May 17 08:01:45.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 08:01:45.041
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:45.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:45.049
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May 17 08:01:45.051: INFO: Creating simple deployment test-new-deployment
May 17 08:01:45.058: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 05/17/23 08:01:47.064
STEP: updating a scale subresource 05/17/23 08:01:47.065
STEP: verifying the deployment Spec.Replicas was modified 05/17/23 08:01:47.069
STEP: Patch a scale subresource 05/17/23 08:01:47.07
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 08:01:47.080: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9819  74e5e644-8e3e-47ed-bd29-5c1c1a66b781 1196225 3 2023-05-17 08:01:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-17 08:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:01:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b35eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 08:01:46 +0000 UTC,LastTransitionTime:2023-05-17 08:01:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-17 08:01:46 +0000 UTC,LastTransitionTime:2023-05-17 08:01:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 08:01:47.082: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9819  fc82e90e-851d-48ba-a31e-c74464503a1e 1196229 2 2023-05-17 08:01:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 74e5e644-8e3e-47ed-bd29-5c1c1a66b781 0xc004766967 0xc004766968}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:01:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74e5e644-8e3e-47ed-bd29-5c1c1a66b781\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:01:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004766a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 08:01:47.085: INFO: Pod "test-new-deployment-7f5969cbc7-h4hj6" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-h4hj6 test-new-deployment-7f5969cbc7- deployment-9819  2f4399ce-23b2-49fc-a107-53f56d9dcc0a 1196230 0 2023-05-17 08:01:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fc82e90e-851d-48ba-a31e-c74464503a1e 0xc004766e47 0xc004766e48}] [] [{kube-controller-manager Update v1 2023-05-17 08:01:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc82e90e-851d-48ba-a31e-c74464503a1e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-998ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-998ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:01:47.085: INFO: Pod "test-new-deployment-7f5969cbc7-qvn24" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qvn24 test-new-deployment-7f5969cbc7- deployment-9819  df0e69aa-613f-4208-b321-23b1ee370042 1196220 0 2023-05-17 08:01:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4e64c435dd76d38f33d1ec23ba2192cd2c0285271e273b36f6a35b35a022bd86 cni.projectcalico.org/podIP:192.168.36.120/32 cni.projectcalico.org/podIPs:192.168.36.120/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fc82e90e-851d-48ba-a31e-c74464503a1e 0xc004766ff7 0xc004766ff8}] [] [{calico Update v1 2023-05-17 08:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 08:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc82e90e-851d-48ba-a31e-c74464503a1e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:01:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dxjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dxjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.120,StartTime:2023-05-17 08:01:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:01:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c85f2bd35097cd06d79fa6a05a77dde8f1b2e430a7340761f5b313e050ec3f7d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 08:01:47.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9819" for this suite. 05/17/23 08:01:47.088
------------------------------
â€¢ [2.052 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:45.04
    May 17 08:01:45.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 08:01:45.041
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:45.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:45.049
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May 17 08:01:45.051: INFO: Creating simple deployment test-new-deployment
    May 17 08:01:45.058: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 05/17/23 08:01:47.064
    STEP: updating a scale subresource 05/17/23 08:01:47.065
    STEP: verifying the deployment Spec.Replicas was modified 05/17/23 08:01:47.069
    STEP: Patch a scale subresource 05/17/23 08:01:47.07
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 08:01:47.080: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9819  74e5e644-8e3e-47ed-bd29-5c1c1a66b781 1196225 3 2023-05-17 08:01:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-17 08:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:01:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b35eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 08:01:46 +0000 UTC,LastTransitionTime:2023-05-17 08:01:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-17 08:01:46 +0000 UTC,LastTransitionTime:2023-05-17 08:01:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 08:01:47.082: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9819  fc82e90e-851d-48ba-a31e-c74464503a1e 1196229 2 2023-05-17 08:01:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 74e5e644-8e3e-47ed-bd29-5c1c1a66b781 0xc004766967 0xc004766968}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:01:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74e5e644-8e3e-47ed-bd29-5c1c1a66b781\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:01:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004766a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 08:01:47.085: INFO: Pod "test-new-deployment-7f5969cbc7-h4hj6" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-h4hj6 test-new-deployment-7f5969cbc7- deployment-9819  2f4399ce-23b2-49fc-a107-53f56d9dcc0a 1196230 0 2023-05-17 08:01:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fc82e90e-851d-48ba-a31e-c74464503a1e 0xc004766e47 0xc004766e48}] [] [{kube-controller-manager Update v1 2023-05-17 08:01:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc82e90e-851d-48ba-a31e-c74464503a1e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-998ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-998ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:01:47.085: INFO: Pod "test-new-deployment-7f5969cbc7-qvn24" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qvn24 test-new-deployment-7f5969cbc7- deployment-9819  df0e69aa-613f-4208-b321-23b1ee370042 1196220 0 2023-05-17 08:01:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4e64c435dd76d38f33d1ec23ba2192cd2c0285271e273b36f6a35b35a022bd86 cni.projectcalico.org/podIP:192.168.36.120/32 cni.projectcalico.org/podIPs:192.168.36.120/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fc82e90e-851d-48ba-a31e-c74464503a1e 0xc004766ff7 0xc004766ff8}] [] [{calico Update v1 2023-05-17 08:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 08:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc82e90e-851d-48ba-a31e-c74464503a1e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:01:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dxjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dxjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:01:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.120,StartTime:2023-05-17 08:01:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:01:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c85f2bd35097cd06d79fa6a05a77dde8f1b2e430a7340761f5b313e050ec3f7d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:47.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9819" for this suite. 05/17/23 08:01:47.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:47.092
May 17 08:01:47.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:01:47.093
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:47.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:47.102
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 08:01:47.104
May 17 08:01:47.108: INFO: Waiting up to 5m0s for pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23" in namespace "emptydir-5725" to be "Succeeded or Failed"
May 17 08:01:47.109: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23": Phase="Pending", Reason="", readiness=false. Elapsed: 1.203795ms
May 17 08:01:49.113: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005302004s
May 17 08:01:51.114: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005521477s
STEP: Saw pod success 05/17/23 08:01:51.114
May 17 08:01:51.114: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23" satisfied condition "Succeeded or Failed"
May 17 08:01:51.115: INFO: Trying to get logs from node k8s-node1 pod pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23 container test-container: <nil>
STEP: delete the pod 05/17/23 08:01:51.119
May 17 08:01:51.124: INFO: Waiting for pod pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23 to disappear
May 17 08:01:51.125: INFO: Pod pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:01:51.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5725" for this suite. 05/17/23 08:01:51.127
------------------------------
â€¢ [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:47.092
    May 17 08:01:47.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:01:47.093
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:47.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:47.102
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 08:01:47.104
    May 17 08:01:47.108: INFO: Waiting up to 5m0s for pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23" in namespace "emptydir-5725" to be "Succeeded or Failed"
    May 17 08:01:47.109: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23": Phase="Pending", Reason="", readiness=false. Elapsed: 1.203795ms
    May 17 08:01:49.113: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005302004s
    May 17 08:01:51.114: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005521477s
    STEP: Saw pod success 05/17/23 08:01:51.114
    May 17 08:01:51.114: INFO: Pod "pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23" satisfied condition "Succeeded or Failed"
    May 17 08:01:51.115: INFO: Trying to get logs from node k8s-node1 pod pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:01:51.119
    May 17 08:01:51.124: INFO: Waiting for pod pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23 to disappear
    May 17 08:01:51.125: INFO: Pod pod-32cc82d6-f702-4ed2-9fbd-6dceac303e23 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:51.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5725" for this suite. 05/17/23 08:01:51.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:51.13
May 17 08:01:51.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubelet-test 05/17/23 08:01:51.132
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:51.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:51.14
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 05/17/23 08:01:51.146
May 17 08:01:51.146: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7" in namespace "kubelet-test-7552" to be "completed"
May 17 08:01:51.148: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.388985ms
May 17 08:01:53.151: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004858926s
May 17 08:01:55.150: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003503599s
May 17 08:01:55.150: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 17 08:01:55.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7552" for this suite. 05/17/23 08:01:55.155
------------------------------
â€¢ [4.028 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:51.13
    May 17 08:01:51.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 08:01:51.132
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:51.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:51.14
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 05/17/23 08:01:51.146
    May 17 08:01:51.146: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7" in namespace "kubelet-test-7552" to be "completed"
    May 17 08:01:51.148: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.388985ms
    May 17 08:01:53.151: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004858926s
    May 17 08:01:55.150: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003503599s
    May 17 08:01:55.150: INFO: Pod "agnhost-host-aliases49d02175-6179-478b-bc49-0903ac2118b7" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:55.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7552" for this suite. 05/17/23 08:01:55.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:55.159
May 17 08:01:55.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 08:01:55.16
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:55.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:55.167
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/17/23 08:01:55.168
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/17/23 08:01:55.17
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/17/23 08:01:55.17
STEP: creating a pod to probe DNS 05/17/23 08:01:55.17
STEP: submitting the pod to kubernetes 05/17/23 08:01:55.17
May 17 08:01:55.176: INFO: Waiting up to 15m0s for pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87" in namespace "dns-34" to be "running"
May 17 08:01:55.177: INFO: Pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.296424ms
May 17 08:01:57.179: INFO: Pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87": Phase="Running", Reason="", readiness=true. Elapsed: 2.003205832s
May 17 08:01:57.179: INFO: Pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87" satisfied condition "running"
STEP: retrieving the pod 05/17/23 08:01:57.179
STEP: looking for the results for each expected name from probers 05/17/23 08:01:57.181
May 17 08:01:57.187: INFO: DNS probes using dns-34/dns-test-25b5c301-aa36-419d-b88b-efe603144e87 succeeded

STEP: deleting the pod 05/17/23 08:01:57.187
STEP: deleting the test headless service 05/17/23 08:01:57.193
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 08:01:57.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-34" for this suite. 05/17/23 08:01:57.2
------------------------------
â€¢ [2.044 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:55.159
    May 17 08:01:55.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 08:01:55.16
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:55.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:55.167
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/17/23 08:01:55.168
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/17/23 08:01:55.17
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-34.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/17/23 08:01:55.17
    STEP: creating a pod to probe DNS 05/17/23 08:01:55.17
    STEP: submitting the pod to kubernetes 05/17/23 08:01:55.17
    May 17 08:01:55.176: INFO: Waiting up to 15m0s for pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87" in namespace "dns-34" to be "running"
    May 17 08:01:55.177: INFO: Pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.296424ms
    May 17 08:01:57.179: INFO: Pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87": Phase="Running", Reason="", readiness=true. Elapsed: 2.003205832s
    May 17 08:01:57.179: INFO: Pod "dns-test-25b5c301-aa36-419d-b88b-efe603144e87" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 08:01:57.179
    STEP: looking for the results for each expected name from probers 05/17/23 08:01:57.181
    May 17 08:01:57.187: INFO: DNS probes using dns-34/dns-test-25b5c301-aa36-419d-b88b-efe603144e87 succeeded

    STEP: deleting the pod 05/17/23 08:01:57.187
    STEP: deleting the test headless service 05/17/23 08:01:57.193
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 08:01:57.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-34" for this suite. 05/17/23 08:01:57.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:01:57.203
May 17 08:01:57.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 08:01:57.204
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:57.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:57.211
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9464 05/17/23 08:01:57.212
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-9464 05/17/23 08:01:57.216
May 17 08:01:57.220: INFO: Found 0 stateful pods, waiting for 1
May 17 08:02:07.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/17/23 08:02:07.226
STEP: Getting /status 05/17/23 08:02:07.23
May 17 08:02:07.232: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/17/23 08:02:07.232
May 17 08:02:07.237: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/17/23 08:02:07.237
May 17 08:02:07.238: INFO: Observed &StatefulSet event: ADDED
May 17 08:02:07.238: INFO: Found Statefulset ss in namespace statefulset-9464 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 08:02:07.238: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/17/23 08:02:07.238
May 17 08:02:07.238: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 17 08:02:07.242: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/17/23 08:02:07.242
May 17 08:02:07.243: INFO: Observed &StatefulSet event: ADDED
May 17 08:02:07.243: INFO: Observed Statefulset ss in namespace statefulset-9464 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 08:02:07.243: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 08:02:07.243: INFO: Deleting all statefulset in ns statefulset-9464
May 17 08:02:07.244: INFO: Scaling statefulset ss to 0
May 17 08:02:17.254: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:02:17.255: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 08:02:17.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9464" for this suite. 05/17/23 08:02:17.263
------------------------------
â€¢ [SLOW TEST] [20.062 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:01:57.203
    May 17 08:01:57.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 08:01:57.204
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:01:57.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:01:57.211
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9464 05/17/23 08:01:57.212
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-9464 05/17/23 08:01:57.216
    May 17 08:01:57.220: INFO: Found 0 stateful pods, waiting for 1
    May 17 08:02:07.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/17/23 08:02:07.226
    STEP: Getting /status 05/17/23 08:02:07.23
    May 17 08:02:07.232: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/17/23 08:02:07.232
    May 17 08:02:07.237: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/17/23 08:02:07.237
    May 17 08:02:07.238: INFO: Observed &StatefulSet event: ADDED
    May 17 08:02:07.238: INFO: Found Statefulset ss in namespace statefulset-9464 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 08:02:07.238: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/17/23 08:02:07.238
    May 17 08:02:07.238: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 17 08:02:07.242: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/17/23 08:02:07.242
    May 17 08:02:07.243: INFO: Observed &StatefulSet event: ADDED
    May 17 08:02:07.243: INFO: Observed Statefulset ss in namespace statefulset-9464 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 08:02:07.243: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 08:02:07.243: INFO: Deleting all statefulset in ns statefulset-9464
    May 17 08:02:07.244: INFO: Scaling statefulset ss to 0
    May 17 08:02:17.254: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:02:17.255: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:17.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9464" for this suite. 05/17/23 08:02:17.263
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:17.265
May 17 08:02:17.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename tables 05/17/23 08:02:17.266
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:17.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:17.273
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
May 17 08:02:17.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-7278" for this suite. 05/17/23 08:02:17.278
------------------------------
â€¢ [0.015 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:17.265
    May 17 08:02:17.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename tables 05/17/23 08:02:17.266
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:17.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:17.273
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:17.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-7278" for this suite. 05/17/23 08:02:17.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:17.281
May 17 08:02:17.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:02:17.282
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:17.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:17.289
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-62c42e01-064d-4b57-925d-41e00b34e36c 05/17/23 08:02:17.291
STEP: Creating a pod to test consume configMaps 05/17/23 08:02:17.293
May 17 08:02:17.298: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6" in namespace "projected-9839" to be "Succeeded or Failed"
May 17 08:02:17.299: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2723ms
May 17 08:02:19.302: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004746232s
May 17 08:02:21.302: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004566855s
STEP: Saw pod success 05/17/23 08:02:21.302
May 17 08:02:21.302: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6" satisfied condition "Succeeded or Failed"
May 17 08:02:21.304: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6 container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/17/23 08:02:21.309
May 17 08:02:21.316: INFO: Waiting for pod pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6 to disappear
May 17 08:02:21.317: INFO: Pod pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 08:02:21.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9839" for this suite. 05/17/23 08:02:21.319
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:17.281
    May 17 08:02:17.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:02:17.282
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:17.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:17.289
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-62c42e01-064d-4b57-925d-41e00b34e36c 05/17/23 08:02:17.291
    STEP: Creating a pod to test consume configMaps 05/17/23 08:02:17.293
    May 17 08:02:17.298: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6" in namespace "projected-9839" to be "Succeeded or Failed"
    May 17 08:02:17.299: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2723ms
    May 17 08:02:19.302: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004746232s
    May 17 08:02:21.302: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004566855s
    STEP: Saw pod success 05/17/23 08:02:21.302
    May 17 08:02:21.302: INFO: Pod "pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6" satisfied condition "Succeeded or Failed"
    May 17 08:02:21.304: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:02:21.309
    May 17 08:02:21.316: INFO: Waiting for pod pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6 to disappear
    May 17 08:02:21.317: INFO: Pod pod-projected-configmaps-a0e124cc-3102-4742-a516-66b557658bf6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:21.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9839" for this suite. 05/17/23 08:02:21.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:21.322
May 17 08:02:21.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename daemonsets 05/17/23 08:02:21.323
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:21.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:21.33
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
May 17 08:02:21.338: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/17/23 08:02:21.341
May 17 08:02:21.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:02:21.342: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/17/23 08:02:21.342
May 17 08:02:21.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:02:21.351: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:02:22.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:02:22.353: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/17/23 08:02:22.355
May 17 08:02:22.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:02:22.365: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May 17 08:02:23.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:02:23.368: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/17/23 08:02:23.368
May 17 08:02:23.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:02:23.375: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:02:24.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:02:24.377: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:02:25.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:02:25.377: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:02:26.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:02:26.376: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:02:26.379
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5481, will wait for the garbage collector to delete the pods 05/17/23 08:02:26.379
May 17 08:02:26.434: INFO: Deleting DaemonSet.extensions daemon-set took: 2.805079ms
May 17 08:02:26.535: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.986938ms
May 17 08:02:29.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:02:29.438: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 08:02:29.439: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1196628"},"items":null}

May 17 08:02:29.440: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1196628"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:02:29.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5481" for this suite. 05/17/23 08:02:29.453
------------------------------
â€¢ [SLOW TEST] [8.134 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:21.322
    May 17 08:02:21.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename daemonsets 05/17/23 08:02:21.323
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:21.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:21.33
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    May 17 08:02:21.338: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/17/23 08:02:21.341
    May 17 08:02:21.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:02:21.342: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/17/23 08:02:21.342
    May 17 08:02:21.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:02:21.351: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:02:22.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:02:22.353: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/17/23 08:02:22.355
    May 17 08:02:22.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:02:22.365: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    May 17 08:02:23.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:02:23.368: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/17/23 08:02:23.368
    May 17 08:02:23.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:02:23.375: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:02:24.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:02:24.377: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:02:25.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:02:25.377: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:02:26.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:02:26.376: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:02:26.379
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5481, will wait for the garbage collector to delete the pods 05/17/23 08:02:26.379
    May 17 08:02:26.434: INFO: Deleting DaemonSet.extensions daemon-set took: 2.805079ms
    May 17 08:02:26.535: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.986938ms
    May 17 08:02:29.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:02:29.438: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 08:02:29.439: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1196628"},"items":null}

    May 17 08:02:29.440: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1196628"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:29.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5481" for this suite. 05/17/23 08:02:29.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:29.457
May 17 08:02:29.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:02:29.457
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:29.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:29.467
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 05/17/23 08:02:29.469
May 17 08:02:29.469: INFO: namespace kubectl-9354
May 17 08:02:29.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 create -f -'
May 17 08:02:29.610: INFO: stderr: ""
May 17 08:02:29.610: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/17/23 08:02:29.61
May 17 08:02:30.614: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:02:30.614: INFO: Found 0 / 1
May 17 08:02:31.614: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:02:31.614: INFO: Found 1 / 1
May 17 08:02:31.614: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 17 08:02:31.616: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:02:31.616: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 08:02:31.616: INFO: wait on agnhost-primary startup in kubectl-9354 
May 17 08:02:31.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 logs agnhost-primary-2c2mq agnhost-primary'
May 17 08:02:31.672: INFO: stderr: ""
May 17 08:02:31.672: INFO: stdout: "Paused\n"
STEP: exposing RC 05/17/23 08:02:31.672
May 17 08:02:31.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 17 08:02:31.732: INFO: stderr: ""
May 17 08:02:31.732: INFO: stdout: "service/rm2 exposed\n"
May 17 08:02:31.734: INFO: Service rm2 in namespace kubectl-9354 found.
STEP: exposing service 05/17/23 08:02:33.738
May 17 08:02:33.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 17 08:02:33.807: INFO: stderr: ""
May 17 08:02:33.807: INFO: stdout: "service/rm3 exposed\n"
May 17 08:02:33.809: INFO: Service rm3 in namespace kubectl-9354 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:02:35.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9354" for this suite. 05/17/23 08:02:35.815
------------------------------
â€¢ [SLOW TEST] [6.362 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:29.457
    May 17 08:02:29.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:02:29.457
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:29.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:29.467
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 05/17/23 08:02:29.469
    May 17 08:02:29.469: INFO: namespace kubectl-9354
    May 17 08:02:29.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 create -f -'
    May 17 08:02:29.610: INFO: stderr: ""
    May 17 08:02:29.610: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/17/23 08:02:29.61
    May 17 08:02:30.614: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:02:30.614: INFO: Found 0 / 1
    May 17 08:02:31.614: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:02:31.614: INFO: Found 1 / 1
    May 17 08:02:31.614: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May 17 08:02:31.616: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:02:31.616: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 17 08:02:31.616: INFO: wait on agnhost-primary startup in kubectl-9354 
    May 17 08:02:31.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 logs agnhost-primary-2c2mq agnhost-primary'
    May 17 08:02:31.672: INFO: stderr: ""
    May 17 08:02:31.672: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/17/23 08:02:31.672
    May 17 08:02:31.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May 17 08:02:31.732: INFO: stderr: ""
    May 17 08:02:31.732: INFO: stdout: "service/rm2 exposed\n"
    May 17 08:02:31.734: INFO: Service rm2 in namespace kubectl-9354 found.
    STEP: exposing service 05/17/23 08:02:33.738
    May 17 08:02:33.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9354 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May 17 08:02:33.807: INFO: stderr: ""
    May 17 08:02:33.807: INFO: stdout: "service/rm3 exposed\n"
    May 17 08:02:33.809: INFO: Service rm3 in namespace kubectl-9354 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:35.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9354" for this suite. 05/17/23 08:02:35.815
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:35.819
May 17 08:02:35.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-webhook 05/17/23 08:02:35.819
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:35.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:35.828
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/17/23 08:02:35.829
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 08:02:36.067
STEP: Deploying the custom resource conversion webhook pod 05/17/23 08:02:36.074
STEP: Wait for the deployment to be ready 05/17/23 08:02:36.081
May 17 08:02:36.084: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 08:02:38.09
STEP: Verifying the service has paired with the endpoint 05/17/23 08:02:38.099
May 17 08:02:39.099: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May 17 08:02:39.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Creating a v1 custom resource 05/17/23 08:02:41.672
STEP: v2 custom resource should be converted 05/17/23 08:02:41.675
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:02:42.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7287" for this suite. 05/17/23 08:02:42.206
------------------------------
â€¢ [SLOW TEST] [6.390 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:35.819
    May 17 08:02:35.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-webhook 05/17/23 08:02:35.819
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:35.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:35.828
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/17/23 08:02:35.829
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 08:02:36.067
    STEP: Deploying the custom resource conversion webhook pod 05/17/23 08:02:36.074
    STEP: Wait for the deployment to be ready 05/17/23 08:02:36.081
    May 17 08:02:36.084: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 08:02:38.09
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:02:38.099
    May 17 08:02:39.099: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May 17 08:02:39.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Creating a v1 custom resource 05/17/23 08:02:41.672
    STEP: v2 custom resource should be converted 05/17/23 08:02:41.675
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:42.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7287" for this suite. 05/17/23 08:02:42.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:42.21
May 17 08:02:42.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubelet-test 05/17/23 08:02:42.21
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:42.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:42.22
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May 17 08:02:42.224: INFO: Waiting up to 5m0s for pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da" in namespace "kubelet-test-1105" to be "running and ready"
May 17 08:02:42.226: INFO: Pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.227825ms
May 17 08:02:42.226: INFO: The phase of Pod busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da is Pending, waiting for it to be Running (with Ready = true)
May 17 08:02:44.228: INFO: Pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da": Phase="Running", Reason="", readiness=true. Elapsed: 2.003527036s
May 17 08:02:44.228: INFO: The phase of Pod busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da is Running (Ready = true)
May 17 08:02:44.228: INFO: Pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 17 08:02:44.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1105" for this suite. 05/17/23 08:02:44.235
------------------------------
â€¢ [2.028 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:42.21
    May 17 08:02:42.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 08:02:42.21
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:42.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:42.22
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May 17 08:02:42.224: INFO: Waiting up to 5m0s for pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da" in namespace "kubelet-test-1105" to be "running and ready"
    May 17 08:02:42.226: INFO: Pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.227825ms
    May 17 08:02:42.226: INFO: The phase of Pod busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:02:44.228: INFO: Pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da": Phase="Running", Reason="", readiness=true. Elapsed: 2.003527036s
    May 17 08:02:44.228: INFO: The phase of Pod busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da is Running (Ready = true)
    May 17 08:02:44.228: INFO: Pod "busybox-scheduling-57eeac75-1057-4e2c-84de-b1b37ba247da" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:44.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1105" for this suite. 05/17/23 08:02:44.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:44.238
May 17 08:02:44.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename security-context-test 05/17/23 08:02:44.239
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:44.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:44.248
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
May 17 08:02:44.253: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67" in namespace "security-context-test-2133" to be "Succeeded or Failed"
May 17 08:02:44.255: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.393143ms
May 17 08:02:46.257: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003790641s
May 17 08:02:48.258: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004266656s
May 17 08:02:48.258: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67" satisfied condition "Succeeded or Failed"
May 17 08:02:48.261: INFO: Got logs for pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 17 08:02:48.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2133" for this suite. 05/17/23 08:02:48.263
------------------------------
â€¢ [4.028 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:44.238
    May 17 08:02:44.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename security-context-test 05/17/23 08:02:44.239
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:44.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:44.248
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    May 17 08:02:44.253: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67" in namespace "security-context-test-2133" to be "Succeeded or Failed"
    May 17 08:02:44.255: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.393143ms
    May 17 08:02:46.257: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003790641s
    May 17 08:02:48.258: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004266656s
    May 17 08:02:48.258: INFO: Pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67" satisfied condition "Succeeded or Failed"
    May 17 08:02:48.261: INFO: Got logs for pod "busybox-privileged-false-3d87c760-09f1-48ed-bcd2-7eb2ff50ad67": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:48.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2133" for this suite. 05/17/23 08:02:48.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:48.268
May 17 08:02:48.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 08:02:48.268
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:48.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:48.278
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 05/17/23 08:02:48.28
May 17 08:02:48.284: INFO: Waiting up to 5m0s for pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388" in namespace "var-expansion-8226" to be "Succeeded or Failed"
May 17 08:02:48.285: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420372ms
May 17 08:02:50.288: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004426597s
May 17 08:02:52.288: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003844568s
STEP: Saw pod success 05/17/23 08:02:52.288
May 17 08:02:52.288: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388" satisfied condition "Succeeded or Failed"
May 17 08:02:52.289: INFO: Trying to get logs from node k8s-node1 pod var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388 container dapi-container: <nil>
STEP: delete the pod 05/17/23 08:02:52.292
May 17 08:02:52.298: INFO: Waiting for pod var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388 to disappear
May 17 08:02:52.299: INFO: Pod var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 08:02:52.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8226" for this suite. 05/17/23 08:02:52.301
------------------------------
â€¢ [4.037 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:48.268
    May 17 08:02:48.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 08:02:48.268
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:48.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:48.278
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 05/17/23 08:02:48.28
    May 17 08:02:48.284: INFO: Waiting up to 5m0s for pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388" in namespace "var-expansion-8226" to be "Succeeded or Failed"
    May 17 08:02:48.285: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420372ms
    May 17 08:02:50.288: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004426597s
    May 17 08:02:52.288: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003844568s
    STEP: Saw pod success 05/17/23 08:02:52.288
    May 17 08:02:52.288: INFO: Pod "var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388" satisfied condition "Succeeded or Failed"
    May 17 08:02:52.289: INFO: Trying to get logs from node k8s-node1 pod var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 08:02:52.292
    May 17 08:02:52.298: INFO: Waiting for pod var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388 to disappear
    May 17 08:02:52.299: INFO: Pod var-expansion-a3fd27b7-b655-474d-adb8-2b365ca41388 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:52.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8226" for this suite. 05/17/23 08:02:52.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:52.304
May 17 08:02:52.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:02:52.305
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:52.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:52.313
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 05/17/23 08:02:52.314
May 17 08:02:52.318: INFO: Waiting up to 5m0s for pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1" in namespace "projected-6875" to be "running and ready"
May 17 08:02:52.319: INFO: Pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.160405ms
May 17 08:02:52.319: INFO: The phase of Pod annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:02:54.322: INFO: Pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003509863s
May 17 08:02:54.322: INFO: The phase of Pod annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1 is Running (Ready = true)
May 17 08:02:54.322: INFO: Pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1" satisfied condition "running and ready"
May 17 08:02:54.835: INFO: Successfully updated pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:02:58.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6875" for this suite. 05/17/23 08:02:58.85
------------------------------
â€¢ [SLOW TEST] [6.548 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:52.304
    May 17 08:02:52.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:02:52.305
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:52.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:52.313
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 05/17/23 08:02:52.314
    May 17 08:02:52.318: INFO: Waiting up to 5m0s for pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1" in namespace "projected-6875" to be "running and ready"
    May 17 08:02:52.319: INFO: Pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.160405ms
    May 17 08:02:52.319: INFO: The phase of Pod annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:02:54.322: INFO: Pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003509863s
    May 17 08:02:54.322: INFO: The phase of Pod annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1 is Running (Ready = true)
    May 17 08:02:54.322: INFO: Pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1" satisfied condition "running and ready"
    May 17 08:02:54.835: INFO: Successfully updated pod "annotationupdate01e19831-e590-4195-b7a2-e568a8466dd1"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:58.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6875" for this suite. 05/17/23 08:02:58.85
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:58.853
May 17 08:02:58.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:02:58.854
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:58.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:58.861
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 05/17/23 08:02:58.863
STEP: Getting a ResourceQuota 05/17/23 08:02:58.865
STEP: Updating a ResourceQuota 05/17/23 08:02:58.866
STEP: Verifying a ResourceQuota was modified 05/17/23 08:02:58.87
STEP: Deleting a ResourceQuota 05/17/23 08:02:58.871
STEP: Verifying the deleted ResourceQuota 05/17/23 08:02:58.874
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:02:58.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5548" for this suite. 05/17/23 08:02:58.876
------------------------------
â€¢ [0.026 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:58.853
    May 17 08:02:58.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:02:58.854
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:58.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:58.861
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 05/17/23 08:02:58.863
    STEP: Getting a ResourceQuota 05/17/23 08:02:58.865
    STEP: Updating a ResourceQuota 05/17/23 08:02:58.866
    STEP: Verifying a ResourceQuota was modified 05/17/23 08:02:58.87
    STEP: Deleting a ResourceQuota 05/17/23 08:02:58.871
    STEP: Verifying the deleted ResourceQuota 05/17/23 08:02:58.874
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:02:58.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5548" for this suite. 05/17/23 08:02:58.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:02:58.879
May 17 08:02:58.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:02:58.88
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:58.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:58.886
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/17/23 08:02:58.887
May 17 08:02:58.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/17/23 08:03:04.148
May 17 08:03:04.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:03:05.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:03:11.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3365" for this suite. 05/17/23 08:03:11.22
------------------------------
â€¢ [SLOW TEST] [12.344 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:02:58.879
    May 17 08:02:58.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:02:58.88
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:02:58.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:02:58.886
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/17/23 08:02:58.887
    May 17 08:02:58.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/17/23 08:03:04.148
    May 17 08:03:04.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:03:05.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:03:11.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3365" for this suite. 05/17/23 08:03:11.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:03:11.223
May 17 08:03:11.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:03:11.224
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:11.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:11.231
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 08:03:11.232
May 17 08:03:11.236: INFO: Waiting up to 5m0s for pod "pod-db06135f-1459-4211-99f3-3a09aef978d8" in namespace "emptydir-3799" to be "Succeeded or Failed"
May 17 08:03:11.238: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31421ms
May 17 08:03:13.240: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003662621s
May 17 08:03:15.241: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004552659s
STEP: Saw pod success 05/17/23 08:03:15.241
May 17 08:03:15.241: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8" satisfied condition "Succeeded or Failed"
May 17 08:03:15.242: INFO: Trying to get logs from node k8s-node1 pod pod-db06135f-1459-4211-99f3-3a09aef978d8 container test-container: <nil>
STEP: delete the pod 05/17/23 08:03:15.246
May 17 08:03:15.251: INFO: Waiting for pod pod-db06135f-1459-4211-99f3-3a09aef978d8 to disappear
May 17 08:03:15.254: INFO: Pod pod-db06135f-1459-4211-99f3-3a09aef978d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:03:15.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3799" for this suite. 05/17/23 08:03:15.256
------------------------------
â€¢ [4.035 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:03:11.223
    May 17 08:03:11.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:03:11.224
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:11.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:11.231
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 08:03:11.232
    May 17 08:03:11.236: INFO: Waiting up to 5m0s for pod "pod-db06135f-1459-4211-99f3-3a09aef978d8" in namespace "emptydir-3799" to be "Succeeded or Failed"
    May 17 08:03:11.238: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31421ms
    May 17 08:03:13.240: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003662621s
    May 17 08:03:15.241: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004552659s
    STEP: Saw pod success 05/17/23 08:03:15.241
    May 17 08:03:15.241: INFO: Pod "pod-db06135f-1459-4211-99f3-3a09aef978d8" satisfied condition "Succeeded or Failed"
    May 17 08:03:15.242: INFO: Trying to get logs from node k8s-node1 pod pod-db06135f-1459-4211-99f3-3a09aef978d8 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:03:15.246
    May 17 08:03:15.251: INFO: Waiting for pod pod-db06135f-1459-4211-99f3-3a09aef978d8 to disappear
    May 17 08:03:15.254: INFO: Pod pod-db06135f-1459-4211-99f3-3a09aef978d8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:03:15.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3799" for this suite. 05/17/23 08:03:15.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:03:15.259
May 17 08:03:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 08:03:15.26
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:15.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:15.266
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 05/17/23 08:03:15.268
May 17 08:03:15.271: INFO: created test-pod-1
May 17 08:03:15.274: INFO: created test-pod-2
May 17 08:03:15.276: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/17/23 08:03:15.276
May 17 08:03:15.276: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2690' to be running and ready
May 17 08:03:15.282: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 17 08:03:15.282: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 17 08:03:15.282: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 17 08:03:15.282: INFO: 0 / 3 pods in namespace 'pods-2690' are running and ready (0 seconds elapsed)
May 17 08:03:15.282: INFO: expected 0 pod replicas in namespace 'pods-2690', 0 are Running and Ready.
May 17 08:03:15.282: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
May 17 08:03:15.282: INFO: test-pod-1  k8s-node1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  }]
May 17 08:03:15.282: INFO: test-pod-2  k8s-node1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  }]
May 17 08:03:15.282: INFO: test-pod-3  k8s-node1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  }]
May 17 08:03:15.282: INFO: 
May 17 08:03:17.287: INFO: 3 / 3 pods in namespace 'pods-2690' are running and ready (2 seconds elapsed)
May 17 08:03:17.287: INFO: expected 0 pod replicas in namespace 'pods-2690', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/17/23 08:03:17.296
May 17 08:03:17.298: INFO: Pod quantity 3 is different from expected quantity 0
May 17 08:03:18.300: INFO: Pod quantity 3 is different from expected quantity 0
May 17 08:03:19.301: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 08:03:20.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2690" for this suite. 05/17/23 08:03:20.301
------------------------------
â€¢ [SLOW TEST] [5.045 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:03:15.259
    May 17 08:03:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 08:03:15.26
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:15.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:15.266
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 05/17/23 08:03:15.268
    May 17 08:03:15.271: INFO: created test-pod-1
    May 17 08:03:15.274: INFO: created test-pod-2
    May 17 08:03:15.276: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/17/23 08:03:15.276
    May 17 08:03:15.276: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2690' to be running and ready
    May 17 08:03:15.282: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 17 08:03:15.282: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 17 08:03:15.282: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 17 08:03:15.282: INFO: 0 / 3 pods in namespace 'pods-2690' are running and ready (0 seconds elapsed)
    May 17 08:03:15.282: INFO: expected 0 pod replicas in namespace 'pods-2690', 0 are Running and Ready.
    May 17 08:03:15.282: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
    May 17 08:03:15.282: INFO: test-pod-1  k8s-node1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  }]
    May 17 08:03:15.282: INFO: test-pod-2  k8s-node1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  }]
    May 17 08:03:15.282: INFO: test-pod-3  k8s-node1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:03:15 +0000 UTC  }]
    May 17 08:03:15.282: INFO: 
    May 17 08:03:17.287: INFO: 3 / 3 pods in namespace 'pods-2690' are running and ready (2 seconds elapsed)
    May 17 08:03:17.287: INFO: expected 0 pod replicas in namespace 'pods-2690', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/17/23 08:03:17.296
    May 17 08:03:17.298: INFO: Pod quantity 3 is different from expected quantity 0
    May 17 08:03:18.300: INFO: Pod quantity 3 is different from expected quantity 0
    May 17 08:03:19.301: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 08:03:20.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2690" for this suite. 05/17/23 08:03:20.301
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:03:20.304
May 17 08:03:20.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename gc 05/17/23 08:03:20.305
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:20.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:20.313
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/17/23 08:03:20.316
STEP: create the rc2 05/17/23 08:03:20.319
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/17/23 08:03:25.325
STEP: delete the rc simpletest-rc-to-be-deleted 05/17/23 08:03:25.522
STEP: wait for the rc to be deleted 05/17/23 08:03:25.525
May 17 08:03:30.532: INFO: 71 pods remaining
May 17 08:03:30.533: INFO: 71 pods has nil DeletionTimestamp
May 17 08:03:30.533: INFO: 
STEP: Gathering metrics 05/17/23 08:03:35.532
May 17 08:03:35.544: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
May 17 08:03:35.546: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.425639ms
May 17 08:03:35.546: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
May 17 08:03:35.546: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
May 17 08:03:35.589: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 17 08:03:35.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-26lv8" in namespace "gc-3954"
May 17 08:03:35.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-27mf8" in namespace "gc-3954"
May 17 08:03:35.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bjwg" in namespace "gc-3954"
May 17 08:03:35.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-2czzx" in namespace "gc-3954"
May 17 08:03:35.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w6jn" in namespace "gc-3954"
May 17 08:03:35.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-45tpp" in namespace "gc-3954"
May 17 08:03:35.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kxvq" in namespace "gc-3954"
May 17 08:03:35.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nwm8" in namespace "gc-3954"
May 17 08:03:35.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pgfg" in namespace "gc-3954"
May 17 08:03:35.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zr6h" in namespace "gc-3954"
May 17 08:03:35.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-59xhx" in namespace "gc-3954"
May 17 08:03:35.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-67lq2" in namespace "gc-3954"
May 17 08:03:35.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fgmw" in namespace "gc-3954"
May 17 08:03:35.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kd2r" in namespace "gc-3954"
May 17 08:03:35.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kxfw" in namespace "gc-3954"
May 17 08:03:35.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-6q4vz" in namespace "gc-3954"
May 17 08:03:35.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bn6f" in namespace "gc-3954"
May 17 08:03:35.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fvrr" in namespace "gc-3954"
May 17 08:03:35.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-86gm8" in namespace "gc-3954"
May 17 08:03:35.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-875nw" in namespace "gc-3954"
May 17 08:03:35.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-87bcx" in namespace "gc-3954"
May 17 08:03:35.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-89pn5" in namespace "gc-3954"
May 17 08:03:35.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jrg4" in namespace "gc-3954"
May 17 08:03:35.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z9vw" in namespace "gc-3954"
May 17 08:03:35.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-987fj" in namespace "gc-3954"
May 17 08:03:35.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lxpl" in namespace "gc-3954"
May 17 08:03:35.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r4mf" in namespace "gc-3954"
May 17 08:03:35.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wft9" in namespace "gc-3954"
May 17 08:03:35.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wttf" in namespace "gc-3954"
May 17 08:03:35.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2ckd" in namespace "gc-3954"
May 17 08:03:35.762: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfqg6" in namespace "gc-3954"
May 17 08:03:35.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn4z6" in namespace "gc-3954"
May 17 08:03:35.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnk68" in namespace "gc-3954"
May 17 08:03:35.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx9gg" in namespace "gc-3954"
May 17 08:03:35.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-c47xm" in namespace "gc-3954"
May 17 08:03:35.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9h62" in namespace "gc-3954"
May 17 08:03:35.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc68l" in namespace "gc-3954"
May 17 08:03:35.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqgth" in namespace "gc-3954"
May 17 08:03:35.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctgnx" in namespace "gc-3954"
May 17 08:03:35.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsl6g" in namespace "gc-3954"
May 17 08:03:35.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsqfd" in namespace "gc-3954"
May 17 08:03:35.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm59b" in namespace "gc-3954"
May 17 08:03:35.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt2bc" in namespace "gc-3954"
May 17 08:03:35.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-h66mh" in namespace "gc-3954"
May 17 08:03:35.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7tz2" in namespace "gc-3954"
May 17 08:03:35.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm5vq" in namespace "gc-3954"
May 17 08:03:35.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvfpm" in namespace "gc-3954"
May 17 08:03:35.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxp98" in namespace "gc-3954"
May 17 08:03:35.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-jbxfh" in namespace "gc-3954"
May 17 08:03:35.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqwxd" in namespace "gc-3954"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 17 08:03:35.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3954" for this suite. 05/17/23 08:03:35.87
------------------------------
â€¢ [SLOW TEST] [15.575 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:03:20.304
    May 17 08:03:20.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename gc 05/17/23 08:03:20.305
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:20.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:20.313
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/17/23 08:03:20.316
    STEP: create the rc2 05/17/23 08:03:20.319
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/17/23 08:03:25.325
    STEP: delete the rc simpletest-rc-to-be-deleted 05/17/23 08:03:25.522
    STEP: wait for the rc to be deleted 05/17/23 08:03:25.525
    May 17 08:03:30.532: INFO: 71 pods remaining
    May 17 08:03:30.533: INFO: 71 pods has nil DeletionTimestamp
    May 17 08:03:30.533: INFO: 
    STEP: Gathering metrics 05/17/23 08:03:35.532
    May 17 08:03:35.544: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
    May 17 08:03:35.546: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.425639ms
    May 17 08:03:35.546: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
    May 17 08:03:35.546: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
    May 17 08:03:35.589: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May 17 08:03:35.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-26lv8" in namespace "gc-3954"
    May 17 08:03:35.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-27mf8" in namespace "gc-3954"
    May 17 08:03:35.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bjwg" in namespace "gc-3954"
    May 17 08:03:35.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-2czzx" in namespace "gc-3954"
    May 17 08:03:35.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w6jn" in namespace "gc-3954"
    May 17 08:03:35.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-45tpp" in namespace "gc-3954"
    May 17 08:03:35.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kxvq" in namespace "gc-3954"
    May 17 08:03:35.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nwm8" in namespace "gc-3954"
    May 17 08:03:35.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pgfg" in namespace "gc-3954"
    May 17 08:03:35.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zr6h" in namespace "gc-3954"
    May 17 08:03:35.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-59xhx" in namespace "gc-3954"
    May 17 08:03:35.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-67lq2" in namespace "gc-3954"
    May 17 08:03:35.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fgmw" in namespace "gc-3954"
    May 17 08:03:35.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kd2r" in namespace "gc-3954"
    May 17 08:03:35.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kxfw" in namespace "gc-3954"
    May 17 08:03:35.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-6q4vz" in namespace "gc-3954"
    May 17 08:03:35.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bn6f" in namespace "gc-3954"
    May 17 08:03:35.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fvrr" in namespace "gc-3954"
    May 17 08:03:35.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-86gm8" in namespace "gc-3954"
    May 17 08:03:35.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-875nw" in namespace "gc-3954"
    May 17 08:03:35.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-87bcx" in namespace "gc-3954"
    May 17 08:03:35.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-89pn5" in namespace "gc-3954"
    May 17 08:03:35.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jrg4" in namespace "gc-3954"
    May 17 08:03:35.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z9vw" in namespace "gc-3954"
    May 17 08:03:35.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-987fj" in namespace "gc-3954"
    May 17 08:03:35.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lxpl" in namespace "gc-3954"
    May 17 08:03:35.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r4mf" in namespace "gc-3954"
    May 17 08:03:35.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wft9" in namespace "gc-3954"
    May 17 08:03:35.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wttf" in namespace "gc-3954"
    May 17 08:03:35.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2ckd" in namespace "gc-3954"
    May 17 08:03:35.762: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfqg6" in namespace "gc-3954"
    May 17 08:03:35.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn4z6" in namespace "gc-3954"
    May 17 08:03:35.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnk68" in namespace "gc-3954"
    May 17 08:03:35.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx9gg" in namespace "gc-3954"
    May 17 08:03:35.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-c47xm" in namespace "gc-3954"
    May 17 08:03:35.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9h62" in namespace "gc-3954"
    May 17 08:03:35.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc68l" in namespace "gc-3954"
    May 17 08:03:35.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqgth" in namespace "gc-3954"
    May 17 08:03:35.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctgnx" in namespace "gc-3954"
    May 17 08:03:35.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsl6g" in namespace "gc-3954"
    May 17 08:03:35.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsqfd" in namespace "gc-3954"
    May 17 08:03:35.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm59b" in namespace "gc-3954"
    May 17 08:03:35.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt2bc" in namespace "gc-3954"
    May 17 08:03:35.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-h66mh" in namespace "gc-3954"
    May 17 08:03:35.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7tz2" in namespace "gc-3954"
    May 17 08:03:35.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm5vq" in namespace "gc-3954"
    May 17 08:03:35.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvfpm" in namespace "gc-3954"
    May 17 08:03:35.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxp98" in namespace "gc-3954"
    May 17 08:03:35.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-jbxfh" in namespace "gc-3954"
    May 17 08:03:35.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqwxd" in namespace "gc-3954"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 17 08:03:35.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3954" for this suite. 05/17/23 08:03:35.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:03:35.879
May 17 08:03:35.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename job 05/17/23 08:03:35.88
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:35.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:35.908
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 05/17/23 08:03:35.91
STEP: Ensuring active pods == parallelism 05/17/23 08:03:35.932
STEP: delete a job 05/17/23 08:03:45.934
STEP: deleting Job.batch foo in namespace job-7915, will wait for the garbage collector to delete the pods 05/17/23 08:03:45.934
May 17 08:03:45.990: INFO: Deleting Job.batch foo took: 2.935823ms
May 17 08:03:46.091: INFO: Terminating Job.batch foo pods took: 101.039296ms
STEP: Ensuring job was deleted 05/17/23 08:04:17.091
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 17 08:04:17.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7915" for this suite. 05/17/23 08:04:17.098
------------------------------
â€¢ [SLOW TEST] [41.223 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:03:35.879
    May 17 08:03:35.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename job 05/17/23 08:03:35.88
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:03:35.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:03:35.908
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 05/17/23 08:03:35.91
    STEP: Ensuring active pods == parallelism 05/17/23 08:03:35.932
    STEP: delete a job 05/17/23 08:03:45.934
    STEP: deleting Job.batch foo in namespace job-7915, will wait for the garbage collector to delete the pods 05/17/23 08:03:45.934
    May 17 08:03:45.990: INFO: Deleting Job.batch foo took: 2.935823ms
    May 17 08:03:46.091: INFO: Terminating Job.batch foo pods took: 101.039296ms
    STEP: Ensuring job was deleted 05/17/23 08:04:17.091
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:17.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7915" for this suite. 05/17/23 08:04:17.098
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:17.103
May 17 08:04:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:04:17.104
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:17.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:17.112
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 05/17/23 08:04:17.114
May 17 08:04:17.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-2113 cluster-info'
May 17 08:04:17.164: INFO: stderr: ""
May 17 08:04:17.164: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:04:17.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2113" for this suite. 05/17/23 08:04:17.167
------------------------------
â€¢ [0.068 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:17.103
    May 17 08:04:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:04:17.104
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:17.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:17.112
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 05/17/23 08:04:17.114
    May 17 08:04:17.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-2113 cluster-info'
    May 17 08:04:17.164: INFO: stderr: ""
    May 17 08:04:17.164: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:17.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2113" for this suite. 05/17/23 08:04:17.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:17.171
May 17 08:04:17.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:04:17.172
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:17.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:17.179
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:04:17.188
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:04:17.508
STEP: Deploying the webhook pod 05/17/23 08:04:17.513
STEP: Wait for the deployment to be ready 05/17/23 08:04:17.52
May 17 08:04:17.524: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:04:19.529
STEP: Verifying the service has paired with the endpoint 05/17/23 08:04:19.536
May 17 08:04:20.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 05/17/23 08:04:20.538
STEP: Creating a custom resource definition that should be denied by the webhook 05/17/23 08:04:20.549
May 17 08:04:20.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:04:20.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1209" for this suite. 05/17/23 08:04:20.591
STEP: Destroying namespace "webhook-1209-markers" for this suite. 05/17/23 08:04:20.595
------------------------------
â€¢ [3.429 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:17.171
    May 17 08:04:17.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:04:17.172
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:17.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:17.179
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:04:17.188
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:04:17.508
    STEP: Deploying the webhook pod 05/17/23 08:04:17.513
    STEP: Wait for the deployment to be ready 05/17/23 08:04:17.52
    May 17 08:04:17.524: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:04:19.529
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:04:19.536
    May 17 08:04:20.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/17/23 08:04:20.538
    STEP: Creating a custom resource definition that should be denied by the webhook 05/17/23 08:04:20.549
    May 17 08:04:20.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:20.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1209" for this suite. 05/17/23 08:04:20.591
    STEP: Destroying namespace "webhook-1209-markers" for this suite. 05/17/23 08:04:20.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:20.6
May 17 08:04:20.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename events 05/17/23 08:04:20.601
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:20.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:20.611
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/17/23 08:04:20.613
STEP: listing events in all namespaces 05/17/23 08:04:20.617
STEP: listing events in test namespace 05/17/23 08:04:20.619
STEP: listing events with field selection filtering on source 05/17/23 08:04:20.621
STEP: listing events with field selection filtering on reportingController 05/17/23 08:04:20.622
STEP: getting the test event 05/17/23 08:04:20.624
STEP: patching the test event 05/17/23 08:04:20.625
STEP: getting the test event 05/17/23 08:04:20.628
STEP: updating the test event 05/17/23 08:04:20.63
STEP: getting the test event 05/17/23 08:04:20.633
STEP: deleting the test event 05/17/23 08:04:20.634
STEP: listing events in all namespaces 05/17/23 08:04:20.637
STEP: listing events in test namespace 05/17/23 08:04:20.64
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May 17 08:04:20.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4305" for this suite. 05/17/23 08:04:20.643
------------------------------
â€¢ [0.047 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:20.6
    May 17 08:04:20.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename events 05/17/23 08:04:20.601
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:20.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:20.611
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/17/23 08:04:20.613
    STEP: listing events in all namespaces 05/17/23 08:04:20.617
    STEP: listing events in test namespace 05/17/23 08:04:20.619
    STEP: listing events with field selection filtering on source 05/17/23 08:04:20.621
    STEP: listing events with field selection filtering on reportingController 05/17/23 08:04:20.622
    STEP: getting the test event 05/17/23 08:04:20.624
    STEP: patching the test event 05/17/23 08:04:20.625
    STEP: getting the test event 05/17/23 08:04:20.628
    STEP: updating the test event 05/17/23 08:04:20.63
    STEP: getting the test event 05/17/23 08:04:20.633
    STEP: deleting the test event 05/17/23 08:04:20.634
    STEP: listing events in all namespaces 05/17/23 08:04:20.637
    STEP: listing events in test namespace 05/17/23 08:04:20.64
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:20.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4305" for this suite. 05/17/23 08:04:20.643
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:20.647
May 17 08:04:20.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:04:20.648
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:20.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:20.657
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:04:20.658
May 17 08:04:20.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-3546 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
May 17 08:04:20.715: INFO: stderr: ""
May 17 08:04:20.715: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 08:04:20.715
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
May 17 08:04:20.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-3546 delete pods e2e-test-httpd-pod'
May 17 08:04:23.041: INFO: stderr: ""
May 17 08:04:23.041: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:04:23.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3546" for this suite. 05/17/23 08:04:23.043
------------------------------
â€¢ [2.398 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:20.647
    May 17 08:04:20.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:04:20.648
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:20.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:20.657
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:04:20.658
    May 17 08:04:20.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-3546 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    May 17 08:04:20.715: INFO: stderr: ""
    May 17 08:04:20.715: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 08:04:20.715
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    May 17 08:04:20.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-3546 delete pods e2e-test-httpd-pod'
    May 17 08:04:23.041: INFO: stderr: ""
    May 17 08:04:23.041: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:23.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3546" for this suite. 05/17/23 08:04:23.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:23.049
May 17 08:04:23.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:04:23.049
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:23.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:23.058
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:04:23.06
May 17 08:04:23.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb" in namespace "projected-4160" to be "Succeeded or Failed"
May 17 08:04:23.066: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.477751ms
May 17 08:04:25.070: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005226977s
May 17 08:04:27.069: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005035378s
STEP: Saw pod success 05/17/23 08:04:27.07
May 17 08:04:27.070: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb" satisfied condition "Succeeded or Failed"
May 17 08:04:27.071: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb container client-container: <nil>
STEP: delete the pod 05/17/23 08:04:27.074
May 17 08:04:27.080: INFO: Waiting for pod downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb to disappear
May 17 08:04:27.081: INFO: Pod downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:04:27.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4160" for this suite. 05/17/23 08:04:27.083
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:23.049
    May 17 08:04:23.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:04:23.049
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:23.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:23.058
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:04:23.06
    May 17 08:04:23.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb" in namespace "projected-4160" to be "Succeeded or Failed"
    May 17 08:04:23.066: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.477751ms
    May 17 08:04:25.070: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005226977s
    May 17 08:04:27.069: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005035378s
    STEP: Saw pod success 05/17/23 08:04:27.07
    May 17 08:04:27.070: INFO: Pod "downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb" satisfied condition "Succeeded or Failed"
    May 17 08:04:27.071: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb container client-container: <nil>
    STEP: delete the pod 05/17/23 08:04:27.074
    May 17 08:04:27.080: INFO: Waiting for pod downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb to disappear
    May 17 08:04:27.081: INFO: Pod downwardapi-volume-eebd2ac9-3a21-432e-af35-44e0325fefeb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:27.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4160" for this suite. 05/17/23 08:04:27.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:27.087
May 17 08:04:27.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:04:27.087
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:27.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:27.095
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 08:04:27.096
May 17 08:04:27.100: INFO: Waiting up to 5m0s for pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320" in namespace "emptydir-5235" to be "Succeeded or Failed"
May 17 08:04:27.101: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320": Phase="Pending", Reason="", readiness=false. Elapsed: 1.493064ms
May 17 08:04:29.105: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004798049s
May 17 08:04:31.104: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004193911s
STEP: Saw pod success 05/17/23 08:04:31.104
May 17 08:04:31.104: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320" satisfied condition "Succeeded or Failed"
May 17 08:04:31.106: INFO: Trying to get logs from node k8s-node1 pod pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320 container test-container: <nil>
STEP: delete the pod 05/17/23 08:04:31.109
May 17 08:04:31.114: INFO: Waiting for pod pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320 to disappear
May 17 08:04:31.115: INFO: Pod pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:04:31.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5235" for this suite. 05/17/23 08:04:31.117
------------------------------
â€¢ [4.033 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:27.087
    May 17 08:04:27.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:04:27.087
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:27.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:27.095
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 08:04:27.096
    May 17 08:04:27.100: INFO: Waiting up to 5m0s for pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320" in namespace "emptydir-5235" to be "Succeeded or Failed"
    May 17 08:04:27.101: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320": Phase="Pending", Reason="", readiness=false. Elapsed: 1.493064ms
    May 17 08:04:29.105: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004798049s
    May 17 08:04:31.104: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004193911s
    STEP: Saw pod success 05/17/23 08:04:31.104
    May 17 08:04:31.104: INFO: Pod "pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320" satisfied condition "Succeeded or Failed"
    May 17 08:04:31.106: INFO: Trying to get logs from node k8s-node1 pod pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:04:31.109
    May 17 08:04:31.114: INFO: Waiting for pod pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320 to disappear
    May 17 08:04:31.115: INFO: Pod pod-ca9439d7-5492-46f3-a8ce-184a6fc1a320 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:31.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5235" for this suite. 05/17/23 08:04:31.117
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:31.12
May 17 08:04:31.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/17/23 08:04:31.12
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:31.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:31.128
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/17/23 08:04:31.13
STEP: Creating hostNetwork=false pod 05/17/23 08:04:31.13
May 17 08:04:31.134: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5745" to be "running and ready"
May 17 08:04:31.135: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.122631ms
May 17 08:04:31.135: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 17 08:04:33.138: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003673129s
May 17 08:04:33.138: INFO: The phase of Pod test-pod is Running (Ready = true)
May 17 08:04:33.138: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/17/23 08:04:33.139
May 17 08:04:33.142: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5745" to be "running and ready"
May 17 08:04:33.143: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.291007ms
May 17 08:04:33.143: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 17 08:04:35.146: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003625691s
May 17 08:04:35.146: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May 17 08:04:35.146: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/17/23 08:04:35.147
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/17/23 08:04:35.147
May 17 08:04:35.147: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.148: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.148: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 08:04:35.190: INFO: Exec stderr: ""
May 17 08:04:35.190: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.191: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.191: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 08:04:35.228: INFO: Exec stderr: ""
May 17 08:04:35.228: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.228: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.228: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 08:04:35.269: INFO: Exec stderr: ""
May 17 08:04:35.269: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.269: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.269: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 08:04:35.312: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/17/23 08:04:35.312
May 17 08:04:35.313: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.313: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.313: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May 17 08:04:35.356: INFO: Exec stderr: ""
May 17 08:04:35.356: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.356: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.356: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May 17 08:04:35.395: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/17/23 08:04:35.395
May 17 08:04:35.395: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.396: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.396: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 08:04:35.442: INFO: Exec stderr: ""
May 17 08:04:35.442: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.443: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.443: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 08:04:35.486: INFO: Exec stderr: ""
May 17 08:04:35.486: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.487: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.487: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 08:04:35.526: INFO: Exec stderr: ""
May 17 08:04:35.526: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:35.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:35.527: INFO: ExecWithOptions: Clientset creation
May 17 08:04:35.527: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 08:04:35.568: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
May 17 08:04:35.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5745" for this suite. 05/17/23 08:04:35.571
------------------------------
â€¢ [4.454 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:31.12
    May 17 08:04:31.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/17/23 08:04:31.12
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:31.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:31.128
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/17/23 08:04:31.13
    STEP: Creating hostNetwork=false pod 05/17/23 08:04:31.13
    May 17 08:04:31.134: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5745" to be "running and ready"
    May 17 08:04:31.135: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.122631ms
    May 17 08:04:31.135: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:04:33.138: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003673129s
    May 17 08:04:33.138: INFO: The phase of Pod test-pod is Running (Ready = true)
    May 17 08:04:33.138: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/17/23 08:04:33.139
    May 17 08:04:33.142: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5745" to be "running and ready"
    May 17 08:04:33.143: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.291007ms
    May 17 08:04:33.143: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:04:35.146: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003625691s
    May 17 08:04:35.146: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May 17 08:04:35.146: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/17/23 08:04:35.147
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/17/23 08:04:35.147
    May 17 08:04:35.147: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.148: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.148: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 08:04:35.190: INFO: Exec stderr: ""
    May 17 08:04:35.190: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.191: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.191: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 08:04:35.228: INFO: Exec stderr: ""
    May 17 08:04:35.228: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.228: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.228: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 08:04:35.269: INFO: Exec stderr: ""
    May 17 08:04:35.269: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.269: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.269: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 08:04:35.312: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/17/23 08:04:35.312
    May 17 08:04:35.313: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.313: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.313: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May 17 08:04:35.356: INFO: Exec stderr: ""
    May 17 08:04:35.356: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.356: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.356: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May 17 08:04:35.395: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/17/23 08:04:35.395
    May 17 08:04:35.395: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.396: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.396: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 08:04:35.442: INFO: Exec stderr: ""
    May 17 08:04:35.442: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.443: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.443: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 08:04:35.486: INFO: Exec stderr: ""
    May 17 08:04:35.486: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.487: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.487: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 08:04:35.526: INFO: Exec stderr: ""
    May 17 08:04:35.526: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5745 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:35.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:35.527: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:35.527: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5745/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 08:04:35.568: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:35.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5745" for this suite. 05/17/23 08:04:35.571
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:35.574
May 17 08:04:35.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename ephemeral-containers-test 05/17/23 08:04:35.575
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:35.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:35.582
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/17/23 08:04:35.583
May 17 08:04:35.587: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9496" to be "running and ready"
May 17 08:04:35.589: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318308ms
May 17 08:04:35.589: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May 17 08:04:37.590: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003157314s
May 17 08:04:37.590: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May 17 08:04:37.590: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/17/23 08:04:37.592
May 17 08:04:37.598: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9496" to be "container debugger running"
May 17 08:04:37.599: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.328738ms
May 17 08:04:39.602: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004762775s
May 17 08:04:41.601: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.003385194s
May 17 08:04:41.601: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/17/23 08:04:41.601
May 17 08:04:41.601: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9496 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:04:41.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:04:41.602: INFO: ExecWithOptions: Clientset creation
May 17 08:04:41.602: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-9496/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May 17 08:04:41.640: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 17 08:04:41.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-9496" for this suite. 05/17/23 08:04:41.645
------------------------------
â€¢ [SLOW TEST] [6.074 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:35.574
    May 17 08:04:35.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/17/23 08:04:35.575
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:35.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:35.582
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/17/23 08:04:35.583
    May 17 08:04:35.587: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9496" to be "running and ready"
    May 17 08:04:35.589: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318308ms
    May 17 08:04:35.589: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:04:37.590: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003157314s
    May 17 08:04:37.590: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May 17 08:04:37.590: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/17/23 08:04:37.592
    May 17 08:04:37.598: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9496" to be "container debugger running"
    May 17 08:04:37.599: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.328738ms
    May 17 08:04:39.602: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004762775s
    May 17 08:04:41.601: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.003385194s
    May 17 08:04:41.601: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/17/23 08:04:41.601
    May 17 08:04:41.601: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9496 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:04:41.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:04:41.602: INFO: ExecWithOptions: Clientset creation
    May 17 08:04:41.602: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-9496/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May 17 08:04:41.640: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:41.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-9496" for this suite. 05/17/23 08:04:41.645
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:41.648
May 17 08:04:41.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 08:04:41.649
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:41.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:41.656
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 05/17/23 08:04:41.658
May 17 08:04:41.662: INFO: Waiting up to 5m0s for pod "pod-pdpph" in namespace "pods-9938" to be "running"
May 17 08:04:41.664: INFO: Pod "pod-pdpph": Phase="Pending", Reason="", readiness=false. Elapsed: 1.219606ms
May 17 08:04:43.666: INFO: Pod "pod-pdpph": Phase="Running", Reason="", readiness=true. Elapsed: 2.003474713s
May 17 08:04:43.666: INFO: Pod "pod-pdpph" satisfied condition "running"
STEP: patching /status 05/17/23 08:04:43.666
May 17 08:04:43.671: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 08:04:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9938" for this suite. 05/17/23 08:04:43.673
------------------------------
â€¢ [2.027 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:41.648
    May 17 08:04:41.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 08:04:41.649
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:41.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:41.656
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 05/17/23 08:04:41.658
    May 17 08:04:41.662: INFO: Waiting up to 5m0s for pod "pod-pdpph" in namespace "pods-9938" to be "running"
    May 17 08:04:41.664: INFO: Pod "pod-pdpph": Phase="Pending", Reason="", readiness=false. Elapsed: 1.219606ms
    May 17 08:04:43.666: INFO: Pod "pod-pdpph": Phase="Running", Reason="", readiness=true. Elapsed: 2.003474713s
    May 17 08:04:43.666: INFO: Pod "pod-pdpph" satisfied condition "running"
    STEP: patching /status 05/17/23 08:04:43.666
    May 17 08:04:43.671: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9938" for this suite. 05/17/23 08:04:43.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:43.676
May 17 08:04:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:04:43.677
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:43.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:43.683
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 05/17/23 08:04:43.685
May 17 08:04:43.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: rename a version 05/17/23 08:04:46.985
STEP: check the new version name is served 05/17/23 08:04:46.994
STEP: check the old version name is removed 05/17/23 08:04:48.314
STEP: check the other version is not changed 05/17/23 08:04:48.961
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:04:52.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4322" for this suite. 05/17/23 08:04:52.104
------------------------------
â€¢ [SLOW TEST] [8.430 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:43.676
    May 17 08:04:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:04:43.677
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:43.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:43.683
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 05/17/23 08:04:43.685
    May 17 08:04:43.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: rename a version 05/17/23 08:04:46.985
    STEP: check the new version name is served 05/17/23 08:04:46.994
    STEP: check the old version name is removed 05/17/23 08:04:48.314
    STEP: check the other version is not changed 05/17/23 08:04:48.961
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:52.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4322" for this suite. 05/17/23 08:04:52.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:52.107
May 17 08:04:52.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename limitrange 05/17/23 08:04:52.108
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:52.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:52.116
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-pwpsg" in namespace "limitrange-1175" 05/17/23 08:04:52.117
STEP: Creating another limitRange in another namespace 05/17/23 08:04:52.12
May 17 08:04:52.125: INFO: Namespace "e2e-limitrange-pwpsg-3411" created
May 17 08:04:52.125: INFO: Creating LimitRange "e2e-limitrange-pwpsg" in namespace "e2e-limitrange-pwpsg-3411"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-pwpsg" 05/17/23 08:04:52.127
May 17 08:04:52.128: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-pwpsg" in "limitrange-1175" namespace 05/17/23 08:04:52.128
May 17 08:04:52.131: INFO: LimitRange "e2e-limitrange-pwpsg" has been patched
STEP: Delete LimitRange "e2e-limitrange-pwpsg" by Collection with labelSelector: "e2e-limitrange-pwpsg=patched" 05/17/23 08:04:52.131
STEP: Confirm that the limitRange "e2e-limitrange-pwpsg" has been deleted 05/17/23 08:04:52.134
May 17 08:04:52.134: INFO: Requesting list of LimitRange to confirm quantity
May 17 08:04:52.136: INFO: Found 0 LimitRange with label "e2e-limitrange-pwpsg=patched"
May 17 08:04:52.136: INFO: LimitRange "e2e-limitrange-pwpsg" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-pwpsg" 05/17/23 08:04:52.136
May 17 08:04:52.137: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May 17 08:04:52.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1175" for this suite. 05/17/23 08:04:52.139
STEP: Destroying namespace "e2e-limitrange-pwpsg-3411" for this suite. 05/17/23 08:04:52.142
------------------------------
â€¢ [0.038 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:52.107
    May 17 08:04:52.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename limitrange 05/17/23 08:04:52.108
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:52.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:52.116
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-pwpsg" in namespace "limitrange-1175" 05/17/23 08:04:52.117
    STEP: Creating another limitRange in another namespace 05/17/23 08:04:52.12
    May 17 08:04:52.125: INFO: Namespace "e2e-limitrange-pwpsg-3411" created
    May 17 08:04:52.125: INFO: Creating LimitRange "e2e-limitrange-pwpsg" in namespace "e2e-limitrange-pwpsg-3411"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-pwpsg" 05/17/23 08:04:52.127
    May 17 08:04:52.128: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-pwpsg" in "limitrange-1175" namespace 05/17/23 08:04:52.128
    May 17 08:04:52.131: INFO: LimitRange "e2e-limitrange-pwpsg" has been patched
    STEP: Delete LimitRange "e2e-limitrange-pwpsg" by Collection with labelSelector: "e2e-limitrange-pwpsg=patched" 05/17/23 08:04:52.131
    STEP: Confirm that the limitRange "e2e-limitrange-pwpsg" has been deleted 05/17/23 08:04:52.134
    May 17 08:04:52.134: INFO: Requesting list of LimitRange to confirm quantity
    May 17 08:04:52.136: INFO: Found 0 LimitRange with label "e2e-limitrange-pwpsg=patched"
    May 17 08:04:52.136: INFO: LimitRange "e2e-limitrange-pwpsg" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-pwpsg" 05/17/23 08:04:52.136
    May 17 08:04:52.137: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:52.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1175" for this suite. 05/17/23 08:04:52.139
    STEP: Destroying namespace "e2e-limitrange-pwpsg-3411" for this suite. 05/17/23 08:04:52.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:52.145
May 17 08:04:52.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename proxy 05/17/23 08:04:52.146
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:52.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:52.153
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/17/23 08:04:52.16
STEP: creating replication controller proxy-service-pvwv7 in namespace proxy-1338 05/17/23 08:04:52.16
I0517 08:04:52.164930      23 runners.go:193] Created replication controller with name: proxy-service-pvwv7, namespace: proxy-1338, replica count: 1
I0517 08:04:53.216396      23 runners.go:193] proxy-service-pvwv7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 08:04:53.218: INFO: setup took 1.063206534s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/17/23 08:04:53.218
May 17 08:04:53.221: INFO: (0) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.904199ms)
May 17 08:04:53.221: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.889073ms)
May 17 08:04:53.221: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.887729ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 4.014756ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 3.967377ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 3.898011ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 4.152027ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 4.148835ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 4.226893ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 4.256284ms)
May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 4.208624ms)
May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 7.742369ms)
May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 7.909039ms)
May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 7.907547ms)
May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 7.980099ms)
May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 7.940449ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.506789ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.432273ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.542187ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.560219ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.51352ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.517731ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.621808ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.622528ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.559961ms)
May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.609118ms)
May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.738742ms)
May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.816369ms)
May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.732881ms)
May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.790589ms)
May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.70556ms)
May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.767752ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.05587ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.149407ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.139517ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.134625ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.269552ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.247955ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.216689ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.304996ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.250957ms)
May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.32481ms)
May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.988207ms)
May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 3.000958ms)
May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 3.033191ms)
May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 3.039609ms)
May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 3.078495ms)
May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.157217ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.941897ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.056632ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.945254ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.969283ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.048865ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.120813ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.005055ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.050495ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.152283ms)
May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.249462ms)
May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.68026ms)
May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.686686ms)
May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.682036ms)
May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.738866ms)
May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.738227ms)
May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.755538ms)
May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.500435ms)
May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.557346ms)
May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.598595ms)
May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.607984ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.617533ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.793435ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.804825ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.870769ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.88867ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.904718ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.088488ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 3.065761ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 3.030304ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 3.088303ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 3.053398ms)
May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 3.216051ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.040671ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.388493ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.346033ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.344027ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.356514ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.383523ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.380198ms)
May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.39239ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.748944ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.80835ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.751622ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.8275ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.780427ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.816639ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.812351ms)
May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.861823ms)
May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 7.675847ms)
May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 7.764444ms)
May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 7.909535ms)
May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 7.899114ms)
May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 7.959974ms)
May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 8.015294ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 8.866996ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 8.892524ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 8.979753ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 9.038728ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 9.077556ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 9.030283ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 9.242053ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 9.257291ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 9.217119ms)
May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 9.290466ms)
May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 6.750108ms)
May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 6.980831ms)
May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 7.05018ms)
May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 7.021087ms)
May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 7.054479ms)
May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 7.026452ms)
May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 7.116407ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 7.168795ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 7.43901ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 7.36085ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 7.468621ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 7.450391ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 7.474897ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 7.496899ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 7.533721ms)
May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 7.659834ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.888223ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.895641ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 3.912124ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 3.915219ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 4.070507ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 4.096004ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 4.186154ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 4.2243ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 4.278685ms)
May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 4.275147ms)
May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 8.208889ms)
May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 8.254845ms)
May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 8.244016ms)
May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 8.267149ms)
May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 8.278724ms)
May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 8.265074ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 3.746894ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 3.840601ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.823896ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 3.77624ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 3.761672ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 3.785354ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.79942ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 3.795729ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 3.831519ms)
May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 3.872344ms)
May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 4.034502ms)
May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 4.087428ms)
May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 4.074739ms)
May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 4.064471ms)
May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 4.164785ms)
May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 4.072377ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.016166ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.042528ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.025947ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.028518ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.05369ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.21226ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.165682ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.248341ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.243401ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.325598ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.449135ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.459399ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.489004ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.473553ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.571836ms)
May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.562138ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.806123ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.776021ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.815051ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 1.798321ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 1.808955ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.803452ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.916187ms)
May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 1.941361ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.127807ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.17201ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.426467ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.441967ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.484183ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.466962ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.441678ms)
May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.45609ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.030024ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.16352ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.201671ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.112042ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.463823ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.427172ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.417041ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.443764ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.479294ms)
May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.450789ms)
May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.776689ms)
May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.731254ms)
May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.80831ms)
May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.870717ms)
May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.881221ms)
May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.893549ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.664944ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.855301ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 1.870867ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.894613ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 1.953114ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 1.950841ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.902284ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.941484ms)
May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 1.952038ms)
May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.543185ms)
May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.580642ms)
May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.582786ms)
May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.838923ms)
May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.81797ms)
May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 3.008275ms)
May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.04216ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.668621ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.654802ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 1.770964ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.24557ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.187634ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.15905ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.215396ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.219499ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.176873ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.213813ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.261226ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.310495ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.340009ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.335629ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.331802ms)
May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.403668ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.069958ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.0622ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.071395ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.076781ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.11072ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.059786ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.137917ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.056008ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.090669ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.288293ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.509859ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.559003ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.713392ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.712828ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.71536ms)
May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.782335ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.497508ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.56309ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.5334ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.605381ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.521255ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.529507ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.619927ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.611257ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.8044ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.889445ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.931332ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.870992ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.864915ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.924122ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.902423ms)
May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.012286ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.086095ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.132944ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.134751ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.221644ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.229996ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.209127ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.293739ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.312789ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.28585ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.351401ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.805314ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.954618ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 3.037111ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 3.013466ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.042213ms)
May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 3.055967ms)
May 17 08:04:53.295: INFO: (18) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.637267ms)
May 17 08:04:53.295: INFO: (18) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 1.813619ms)
May 17 08:04:53.295: INFO: (18) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.826313ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.967226ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.070549ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.092796ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.127928ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.096244ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.087304ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.181149ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.16719ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.232084ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.21323ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.282589ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.294095ms)
May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.286336ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 1.775716ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.801382ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 1.913483ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.438895ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.44896ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.361592ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.423015ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.389829ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.387515ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.454683ms)
May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.532101ms)
May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.596941ms)
May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.589893ms)
May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.617855ms)
May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.632602ms)
May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.614912ms)
STEP: deleting ReplicationController proxy-service-pvwv7 in namespace proxy-1338, will wait for the garbage collector to delete the pods 05/17/23 08:04:53.299
May 17 08:04:53.354: INFO: Deleting ReplicationController proxy-service-pvwv7 took: 2.95132ms
May 17 08:04:53.454: INFO: Terminating ReplicationController proxy-service-pvwv7 pods took: 100.614994ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May 17 08:04:56.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1338" for this suite. 05/17/23 08:04:56.159
------------------------------
â€¢ [4.017 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:52.145
    May 17 08:04:52.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename proxy 05/17/23 08:04:52.146
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:52.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:52.153
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/17/23 08:04:52.16
    STEP: creating replication controller proxy-service-pvwv7 in namespace proxy-1338 05/17/23 08:04:52.16
    I0517 08:04:52.164930      23 runners.go:193] Created replication controller with name: proxy-service-pvwv7, namespace: proxy-1338, replica count: 1
    I0517 08:04:53.216396      23 runners.go:193] proxy-service-pvwv7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 08:04:53.218: INFO: setup took 1.063206534s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/17/23 08:04:53.218
    May 17 08:04:53.221: INFO: (0) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.904199ms)
    May 17 08:04:53.221: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.889073ms)
    May 17 08:04:53.221: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.887729ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 4.014756ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 3.967377ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 3.898011ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 4.152027ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 4.148835ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 4.226893ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 4.256284ms)
    May 17 08:04:53.222: INFO: (0) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 4.208624ms)
    May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 7.742369ms)
    May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 7.909039ms)
    May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 7.907547ms)
    May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 7.980099ms)
    May 17 08:04:53.226: INFO: (0) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 7.940449ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.506789ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.432273ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.542187ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.560219ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.51352ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.517731ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.621808ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.622528ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.559961ms)
    May 17 08:04:53.228: INFO: (1) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.609118ms)
    May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.738742ms)
    May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.816369ms)
    May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.732881ms)
    May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.790589ms)
    May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.70556ms)
    May 17 08:04:53.229: INFO: (1) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.767752ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.05587ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.149407ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.139517ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.134625ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.269552ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.247955ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.216689ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.304996ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.250957ms)
    May 17 08:04:53.231: INFO: (2) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.32481ms)
    May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.988207ms)
    May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 3.000958ms)
    May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 3.033191ms)
    May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 3.039609ms)
    May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 3.078495ms)
    May 17 08:04:53.232: INFO: (2) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.157217ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.941897ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.056632ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.945254ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.969283ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.048865ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.120813ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.005055ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.050495ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.152283ms)
    May 17 08:04:53.234: INFO: (3) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.249462ms)
    May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.68026ms)
    May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.686686ms)
    May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.682036ms)
    May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.738866ms)
    May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.738227ms)
    May 17 08:04:53.235: INFO: (3) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.755538ms)
    May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.500435ms)
    May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.557346ms)
    May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.598595ms)
    May 17 08:04:53.237: INFO: (4) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.607984ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.617533ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.793435ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.804825ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.870769ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.88867ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.904718ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.088488ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 3.065761ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 3.030304ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 3.088303ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 3.053398ms)
    May 17 08:04:53.238: INFO: (4) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 3.216051ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.040671ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.388493ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.346033ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.344027ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.356514ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.383523ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.380198ms)
    May 17 08:04:53.240: INFO: (5) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.39239ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.748944ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.80835ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.751622ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.8275ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.780427ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.816639ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.812351ms)
    May 17 08:04:53.241: INFO: (5) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.861823ms)
    May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 7.675847ms)
    May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 7.764444ms)
    May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 7.909535ms)
    May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 7.899114ms)
    May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 7.959974ms)
    May 17 08:04:53.249: INFO: (6) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 8.015294ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 8.866996ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 8.892524ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 8.979753ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 9.038728ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 9.077556ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 9.030283ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 9.242053ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 9.257291ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 9.217119ms)
    May 17 08:04:53.250: INFO: (6) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 9.290466ms)
    May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 6.750108ms)
    May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 6.980831ms)
    May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 7.05018ms)
    May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 7.021087ms)
    May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 7.054479ms)
    May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 7.026452ms)
    May 17 08:04:53.257: INFO: (7) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 7.116407ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 7.168795ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 7.43901ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 7.36085ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 7.468621ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 7.450391ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 7.474897ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 7.496899ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 7.533721ms)
    May 17 08:04:53.258: INFO: (7) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 7.659834ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.888223ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.895641ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 3.912124ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 3.915219ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 4.070507ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 4.096004ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 4.186154ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 4.2243ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 4.278685ms)
    May 17 08:04:53.262: INFO: (8) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 4.275147ms)
    May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 8.208889ms)
    May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 8.254845ms)
    May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 8.244016ms)
    May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 8.267149ms)
    May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 8.278724ms)
    May 17 08:04:53.266: INFO: (8) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 8.265074ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 3.746894ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 3.840601ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.823896ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 3.77624ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 3.761672ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 3.785354ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 3.79942ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 3.795729ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 3.831519ms)
    May 17 08:04:53.270: INFO: (9) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 3.872344ms)
    May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 4.034502ms)
    May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 4.087428ms)
    May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 4.074739ms)
    May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 4.064471ms)
    May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 4.164785ms)
    May 17 08:04:53.271: INFO: (9) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 4.072377ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.016166ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.042528ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.025947ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.028518ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.05369ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.21226ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.165682ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.248341ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.243401ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.325598ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.449135ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.459399ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.489004ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.473553ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.571836ms)
    May 17 08:04:53.273: INFO: (10) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.562138ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.806123ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.776021ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.815051ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 1.798321ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 1.808955ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.803452ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.916187ms)
    May 17 08:04:53.275: INFO: (11) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 1.941361ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.127807ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.17201ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.426467ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.441967ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.484183ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.466962ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.441678ms)
    May 17 08:04:53.276: INFO: (11) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.45609ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.030024ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.16352ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.201671ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.112042ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.463823ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.427172ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.417041ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.443764ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.479294ms)
    May 17 08:04:53.278: INFO: (12) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.450789ms)
    May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.776689ms)
    May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.731254ms)
    May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.80831ms)
    May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.870717ms)
    May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.881221ms)
    May 17 08:04:53.279: INFO: (12) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.893549ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.664944ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.855301ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 1.870867ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.894613ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 1.953114ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 1.950841ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.902284ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.941484ms)
    May 17 08:04:53.281: INFO: (13) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 1.952038ms)
    May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.543185ms)
    May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.580642ms)
    May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.582786ms)
    May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.838923ms)
    May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.81797ms)
    May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 3.008275ms)
    May 17 08:04:53.282: INFO: (13) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.04216ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.668621ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.654802ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 1.770964ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.24557ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.187634ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.15905ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.215396ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.219499ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.176873ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.213813ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.261226ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.310495ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.340009ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.335629ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.331802ms)
    May 17 08:04:53.284: INFO: (14) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.403668ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.069958ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.0622ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.071395ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.076781ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.11072ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.059786ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.137917ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.056008ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.090669ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.288293ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.509859ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.559003ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.713392ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.712828ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.71536ms)
    May 17 08:04:53.287: INFO: (15) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.782335ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.497508ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.56309ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.5334ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.605381ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.521255ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.529507ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.619927ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.611257ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.8044ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.889445ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.931332ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.870992ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.864915ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.924122ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.902423ms)
    May 17 08:04:53.290: INFO: (16) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.012286ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.086095ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.132944ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.134751ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.221644ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.229996ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.209127ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.293739ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.312789ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.28585ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.351401ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.805314ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.954618ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 3.037111ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 3.013466ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 3.042213ms)
    May 17 08:04:53.293: INFO: (17) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 3.055967ms)
    May 17 08:04:53.295: INFO: (18) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.637267ms)
    May 17 08:04:53.295: INFO: (18) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 1.813619ms)
    May 17 08:04:53.295: INFO: (18) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 1.826313ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 1.967226ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 2.070549ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.092796ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.127928ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.096244ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.087304ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.181149ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 2.16719ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.232084ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.21323ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.282589ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.294095ms)
    May 17 08:04:53.296: INFO: (18) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.286336ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">test<... (200; 1.775716ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 1.801382ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:443/proxy/tlsrewritem... (200; 1.913483ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.438895ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:462/proxy/: tls qux (200; 2.44896ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh/proxy/rewriteme">test</a> (200; 2.361592ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/proxy-service-pvwv7-t4lvh:162/proxy/: bar (200; 2.423015ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:160/proxy/: foo (200; 2.389829ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/https:proxy-service-pvwv7-t4lvh:460/proxy/: tls baz (200; 2.387515ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1338/pods/http:proxy-service-pvwv7-t4lvh:1080/proxy/rewriteme">... (200; 2.454683ms)
    May 17 08:04:53.298: INFO: (19) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname1/proxy/: foo (200; 2.532101ms)
    May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname1/proxy/: tls baz (200; 2.596941ms)
    May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname1/proxy/: foo (200; 2.589893ms)
    May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/http:proxy-service-pvwv7:portname2/proxy/: bar (200; 2.617855ms)
    May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/proxy-service-pvwv7:portname2/proxy/: bar (200; 2.632602ms)
    May 17 08:04:53.299: INFO: (19) /api/v1/namespaces/proxy-1338/services/https:proxy-service-pvwv7:tlsportname2/proxy/: tls qux (200; 2.614912ms)
    STEP: deleting ReplicationController proxy-service-pvwv7 in namespace proxy-1338, will wait for the garbage collector to delete the pods 05/17/23 08:04:53.299
    May 17 08:04:53.354: INFO: Deleting ReplicationController proxy-service-pvwv7 took: 2.95132ms
    May 17 08:04:53.454: INFO: Terminating ReplicationController proxy-service-pvwv7 pods took: 100.614994ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May 17 08:04:56.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1338" for this suite. 05/17/23 08:04:56.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:04:56.163
May 17 08:04:56.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename subpath 05/17/23 08:04:56.164
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:56.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:56.171
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 08:04:56.173
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-spnm 05/17/23 08:04:56.177
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 08:04:56.177
May 17 08:04:56.180: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-spnm" in namespace "subpath-8758" to be "Succeeded or Failed"
May 17 08:04:56.183: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691881ms
May 17 08:04:58.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 2.004721372s
May 17 08:05:00.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 4.005471927s
May 17 08:05:02.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 6.005162667s
May 17 08:05:04.187: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 8.006582905s
May 17 08:05:06.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 10.004594488s
May 17 08:05:08.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005477341s
May 17 08:05:10.188: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 14.007384097s
May 17 08:05:12.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 16.005548889s
May 17 08:05:14.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 18.005237542s
May 17 08:05:16.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 20.00492168s
May 17 08:05:18.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=false. Elapsed: 22.004940802s
May 17 08:05:20.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005953173s
STEP: Saw pod success 05/17/23 08:05:20.186
May 17 08:05:20.186: INFO: Pod "pod-subpath-test-configmap-spnm" satisfied condition "Succeeded or Failed"
May 17 08:05:20.188: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-configmap-spnm container test-container-subpath-configmap-spnm: <nil>
STEP: delete the pod 05/17/23 08:05:20.192
May 17 08:05:20.199: INFO: Waiting for pod pod-subpath-test-configmap-spnm to disappear
May 17 08:05:20.200: INFO: Pod pod-subpath-test-configmap-spnm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-spnm 05/17/23 08:05:20.2
May 17 08:05:20.200: INFO: Deleting pod "pod-subpath-test-configmap-spnm" in namespace "subpath-8758"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 17 08:05:20.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8758" for this suite. 05/17/23 08:05:20.204
------------------------------
â€¢ [SLOW TEST] [24.043 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:04:56.163
    May 17 08:04:56.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename subpath 05/17/23 08:04:56.164
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:04:56.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:04:56.171
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 08:04:56.173
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-spnm 05/17/23 08:04:56.177
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 08:04:56.177
    May 17 08:04:56.180: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-spnm" in namespace "subpath-8758" to be "Succeeded or Failed"
    May 17 08:04:56.183: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691881ms
    May 17 08:04:58.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 2.004721372s
    May 17 08:05:00.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 4.005471927s
    May 17 08:05:02.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 6.005162667s
    May 17 08:05:04.187: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 8.006582905s
    May 17 08:05:06.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 10.004594488s
    May 17 08:05:08.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005477341s
    May 17 08:05:10.188: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 14.007384097s
    May 17 08:05:12.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 16.005548889s
    May 17 08:05:14.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 18.005237542s
    May 17 08:05:16.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=true. Elapsed: 20.00492168s
    May 17 08:05:18.185: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Running", Reason="", readiness=false. Elapsed: 22.004940802s
    May 17 08:05:20.186: INFO: Pod "pod-subpath-test-configmap-spnm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005953173s
    STEP: Saw pod success 05/17/23 08:05:20.186
    May 17 08:05:20.186: INFO: Pod "pod-subpath-test-configmap-spnm" satisfied condition "Succeeded or Failed"
    May 17 08:05:20.188: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-configmap-spnm container test-container-subpath-configmap-spnm: <nil>
    STEP: delete the pod 05/17/23 08:05:20.192
    May 17 08:05:20.199: INFO: Waiting for pod pod-subpath-test-configmap-spnm to disappear
    May 17 08:05:20.200: INFO: Pod pod-subpath-test-configmap-spnm no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-spnm 05/17/23 08:05:20.2
    May 17 08:05:20.200: INFO: Deleting pod "pod-subpath-test-configmap-spnm" in namespace "subpath-8758"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 17 08:05:20.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8758" for this suite. 05/17/23 08:05:20.204
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:05:20.207
May 17 08:05:20.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 08:05:20.207
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:05:20.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:05:20.215
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 05/17/23 08:05:20.216
STEP: fetching the ConfigMap 05/17/23 08:05:20.219
STEP: patching the ConfigMap 05/17/23 08:05:20.221
STEP: listing all ConfigMaps in all namespaces with a label selector 05/17/23 08:05:20.223
STEP: deleting the ConfigMap by collection with a label selector 05/17/23 08:05:20.225
STEP: listing all ConfigMaps in test namespace 05/17/23 08:05:20.228
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 08:05:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7868" for this suite. 05/17/23 08:05:20.231
------------------------------
â€¢ [0.026 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:05:20.207
    May 17 08:05:20.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 08:05:20.207
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:05:20.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:05:20.215
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 05/17/23 08:05:20.216
    STEP: fetching the ConfigMap 05/17/23 08:05:20.219
    STEP: patching the ConfigMap 05/17/23 08:05:20.221
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/17/23 08:05:20.223
    STEP: deleting the ConfigMap by collection with a label selector 05/17/23 08:05:20.225
    STEP: listing all ConfigMaps in test namespace 05/17/23 08:05:20.228
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:05:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7868" for this suite. 05/17/23 08:05:20.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:05:20.233
May 17 08:05:20.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:05:20.234
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:05:20.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:05:20.241
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 08:05:20.243
May 17 08:05:20.247: INFO: Waiting up to 5m0s for pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3" in namespace "emptydir-16" to be "Succeeded or Failed"
May 17 08:05:20.248: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.276218ms
May 17 08:05:22.250: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003324978s
May 17 08:05:24.253: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005970737s
STEP: Saw pod success 05/17/23 08:05:24.253
May 17 08:05:24.253: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3" satisfied condition "Succeeded or Failed"
May 17 08:05:24.255: INFO: Trying to get logs from node k8s-node1 pod pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3 container test-container: <nil>
STEP: delete the pod 05/17/23 08:05:24.258
May 17 08:05:24.264: INFO: Waiting for pod pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3 to disappear
May 17 08:05:24.265: INFO: Pod pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:05:24.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-16" for this suite. 05/17/23 08:05:24.267
------------------------------
â€¢ [4.036 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:05:20.233
    May 17 08:05:20.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:05:20.234
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:05:20.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:05:20.241
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 08:05:20.243
    May 17 08:05:20.247: INFO: Waiting up to 5m0s for pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3" in namespace "emptydir-16" to be "Succeeded or Failed"
    May 17 08:05:20.248: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.276218ms
    May 17 08:05:22.250: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003324978s
    May 17 08:05:24.253: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005970737s
    STEP: Saw pod success 05/17/23 08:05:24.253
    May 17 08:05:24.253: INFO: Pod "pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3" satisfied condition "Succeeded or Failed"
    May 17 08:05:24.255: INFO: Trying to get logs from node k8s-node1 pod pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:05:24.258
    May 17 08:05:24.264: INFO: Waiting for pod pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3 to disappear
    May 17 08:05:24.265: INFO: Pod pod-ca6ffaa0-a7a4-4476-9880-0d07ee4d89a3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:05:24.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-16" for this suite. 05/17/23 08:05:24.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:05:24.27
May 17 08:05:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 08:05:24.27
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:05:24.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:05:24.277
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-09af85b6-5ad7-4727-a24c-b7f7c8a2d529 05/17/23 08:05:24.28
STEP: Creating the pod 05/17/23 08:05:24.283
May 17 08:05:24.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c" in namespace "configmap-1525" to be "running and ready"
May 17 08:05:24.288: INFO: Pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721674ms
May 17 08:05:24.288: INFO: The phase of Pod pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c is Pending, waiting for it to be Running (with Ready = true)
May 17 08:05:26.291: INFO: Pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004764612s
May 17 08:05:26.292: INFO: The phase of Pod pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c is Running (Ready = true)
May 17 08:05:26.292: INFO: Pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-09af85b6-5ad7-4727-a24c-b7f7c8a2d529 05/17/23 08:05:26.296
STEP: waiting to observe update in volume 05/17/23 08:05:26.299
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 08:06:42.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1525" for this suite. 05/17/23 08:06:42.499
------------------------------
â€¢ [SLOW TEST] [78.233 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:05:24.27
    May 17 08:05:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 08:05:24.27
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:05:24.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:05:24.277
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-09af85b6-5ad7-4727-a24c-b7f7c8a2d529 05/17/23 08:05:24.28
    STEP: Creating the pod 05/17/23 08:05:24.283
    May 17 08:05:24.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c" in namespace "configmap-1525" to be "running and ready"
    May 17 08:05:24.288: INFO: Pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721674ms
    May 17 08:05:24.288: INFO: The phase of Pod pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:05:26.291: INFO: Pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004764612s
    May 17 08:05:26.292: INFO: The phase of Pod pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c is Running (Ready = true)
    May 17 08:05:26.292: INFO: Pod "pod-configmaps-99845ce8-d1a0-43ad-a276-533e98d2e74c" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-09af85b6-5ad7-4727-a24c-b7f7c8a2d529 05/17/23 08:05:26.296
    STEP: waiting to observe update in volume 05/17/23 08:05:26.299
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:06:42.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1525" for this suite. 05/17/23 08:06:42.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:06:42.503
May 17 08:06:42.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename disruption 05/17/23 08:06:42.504
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:42.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:42.511
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:06:42.513
May 17 08:06:42.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename disruption-2 05/17/23 08:06:42.513
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:42.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:42.521
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 05/17/23 08:06:42.525
STEP: Waiting for the pdb to be processed 05/17/23 08:06:44.532
STEP: Waiting for the pdb to be processed 05/17/23 08:06:46.539
STEP: listing a collection of PDBs across all namespaces 05/17/23 08:06:48.543
STEP: listing a collection of PDBs in namespace disruption-150 05/17/23 08:06:48.545
STEP: deleting a collection of PDBs 05/17/23 08:06:48.547
STEP: Waiting for the PDB collection to be deleted 05/17/23 08:06:48.552
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
May 17 08:06:48.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 17 08:06:48.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-1764" for this suite. 05/17/23 08:06:48.557
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-150" for this suite. 05/17/23 08:06:48.561
------------------------------
â€¢ [SLOW TEST] [6.061 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:06:42.503
    May 17 08:06:42.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename disruption 05/17/23 08:06:42.504
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:42.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:42.511
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:06:42.513
    May 17 08:06:42.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename disruption-2 05/17/23 08:06:42.513
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:42.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:42.521
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 05/17/23 08:06:42.525
    STEP: Waiting for the pdb to be processed 05/17/23 08:06:44.532
    STEP: Waiting for the pdb to be processed 05/17/23 08:06:46.539
    STEP: listing a collection of PDBs across all namespaces 05/17/23 08:06:48.543
    STEP: listing a collection of PDBs in namespace disruption-150 05/17/23 08:06:48.545
    STEP: deleting a collection of PDBs 05/17/23 08:06:48.547
    STEP: Waiting for the PDB collection to be deleted 05/17/23 08:06:48.552
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    May 17 08:06:48.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 17 08:06:48.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-1764" for this suite. 05/17/23 08:06:48.557
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-150" for this suite. 05/17/23 08:06:48.561
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:06:48.565
May 17 08:06:48.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename security-context 05/17/23 08:06:48.565
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:48.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:48.573
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 08:06:48.574
May 17 08:06:48.578: INFO: Waiting up to 5m0s for pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d" in namespace "security-context-5578" to be "Succeeded or Failed"
May 17 08:06:48.579: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275716ms
May 17 08:06:50.583: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004435283s
May 17 08:06:52.583: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004871495s
STEP: Saw pod success 05/17/23 08:06:52.583
May 17 08:06:52.583: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d" satisfied condition "Succeeded or Failed"
May 17 08:06:52.585: INFO: Trying to get logs from node k8s-node1 pod security-context-9000db85-3357-4170-88cf-7879929e4f1d container test-container: <nil>
STEP: delete the pod 05/17/23 08:06:52.589
May 17 08:06:52.596: INFO: Waiting for pod security-context-9000db85-3357-4170-88cf-7879929e4f1d to disappear
May 17 08:06:52.597: INFO: Pod security-context-9000db85-3357-4170-88cf-7879929e4f1d no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 17 08:06:52.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5578" for this suite. 05/17/23 08:06:52.6
------------------------------
â€¢ [4.038 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:06:48.565
    May 17 08:06:48.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename security-context 05/17/23 08:06:48.565
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:48.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:48.573
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 08:06:48.574
    May 17 08:06:48.578: INFO: Waiting up to 5m0s for pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d" in namespace "security-context-5578" to be "Succeeded or Failed"
    May 17 08:06:48.579: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275716ms
    May 17 08:06:50.583: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004435283s
    May 17 08:06:52.583: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004871495s
    STEP: Saw pod success 05/17/23 08:06:52.583
    May 17 08:06:52.583: INFO: Pod "security-context-9000db85-3357-4170-88cf-7879929e4f1d" satisfied condition "Succeeded or Failed"
    May 17 08:06:52.585: INFO: Trying to get logs from node k8s-node1 pod security-context-9000db85-3357-4170-88cf-7879929e4f1d container test-container: <nil>
    STEP: delete the pod 05/17/23 08:06:52.589
    May 17 08:06:52.596: INFO: Waiting for pod security-context-9000db85-3357-4170-88cf-7879929e4f1d to disappear
    May 17 08:06:52.597: INFO: Pod security-context-9000db85-3357-4170-88cf-7879929e4f1d no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 17 08:06:52.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5578" for this suite. 05/17/23 08:06:52.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:06:52.604
May 17 08:06:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:06:52.604
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:52.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:52.612
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 05/17/23 08:06:52.613
STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:06:52.616
STEP: Creating a ResourceQuota with not terminating scope 05/17/23 08:06:54.619
STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:06:54.622
STEP: Creating a long running pod 05/17/23 08:06:56.626
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/17/23 08:06:56.633
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/17/23 08:06:58.636
STEP: Deleting the pod 05/17/23 08:07:00.638
STEP: Ensuring resource quota status released the pod usage 05/17/23 08:07:00.645
STEP: Creating a terminating pod 05/17/23 08:07:02.647
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/17/23 08:07:02.653
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/17/23 08:07:04.656
STEP: Deleting the pod 05/17/23 08:07:06.66
STEP: Ensuring resource quota status released the pod usage 05/17/23 08:07:06.666
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:07:08.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2116" for this suite. 05/17/23 08:07:08.671
------------------------------
â€¢ [SLOW TEST] [16.071 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:06:52.604
    May 17 08:06:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:06:52.604
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:06:52.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:06:52.612
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 05/17/23 08:06:52.613
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:06:52.616
    STEP: Creating a ResourceQuota with not terminating scope 05/17/23 08:06:54.619
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:06:54.622
    STEP: Creating a long running pod 05/17/23 08:06:56.626
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/17/23 08:06:56.633
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/17/23 08:06:58.636
    STEP: Deleting the pod 05/17/23 08:07:00.638
    STEP: Ensuring resource quota status released the pod usage 05/17/23 08:07:00.645
    STEP: Creating a terminating pod 05/17/23 08:07:02.647
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/17/23 08:07:02.653
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/17/23 08:07:04.656
    STEP: Deleting the pod 05/17/23 08:07:06.66
    STEP: Ensuring resource quota status released the pod usage 05/17/23 08:07:06.666
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:08.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2116" for this suite. 05/17/23 08:07:08.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:08.676
May 17 08:07:08.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:07:08.677
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:08.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:08.684
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
May 17 08:07:08.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-1264 version'
May 17 08:07:08.733: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May 17 08:07:08.733: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:07:08.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1264" for this suite. 05/17/23 08:07:08.735
------------------------------
â€¢ [0.062 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:08.676
    May 17 08:07:08.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:07:08.677
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:08.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:08.684
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    May 17 08:07:08.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-1264 version'
    May 17 08:07:08.733: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May 17 08:07:08.733: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:08.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1264" for this suite. 05/17/23 08:07:08.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:08.738
May 17 08:07:08.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 08:07:08.739
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:08.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:08.747
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-6f64041a-cb13-4532-a3d4-46538833c1cb in namespace container-probe-8083 05/17/23 08:07:08.748
May 17 08:07:08.752: INFO: Waiting up to 5m0s for pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb" in namespace "container-probe-8083" to be "not pending"
May 17 08:07:08.754: INFO: Pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.217467ms
May 17 08:07:10.757: INFO: Pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004543842s
May 17 08:07:10.757: INFO: Pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb" satisfied condition "not pending"
May 17 08:07:10.757: INFO: Started pod liveness-6f64041a-cb13-4532-a3d4-46538833c1cb in namespace container-probe-8083
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:07:10.757
May 17 08:07:10.758: INFO: Initial restart count of pod liveness-6f64041a-cb13-4532-a3d4-46538833c1cb is 0
May 17 08:07:30.788: INFO: Restart count of pod container-probe-8083/liveness-6f64041a-cb13-4532-a3d4-46538833c1cb is now 1 (20.029791082s elapsed)
STEP: deleting the pod 05/17/23 08:07:30.788
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 08:07:30.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8083" for this suite. 05/17/23 08:07:30.797
------------------------------
â€¢ [SLOW TEST] [22.063 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:08.738
    May 17 08:07:08.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 08:07:08.739
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:08.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:08.747
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-6f64041a-cb13-4532-a3d4-46538833c1cb in namespace container-probe-8083 05/17/23 08:07:08.748
    May 17 08:07:08.752: INFO: Waiting up to 5m0s for pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb" in namespace "container-probe-8083" to be "not pending"
    May 17 08:07:08.754: INFO: Pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.217467ms
    May 17 08:07:10.757: INFO: Pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004543842s
    May 17 08:07:10.757: INFO: Pod "liveness-6f64041a-cb13-4532-a3d4-46538833c1cb" satisfied condition "not pending"
    May 17 08:07:10.757: INFO: Started pod liveness-6f64041a-cb13-4532-a3d4-46538833c1cb in namespace container-probe-8083
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:07:10.757
    May 17 08:07:10.758: INFO: Initial restart count of pod liveness-6f64041a-cb13-4532-a3d4-46538833c1cb is 0
    May 17 08:07:30.788: INFO: Restart count of pod container-probe-8083/liveness-6f64041a-cb13-4532-a3d4-46538833c1cb is now 1 (20.029791082s elapsed)
    STEP: deleting the pod 05/17/23 08:07:30.788
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:30.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8083" for this suite. 05/17/23 08:07:30.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:30.801
May 17 08:07:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:07:30.802
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:30.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:30.81
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:07:30.812
May 17 08:07:30.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286" in namespace "downward-api-5814" to be "Succeeded or Failed"
May 17 08:07:30.818: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286": Phase="Pending", Reason="", readiness=false. Elapsed: 1.334528ms
May 17 08:07:32.820: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003848893s
May 17 08:07:34.822: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005606424s
STEP: Saw pod success 05/17/23 08:07:34.822
May 17 08:07:34.822: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286" satisfied condition "Succeeded or Failed"
May 17 08:07:34.824: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286 container client-container: <nil>
STEP: delete the pod 05/17/23 08:07:34.829
May 17 08:07:34.835: INFO: Waiting for pod downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286 to disappear
May 17 08:07:34.836: INFO: Pod downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:07:34.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5814" for this suite. 05/17/23 08:07:34.838
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:30.801
    May 17 08:07:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:07:30.802
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:30.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:30.81
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:07:30.812
    May 17 08:07:30.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286" in namespace "downward-api-5814" to be "Succeeded or Failed"
    May 17 08:07:30.818: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286": Phase="Pending", Reason="", readiness=false. Elapsed: 1.334528ms
    May 17 08:07:32.820: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003848893s
    May 17 08:07:34.822: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005606424s
    STEP: Saw pod success 05/17/23 08:07:34.822
    May 17 08:07:34.822: INFO: Pod "downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286" satisfied condition "Succeeded or Failed"
    May 17 08:07:34.824: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:07:34.829
    May 17 08:07:34.835: INFO: Waiting for pod downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286 to disappear
    May 17 08:07:34.836: INFO: Pod downwardapi-volume-c5f5888d-330e-453c-b167-59c4e8d07286 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:34.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5814" for this suite. 05/17/23 08:07:34.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:34.841
May 17 08:07:34.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replicaset 05/17/23 08:07:34.842
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:34.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:34.851
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/17/23 08:07:34.853
STEP: Verify that the required pods have come up 05/17/23 08:07:34.855
May 17 08:07:34.857: INFO: Pod name sample-pod: Found 0 pods out of 3
May 17 08:07:39.861: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/17/23 08:07:39.861
May 17 08:07:39.862: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/17/23 08:07:39.862
STEP: DeleteCollection of the ReplicaSets 05/17/23 08:07:39.863
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/17/23 08:07:39.867
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 17 08:07:39.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-996" for this suite. 05/17/23 08:07:39.873
------------------------------
â€¢ [SLOW TEST] [5.034 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:34.841
    May 17 08:07:34.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replicaset 05/17/23 08:07:34.842
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:34.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:34.851
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/17/23 08:07:34.853
    STEP: Verify that the required pods have come up 05/17/23 08:07:34.855
    May 17 08:07:34.857: INFO: Pod name sample-pod: Found 0 pods out of 3
    May 17 08:07:39.861: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/17/23 08:07:39.861
    May 17 08:07:39.862: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/17/23 08:07:39.862
    STEP: DeleteCollection of the ReplicaSets 05/17/23 08:07:39.863
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/17/23 08:07:39.867
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:39.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-996" for this suite. 05/17/23 08:07:39.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:39.876
May 17 08:07:39.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 08:07:39.877
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:39.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:39.886
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-4f97ddd3-e366-4e9b-9b8f-a72ecd393473 05/17/23 08:07:39.889
STEP: Creating the pod 05/17/23 08:07:39.891
May 17 08:07:39.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b" in namespace "configmap-9627" to be "running"
May 17 08:07:39.897: INFO: Pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.174507ms
May 17 08:07:41.899: INFO: Pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b": Phase="Running", Reason="", readiness=false. Elapsed: 2.003360213s
May 17 08:07:41.899: INFO: Pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b" satisfied condition "running"
STEP: Waiting for pod with text data 05/17/23 08:07:41.899
STEP: Waiting for pod with binary data 05/17/23 08:07:41.902
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 08:07:41.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9627" for this suite. 05/17/23 08:07:41.907
------------------------------
â€¢ [2.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:39.876
    May 17 08:07:39.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 08:07:39.877
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:39.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:39.886
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-4f97ddd3-e366-4e9b-9b8f-a72ecd393473 05/17/23 08:07:39.889
    STEP: Creating the pod 05/17/23 08:07:39.891
    May 17 08:07:39.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b" in namespace "configmap-9627" to be "running"
    May 17 08:07:39.897: INFO: Pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.174507ms
    May 17 08:07:41.899: INFO: Pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b": Phase="Running", Reason="", readiness=false. Elapsed: 2.003360213s
    May 17 08:07:41.899: INFO: Pod "pod-configmaps-8985ed56-0be7-4373-b1d5-1054e21ed27b" satisfied condition "running"
    STEP: Waiting for pod with text data 05/17/23 08:07:41.899
    STEP: Waiting for pod with binary data 05/17/23 08:07:41.902
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:41.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9627" for this suite. 05/17/23 08:07:41.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:41.91
May 17 08:07:41.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename watch 05/17/23 08:07:41.911
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:41.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:41.921
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/17/23 08:07:41.922
STEP: creating a new configmap 05/17/23 08:07:41.923
STEP: modifying the configmap once 05/17/23 08:07:41.925
STEP: closing the watch once it receives two notifications 05/17/23 08:07:41.928
May 17 08:07:41.928: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200146 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:07:41.929: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200147 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/17/23 08:07:41.929
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/17/23 08:07:41.933
STEP: deleting the configmap 05/17/23 08:07:41.933
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/17/23 08:07:41.936
May 17 08:07:41.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200148 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:07:41.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200149 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 17 08:07:41.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9836" for this suite. 05/17/23 08:07:41.938
------------------------------
â€¢ [0.030 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:41.91
    May 17 08:07:41.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename watch 05/17/23 08:07:41.911
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:41.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:41.921
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/17/23 08:07:41.922
    STEP: creating a new configmap 05/17/23 08:07:41.923
    STEP: modifying the configmap once 05/17/23 08:07:41.925
    STEP: closing the watch once it receives two notifications 05/17/23 08:07:41.928
    May 17 08:07:41.928: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200146 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:07:41.929: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200147 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/17/23 08:07:41.929
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/17/23 08:07:41.933
    STEP: deleting the configmap 05/17/23 08:07:41.933
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/17/23 08:07:41.936
    May 17 08:07:41.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200148 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:07:41.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9836  e023fd32-937e-42a8-bec8-e81510f6139d 1200149 0 2023-05-17 08:07:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 08:07:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:41.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9836" for this suite. 05/17/23 08:07:41.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:41.94
May 17 08:07:41.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-runtime 05/17/23 08:07:41.941
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:41.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:41.949
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 05/17/23 08:07:41.95
STEP: wait for the container to reach Succeeded 05/17/23 08:07:41.954
STEP: get the container status 05/17/23 08:07:45.966
STEP: the container should be terminated 05/17/23 08:07:45.968
STEP: the termination message should be set 05/17/23 08:07:45.968
May 17 08:07:45.968: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/17/23 08:07:45.968
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 17 08:07:45.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7165" for this suite. 05/17/23 08:07:45.979
------------------------------
â€¢ [4.041 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:41.94
    May 17 08:07:41.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-runtime 05/17/23 08:07:41.941
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:41.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:41.949
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 05/17/23 08:07:41.95
    STEP: wait for the container to reach Succeeded 05/17/23 08:07:41.954
    STEP: get the container status 05/17/23 08:07:45.966
    STEP: the container should be terminated 05/17/23 08:07:45.968
    STEP: the termination message should be set 05/17/23 08:07:45.968
    May 17 08:07:45.968: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/17/23 08:07:45.968
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:45.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7165" for this suite. 05/17/23 08:07:45.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:45.983
May 17 08:07:45.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubelet-test 05/17/23 08:07:45.984
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:45.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:45.991
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
May 17 08:07:45.998: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1" in namespace "kubelet-test-8537" to be "running and ready"
May 17 08:07:45.999: INFO: Pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.255078ms
May 17 08:07:45.999: INFO: The phase of Pod busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:07:48.001: INFO: Pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003847093s
May 17 08:07:48.001: INFO: The phase of Pod busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1 is Running (Ready = true)
May 17 08:07:48.001: INFO: Pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 17 08:07:48.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8537" for this suite. 05/17/23 08:07:48.008
------------------------------
â€¢ [2.030 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:45.983
    May 17 08:07:45.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 08:07:45.984
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:45.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:45.991
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    May 17 08:07:45.998: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1" in namespace "kubelet-test-8537" to be "running and ready"
    May 17 08:07:45.999: INFO: Pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.255078ms
    May 17 08:07:45.999: INFO: The phase of Pod busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:07:48.001: INFO: Pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003847093s
    May 17 08:07:48.001: INFO: The phase of Pod busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1 is Running (Ready = true)
    May 17 08:07:48.001: INFO: Pod "busybox-readonly-fsb173c2c0-b4b3-4b87-a1b9-e53168d844f1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:48.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8537" for this suite. 05/17/23 08:07:48.008
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:48.012
May 17 08:07:48.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:07:48.013
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:48.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:48.02
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 05/17/23 08:07:48.022
STEP: Creating a ResourceQuota 05/17/23 08:07:53.025
STEP: Ensuring resource quota status is calculated 05/17/23 08:07:53.027
STEP: Creating a ReplicationController 05/17/23 08:07:55.03
STEP: Ensuring resource quota status captures replication controller creation 05/17/23 08:07:55.039
STEP: Deleting a ReplicationController 05/17/23 08:07:57.042
STEP: Ensuring resource quota status released usage 05/17/23 08:07:57.045
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:07:59.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8908" for this suite. 05/17/23 08:07:59.051
------------------------------
â€¢ [SLOW TEST] [11.043 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:48.012
    May 17 08:07:48.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:07:48.013
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:48.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:48.02
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 05/17/23 08:07:48.022
    STEP: Creating a ResourceQuota 05/17/23 08:07:53.025
    STEP: Ensuring resource quota status is calculated 05/17/23 08:07:53.027
    STEP: Creating a ReplicationController 05/17/23 08:07:55.03
    STEP: Ensuring resource quota status captures replication controller creation 05/17/23 08:07:55.039
    STEP: Deleting a ReplicationController 05/17/23 08:07:57.042
    STEP: Ensuring resource quota status released usage 05/17/23 08:07:57.045
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:07:59.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8908" for this suite. 05/17/23 08:07:59.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:07:59.056
May 17 08:07:59.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 08:07:59.057
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:59.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:59.066
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-036ae20e-878f-4a33-b9ca-dc81a48ed84f 05/17/23 08:07:59.067
STEP: Creating a pod to test consume configMaps 05/17/23 08:07:59.07
May 17 08:07:59.074: INFO: Waiting up to 5m0s for pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321" in namespace "configmap-1567" to be "Succeeded or Failed"
May 17 08:07:59.075: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349931ms
May 17 08:08:01.079: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004940349s
May 17 08:08:03.079: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004843223s
STEP: Saw pod success 05/17/23 08:08:03.079
May 17 08:08:03.079: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321" satisfied condition "Succeeded or Failed"
May 17 08:08:03.081: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 08:08:03.085
May 17 08:08:03.092: INFO: Waiting for pod pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321 to disappear
May 17 08:08:03.094: INFO: Pod pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 08:08:03.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1567" for this suite. 05/17/23 08:08:03.096
------------------------------
â€¢ [4.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:07:59.056
    May 17 08:07:59.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 08:07:59.057
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:07:59.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:07:59.066
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-036ae20e-878f-4a33-b9ca-dc81a48ed84f 05/17/23 08:07:59.067
    STEP: Creating a pod to test consume configMaps 05/17/23 08:07:59.07
    May 17 08:07:59.074: INFO: Waiting up to 5m0s for pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321" in namespace "configmap-1567" to be "Succeeded or Failed"
    May 17 08:07:59.075: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349931ms
    May 17 08:08:01.079: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004940349s
    May 17 08:08:03.079: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004843223s
    STEP: Saw pod success 05/17/23 08:08:03.079
    May 17 08:08:03.079: INFO: Pod "pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321" satisfied condition "Succeeded or Failed"
    May 17 08:08:03.081: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 08:08:03.085
    May 17 08:08:03.092: INFO: Waiting for pod pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321 to disappear
    May 17 08:08:03.094: INFO: Pod pod-configmaps-a30ac983-7c62-4e4c-9fb2-c0eb8e562321 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:08:03.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1567" for this suite. 05/17/23 08:08:03.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:08:03.099
May 17 08:08:03.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 08:08:03.099
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:08:03.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:08:03.109
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-664 05/17/23 08:08:03.111
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[] 05/17/23 08:08:03.117
May 17 08:08:03.119: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 17 08:08:04.122: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-664 05/17/23 08:08:04.122
May 17 08:08:04.127: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-664" to be "running and ready"
May 17 08:08:04.129: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.247325ms
May 17 08:08:04.129: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:08:06.132: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005053503s
May 17 08:08:06.132: INFO: The phase of Pod pod1 is Running (Ready = true)
May 17 08:08:06.132: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[pod1:[80]] 05/17/23 08:08:06.134
May 17 08:08:06.138: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/17/23 08:08:06.138
May 17 08:08:06.139: INFO: Creating new exec pod
May 17 08:08:06.142: INFO: Waiting up to 5m0s for pod "execpodkn22d" in namespace "services-664" to be "running"
May 17 08:08:06.144: INFO: Pod "execpodkn22d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238387ms
May 17 08:08:08.146: INFO: Pod "execpodkn22d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003192207s
May 17 08:08:08.146: INFO: Pod "execpodkn22d" satisfied condition "running"
May 17 08:08:09.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May 17 08:08:09.253: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 17 08:08:09.253: INFO: stdout: ""
May 17 08:08:09.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 10.104.52.176 80'
May 17 08:08:09.360: INFO: stderr: "+ nc -v -z -w 2 10.104.52.176 80\nConnection to 10.104.52.176 80 port [tcp/http] succeeded!\n"
May 17 08:08:09.360: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-664 05/17/23 08:08:09.36
May 17 08:08:09.364: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-664" to be "running and ready"
May 17 08:08:09.366: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.324141ms
May 17 08:08:09.366: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:08:11.368: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.0041169s
May 17 08:08:11.368: INFO: The phase of Pod pod2 is Running (Ready = true)
May 17 08:08:11.368: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[pod1:[80] pod2:[80]] 05/17/23 08:08:11.37
May 17 08:08:11.376: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/17/23 08:08:11.376
May 17 08:08:12.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May 17 08:08:12.471: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 17 08:08:12.471: INFO: stdout: ""
May 17 08:08:12.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 10.104.52.176 80'
May 17 08:08:12.571: INFO: stderr: "+ nc -v -z -w 2 10.104.52.176 80\nConnection to 10.104.52.176 80 port [tcp/http] succeeded!\n"
May 17 08:08:12.571: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-664 05/17/23 08:08:12.571
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[pod2:[80]] 05/17/23 08:08:12.577
May 17 08:08:13.586: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/17/23 08:08:13.586
May 17 08:08:14.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May 17 08:08:14.683: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 17 08:08:14.683: INFO: stdout: ""
May 17 08:08:14.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 10.104.52.176 80'
May 17 08:08:14.773: INFO: stderr: "+ nc -v -z -w 2 10.104.52.176 80\nConnection to 10.104.52.176 80 port [tcp/http] succeeded!\n"
May 17 08:08:14.773: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-664 05/17/23 08:08:14.773
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[] 05/17/23 08:08:14.779
May 17 08:08:14.784: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 08:08:14.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-664" for this suite. 05/17/23 08:08:14.793
------------------------------
â€¢ [SLOW TEST] [11.697 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:08:03.099
    May 17 08:08:03.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 08:08:03.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:08:03.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:08:03.109
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-664 05/17/23 08:08:03.111
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[] 05/17/23 08:08:03.117
    May 17 08:08:03.119: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    May 17 08:08:04.122: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-664 05/17/23 08:08:04.122
    May 17 08:08:04.127: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-664" to be "running and ready"
    May 17 08:08:04.129: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.247325ms
    May 17 08:08:04.129: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:08:06.132: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005053503s
    May 17 08:08:06.132: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 17 08:08:06.132: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[pod1:[80]] 05/17/23 08:08:06.134
    May 17 08:08:06.138: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/17/23 08:08:06.138
    May 17 08:08:06.139: INFO: Creating new exec pod
    May 17 08:08:06.142: INFO: Waiting up to 5m0s for pod "execpodkn22d" in namespace "services-664" to be "running"
    May 17 08:08:06.144: INFO: Pod "execpodkn22d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238387ms
    May 17 08:08:08.146: INFO: Pod "execpodkn22d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003192207s
    May 17 08:08:08.146: INFO: Pod "execpodkn22d" satisfied condition "running"
    May 17 08:08:09.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May 17 08:08:09.253: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 17 08:08:09.253: INFO: stdout: ""
    May 17 08:08:09.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 10.104.52.176 80'
    May 17 08:08:09.360: INFO: stderr: "+ nc -v -z -w 2 10.104.52.176 80\nConnection to 10.104.52.176 80 port [tcp/http] succeeded!\n"
    May 17 08:08:09.360: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-664 05/17/23 08:08:09.36
    May 17 08:08:09.364: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-664" to be "running and ready"
    May 17 08:08:09.366: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.324141ms
    May 17 08:08:09.366: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:08:11.368: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.0041169s
    May 17 08:08:11.368: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 17 08:08:11.368: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[pod1:[80] pod2:[80]] 05/17/23 08:08:11.37
    May 17 08:08:11.376: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/17/23 08:08:11.376
    May 17 08:08:12.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May 17 08:08:12.471: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 17 08:08:12.471: INFO: stdout: ""
    May 17 08:08:12.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 10.104.52.176 80'
    May 17 08:08:12.571: INFO: stderr: "+ nc -v -z -w 2 10.104.52.176 80\nConnection to 10.104.52.176 80 port [tcp/http] succeeded!\n"
    May 17 08:08:12.571: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-664 05/17/23 08:08:12.571
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[pod2:[80]] 05/17/23 08:08:12.577
    May 17 08:08:13.586: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/17/23 08:08:13.586
    May 17 08:08:14.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May 17 08:08:14.683: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 17 08:08:14.683: INFO: stdout: ""
    May 17 08:08:14.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-664 exec execpodkn22d -- /bin/sh -x -c nc -v -z -w 2 10.104.52.176 80'
    May 17 08:08:14.773: INFO: stderr: "+ nc -v -z -w 2 10.104.52.176 80\nConnection to 10.104.52.176 80 port [tcp/http] succeeded!\n"
    May 17 08:08:14.773: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-664 05/17/23 08:08:14.773
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-664 to expose endpoints map[] 05/17/23 08:08:14.779
    May 17 08:08:14.784: INFO: successfully validated that service endpoint-test2 in namespace services-664 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 08:08:14.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-664" for this suite. 05/17/23 08:08:14.793
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:08:14.797
May 17 08:08:14.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-preemption 05/17/23 08:08:14.797
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:08:14.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:08:14.806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 17 08:08:14.816: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 08:09:14.831: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 05/17/23 08:09:14.832
May 17 08:09:14.846: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 17 08:09:14.848: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 17 08:09:14.860: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 17 08:09:14.863: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/17/23 08:09:14.863
May 17 08:09:14.863: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5710" to be "running"
May 17 08:09:14.865: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.172627ms
May 17 08:09:16.868: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005009395s
May 17 08:09:16.868: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May 17 08:09:16.868: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5710" to be "running"
May 17 08:09:16.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.858814ms
May 17 08:09:16.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May 17 08:09:16.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5710" to be "running"
May 17 08:09:16.872: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.540148ms
May 17 08:09:16.872: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May 17 08:09:16.872: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5710" to be "running"
May 17 08:09:16.873: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.408479ms
May 17 08:09:16.873: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/17/23 08:09:16.873
May 17 08:09:16.879: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May 17 08:09:16.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319381ms
May 17 08:09:18.884: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004234769s
May 17 08:09:20.885: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005636459s
May 17 08:09:20.885: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:09:20.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5710" for this suite. 05/17/23 08:09:20.919
------------------------------
â€¢ [SLOW TEST] [66.126 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:08:14.797
    May 17 08:08:14.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 08:08:14.797
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:08:14.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:08:14.806
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 17 08:08:14.816: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 08:09:14.831: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 05/17/23 08:09:14.832
    May 17 08:09:14.846: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May 17 08:09:14.848: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May 17 08:09:14.860: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May 17 08:09:14.863: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/17/23 08:09:14.863
    May 17 08:09:14.863: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5710" to be "running"
    May 17 08:09:14.865: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.172627ms
    May 17 08:09:16.868: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005009395s
    May 17 08:09:16.868: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May 17 08:09:16.868: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5710" to be "running"
    May 17 08:09:16.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.858814ms
    May 17 08:09:16.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May 17 08:09:16.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5710" to be "running"
    May 17 08:09:16.872: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.540148ms
    May 17 08:09:16.872: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May 17 08:09:16.872: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5710" to be "running"
    May 17 08:09:16.873: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.408479ms
    May 17 08:09:16.873: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/17/23 08:09:16.873
    May 17 08:09:16.879: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May 17 08:09:16.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319381ms
    May 17 08:09:18.884: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004234769s
    May 17 08:09:20.885: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005636459s
    May 17 08:09:20.885: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:20.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5710" for this suite. 05/17/23 08:09:20.919
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:20.922
May 17 08:09:20.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:09:20.923
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:20.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:20.93
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
May 17 08:09:20.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 08:09:22.258
May 17 08:09:22.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 create -f -'
May 17 08:09:22.748: INFO: stderr: ""
May 17 08:09:22.748: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 17 08:09:22.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 delete e2e-test-crd-publish-openapi-9574-crds test-cr'
May 17 08:09:22.804: INFO: stderr: ""
May 17 08:09:22.804: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 17 08:09:22.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 apply -f -'
May 17 08:09:22.963: INFO: stderr: ""
May 17 08:09:22.963: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 17 08:09:22.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 delete e2e-test-crd-publish-openapi-9574-crds test-cr'
May 17 08:09:23.027: INFO: stderr: ""
May 17 08:09:23.027: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/17/23 08:09:23.027
May 17 08:09:23.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 explain e2e-test-crd-publish-openapi-9574-crds'
May 17 08:09:23.174: INFO: stderr: ""
May 17 08:09:23.174: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9574-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:09:24.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-777" for this suite. 05/17/23 08:09:24.506
------------------------------
â€¢ [3.587 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:20.922
    May 17 08:09:20.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:09:20.923
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:20.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:20.93
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    May 17 08:09:20.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 08:09:22.258
    May 17 08:09:22.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 create -f -'
    May 17 08:09:22.748: INFO: stderr: ""
    May 17 08:09:22.748: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May 17 08:09:22.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 delete e2e-test-crd-publish-openapi-9574-crds test-cr'
    May 17 08:09:22.804: INFO: stderr: ""
    May 17 08:09:22.804: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May 17 08:09:22.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 apply -f -'
    May 17 08:09:22.963: INFO: stderr: ""
    May 17 08:09:22.963: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May 17 08:09:22.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 --namespace=crd-publish-openapi-777 delete e2e-test-crd-publish-openapi-9574-crds test-cr'
    May 17 08:09:23.027: INFO: stderr: ""
    May 17 08:09:23.027: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/17/23 08:09:23.027
    May 17 08:09:23.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-777 explain e2e-test-crd-publish-openapi-9574-crds'
    May 17 08:09:23.174: INFO: stderr: ""
    May 17 08:09:23.174: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9574-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:24.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-777" for this suite. 05/17/23 08:09:24.506
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:24.509
May 17 08:09:24.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 08:09:24.51
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:24.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:24.521
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May 17 08:09:24.523: INFO: Creating deployment "webserver-deployment"
May 17 08:09:24.525: INFO: Waiting for observed generation 1
May 17 08:09:26.529: INFO: Waiting for all required pods to come up
May 17 08:09:26.532: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/17/23 08:09:26.532
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-zfvx5" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gdr7q" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sfxnv" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gtrsd" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tk967" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-t4pkr" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vz7qj" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rvs4d" in namespace "deployment-3448" to be "running"
May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rgwb4" in namespace "deployment-3448" to be "running"
May 17 08:09:26.534: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.871403ms
May 17 08:09:26.535: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869646ms
May 17 08:09:26.535: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51116ms
May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034829ms
May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-gdr7q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148118ms
May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-tk967": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127328ms
May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383664ms
May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-sfxnv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.58771ms
May 17 08:09:26.537: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.819404ms
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004396366s
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd" satisfied condition "running"
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004494425s
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5" satisfied condition "running"
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-tk967": Phase="Running", Reason="", readiness=true. Elapsed: 2.004719635s
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-tk967" satisfied condition "running"
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004681239s
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d" satisfied condition "running"
May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004765017s
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4" satisfied condition "running"
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-gdr7q": Phase="Running", Reason="", readiness=true. Elapsed: 2.004944622s
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-gdr7q" satisfied condition "running"
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005465217s
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr" satisfied condition "running"
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj": Phase="Running", Reason="", readiness=true. Elapsed: 2.005566542s
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj" satisfied condition "running"
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-sfxnv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005708888s
May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-sfxnv" satisfied condition "running"
May 17 08:09:28.538: INFO: Waiting for deployment "webserver-deployment" to complete
May 17 08:09:28.541: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 17 08:09:28.546: INFO: Updating deployment webserver-deployment
May 17 08:09:28.546: INFO: Waiting for observed generation 2
May 17 08:09:30.553: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 17 08:09:30.554: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 17 08:09:30.555: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 17 08:09:30.559: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 17 08:09:30.559: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 17 08:09:30.561: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 17 08:09:30.563: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 17 08:09:30.563: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 17 08:09:30.568: INFO: Updating deployment webserver-deployment
May 17 08:09:30.568: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 17 08:09:30.571: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 17 08:09:30.573: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 08:09:32.578: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3448  6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f 1201024 3 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da4768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 08:09:30 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-17 08:09:30 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 17 08:09:32.580: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3448  81583337-20f3-4f5e-aaf5-bb5402e524d4 1201020 3 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f 0xc002da4cb7 0xc002da4cb8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da4d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 08:09:32.581: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 17 08:09:32.581: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-3448  aaaea7bd-c147-4edf-a575-c42d20f9e7d9 1201023 3 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f 0xc002da4bc7 0xc002da4bc8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da4c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-4hpc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4hpc4 webserver-deployment-7f5969cbc7- deployment-3448  d29dc6ba-08a1-41ad-817a-ff4689a16768 1200994 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0030711e7 0xc0030711e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7f8dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7f8dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-6jbbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6jbbz webserver-deployment-7f5969cbc7- deployment-3448  9fb8f01c-325b-4e14-bbd3-55d6d406e59b 1201114 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:498bb8ff77c89fa55a5109903b3601f82d7251c138dbf24e63c741b87ce9c091 cni.projectcalico.org/podIP:192.168.169.177/32 cni.projectcalico.org/podIPs:192.168.169.177/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071357 0xc003071358}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4vq2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4vq2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-8kqrh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8kqrh webserver-deployment-7f5969cbc7- deployment-3448  90ab98f9-e679-4d11-8ba4-91f46a7488fc 1201054 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:89faf75fa6968ea3aed37ae85572b3c13e367f09d3ed6d49fa86fe9d98c2c400 cni.projectcalico.org/podIP:192.168.36.96/32 cni.projectcalico.org/podIPs:192.168.36.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071507 0xc003071508}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c42mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c42mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-f8wt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f8wt4 webserver-deployment-7f5969cbc7- deployment-3448  4e0a42fd-442d-498b-b5b9-493a0466ddc5 1201117 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:04017c5c1da25821dc0e310b9215dd424eeafbfa145fe2d4f9f23d222ec8bdfd cni.projectcalico.org/podIP:192.168.36.79/32 cni.projectcalico.org/podIPs:192.168.36.79/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0030716b7 0xc0030716b8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bq97m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bq97m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-g4qkp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g4qkp webserver-deployment-7f5969cbc7- deployment-3448  ec474d96-ea78-4b37-9378-3de5489c0de2 1200801 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:26e423bec909b2e20e9ff83d59709d31d8a18edd0868a4c8b7ce00cecb0b3cea cni.projectcalico.org/podIP:192.168.36.106/32 cni.projectcalico.org/podIPs:192.168.36.106/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071847 0xc003071848}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdp25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdp25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.106,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://87fd8cdde51729c87a752b3dcbf84946fca6e6569d1f3de43dd4e3b41cd5ea75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gtrsd webserver-deployment-7f5969cbc7- deployment-3448  bde6ff41-aed7-4509-be03-1e9808a2938b 1200842 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:81ee7124f3e79e5b711301a179b8621a784f5d855f514f0b9cfee1d14ae94229 cni.projectcalico.org/podIP:192.168.169.132/32 cni.projectcalico.org/podIPs:192.168.169.132/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071a57 0xc003071a58}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh27l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh27l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.132,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://2e3590e8d9a85084f5a907eba136143edf6b778b16241fe04cc51bce7580cc2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-kltlb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kltlb webserver-deployment-7f5969cbc7- deployment-3448  b033d797-b098-4fbc-a68e-8c30abe15ef6 1201084 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4fca16a842875e204baf9142b3e2d378f0297525bd519f6481fbd056c4748ab7 cni.projectcalico.org/podIP:192.168.169.186/32 cni.projectcalico.org/podIPs:192.168.169.186/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071c67 0xc003071c68}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xsbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xsbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rgwb4 webserver-deployment-7f5969cbc7- deployment-3448  4bd3ecaf-ee3c-4168-9012-7a8832e2f613 1200845 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7536987a662dbf88d5f499f5162c7023c4d75a17a5df334d9f6eb8d3cb3b0008 cni.projectcalico.org/podIP:192.168.36.91/32 cni.projectcalico.org/podIPs:192.168.36.91/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071e17 0xc003071e18}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qzvlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qzvlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.91,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://41a07c8d21a41d07158f132d5a009183539a77d398c463c8ae88c70d7545e871,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rn8th" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rn8th webserver-deployment-7f5969cbc7- deployment-3448  a530ab16-910f-493a-af63-94eab13ba66f 1200977 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934027 0xc004934028}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dszzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dszzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rvs4d webserver-deployment-7f5969cbc7- deployment-3448  33dd0f47-1fac-4981-a267-3f12bc32a277 1200819 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e47cc3d7122592809aa477645b622a8f515e8972084da25ace13a92ffdbdd020 cni.projectcalico.org/podIP:192.168.36.95/32 cni.projectcalico.org/podIPs:192.168.36.95/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049341b7 0xc0049341b8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4f68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4f68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.95,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://6dfc1d6469b0f3bb50e753fa060ab5191c709a19a6b04620aa9525108c970fda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rwxvs" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rwxvs webserver-deployment-7f5969cbc7- deployment-3448  89295884-211c-4a82-8afc-5abe13d61f63 1200953 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049343e7 0xc0049343e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlczj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlczj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-sh2w8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sh2w8 webserver-deployment-7f5969cbc7- deployment-3448  080a0260-7e35-4878-bae3-375156b493ed 1201083 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0578a12c885f9c84864a4f6216302681b1f2a2840b9e3066a6b893e067194156 cni.projectcalico.org/podIP:192.168.36.124/32 cni.projectcalico.org/podIPs:192.168.36.124/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934557 0xc004934558}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p2vtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p2vtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t4pkr webserver-deployment-7f5969cbc7- deployment-3448  65d822e5-b66a-4364-89bd-c157eed23130 1200834 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f77e5bc6a8182cdbefb5215100ec9a1a24296b2e7495772c38c0a40f3f47a472 cni.projectcalico.org/podIP:192.168.169.156/32 cni.projectcalico.org/podIPs:192.168.169.156/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049346e7 0xc0049346e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hbg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hbg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.156,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://4a8fe90748c036a11406dca412fb1d96cd926e37efa73cb473c53ef13c177e23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-tfnfc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tfnfc webserver-deployment-7f5969cbc7- deployment-3448  5d512719-6eaf-4de2-aade-0733fbe6516f 1201118 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c6d0cb6cda4cee6bbd91d04cc8cf9ce7a9363413797e240ca48c98805ad529a3 cni.projectcalico.org/podIP:192.168.169.154/32 cni.projectcalico.org/podIPs:192.168.169.154/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049348f7 0xc0049348f8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gpk7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gpk7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-tk967" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tk967 webserver-deployment-7f5969cbc7- deployment-3448  0ce61180-beb4-4e5a-9c8f-ea065808b59e 1200820 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:014bee4ecb71aaa827089a24c9dc03fd4c53cf0ebde6c8e7b4b36b9532631748 cni.projectcalico.org/podIP:192.168.169.155/32 cni.projectcalico.org/podIPs:192.168.169.155/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934a87 0xc004934a88}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stmvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stmvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.155,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://ee4bdbfd0937cd0224fd377d02b4de5ac3dbb7dbd2141bc09f9fe323167a2d0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-tnkfv" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tnkfv webserver-deployment-7f5969cbc7- deployment-3448  cac0b57e-c69f-46e1-951c-24299911d87d 1201050 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9eacf5cadedac616399cc35a5a96d1c5faca607a048ec6aa342b2c3ddd299fef cni.projectcalico.org/podIP:192.168.169.158/32 cni.projectcalico.org/podIPs:192.168.169.158/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934c97 0xc004934c98}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6j8gc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6j8gc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vz7qj webserver-deployment-7f5969cbc7- deployment-3448  d354e260-67c3-497c-bb26-68984e2a2433 1200848 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f29b22a5023de7349734d62aa4910d1cb517ca5d925a267b16d57c1f30123d4b cni.projectcalico.org/podIP:192.168.169.165/32 cni.projectcalico.org/podIPs:192.168.169.165/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934e27 0xc004934e28}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87pfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87pfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.165,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://94264a0f01f2eac382750e6799b83bfc769d2be8409d9bf00d738a171c6c112f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-wqd7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wqd7g webserver-deployment-7f5969cbc7- deployment-3448  9c816ef8-acd6-42ba-8727-9244421f6a51 1201101 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2918025b8bccbe51b81e5abc815c19dc262917d2b0eb8402ea07c2d6bb52858d cni.projectcalico.org/podIP:192.168.36.94/32 cni.projectcalico.org/podIPs:192.168.36.94/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004935057 0xc004935058}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4z9xr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4z9xr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-zbzsr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zbzsr webserver-deployment-7f5969cbc7- deployment-3448  3dc8f64a-fbef-4c8a-8086-c32d69e586b4 1201079 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a57d666a6cb85d97b3e7f35cc6c5c1754a9cdc2005a1cb3146665cb69d28e030 cni.projectcalico.org/podIP:192.168.36.100/32 cni.projectcalico.org/podIPs:192.168.36.100/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049351e7 0xc0049351e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdtwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdtwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zfvx5 webserver-deployment-7f5969cbc7- deployment-3448  a013ce95-eb90-43db-afe6-73dcd39ee8eb 1200827 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9f8513624a67bf5ad835dc06b7a66bb39f5e8760eb53b77dd66ddc580e7f3e9e cni.projectcalico.org/podIP:192.168.169.167/32 cni.projectcalico.org/podIPs:192.168.169.167/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004935377 0xc004935378}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgnqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgnqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.167,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c2644d1d17d603f8c8cce5b0e8cf51866fe5155f2415ade45aa1baafcb825ab8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-d9f79cb5-552tf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-552tf webserver-deployment-d9f79cb5- deployment-3448  0319c5ed-abd4-494d-b469-d00e569b3c60 1200945 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:8feb4bf768bfb762074498624d34458386b26942fea9cc61c20292c8591b4565 cni.projectcalico.org/podIP:192.168.36.71/32 cni.projectcalico.org/podIPs:192.168.36.71/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc0049355a7 0xc0049355a8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nq5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nq5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.587: INFO: Pod "webserver-deployment-d9f79cb5-6j6tl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6j6tl webserver-deployment-d9f79cb5- deployment-3448  97309774-e62c-43a6-88b2-b3508efae0de 1200942 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cbbd83c824935794d41375d8e2f669b60be0c9fb3b060e28841bdb7e446dd040 cni.projectcalico.org/podIP:192.168.36.81/32 cni.projectcalico.org/podIPs:192.168.36.81/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc0049357d7 0xc0049357d8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mf6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mf6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-c9psl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-c9psl webserver-deployment-d9f79cb5- deployment-3448  94c089b7-bb9b-4845-845b-81cda3307876 1201008 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc0049359e7 0xc0049359e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nn64b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn64b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-djx2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-djx2m webserver-deployment-d9f79cb5- deployment-3448  7f030a3a-63ec-42c7-8f2d-317ecd7394fb 1201105 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4dbadaabd167d2375eaeb9a74015c7bdc95e3fb435c04571971563c05728e618 cni.projectcalico.org/podIP:192.168.36.114/32 cni.projectcalico.org/podIPs:192.168.36.114/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004935b67 0xc004935b68}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s45xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s45xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-jcqrk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jcqrk webserver-deployment-d9f79cb5- deployment-3448  7d21c094-6cb9-46c5-9edf-d7ff66639c9f 1200995 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004935d07 0xc004935d08}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhmc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhmc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-jv4k8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jv4k8 webserver-deployment-d9f79cb5- deployment-3448  496282a6-3ae7-4ce0-aef1-cc3ad0987bb4 1201077 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:29e35d112cc96d5b0d882594820d915160704b71cc91d255737c7f39f07cbcf7 cni.projectcalico.org/podIP:192.168.169.131/32 cni.projectcalico.org/podIPs:192.168.169.131/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004935e87 0xc004935e88}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmht5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmht5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-k5gms" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k5gms webserver-deployment-d9f79cb5- deployment-3448  14bd82a4-7312-42a2-a7e7-19263d2eb374 1201104 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d1f6d5e3c8ea5c75dc8ff6b19f8232dde5a3b88e9337e31b1db6b9305c553cfb cni.projectcalico.org/podIP:192.168.169.145/32 cni.projectcalico.org/podIPs:192.168.169.145/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a22027 0xc004a22028}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6knl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6knl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-lm5zn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lm5zn webserver-deployment-d9f79cb5- deployment-3448  eb46e38a-434c-4f43-ad2b-8aacc9979fc6 1201067 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e7c0818668da70a908f201656c89f7b825d21095f2d0010838595a94fe851c44 cni.projectcalico.org/podIP:192.168.169.140/32 cni.projectcalico.org/podIPs:192.168.169.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a221c7 0xc004a221c8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7s7w9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7s7w9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-lrtkl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lrtkl webserver-deployment-d9f79cb5- deployment-3448  be9ffbb5-15d5-4123-9598-af4722f6e4c8 1201034 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:366c7181b5f41e503ecaecb14d479c10e5d453bed83dba435da44a904a0fa6fd cni.projectcalico.org/podIP:192.168.169.130/32 cni.projectcalico.org/podIPs:192.168.169.130/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a224b7 0xc004a224b8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q8m9x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q8m9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-m8jjg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m8jjg webserver-deployment-d9f79cb5- deployment-3448  5b8197ca-7e4b-4236-aa3d-e2c0c2a11aa6 1200996 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a22797 0xc004a22798}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lmkl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmkl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-mr4gz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mr4gz webserver-deployment-d9f79cb5- deployment-3448  3169ebc4-55bc-422b-ae9a-1731cd46d849 1201065 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:934113c9760ae5986265021cdebe3b0edc7d7092fb94589d23342b960d984bb8 cni.projectcalico.org/podIP:192.168.36.111/32 cni.projectcalico.org/podIPs:192.168.36.111/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a22ae7 0xc004a22ae8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9qg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9qg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.589: INFO: Pod "webserver-deployment-d9f79cb5-s74sf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s74sf webserver-deployment-d9f79cb5- deployment-3448  45b51996-7331-4919-89c0-4e80ec2da96f 1200944 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d121aa0587374d5ff159b474156961944960046a647f7439599be0793a764306 cni.projectcalico.org/podIP:192.168.36.85/32 cni.projectcalico.org/podIPs:192.168.36.85/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a230d7 0xc004a230d8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwmzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwmzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:09:32.589: INFO: Pod "webserver-deployment-d9f79cb5-whdb6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-whdb6 webserver-deployment-d9f79cb5- deployment-3448  7865cd15-29e6-443c-a66c-6a8b81daa21b 1200946 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:818d97b8a684ec6a3838a41f3cb6cc10fc44fe107a759e4dcef442c88248b449 cni.projectcalico.org/podIP:192.168.169.176/32 cni.projectcalico.org/podIPs:192.168.169.176/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a232e7 0xc004a232e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qzb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qzb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 08:09:32.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3448" for this suite. 05/17/23 08:09:32.591
------------------------------
â€¢ [SLOW TEST] [8.085 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:24.509
    May 17 08:09:24.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 08:09:24.51
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:24.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:24.521
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May 17 08:09:24.523: INFO: Creating deployment "webserver-deployment"
    May 17 08:09:24.525: INFO: Waiting for observed generation 1
    May 17 08:09:26.529: INFO: Waiting for all required pods to come up
    May 17 08:09:26.532: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/17/23 08:09:26.532
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-zfvx5" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gdr7q" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sfxnv" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gtrsd" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tk967" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-t4pkr" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vz7qj" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rvs4d" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.533: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rgwb4" in namespace "deployment-3448" to be "running"
    May 17 08:09:26.534: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.871403ms
    May 17 08:09:26.535: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869646ms
    May 17 08:09:26.535: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51116ms
    May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034829ms
    May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-gdr7q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148118ms
    May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-tk967": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127328ms
    May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383664ms
    May 17 08:09:26.536: INFO: Pod "webserver-deployment-7f5969cbc7-sfxnv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.58771ms
    May 17 08:09:26.537: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.819404ms
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004396366s
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd" satisfied condition "running"
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004494425s
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5" satisfied condition "running"
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-tk967": Phase="Running", Reason="", readiness=true. Elapsed: 2.004719635s
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-tk967" satisfied condition "running"
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004681239s
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d" satisfied condition "running"
    May 17 08:09:28.537: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004765017s
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4" satisfied condition "running"
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-gdr7q": Phase="Running", Reason="", readiness=true. Elapsed: 2.004944622s
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-gdr7q" satisfied condition "running"
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005465217s
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr" satisfied condition "running"
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj": Phase="Running", Reason="", readiness=true. Elapsed: 2.005566542s
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj" satisfied condition "running"
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-sfxnv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005708888s
    May 17 08:09:28.538: INFO: Pod "webserver-deployment-7f5969cbc7-sfxnv" satisfied condition "running"
    May 17 08:09:28.538: INFO: Waiting for deployment "webserver-deployment" to complete
    May 17 08:09:28.541: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May 17 08:09:28.546: INFO: Updating deployment webserver-deployment
    May 17 08:09:28.546: INFO: Waiting for observed generation 2
    May 17 08:09:30.553: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May 17 08:09:30.554: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May 17 08:09:30.555: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May 17 08:09:30.559: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May 17 08:09:30.559: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May 17 08:09:30.561: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May 17 08:09:30.563: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May 17 08:09:30.563: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May 17 08:09:30.568: INFO: Updating deployment webserver-deployment
    May 17 08:09:30.568: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May 17 08:09:30.571: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May 17 08:09:30.573: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 08:09:32.578: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-3448  6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f 1201024 3 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da4768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 08:09:30 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-17 08:09:30 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    May 17 08:09:32.580: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3448  81583337-20f3-4f5e-aaf5-bb5402e524d4 1201020 3 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f 0xc002da4cb7 0xc002da4cb8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da4d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 08:09:32.581: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May 17 08:09:32.581: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-3448  aaaea7bd-c147-4edf-a575-c42d20f9e7d9 1201023 3 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f 0xc002da4bc7 0xc002da4bc8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd9702e-9eb9-4cfd-bb49-e2b2e27e484f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da4c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-4hpc4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4hpc4 webserver-deployment-7f5969cbc7- deployment-3448  d29dc6ba-08a1-41ad-817a-ff4689a16768 1200994 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0030711e7 0xc0030711e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7f8dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7f8dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-6jbbz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6jbbz webserver-deployment-7f5969cbc7- deployment-3448  9fb8f01c-325b-4e14-bbd3-55d6d406e59b 1201114 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:498bb8ff77c89fa55a5109903b3601f82d7251c138dbf24e63c741b87ce9c091 cni.projectcalico.org/podIP:192.168.169.177/32 cni.projectcalico.org/podIPs:192.168.169.177/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071357 0xc003071358}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4vq2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4vq2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-8kqrh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8kqrh webserver-deployment-7f5969cbc7- deployment-3448  90ab98f9-e679-4d11-8ba4-91f46a7488fc 1201054 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:89faf75fa6968ea3aed37ae85572b3c13e367f09d3ed6d49fa86fe9d98c2c400 cni.projectcalico.org/podIP:192.168.36.96/32 cni.projectcalico.org/podIPs:192.168.36.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071507 0xc003071508}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c42mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c42mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-f8wt4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f8wt4 webserver-deployment-7f5969cbc7- deployment-3448  4e0a42fd-442d-498b-b5b9-493a0466ddc5 1201117 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:04017c5c1da25821dc0e310b9215dd424eeafbfa145fe2d4f9f23d222ec8bdfd cni.projectcalico.org/podIP:192.168.36.79/32 cni.projectcalico.org/podIPs:192.168.36.79/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0030716b7 0xc0030716b8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bq97m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bq97m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.585: INFO: Pod "webserver-deployment-7f5969cbc7-g4qkp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g4qkp webserver-deployment-7f5969cbc7- deployment-3448  ec474d96-ea78-4b37-9378-3de5489c0de2 1200801 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:26e423bec909b2e20e9ff83d59709d31d8a18edd0868a4c8b7ce00cecb0b3cea cni.projectcalico.org/podIP:192.168.36.106/32 cni.projectcalico.org/podIPs:192.168.36.106/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071847 0xc003071848}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdp25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdp25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.106,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://87fd8cdde51729c87a752b3dcbf84946fca6e6569d1f3de43dd4e3b41cd5ea75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-gtrsd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gtrsd webserver-deployment-7f5969cbc7- deployment-3448  bde6ff41-aed7-4509-be03-1e9808a2938b 1200842 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:81ee7124f3e79e5b711301a179b8621a784f5d855f514f0b9cfee1d14ae94229 cni.projectcalico.org/podIP:192.168.169.132/32 cni.projectcalico.org/podIPs:192.168.169.132/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071a57 0xc003071a58}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh27l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh27l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.132,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://2e3590e8d9a85084f5a907eba136143edf6b778b16241fe04cc51bce7580cc2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-kltlb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kltlb webserver-deployment-7f5969cbc7- deployment-3448  b033d797-b098-4fbc-a68e-8c30abe15ef6 1201084 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4fca16a842875e204baf9142b3e2d378f0297525bd519f6481fbd056c4748ab7 cni.projectcalico.org/podIP:192.168.169.186/32 cni.projectcalico.org/podIPs:192.168.169.186/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071c67 0xc003071c68}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xsbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xsbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rgwb4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rgwb4 webserver-deployment-7f5969cbc7- deployment-3448  4bd3ecaf-ee3c-4168-9012-7a8832e2f613 1200845 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7536987a662dbf88d5f499f5162c7023c4d75a17a5df334d9f6eb8d3cb3b0008 cni.projectcalico.org/podIP:192.168.36.91/32 cni.projectcalico.org/podIPs:192.168.36.91/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc003071e17 0xc003071e18}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qzvlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qzvlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.91,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://41a07c8d21a41d07158f132d5a009183539a77d398c463c8ae88c70d7545e871,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rn8th" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rn8th webserver-deployment-7f5969cbc7- deployment-3448  a530ab16-910f-493a-af63-94eab13ba66f 1200977 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934027 0xc004934028}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dszzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dszzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rvs4d" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rvs4d webserver-deployment-7f5969cbc7- deployment-3448  33dd0f47-1fac-4981-a267-3f12bc32a277 1200819 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e47cc3d7122592809aa477645b622a8f515e8972084da25ace13a92ffdbdd020 cni.projectcalico.org/podIP:192.168.36.95/32 cni.projectcalico.org/podIPs:192.168.36.95/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049341b7 0xc0049341b8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4f68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4f68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.95,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://6dfc1d6469b0f3bb50e753fa060ab5191c709a19a6b04620aa9525108c970fda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-rwxvs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rwxvs webserver-deployment-7f5969cbc7- deployment-3448  89295884-211c-4a82-8afc-5abe13d61f63 1200953 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049343e7 0xc0049343e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlczj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlczj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-sh2w8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sh2w8 webserver-deployment-7f5969cbc7- deployment-3448  080a0260-7e35-4878-bae3-375156b493ed 1201083 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0578a12c885f9c84864a4f6216302681b1f2a2840b9e3066a6b893e067194156 cni.projectcalico.org/podIP:192.168.36.124/32 cni.projectcalico.org/podIPs:192.168.36.124/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934557 0xc004934558}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p2vtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p2vtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.586: INFO: Pod "webserver-deployment-7f5969cbc7-t4pkr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t4pkr webserver-deployment-7f5969cbc7- deployment-3448  65d822e5-b66a-4364-89bd-c157eed23130 1200834 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f77e5bc6a8182cdbefb5215100ec9a1a24296b2e7495772c38c0a40f3f47a472 cni.projectcalico.org/podIP:192.168.169.156/32 cni.projectcalico.org/podIPs:192.168.169.156/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049346e7 0xc0049346e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hbg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hbg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.156,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://4a8fe90748c036a11406dca412fb1d96cd926e37efa73cb473c53ef13c177e23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-tfnfc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tfnfc webserver-deployment-7f5969cbc7- deployment-3448  5d512719-6eaf-4de2-aade-0733fbe6516f 1201118 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c6d0cb6cda4cee6bbd91d04cc8cf9ce7a9363413797e240ca48c98805ad529a3 cni.projectcalico.org/podIP:192.168.169.154/32 cni.projectcalico.org/podIPs:192.168.169.154/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049348f7 0xc0049348f8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gpk7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gpk7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-tk967" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tk967 webserver-deployment-7f5969cbc7- deployment-3448  0ce61180-beb4-4e5a-9c8f-ea065808b59e 1200820 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:014bee4ecb71aaa827089a24c9dc03fd4c53cf0ebde6c8e7b4b36b9532631748 cni.projectcalico.org/podIP:192.168.169.155/32 cni.projectcalico.org/podIPs:192.168.169.155/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934a87 0xc004934a88}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stmvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stmvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.155,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://ee4bdbfd0937cd0224fd377d02b4de5ac3dbb7dbd2141bc09f9fe323167a2d0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-tnkfv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tnkfv webserver-deployment-7f5969cbc7- deployment-3448  cac0b57e-c69f-46e1-951c-24299911d87d 1201050 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9eacf5cadedac616399cc35a5a96d1c5faca607a048ec6aa342b2c3ddd299fef cni.projectcalico.org/podIP:192.168.169.158/32 cni.projectcalico.org/podIPs:192.168.169.158/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934c97 0xc004934c98}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6j8gc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6j8gc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-vz7qj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vz7qj webserver-deployment-7f5969cbc7- deployment-3448  d354e260-67c3-497c-bb26-68984e2a2433 1200848 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f29b22a5023de7349734d62aa4910d1cb517ca5d925a267b16d57c1f30123d4b cni.projectcalico.org/podIP:192.168.169.165/32 cni.projectcalico.org/podIPs:192.168.169.165/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004934e27 0xc004934e28}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87pfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87pfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.165,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://94264a0f01f2eac382750e6799b83bfc769d2be8409d9bf00d738a171c6c112f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-wqd7g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wqd7g webserver-deployment-7f5969cbc7- deployment-3448  9c816ef8-acd6-42ba-8727-9244421f6a51 1201101 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2918025b8bccbe51b81e5abc815c19dc262917d2b0eb8402ea07c2d6bb52858d cni.projectcalico.org/podIP:192.168.36.94/32 cni.projectcalico.org/podIPs:192.168.36.94/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004935057 0xc004935058}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4z9xr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4z9xr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-zbzsr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zbzsr webserver-deployment-7f5969cbc7- deployment-3448  3dc8f64a-fbef-4c8a-8086-c32d69e586b4 1201079 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a57d666a6cb85d97b3e7f35cc6c5c1754a9cdc2005a1cb3146665cb69d28e030 cni.projectcalico.org/podIP:192.168.36.100/32 cni.projectcalico.org/podIPs:192.168.36.100/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc0049351e7 0xc0049351e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdtwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdtwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-7f5969cbc7-zfvx5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zfvx5 webserver-deployment-7f5969cbc7- deployment-3448  a013ce95-eb90-43db-afe6-73dcd39ee8eb 1200827 0 2023-05-17 08:09:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9f8513624a67bf5ad835dc06b7a66bb39f5e8760eb53b77dd66ddc580e7f3e9e cni.projectcalico.org/podIP:192.168.169.167/32 cni.projectcalico.org/podIPs:192.168.169.167/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 aaaea7bd-c147-4edf-a575-c42d20f9e7d9 0xc004935377 0xc004935378}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aaaea7bd-c147-4edf-a575-c42d20f9e7d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgnqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgnqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.167,StartTime:2023-05-17 08:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:09:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c2644d1d17d603f8c8cce5b0e8cf51866fe5155f2415ade45aa1baafcb825ab8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-d9f79cb5-552tf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-552tf webserver-deployment-d9f79cb5- deployment-3448  0319c5ed-abd4-494d-b469-d00e569b3c60 1200945 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:8feb4bf768bfb762074498624d34458386b26942fea9cc61c20292c8591b4565 cni.projectcalico.org/podIP:192.168.36.71/32 cni.projectcalico.org/podIPs:192.168.36.71/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc0049355a7 0xc0049355a8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nq5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nq5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.587: INFO: Pod "webserver-deployment-d9f79cb5-6j6tl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6j6tl webserver-deployment-d9f79cb5- deployment-3448  97309774-e62c-43a6-88b2-b3508efae0de 1200942 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cbbd83c824935794d41375d8e2f669b60be0c9fb3b060e28841bdb7e446dd040 cni.projectcalico.org/podIP:192.168.36.81/32 cni.projectcalico.org/podIPs:192.168.36.81/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc0049357d7 0xc0049357d8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mf6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mf6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-c9psl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-c9psl webserver-deployment-d9f79cb5- deployment-3448  94c089b7-bb9b-4845-845b-81cda3307876 1201008 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc0049359e7 0xc0049359e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nn64b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn64b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-djx2m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-djx2m webserver-deployment-d9f79cb5- deployment-3448  7f030a3a-63ec-42c7-8f2d-317ecd7394fb 1201105 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4dbadaabd167d2375eaeb9a74015c7bdc95e3fb435c04571971563c05728e618 cni.projectcalico.org/podIP:192.168.36.114/32 cni.projectcalico.org/podIPs:192.168.36.114/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004935b67 0xc004935b68}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s45xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s45xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-jcqrk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jcqrk webserver-deployment-d9f79cb5- deployment-3448  7d21c094-6cb9-46c5-9edf-d7ff66639c9f 1200995 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004935d07 0xc004935d08}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhmc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhmc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-jv4k8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jv4k8 webserver-deployment-d9f79cb5- deployment-3448  496282a6-3ae7-4ce0-aef1-cc3ad0987bb4 1201077 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:29e35d112cc96d5b0d882594820d915160704b71cc91d255737c7f39f07cbcf7 cni.projectcalico.org/podIP:192.168.169.131/32 cni.projectcalico.org/podIPs:192.168.169.131/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004935e87 0xc004935e88}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmht5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmht5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-k5gms" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k5gms webserver-deployment-d9f79cb5- deployment-3448  14bd82a4-7312-42a2-a7e7-19263d2eb374 1201104 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d1f6d5e3c8ea5c75dc8ff6b19f8232dde5a3b88e9337e31b1db6b9305c553cfb cni.projectcalico.org/podIP:192.168.169.145/32 cni.projectcalico.org/podIPs:192.168.169.145/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a22027 0xc004a22028}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6knl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6knl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-lm5zn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lm5zn webserver-deployment-d9f79cb5- deployment-3448  eb46e38a-434c-4f43-ad2b-8aacc9979fc6 1201067 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e7c0818668da70a908f201656c89f7b825d21095f2d0010838595a94fe851c44 cni.projectcalico.org/podIP:192.168.169.140/32 cni.projectcalico.org/podIPs:192.168.169.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a221c7 0xc004a221c8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7s7w9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7s7w9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-lrtkl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lrtkl webserver-deployment-d9f79cb5- deployment-3448  be9ffbb5-15d5-4123-9598-af4722f6e4c8 1201034 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:366c7181b5f41e503ecaecb14d479c10e5d453bed83dba435da44a904a0fa6fd cni.projectcalico.org/podIP:192.168.169.130/32 cni.projectcalico.org/podIPs:192.168.169.130/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a224b7 0xc004a224b8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q8m9x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q8m9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-m8jjg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m8jjg webserver-deployment-d9f79cb5- deployment-3448  5b8197ca-7e4b-4236-aa3d-e2c0c2a11aa6 1200996 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a22797 0xc004a22798}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lmkl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmkl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.588: INFO: Pod "webserver-deployment-d9f79cb5-mr4gz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mr4gz webserver-deployment-d9f79cb5- deployment-3448  3169ebc4-55bc-422b-ae9a-1731cd46d849 1201065 0 2023-05-17 08:09:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:934113c9760ae5986265021cdebe3b0edc7d7092fb94589d23342b960d984bb8 cni.projectcalico.org/podIP:192.168.36.111/32 cni.projectcalico.org/podIPs:192.168.36.111/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a22ae7 0xc004a22ae8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9qg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9qg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.589: INFO: Pod "webserver-deployment-d9f79cb5-s74sf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s74sf webserver-deployment-d9f79cb5- deployment-3448  45b51996-7331-4919-89c0-4e80ec2da96f 1200944 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d121aa0587374d5ff159b474156961944960046a647f7439599be0793a764306 cni.projectcalico.org/podIP:192.168.36.85/32 cni.projectcalico.org/podIPs:192.168.36.85/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a230d7 0xc004a230d8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwmzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwmzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:09:32.589: INFO: Pod "webserver-deployment-d9f79cb5-whdb6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-whdb6 webserver-deployment-d9f79cb5- deployment-3448  7865cd15-29e6-443c-a66c-6a8b81daa21b 1200946 0 2023-05-17 08:09:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:818d97b8a684ec6a3838a41f3cb6cc10fc44fe107a759e4dcef442c88248b449 cni.projectcalico.org/podIP:192.168.169.176/32 cni.projectcalico.org/podIPs:192.168.169.176/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 81583337-20f3-4f5e-aaf5-bb5402e524d4 0xc004a232e7 0xc004a232e8}] [] [{kube-controller-manager Update v1 2023-05-17 08:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81583337-20f3-4f5e-aaf5-bb5402e524d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:09:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qzb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qzb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:09:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:,StartTime:2023-05-17 08:09:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:32.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3448" for this suite. 05/17/23 08:09:32.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:32.596
May 17 08:09:32.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:09:32.596
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:32.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:32.605
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 05/17/23 08:09:32.607
STEP: watching for the ServiceAccount to be added 05/17/23 08:09:32.61
STEP: patching the ServiceAccount 05/17/23 08:09:32.611
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/17/23 08:09:32.615
STEP: deleting the ServiceAccount 05/17/23 08:09:32.616
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 17 08:09:32.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2757" for this suite. 05/17/23 08:09:32.622
------------------------------
â€¢ [0.029 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:32.596
    May 17 08:09:32.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:09:32.596
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:32.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:32.605
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 05/17/23 08:09:32.607
    STEP: watching for the ServiceAccount to be added 05/17/23 08:09:32.61
    STEP: patching the ServiceAccount 05/17/23 08:09:32.611
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/17/23 08:09:32.615
    STEP: deleting the ServiceAccount 05/17/23 08:09:32.616
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:32.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2757" for this suite. 05/17/23 08:09:32.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:32.625
May 17 08:09:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 08:09:32.626
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:32.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:32.634
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-3935/configmap-test-7d43602a-14ee-4f77-857e-4bb925db05d4 05/17/23 08:09:32.636
STEP: Creating a pod to test consume configMaps 05/17/23 08:09:32.638
May 17 08:09:32.642: INFO: Waiting up to 5m0s for pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f" in namespace "configmap-3935" to be "Succeeded or Failed"
May 17 08:09:32.643: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.136397ms
May 17 08:09:34.646: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004258292s
May 17 08:09:36.646: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004611729s
May 17 08:09:38.646: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003992488s
May 17 08:09:40.647: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.005362147s
STEP: Saw pod success 05/17/23 08:09:40.647
May 17 08:09:40.647: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f" satisfied condition "Succeeded or Failed"
May 17 08:09:40.649: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f container env-test: <nil>
STEP: delete the pod 05/17/23 08:09:40.658
May 17 08:09:40.664: INFO: Waiting for pod pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f to disappear
May 17 08:09:40.665: INFO: Pod pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 08:09:40.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3935" for this suite. 05/17/23 08:09:40.667
------------------------------
â€¢ [SLOW TEST] [8.045 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:32.625
    May 17 08:09:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 08:09:32.626
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:32.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:32.634
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-3935/configmap-test-7d43602a-14ee-4f77-857e-4bb925db05d4 05/17/23 08:09:32.636
    STEP: Creating a pod to test consume configMaps 05/17/23 08:09:32.638
    May 17 08:09:32.642: INFO: Waiting up to 5m0s for pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f" in namespace "configmap-3935" to be "Succeeded or Failed"
    May 17 08:09:32.643: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.136397ms
    May 17 08:09:34.646: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004258292s
    May 17 08:09:36.646: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004611729s
    May 17 08:09:38.646: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003992488s
    May 17 08:09:40.647: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.005362147s
    STEP: Saw pod success 05/17/23 08:09:40.647
    May 17 08:09:40.647: INFO: Pod "pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f" satisfied condition "Succeeded or Failed"
    May 17 08:09:40.649: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f container env-test: <nil>
    STEP: delete the pod 05/17/23 08:09:40.658
    May 17 08:09:40.664: INFO: Waiting for pod pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f to disappear
    May 17 08:09:40.665: INFO: Pod pod-configmaps-95eaa003-101d-46cb-bc19-004518c7839f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:40.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3935" for this suite. 05/17/23 08:09:40.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:40.671
May 17 08:09:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-runtime 05/17/23 08:09:40.672
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:40.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:40.681
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 05/17/23 08:09:40.682
STEP: wait for the container to reach Succeeded 05/17/23 08:09:40.686
STEP: get the container status 05/17/23 08:09:44.697
STEP: the container should be terminated 05/17/23 08:09:44.699
STEP: the termination message should be set 05/17/23 08:09:44.699
May 17 08:09:44.699: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/17/23 08:09:44.699
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 17 08:09:44.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4164" for this suite. 05/17/23 08:09:44.709
------------------------------
â€¢ [4.041 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:40.671
    May 17 08:09:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-runtime 05/17/23 08:09:40.672
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:40.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:40.681
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 05/17/23 08:09:40.682
    STEP: wait for the container to reach Succeeded 05/17/23 08:09:40.686
    STEP: get the container status 05/17/23 08:09:44.697
    STEP: the container should be terminated 05/17/23 08:09:44.699
    STEP: the termination message should be set 05/17/23 08:09:44.699
    May 17 08:09:44.699: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/17/23 08:09:44.699
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:44.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4164" for this suite. 05/17/23 08:09:44.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:44.713
May 17 08:09:44.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename watch 05/17/23 08:09:44.713
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:44.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:44.721
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/17/23 08:09:44.722
STEP: creating a new configmap 05/17/23 08:09:44.723
STEP: modifying the configmap once 05/17/23 08:09:44.725
STEP: changing the label value of the configmap 05/17/23 08:09:44.729
STEP: Expecting to observe a delete notification for the watched object 05/17/23 08:09:44.733
May 17 08:09:44.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201512 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:09:44.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201513 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:09:44.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201514 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/17/23 08:09:44.733
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/17/23 08:09:44.737
STEP: changing the label value of the configmap back 05/17/23 08:09:54.738
STEP: modifying the configmap a third time 05/17/23 08:09:54.743
STEP: deleting the configmap 05/17/23 08:09:54.747
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/17/23 08:09:54.75
May 17 08:09:54.750: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201549 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:09:54.750: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201550 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:09:54.750: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201551 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 17 08:09:54.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2672" for this suite. 05/17/23 08:09:54.752
------------------------------
â€¢ [SLOW TEST] [10.042 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:44.713
    May 17 08:09:44.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename watch 05/17/23 08:09:44.713
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:44.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:44.721
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/17/23 08:09:44.722
    STEP: creating a new configmap 05/17/23 08:09:44.723
    STEP: modifying the configmap once 05/17/23 08:09:44.725
    STEP: changing the label value of the configmap 05/17/23 08:09:44.729
    STEP: Expecting to observe a delete notification for the watched object 05/17/23 08:09:44.733
    May 17 08:09:44.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201512 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:09:44.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201513 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:09:44.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201514 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/17/23 08:09:44.733
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/17/23 08:09:44.737
    STEP: changing the label value of the configmap back 05/17/23 08:09:54.738
    STEP: modifying the configmap a third time 05/17/23 08:09:54.743
    STEP: deleting the configmap 05/17/23 08:09:54.747
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/17/23 08:09:54.75
    May 17 08:09:54.750: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201549 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:09:54.750: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201550 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:09:54.750: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2672  bfd7b7ed-d77e-485b-b6e2-f152c9b07bde 1201551 0 2023-05-17 08:09:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 08:09:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:54.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2672" for this suite. 05/17/23 08:09:54.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:54.755
May 17 08:09:54.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:09:54.756
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:54.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:54.763
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
May 17 08:09:54.766: INFO: Got root ca configmap in namespace "svcaccounts-3750"
May 17 08:09:54.768: INFO: Deleted root ca configmap in namespace "svcaccounts-3750"
STEP: waiting for a new root ca configmap created 05/17/23 08:09:55.269
May 17 08:09:55.271: INFO: Recreated root ca configmap in namespace "svcaccounts-3750"
May 17 08:09:55.275: INFO: Updated root ca configmap in namespace "svcaccounts-3750"
STEP: waiting for the root ca configmap reconciled 05/17/23 08:09:55.776
May 17 08:09:55.778: INFO: Reconciled root ca configmap in namespace "svcaccounts-3750"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 17 08:09:55.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3750" for this suite. 05/17/23 08:09:55.781
------------------------------
â€¢ [1.029 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:54.755
    May 17 08:09:54.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:09:54.756
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:54.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:54.763
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    May 17 08:09:54.766: INFO: Got root ca configmap in namespace "svcaccounts-3750"
    May 17 08:09:54.768: INFO: Deleted root ca configmap in namespace "svcaccounts-3750"
    STEP: waiting for a new root ca configmap created 05/17/23 08:09:55.269
    May 17 08:09:55.271: INFO: Recreated root ca configmap in namespace "svcaccounts-3750"
    May 17 08:09:55.275: INFO: Updated root ca configmap in namespace "svcaccounts-3750"
    STEP: waiting for the root ca configmap reconciled 05/17/23 08:09:55.776
    May 17 08:09:55.778: INFO: Reconciled root ca configmap in namespace "svcaccounts-3750"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:55.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3750" for this suite. 05/17/23 08:09:55.781
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:55.784
May 17 08:09:55.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:09:55.785
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:55.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:55.793
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:09:55.794
May 17 08:09:55.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065" in namespace "projected-2958" to be "Succeeded or Failed"
May 17 08:09:55.800: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065": Phase="Pending", Reason="", readiness=false. Elapsed: 1.333423ms
May 17 08:09:57.803: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004512293s
May 17 08:09:59.802: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004019724s
STEP: Saw pod success 05/17/23 08:09:59.802
May 17 08:09:59.803: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065" satisfied condition "Succeeded or Failed"
May 17 08:09:59.804: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065 container client-container: <nil>
STEP: delete the pod 05/17/23 08:09:59.807
May 17 08:09:59.813: INFO: Waiting for pod downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065 to disappear
May 17 08:09:59.815: INFO: Pod downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:09:59.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2958" for this suite. 05/17/23 08:09:59.817
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:55.784
    May 17 08:09:55.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:09:55.785
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:55.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:55.793
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:09:55.794
    May 17 08:09:55.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065" in namespace "projected-2958" to be "Succeeded or Failed"
    May 17 08:09:55.800: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065": Phase="Pending", Reason="", readiness=false. Elapsed: 1.333423ms
    May 17 08:09:57.803: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004512293s
    May 17 08:09:59.802: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004019724s
    STEP: Saw pod success 05/17/23 08:09:59.802
    May 17 08:09:59.803: INFO: Pod "downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065" satisfied condition "Succeeded or Failed"
    May 17 08:09:59.804: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:09:59.807
    May 17 08:09:59.813: INFO: Waiting for pod downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065 to disappear
    May 17 08:09:59.815: INFO: Pod downwardapi-volume-f006aab7-ffe2-463a-81a7-0022957ec065 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:09:59.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2958" for this suite. 05/17/23 08:09:59.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:09:59.82
May 17 08:09:59.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 08:09:59.821
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:59.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:59.829
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/17/23 08:09:59.831
STEP: Creating RC which spawns configmap-volume pods 05/17/23 08:10:00.076
May 17 08:10:00.175: INFO: Pod name wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/17/23 08:10:00.175
May 17 08:10:00.175: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:00.224: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.604586ms
May 17 08:10:02.228: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052258711s
May 17 08:10:04.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053255952s
May 17 08:10:06.227: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051684075s
May 17 08:10:08.228: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052342813s
May 17 08:10:10.227: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051526274s
May 17 08:10:12.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053148514s
May 17 08:10:14.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Running", Reason="", readiness=true. Elapsed: 14.053596292s
May 17 08:10:14.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9" satisfied condition "running"
May 17 08:10:14.229: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-hwvfq" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:14.231: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-hwvfq": Phase="Running", Reason="", readiness=true. Elapsed: 1.642628ms
May 17 08:10:14.231: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-hwvfq" satisfied condition "running"
May 17 08:10:14.231: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rfv65" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:14.232: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rfv65": Phase="Running", Reason="", readiness=true. Elapsed: 1.67711ms
May 17 08:10:14.232: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rfv65" satisfied condition "running"
May 17 08:10:14.232: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rp4dc" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:14.234: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rp4dc": Phase="Running", Reason="", readiness=true. Elapsed: 1.694187ms
May 17 08:10:14.234: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rp4dc" satisfied condition "running"
May 17 08:10:14.234: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-wpbbh" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:14.236: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-wpbbh": Phase="Running", Reason="", readiness=true. Elapsed: 1.63541ms
May 17 08:10:14.236: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-wpbbh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43 in namespace emptydir-wrapper-9130, will wait for the garbage collector to delete the pods 05/17/23 08:10:14.236
May 17 08:10:14.293: INFO: Deleting ReplicationController wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43 took: 4.759376ms
May 17 08:10:14.394: INFO: Terminating ReplicationController wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43 pods took: 100.883759ms
STEP: Creating RC which spawns configmap-volume pods 05/17/23 08:10:18.597
May 17 08:10:18.605: INFO: Pod name wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf: Found 0 pods out of 5
May 17 08:10:23.611: INFO: Pod name wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/17/23 08:10:23.611
May 17 08:10:23.611: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:23.613: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738938ms
May 17 08:10:25.617: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074596s
May 17 08:10:27.616: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005186381s
May 17 08:10:29.617: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006016759s
May 17 08:10:31.618: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006667736s
May 17 08:10:33.616: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Running", Reason="", readiness=true. Elapsed: 10.005353575s
May 17 08:10:33.616: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l" satisfied condition "running"
May 17 08:10:33.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-dtttd" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:33.619: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-dtttd": Phase="Running", Reason="", readiness=true. Elapsed: 2.148132ms
May 17 08:10:33.619: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-dtttd" satisfied condition "running"
May 17 08:10:33.619: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:33.621: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.99789ms
May 17 08:10:35.625: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt": Phase="Running", Reason="", readiness=true. Elapsed: 2.006634425s
May 17 08:10:35.625: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt" satisfied condition "running"
May 17 08:10:35.625: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kb96h" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:35.627: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kb96h": Phase="Running", Reason="", readiness=true. Elapsed: 1.829304ms
May 17 08:10:35.627: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kb96h" satisfied condition "running"
May 17 08:10:35.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kgbgz" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:35.629: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kgbgz": Phase="Running", Reason="", readiness=true. Elapsed: 1.730643ms
May 17 08:10:35.629: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kgbgz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf in namespace emptydir-wrapper-9130, will wait for the garbage collector to delete the pods 05/17/23 08:10:35.629
May 17 08:10:35.686: INFO: Deleting ReplicationController wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf took: 4.757392ms
May 17 08:10:35.787: INFO: Terminating ReplicationController wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf pods took: 100.891384ms
STEP: Creating RC which spawns configmap-volume pods 05/17/23 08:10:38.391
May 17 08:10:38.399: INFO: Pod name wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8: Found 0 pods out of 5
May 17 08:10:43.405: INFO: Pod name wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/17/23 08:10:43.405
May 17 08:10:43.405: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:43.407: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.831381ms
May 17 08:10:45.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00561645s
May 17 08:10:47.410: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005334538s
May 17 08:10:49.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006032465s
May 17 08:10:51.412: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006866323s
May 17 08:10:53.410: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004777472s
May 17 08:10:55.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Running", Reason="", readiness=true. Elapsed: 12.006167233s
May 17 08:10:55.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4" satisfied condition "running"
May 17 08:10:55.411: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-h5j5v" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:55.413: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-h5j5v": Phase="Running", Reason="", readiness=true. Elapsed: 1.832133ms
May 17 08:10:55.413: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-h5j5v" satisfied condition "running"
May 17 08:10:55.413: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nds9k" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:55.415: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nds9k": Phase="Running", Reason="", readiness=true. Elapsed: 1.680303ms
May 17 08:10:55.415: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nds9k" satisfied condition "running"
May 17 08:10:55.415: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nfnl8" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:55.416: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nfnl8": Phase="Running", Reason="", readiness=true. Elapsed: 1.454936ms
May 17 08:10:55.416: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nfnl8" satisfied condition "running"
May 17 08:10:55.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-pdnkq" in namespace "emptydir-wrapper-9130" to be "running"
May 17 08:10:55.418: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-pdnkq": Phase="Running", Reason="", readiness=true. Elapsed: 1.490899ms
May 17 08:10:55.418: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-pdnkq" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8 in namespace emptydir-wrapper-9130, will wait for the garbage collector to delete the pods 05/17/23 08:10:55.418
May 17 08:10:55.475: INFO: Deleting ReplicationController wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8 took: 4.492319ms
May 17 08:10:55.576: INFO: Terminating ReplicationController wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8 pods took: 101.049884ms
STEP: Cleaning up the configMaps 05/17/23 08:10:59.776
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:10:59.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9130" for this suite. 05/17/23 08:10:59.9
------------------------------
â€¢ [SLOW TEST] [60.082 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:09:59.82
    May 17 08:09:59.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 08:09:59.821
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:09:59.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:09:59.829
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/17/23 08:09:59.831
    STEP: Creating RC which spawns configmap-volume pods 05/17/23 08:10:00.076
    May 17 08:10:00.175: INFO: Pod name wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/17/23 08:10:00.175
    May 17 08:10:00.175: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:00.224: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.604586ms
    May 17 08:10:02.228: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052258711s
    May 17 08:10:04.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053255952s
    May 17 08:10:06.227: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051684075s
    May 17 08:10:08.228: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052342813s
    May 17 08:10:10.227: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051526274s
    May 17 08:10:12.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053148514s
    May 17 08:10:14.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9": Phase="Running", Reason="", readiness=true. Elapsed: 14.053596292s
    May 17 08:10:14.229: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-5cvx9" satisfied condition "running"
    May 17 08:10:14.229: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-hwvfq" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:14.231: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-hwvfq": Phase="Running", Reason="", readiness=true. Elapsed: 1.642628ms
    May 17 08:10:14.231: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-hwvfq" satisfied condition "running"
    May 17 08:10:14.231: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rfv65" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:14.232: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rfv65": Phase="Running", Reason="", readiness=true. Elapsed: 1.67711ms
    May 17 08:10:14.232: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rfv65" satisfied condition "running"
    May 17 08:10:14.232: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rp4dc" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:14.234: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rp4dc": Phase="Running", Reason="", readiness=true. Elapsed: 1.694187ms
    May 17 08:10:14.234: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-rp4dc" satisfied condition "running"
    May 17 08:10:14.234: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-wpbbh" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:14.236: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-wpbbh": Phase="Running", Reason="", readiness=true. Elapsed: 1.63541ms
    May 17 08:10:14.236: INFO: Pod "wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43-wpbbh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43 in namespace emptydir-wrapper-9130, will wait for the garbage collector to delete the pods 05/17/23 08:10:14.236
    May 17 08:10:14.293: INFO: Deleting ReplicationController wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43 took: 4.759376ms
    May 17 08:10:14.394: INFO: Terminating ReplicationController wrapped-volume-race-791d7ed6-abac-49f1-a79e-02b092d16c43 pods took: 100.883759ms
    STEP: Creating RC which spawns configmap-volume pods 05/17/23 08:10:18.597
    May 17 08:10:18.605: INFO: Pod name wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf: Found 0 pods out of 5
    May 17 08:10:23.611: INFO: Pod name wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/17/23 08:10:23.611
    May 17 08:10:23.611: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:23.613: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738938ms
    May 17 08:10:25.617: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074596s
    May 17 08:10:27.616: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005186381s
    May 17 08:10:29.617: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006016759s
    May 17 08:10:31.618: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006667736s
    May 17 08:10:33.616: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l": Phase="Running", Reason="", readiness=true. Elapsed: 10.005353575s
    May 17 08:10:33.616: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-47t8l" satisfied condition "running"
    May 17 08:10:33.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-dtttd" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:33.619: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-dtttd": Phase="Running", Reason="", readiness=true. Elapsed: 2.148132ms
    May 17 08:10:33.619: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-dtttd" satisfied condition "running"
    May 17 08:10:33.619: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:33.621: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.99789ms
    May 17 08:10:35.625: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt": Phase="Running", Reason="", readiness=true. Elapsed: 2.006634425s
    May 17 08:10:35.625: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-gt8nt" satisfied condition "running"
    May 17 08:10:35.625: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kb96h" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:35.627: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kb96h": Phase="Running", Reason="", readiness=true. Elapsed: 1.829304ms
    May 17 08:10:35.627: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kb96h" satisfied condition "running"
    May 17 08:10:35.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kgbgz" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:35.629: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kgbgz": Phase="Running", Reason="", readiness=true. Elapsed: 1.730643ms
    May 17 08:10:35.629: INFO: Pod "wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf-kgbgz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf in namespace emptydir-wrapper-9130, will wait for the garbage collector to delete the pods 05/17/23 08:10:35.629
    May 17 08:10:35.686: INFO: Deleting ReplicationController wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf took: 4.757392ms
    May 17 08:10:35.787: INFO: Terminating ReplicationController wrapped-volume-race-267cd318-590c-46ff-bccf-8230e496d7bf pods took: 100.891384ms
    STEP: Creating RC which spawns configmap-volume pods 05/17/23 08:10:38.391
    May 17 08:10:38.399: INFO: Pod name wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8: Found 0 pods out of 5
    May 17 08:10:43.405: INFO: Pod name wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/17/23 08:10:43.405
    May 17 08:10:43.405: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:43.407: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.831381ms
    May 17 08:10:45.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00561645s
    May 17 08:10:47.410: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005334538s
    May 17 08:10:49.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006032465s
    May 17 08:10:51.412: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006866323s
    May 17 08:10:53.410: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004777472s
    May 17 08:10:55.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4": Phase="Running", Reason="", readiness=true. Elapsed: 12.006167233s
    May 17 08:10:55.411: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-2h9c4" satisfied condition "running"
    May 17 08:10:55.411: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-h5j5v" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:55.413: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-h5j5v": Phase="Running", Reason="", readiness=true. Elapsed: 1.832133ms
    May 17 08:10:55.413: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-h5j5v" satisfied condition "running"
    May 17 08:10:55.413: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nds9k" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:55.415: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nds9k": Phase="Running", Reason="", readiness=true. Elapsed: 1.680303ms
    May 17 08:10:55.415: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nds9k" satisfied condition "running"
    May 17 08:10:55.415: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nfnl8" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:55.416: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nfnl8": Phase="Running", Reason="", readiness=true. Elapsed: 1.454936ms
    May 17 08:10:55.416: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-nfnl8" satisfied condition "running"
    May 17 08:10:55.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-pdnkq" in namespace "emptydir-wrapper-9130" to be "running"
    May 17 08:10:55.418: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-pdnkq": Phase="Running", Reason="", readiness=true. Elapsed: 1.490899ms
    May 17 08:10:55.418: INFO: Pod "wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8-pdnkq" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8 in namespace emptydir-wrapper-9130, will wait for the garbage collector to delete the pods 05/17/23 08:10:55.418
    May 17 08:10:55.475: INFO: Deleting ReplicationController wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8 took: 4.492319ms
    May 17 08:10:55.576: INFO: Terminating ReplicationController wrapped-volume-race-c7a24748-108e-4b1c-b741-c12679cafbf8 pods took: 101.049884ms
    STEP: Cleaning up the configMaps 05/17/23 08:10:59.776
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:10:59.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9130" for this suite. 05/17/23 08:10:59.9
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:10:59.903
May 17 08:10:59.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 08:10:59.903
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:10:59.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:10:59.912
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/17/23 08:10:59.916
May 17 08:10:59.921: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2939" to be "running and ready"
May 17 08:10:59.922: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.364295ms
May 17 08:10:59.923: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 08:11:01.925: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004191627s
May 17 08:11:01.925: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 08:11:01.925: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 05/17/23 08:11:01.927
May 17 08:11:01.930: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2939" to be "running and ready"
May 17 08:11:01.932: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347728ms
May 17 08:11:01.932: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 08:11:03.935: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00447231s
May 17 08:11:03.935: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May 17 08:11:03.935: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/17/23 08:11:03.937
STEP: delete the pod with lifecycle hook 05/17/23 08:11:03.941
May 17 08:11:03.944: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 08:11:03.946: INFO: Pod pod-with-poststart-http-hook still exists
May 17 08:11:05.946: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 08:11:05.948: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 17 08:11:05.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2939" for this suite. 05/17/23 08:11:05.95
------------------------------
â€¢ [SLOW TEST] [6.051 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:10:59.903
    May 17 08:10:59.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 08:10:59.903
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:10:59.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:10:59.912
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 08:10:59.916
    May 17 08:10:59.921: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2939" to be "running and ready"
    May 17 08:10:59.922: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.364295ms
    May 17 08:10:59.923: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:11:01.925: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004191627s
    May 17 08:11:01.925: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 08:11:01.925: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 05/17/23 08:11:01.927
    May 17 08:11:01.930: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2939" to be "running and ready"
    May 17 08:11:01.932: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347728ms
    May 17 08:11:01.932: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:11:03.935: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00447231s
    May 17 08:11:03.935: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May 17 08:11:03.935: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/17/23 08:11:03.937
    STEP: delete the pod with lifecycle hook 05/17/23 08:11:03.941
    May 17 08:11:03.944: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May 17 08:11:03.946: INFO: Pod pod-with-poststart-http-hook still exists
    May 17 08:11:05.946: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May 17 08:11:05.948: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 17 08:11:05.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2939" for this suite. 05/17/23 08:11:05.95
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:11:05.954
May 17 08:11:05.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:11:05.954
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:11:05.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:11:05.963
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:11:05.965
May 17 08:11:05.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1" in namespace "downward-api-8647" to be "Succeeded or Failed"
May 17 08:11:05.970: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.332976ms
May 17 08:11:07.974: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004768343s
May 17 08:11:09.973: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004008461s
STEP: Saw pod success 05/17/23 08:11:09.973
May 17 08:11:09.973: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1" satisfied condition "Succeeded or Failed"
May 17 08:11:09.975: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1 container client-container: <nil>
STEP: delete the pod 05/17/23 08:11:09.978
May 17 08:11:09.984: INFO: Waiting for pod downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1 to disappear
May 17 08:11:09.985: INFO: Pod downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:11:09.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8647" for this suite. 05/17/23 08:11:09.987
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:11:05.954
    May 17 08:11:05.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:11:05.954
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:11:05.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:11:05.963
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:11:05.965
    May 17 08:11:05.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1" in namespace "downward-api-8647" to be "Succeeded or Failed"
    May 17 08:11:05.970: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.332976ms
    May 17 08:11:07.974: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004768343s
    May 17 08:11:09.973: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004008461s
    STEP: Saw pod success 05/17/23 08:11:09.973
    May 17 08:11:09.973: INFO: Pod "downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1" satisfied condition "Succeeded or Failed"
    May 17 08:11:09.975: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:11:09.978
    May 17 08:11:09.984: INFO: Waiting for pod downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1 to disappear
    May 17 08:11:09.985: INFO: Pod downwardapi-volume-754ca7e2-8fe3-4fb6-bc9f-e26f625f63a1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:11:09.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8647" for this suite. 05/17/23 08:11:09.987
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:11:09.989
May 17 08:11:09.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 08:11:09.991
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:11:09.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:11:09.998
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 in namespace container-probe-1849 05/17/23 08:11:10
May 17 08:11:10.005: INFO: Waiting up to 5m0s for pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346" in namespace "container-probe-1849" to be "not pending"
May 17 08:11:10.006: INFO: Pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455047ms
May 17 08:11:12.008: INFO: Pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346": Phase="Running", Reason="", readiness=true. Elapsed: 2.003712881s
May 17 08:11:12.008: INFO: Pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346" satisfied condition "not pending"
May 17 08:11:12.008: INFO: Started pod busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 in namespace container-probe-1849
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:11:12.008
May 17 08:11:12.010: INFO: Initial restart count of pod busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 is 0
May 17 08:12:02.084: INFO: Restart count of pod container-probe-1849/busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 is now 1 (50.074232136s elapsed)
STEP: deleting the pod 05/17/23 08:12:02.084
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 08:12:02.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1849" for this suite. 05/17/23 08:12:02.093
------------------------------
â€¢ [SLOW TEST] [52.106 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:11:09.989
    May 17 08:11:09.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 08:11:09.991
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:11:09.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:11:09.998
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 in namespace container-probe-1849 05/17/23 08:11:10
    May 17 08:11:10.005: INFO: Waiting up to 5m0s for pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346" in namespace "container-probe-1849" to be "not pending"
    May 17 08:11:10.006: INFO: Pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455047ms
    May 17 08:11:12.008: INFO: Pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346": Phase="Running", Reason="", readiness=true. Elapsed: 2.003712881s
    May 17 08:11:12.008: INFO: Pod "busybox-ef65a067-0ce7-4641-9de9-b4df6a586346" satisfied condition "not pending"
    May 17 08:11:12.008: INFO: Started pod busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 in namespace container-probe-1849
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:11:12.008
    May 17 08:11:12.010: INFO: Initial restart count of pod busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 is 0
    May 17 08:12:02.084: INFO: Restart count of pod container-probe-1849/busybox-ef65a067-0ce7-4641-9de9-b4df6a586346 is now 1 (50.074232136s elapsed)
    STEP: deleting the pod 05/17/23 08:12:02.084
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:02.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1849" for this suite. 05/17/23 08:12:02.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:02.097
May 17 08:12:02.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename init-container 05/17/23 08:12:02.098
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:02.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:02.105
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 05/17/23 08:12:02.106
May 17 08:12:02.106: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 17 08:12:06.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2420" for this suite. 05/17/23 08:12:06.05
------------------------------
â€¢ [3.957 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:02.097
    May 17 08:12:02.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename init-container 05/17/23 08:12:02.098
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:02.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:02.105
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 05/17/23 08:12:02.106
    May 17 08:12:02.106: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:06.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2420" for this suite. 05/17/23 08:12:06.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:06.054
May 17 08:12:06.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename endpointslice 05/17/23 08:12:06.055
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:06.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:06.063
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 05/17/23 08:12:06.064
STEP: getting /apis/discovery.k8s.io 05/17/23 08:12:06.066
STEP: getting /apis/discovery.k8s.iov1 05/17/23 08:12:06.067
STEP: creating 05/17/23 08:12:06.067
STEP: getting 05/17/23 08:12:06.074
STEP: listing 05/17/23 08:12:06.076
STEP: watching 05/17/23 08:12:06.077
May 17 08:12:06.077: INFO: starting watch
STEP: cluster-wide listing 05/17/23 08:12:06.078
STEP: cluster-wide watching 05/17/23 08:12:06.079
May 17 08:12:06.079: INFO: starting watch
STEP: patching 05/17/23 08:12:06.08
STEP: updating 05/17/23 08:12:06.082
May 17 08:12:06.086: INFO: waiting for watch events with expected annotations
May 17 08:12:06.086: INFO: saw patched and updated annotations
STEP: deleting 05/17/23 08:12:06.086
STEP: deleting a collection 05/17/23 08:12:06.091
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 17 08:12:06.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8145" for this suite. 05/17/23 08:12:06.101
------------------------------
â€¢ [0.050 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:06.054
    May 17 08:12:06.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename endpointslice 05/17/23 08:12:06.055
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:06.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:06.063
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 05/17/23 08:12:06.064
    STEP: getting /apis/discovery.k8s.io 05/17/23 08:12:06.066
    STEP: getting /apis/discovery.k8s.iov1 05/17/23 08:12:06.067
    STEP: creating 05/17/23 08:12:06.067
    STEP: getting 05/17/23 08:12:06.074
    STEP: listing 05/17/23 08:12:06.076
    STEP: watching 05/17/23 08:12:06.077
    May 17 08:12:06.077: INFO: starting watch
    STEP: cluster-wide listing 05/17/23 08:12:06.078
    STEP: cluster-wide watching 05/17/23 08:12:06.079
    May 17 08:12:06.079: INFO: starting watch
    STEP: patching 05/17/23 08:12:06.08
    STEP: updating 05/17/23 08:12:06.082
    May 17 08:12:06.086: INFO: waiting for watch events with expected annotations
    May 17 08:12:06.086: INFO: saw patched and updated annotations
    STEP: deleting 05/17/23 08:12:06.086
    STEP: deleting a collection 05/17/23 08:12:06.091
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:06.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8145" for this suite. 05/17/23 08:12:06.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:06.104
May 17 08:12:06.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:12:06.105
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:06.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:06.112
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:12:06.12
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:12:06.445
STEP: Deploying the webhook pod 05/17/23 08:12:06.45
STEP: Wait for the deployment to be ready 05/17/23 08:12:06.455
May 17 08:12:06.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:12:08.465
STEP: Verifying the service has paired with the endpoint 05/17/23 08:12:08.472
May 17 08:12:09.472: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 05/17/23 08:12:09.504
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:12:09.529
STEP: Deleting the collection of validation webhooks 05/17/23 08:12:09.546
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:12:09.566
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:12:09.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5654" for this suite. 05/17/23 08:12:09.589
STEP: Destroying namespace "webhook-5654-markers" for this suite. 05/17/23 08:12:09.593
------------------------------
â€¢ [3.493 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:06.104
    May 17 08:12:06.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:12:06.105
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:06.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:06.112
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:12:06.12
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:12:06.445
    STEP: Deploying the webhook pod 05/17/23 08:12:06.45
    STEP: Wait for the deployment to be ready 05/17/23 08:12:06.455
    May 17 08:12:06.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:12:08.465
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:12:08.472
    May 17 08:12:09.472: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 05/17/23 08:12:09.504
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:12:09.529
    STEP: Deleting the collection of validation webhooks 05/17/23 08:12:09.546
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:12:09.566
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:09.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5654" for this suite. 05/17/23 08:12:09.589
    STEP: Destroying namespace "webhook-5654-markers" for this suite. 05/17/23 08:12:09.593
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:09.598
May 17 08:12:09.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename security-context 05/17/23 08:12:09.598
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:09.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:09.606
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 08:12:09.607
May 17 08:12:09.611: INFO: Waiting up to 5m0s for pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437" in namespace "security-context-8811" to be "Succeeded or Failed"
May 17 08:12:09.612: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437": Phase="Pending", Reason="", readiness=false. Elapsed: 1.201025ms
May 17 08:12:11.615: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004328605s
May 17 08:12:13.615: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004096938s
STEP: Saw pod success 05/17/23 08:12:13.615
May 17 08:12:13.615: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437" satisfied condition "Succeeded or Failed"
May 17 08:12:13.617: INFO: Trying to get logs from node k8s-node1 pod security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437 container test-container: <nil>
STEP: delete the pod 05/17/23 08:12:13.62
May 17 08:12:13.627: INFO: Waiting for pod security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437 to disappear
May 17 08:12:13.628: INFO: Pod security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 17 08:12:13.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-8811" for this suite. 05/17/23 08:12:13.63
------------------------------
â€¢ [4.035 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:09.598
    May 17 08:12:09.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename security-context 05/17/23 08:12:09.598
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:09.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:09.606
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 08:12:09.607
    May 17 08:12:09.611: INFO: Waiting up to 5m0s for pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437" in namespace "security-context-8811" to be "Succeeded or Failed"
    May 17 08:12:09.612: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437": Phase="Pending", Reason="", readiness=false. Elapsed: 1.201025ms
    May 17 08:12:11.615: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004328605s
    May 17 08:12:13.615: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004096938s
    STEP: Saw pod success 05/17/23 08:12:13.615
    May 17 08:12:13.615: INFO: Pod "security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437" satisfied condition "Succeeded or Failed"
    May 17 08:12:13.617: INFO: Trying to get logs from node k8s-node1 pod security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:12:13.62
    May 17 08:12:13.627: INFO: Waiting for pod security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437 to disappear
    May 17 08:12:13.628: INFO: Pod security-context-abb7756d-c47e-48b2-bee2-ea67a6d99437 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:13.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-8811" for this suite. 05/17/23 08:12:13.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:13.633
May 17 08:12:13.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replication-controller 05/17/23 08:12:13.634
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:13.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:13.645
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f 05/17/23 08:12:13.646
May 17 08:12:13.650: INFO: Pod name my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f: Found 0 pods out of 1
May 17 08:12:18.654: INFO: Pod name my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f: Found 1 pods out of 1
May 17 08:12:18.654: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f" are running
May 17 08:12:18.654: INFO: Waiting up to 5m0s for pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4" in namespace "replication-controller-8873" to be "running"
May 17 08:12:18.655: INFO: Pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4": Phase="Running", Reason="", readiness=true. Elapsed: 1.438535ms
May 17 08:12:18.655: INFO: Pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4" satisfied condition "running"
May 17 08:12:18.655: INFO: Pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:13 +0000 UTC Reason: Message:}])
May 17 08:12:18.655: INFO: Trying to dial the pod
May 17 08:12:23.661: INFO: Controller my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f: Got expected result from replica 1 [my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4]: "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 17 08:12:23.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8873" for this suite. 05/17/23 08:12:23.663
------------------------------
â€¢ [SLOW TEST] [10.033 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:13.633
    May 17 08:12:13.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replication-controller 05/17/23 08:12:13.634
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:13.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:13.645
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f 05/17/23 08:12:13.646
    May 17 08:12:13.650: INFO: Pod name my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f: Found 0 pods out of 1
    May 17 08:12:18.654: INFO: Pod name my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f: Found 1 pods out of 1
    May 17 08:12:18.654: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f" are running
    May 17 08:12:18.654: INFO: Waiting up to 5m0s for pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4" in namespace "replication-controller-8873" to be "running"
    May 17 08:12:18.655: INFO: Pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4": Phase="Running", Reason="", readiness=true. Elapsed: 1.438535ms
    May 17 08:12:18.655: INFO: Pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4" satisfied condition "running"
    May 17 08:12:18.655: INFO: Pod "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:12:13 +0000 UTC Reason: Message:}])
    May 17 08:12:18.655: INFO: Trying to dial the pod
    May 17 08:12:23.661: INFO: Controller my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f: Got expected result from replica 1 [my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4]: "my-hostname-basic-bf9820e3-de35-4c0b-9abe-6400fef6fe3f-l6rn4", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:23.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8873" for this suite. 05/17/23 08:12:23.663
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:23.667
May 17 08:12:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pod-network-test 05/17/23 08:12:23.667
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:23.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:23.675
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9939 05/17/23 08:12:23.676
STEP: creating a selector 05/17/23 08:12:23.677
STEP: Creating the service pods in kubernetes 05/17/23 08:12:23.677
May 17 08:12:23.677: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 08:12:23.688: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9939" to be "running and ready"
May 17 08:12:23.689: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26524ms
May 17 08:12:23.689: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:12:25.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.0048545s
May 17 08:12:25.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:12:27.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004059339s
May 17 08:12:27.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:12:29.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004793297s
May 17 08:12:29.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:12:31.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004464551s
May 17 08:12:31.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:12:33.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.003753135s
May 17 08:12:33.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:12:35.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004827887s
May 17 08:12:35.692: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 08:12:35.692: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 08:12:35.694: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9939" to be "running and ready"
May 17 08:12:35.695: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.435265ms
May 17 08:12:35.695: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 08:12:35.695: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 08:12:35.697
May 17 08:12:35.703: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9939" to be "running"
May 17 08:12:35.704: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.195517ms
May 17 08:12:37.706: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003412064s
May 17 08:12:37.706: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 08:12:37.708: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9939" to be "running"
May 17 08:12:37.709: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.274542ms
May 17 08:12:37.709: INFO: Pod "host-test-container-pod" satisfied condition "running"
May 17 08:12:37.710: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 17 08:12:37.710: INFO: Going to poll 192.168.36.123 on port 8083 at least 0 times, with a maximum of 34 tries before failing
May 17 08:12:37.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.36.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:12:37.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:12:37.712: INFO: ExecWithOptions: Clientset creation
May 17 08:12:37.712: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.36.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 08:12:37.767: INFO: Found all 1 expected endpoints: [netserver-0]
May 17 08:12:37.767: INFO: Going to poll 192.168.169.142 on port 8083 at least 0 times, with a maximum of 34 tries before failing
May 17 08:12:37.769: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.169.142:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:12:37.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:12:37.769: INFO: ExecWithOptions: Clientset creation
May 17 08:12:37.769: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.169.142%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 08:12:37.811: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 17 08:12:37.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9939" for this suite. 05/17/23 08:12:37.813
------------------------------
â€¢ [SLOW TEST] [14.149 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:23.667
    May 17 08:12:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 08:12:23.667
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:23.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:23.675
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9939 05/17/23 08:12:23.676
    STEP: creating a selector 05/17/23 08:12:23.677
    STEP: Creating the service pods in kubernetes 05/17/23 08:12:23.677
    May 17 08:12:23.677: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 08:12:23.688: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9939" to be "running and ready"
    May 17 08:12:23.689: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26524ms
    May 17 08:12:23.689: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:12:25.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.0048545s
    May 17 08:12:25.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:12:27.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004059339s
    May 17 08:12:27.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:12:29.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004793297s
    May 17 08:12:29.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:12:31.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004464551s
    May 17 08:12:31.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:12:33.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.003753135s
    May 17 08:12:33.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:12:35.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004827887s
    May 17 08:12:35.692: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 08:12:35.692: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 08:12:35.694: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9939" to be "running and ready"
    May 17 08:12:35.695: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.435265ms
    May 17 08:12:35.695: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 08:12:35.695: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 08:12:35.697
    May 17 08:12:35.703: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9939" to be "running"
    May 17 08:12:35.704: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.195517ms
    May 17 08:12:37.706: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003412064s
    May 17 08:12:37.706: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 08:12:37.708: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9939" to be "running"
    May 17 08:12:37.709: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.274542ms
    May 17 08:12:37.709: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May 17 08:12:37.710: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    May 17 08:12:37.710: INFO: Going to poll 192.168.36.123 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    May 17 08:12:37.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.36.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:12:37.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:12:37.712: INFO: ExecWithOptions: Clientset creation
    May 17 08:12:37.712: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.36.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 08:12:37.767: INFO: Found all 1 expected endpoints: [netserver-0]
    May 17 08:12:37.767: INFO: Going to poll 192.168.169.142 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    May 17 08:12:37.769: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.169.142:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:12:37.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:12:37.769: INFO: ExecWithOptions: Clientset creation
    May 17 08:12:37.769: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.169.142%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 08:12:37.811: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:37.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9939" for this suite. 05/17/23 08:12:37.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:37.821
May 17 08:12:37.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:12:37.822
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:37.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:37.83
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 05/17/23 08:12:37.832
May 17 08:12:37.837: INFO: Waiting up to 5m0s for pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae" in namespace "downward-api-3924" to be "Succeeded or Failed"
May 17 08:12:37.838: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.236086ms
May 17 08:12:39.841: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004442978s
May 17 08:12:41.841: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004456827s
STEP: Saw pod success 05/17/23 08:12:41.841
May 17 08:12:41.841: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae" satisfied condition "Succeeded or Failed"
May 17 08:12:41.843: INFO: Trying to get logs from node k8s-node1 pod downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae container dapi-container: <nil>
STEP: delete the pod 05/17/23 08:12:41.847
May 17 08:12:41.852: INFO: Waiting for pod downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae to disappear
May 17 08:12:41.853: INFO: Pod downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 17 08:12:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3924" for this suite. 05/17/23 08:12:41.855
------------------------------
â€¢ [4.036 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:37.821
    May 17 08:12:37.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:12:37.822
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:37.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:37.83
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 05/17/23 08:12:37.832
    May 17 08:12:37.837: INFO: Waiting up to 5m0s for pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae" in namespace "downward-api-3924" to be "Succeeded or Failed"
    May 17 08:12:37.838: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.236086ms
    May 17 08:12:39.841: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004442978s
    May 17 08:12:41.841: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004456827s
    STEP: Saw pod success 05/17/23 08:12:41.841
    May 17 08:12:41.841: INFO: Pod "downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae" satisfied condition "Succeeded or Failed"
    May 17 08:12:41.843: INFO: Trying to get logs from node k8s-node1 pod downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae container dapi-container: <nil>
    STEP: delete the pod 05/17/23 08:12:41.847
    May 17 08:12:41.852: INFO: Waiting for pod downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae to disappear
    May 17 08:12:41.853: INFO: Pod downward-api-e20ecf34-ed1e-4492-8c27-312255ea41ae no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3924" for this suite. 05/17/23 08:12:41.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:41.857
May 17 08:12:41.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 08:12:41.858
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:41.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:41.865
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2342 05/17/23 08:12:41.866
STEP: changing the ExternalName service to type=NodePort 05/17/23 08:12:41.868
STEP: creating replication controller externalname-service in namespace services-2342 05/17/23 08:12:41.877
I0517 08:12:41.879943      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2342, replica count: 2
I0517 08:12:44.931695      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 08:12:44.931: INFO: Creating new exec pod
May 17 08:12:44.935: INFO: Waiting up to 5m0s for pod "execpod6tr2x" in namespace "services-2342" to be "running"
May 17 08:12:44.937: INFO: Pod "execpod6tr2x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.297756ms
May 17 08:12:46.939: INFO: Pod "execpod6tr2x": Phase="Running", Reason="", readiness=true. Elapsed: 2.004001901s
May 17 08:12:46.939: INFO: Pod "execpod6tr2x" satisfied condition "running"
May 17 08:12:47.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May 17 08:12:48.035: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 17 08:12:48.035: INFO: stdout: ""
May 17 08:12:48.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 10.97.123.143 80'
May 17 08:12:48.135: INFO: stderr: "+ nc -v -z -w 2 10.97.123.143 80\nConnection to 10.97.123.143 80 port [tcp/http] succeeded!\n"
May 17 08:12:48.136: INFO: stdout: ""
May 17 08:12:48.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 30574'
May 17 08:12:48.226: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 30574\nConnection to 10.0.79.210 30574 port [tcp/*] succeeded!\n"
May 17 08:12:48.226: INFO: stdout: ""
May 17 08:12:48.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 30574'
May 17 08:12:48.314: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 30574\nConnection to 10.0.79.211 30574 port [tcp/*] succeeded!\n"
May 17 08:12:48.314: INFO: stdout: ""
May 17 08:12:48.314: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 08:12:48.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2342" for this suite. 05/17/23 08:12:48.326
------------------------------
â€¢ [SLOW TEST] [6.471 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:41.857
    May 17 08:12:41.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 08:12:41.858
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:41.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:41.865
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2342 05/17/23 08:12:41.866
    STEP: changing the ExternalName service to type=NodePort 05/17/23 08:12:41.868
    STEP: creating replication controller externalname-service in namespace services-2342 05/17/23 08:12:41.877
    I0517 08:12:41.879943      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2342, replica count: 2
    I0517 08:12:44.931695      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 08:12:44.931: INFO: Creating new exec pod
    May 17 08:12:44.935: INFO: Waiting up to 5m0s for pod "execpod6tr2x" in namespace "services-2342" to be "running"
    May 17 08:12:44.937: INFO: Pod "execpod6tr2x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.297756ms
    May 17 08:12:46.939: INFO: Pod "execpod6tr2x": Phase="Running", Reason="", readiness=true. Elapsed: 2.004001901s
    May 17 08:12:46.939: INFO: Pod "execpod6tr2x" satisfied condition "running"
    May 17 08:12:47.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May 17 08:12:48.035: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May 17 08:12:48.035: INFO: stdout: ""
    May 17 08:12:48.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 10.97.123.143 80'
    May 17 08:12:48.135: INFO: stderr: "+ nc -v -z -w 2 10.97.123.143 80\nConnection to 10.97.123.143 80 port [tcp/http] succeeded!\n"
    May 17 08:12:48.136: INFO: stdout: ""
    May 17 08:12:48.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 30574'
    May 17 08:12:48.226: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 30574\nConnection to 10.0.79.210 30574 port [tcp/*] succeeded!\n"
    May 17 08:12:48.226: INFO: stdout: ""
    May 17 08:12:48.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2342 exec execpod6tr2x -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 30574'
    May 17 08:12:48.314: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 30574\nConnection to 10.0.79.211 30574 port [tcp/*] succeeded!\n"
    May 17 08:12:48.314: INFO: stdout: ""
    May 17 08:12:48.314: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:48.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2342" for this suite. 05/17/23 08:12:48.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:48.33
May 17 08:12:48.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:12:48.331
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:48.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:48.34
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 08:12:48.341
May 17 08:12:48.345: INFO: Waiting up to 5m0s for pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3" in namespace "emptydir-8946" to be "Succeeded or Failed"
May 17 08:12:48.347: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.439491ms
May 17 08:12:50.350: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004650843s
May 17 08:12:52.350: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004481719s
STEP: Saw pod success 05/17/23 08:12:52.35
May 17 08:12:52.350: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3" satisfied condition "Succeeded or Failed"
May 17 08:12:52.351: INFO: Trying to get logs from node k8s-node1 pod pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3 container test-container: <nil>
STEP: delete the pod 05/17/23 08:12:52.355
May 17 08:12:52.363: INFO: Waiting for pod pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3 to disappear
May 17 08:12:52.364: INFO: Pod pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:12:52.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8946" for this suite. 05/17/23 08:12:52.367
------------------------------
â€¢ [4.039 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:48.33
    May 17 08:12:48.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:12:48.331
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:48.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:48.34
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 08:12:48.341
    May 17 08:12:48.345: INFO: Waiting up to 5m0s for pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3" in namespace "emptydir-8946" to be "Succeeded or Failed"
    May 17 08:12:48.347: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.439491ms
    May 17 08:12:50.350: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004650843s
    May 17 08:12:52.350: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004481719s
    STEP: Saw pod success 05/17/23 08:12:52.35
    May 17 08:12:52.350: INFO: Pod "pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3" satisfied condition "Succeeded or Failed"
    May 17 08:12:52.351: INFO: Trying to get logs from node k8s-node1 pod pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:12:52.355
    May 17 08:12:52.363: INFO: Waiting for pod pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3 to disappear
    May 17 08:12:52.364: INFO: Pod pod-65eea906-412f-4e54-a44c-fd6ab18c2dd3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:12:52.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8946" for this suite. 05/17/23 08:12:52.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:12:52.37
May 17 08:12:52.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-watch 05/17/23 08:12:52.371
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:52.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:52.379
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May 17 08:12:52.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Creating first CR  05/17/23 08:12:54.917
May 17 08:12:54.920: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:12:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:12:54Z]] name:name1 resourceVersion:1203301 uid:f372143d-86cc-48cd-ab4d-0dd3a3c28fc0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/17/23 08:13:04.92
May 17 08:13:04.924: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:13:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:04Z]] name:name2 resourceVersion:1203332 uid:2eb0898c-810d-4e26-a588-717b5763143c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/17/23 08:13:14.926
May 17 08:13:14.930: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:12:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:14Z]] name:name1 resourceVersion:1203348 uid:f372143d-86cc-48cd-ab4d-0dd3a3c28fc0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/17/23 08:13:24.932
May 17 08:13:24.937: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:13:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:24Z]] name:name2 resourceVersion:1203364 uid:2eb0898c-810d-4e26-a588-717b5763143c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/17/23 08:13:34.938
May 17 08:13:34.942: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:12:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:14Z]] name:name1 resourceVersion:1203380 uid:f372143d-86cc-48cd-ab4d-0dd3a3c28fc0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/17/23 08:13:44.944
May 17 08:13:44.950: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:13:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:24Z]] name:name2 resourceVersion:1203396 uid:2eb0898c-810d-4e26-a588-717b5763143c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:13:55.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1351" for this suite. 05/17/23 08:13:55.461
------------------------------
â€¢ [SLOW TEST] [63.095 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:12:52.37
    May 17 08:12:52.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-watch 05/17/23 08:12:52.371
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:12:52.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:12:52.379
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May 17 08:12:52.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Creating first CR  05/17/23 08:12:54.917
    May 17 08:12:54.920: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:12:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:12:54Z]] name:name1 resourceVersion:1203301 uid:f372143d-86cc-48cd-ab4d-0dd3a3c28fc0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/17/23 08:13:04.92
    May 17 08:13:04.924: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:13:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:04Z]] name:name2 resourceVersion:1203332 uid:2eb0898c-810d-4e26-a588-717b5763143c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/17/23 08:13:14.926
    May 17 08:13:14.930: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:12:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:14Z]] name:name1 resourceVersion:1203348 uid:f372143d-86cc-48cd-ab4d-0dd3a3c28fc0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/17/23 08:13:24.932
    May 17 08:13:24.937: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:13:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:24Z]] name:name2 resourceVersion:1203364 uid:2eb0898c-810d-4e26-a588-717b5763143c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/17/23 08:13:34.938
    May 17 08:13:34.942: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:12:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:14Z]] name:name1 resourceVersion:1203380 uid:f372143d-86cc-48cd-ab4d-0dd3a3c28fc0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/17/23 08:13:44.944
    May 17 08:13:44.950: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T08:13:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T08:13:24Z]] name:name2 resourceVersion:1203396 uid:2eb0898c-810d-4e26-a588-717b5763143c] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:13:55.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1351" for this suite. 05/17/23 08:13:55.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:13:55.465
May 17 08:13:55.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename events 05/17/23 08:13:55.466
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:13:55.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:13:55.474
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/17/23 08:13:55.476
STEP: listing all events in all namespaces 05/17/23 08:13:55.48
STEP: patching the test event 05/17/23 08:13:55.482
STEP: fetching the test event 05/17/23 08:13:55.486
STEP: updating the test event 05/17/23 08:13:55.487
STEP: getting the test event 05/17/23 08:13:55.491
STEP: deleting the test event 05/17/23 08:13:55.492
STEP: listing all events in all namespaces 05/17/23 08:13:55.495
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May 17 08:13:55.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9746" for this suite. 05/17/23 08:13:55.499
------------------------------
â€¢ [0.037 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:13:55.465
    May 17 08:13:55.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename events 05/17/23 08:13:55.466
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:13:55.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:13:55.474
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/17/23 08:13:55.476
    STEP: listing all events in all namespaces 05/17/23 08:13:55.48
    STEP: patching the test event 05/17/23 08:13:55.482
    STEP: fetching the test event 05/17/23 08:13:55.486
    STEP: updating the test event 05/17/23 08:13:55.487
    STEP: getting the test event 05/17/23 08:13:55.491
    STEP: deleting the test event 05/17/23 08:13:55.492
    STEP: listing all events in all namespaces 05/17/23 08:13:55.495
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May 17 08:13:55.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9746" for this suite. 05/17/23 08:13:55.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:13:55.502
May 17 08:13:55.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename subpath 05/17/23 08:13:55.503
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:13:55.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:13:55.511
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 08:13:55.512
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-9zns 05/17/23 08:13:55.517
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 08:13:55.517
May 17 08:13:55.521: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9zns" in namespace "subpath-8691" to be "Succeeded or Failed"
May 17 08:13:55.523: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Pending", Reason="", readiness=false. Elapsed: 1.242724ms
May 17 08:13:57.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073663s
May 17 08:13:59.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 4.005094597s
May 17 08:14:01.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 6.004326593s
May 17 08:14:03.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 8.004627283s
May 17 08:14:05.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 10.004018372s
May 17 08:14:07.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 12.004330132s
May 17 08:14:09.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 14.004343952s
May 17 08:14:11.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 16.004260595s
May 17 08:14:13.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 18.003688386s
May 17 08:14:15.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 20.004914659s
May 17 08:14:17.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=false. Elapsed: 22.003529434s
May 17 08:14:19.527: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005324444s
STEP: Saw pod success 05/17/23 08:14:19.527
May 17 08:14:19.527: INFO: Pod "pod-subpath-test-downwardapi-9zns" satisfied condition "Succeeded or Failed"
May 17 08:14:19.529: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-downwardapi-9zns container test-container-subpath-downwardapi-9zns: <nil>
STEP: delete the pod 05/17/23 08:14:19.532
May 17 08:14:19.539: INFO: Waiting for pod pod-subpath-test-downwardapi-9zns to disappear
May 17 08:14:19.540: INFO: Pod pod-subpath-test-downwardapi-9zns no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9zns 05/17/23 08:14:19.54
May 17 08:14:19.540: INFO: Deleting pod "pod-subpath-test-downwardapi-9zns" in namespace "subpath-8691"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 17 08:14:19.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8691" for this suite. 05/17/23 08:14:19.543
------------------------------
â€¢ [SLOW TEST] [24.044 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:13:55.502
    May 17 08:13:55.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename subpath 05/17/23 08:13:55.503
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:13:55.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:13:55.511
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 08:13:55.512
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-9zns 05/17/23 08:13:55.517
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 08:13:55.517
    May 17 08:13:55.521: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9zns" in namespace "subpath-8691" to be "Succeeded or Failed"
    May 17 08:13:55.523: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Pending", Reason="", readiness=false. Elapsed: 1.242724ms
    May 17 08:13:57.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073663s
    May 17 08:13:59.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 4.005094597s
    May 17 08:14:01.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 6.004326593s
    May 17 08:14:03.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 8.004627283s
    May 17 08:14:05.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 10.004018372s
    May 17 08:14:07.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 12.004330132s
    May 17 08:14:09.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 14.004343952s
    May 17 08:14:11.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 16.004260595s
    May 17 08:14:13.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 18.003688386s
    May 17 08:14:15.526: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=true. Elapsed: 20.004914659s
    May 17 08:14:17.525: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Running", Reason="", readiness=false. Elapsed: 22.003529434s
    May 17 08:14:19.527: INFO: Pod "pod-subpath-test-downwardapi-9zns": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005324444s
    STEP: Saw pod success 05/17/23 08:14:19.527
    May 17 08:14:19.527: INFO: Pod "pod-subpath-test-downwardapi-9zns" satisfied condition "Succeeded or Failed"
    May 17 08:14:19.529: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-downwardapi-9zns container test-container-subpath-downwardapi-9zns: <nil>
    STEP: delete the pod 05/17/23 08:14:19.532
    May 17 08:14:19.539: INFO: Waiting for pod pod-subpath-test-downwardapi-9zns to disappear
    May 17 08:14:19.540: INFO: Pod pod-subpath-test-downwardapi-9zns no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-9zns 05/17/23 08:14:19.54
    May 17 08:14:19.540: INFO: Deleting pod "pod-subpath-test-downwardapi-9zns" in namespace "subpath-8691"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 17 08:14:19.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8691" for this suite. 05/17/23 08:14:19.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:14:19.547
May 17 08:14:19.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:14:19.547
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:19.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:19.555
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-8774fe2e-dc7b-4add-a653-8a5616f43dec 05/17/23 08:14:19.557
STEP: Creating a pod to test consume secrets 05/17/23 08:14:19.56
May 17 08:14:19.565: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607" in namespace "projected-8765" to be "Succeeded or Failed"
May 17 08:14:19.566: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607": Phase="Pending", Reason="", readiness=false. Elapsed: 1.355732ms
May 17 08:14:21.569: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004421709s
May 17 08:14:23.569: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003720792s
STEP: Saw pod success 05/17/23 08:14:23.569
May 17 08:14:23.569: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607" satisfied condition "Succeeded or Failed"
May 17 08:14:23.570: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:14:23.574
May 17 08:14:23.580: INFO: Waiting for pod pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607 to disappear
May 17 08:14:23.581: INFO: Pod pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 17 08:14:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8765" for this suite. 05/17/23 08:14:23.583
------------------------------
â€¢ [4.039 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:14:19.547
    May 17 08:14:19.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:14:19.547
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:19.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:19.555
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-8774fe2e-dc7b-4add-a653-8a5616f43dec 05/17/23 08:14:19.557
    STEP: Creating a pod to test consume secrets 05/17/23 08:14:19.56
    May 17 08:14:19.565: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607" in namespace "projected-8765" to be "Succeeded or Failed"
    May 17 08:14:19.566: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607": Phase="Pending", Reason="", readiness=false. Elapsed: 1.355732ms
    May 17 08:14:21.569: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004421709s
    May 17 08:14:23.569: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003720792s
    STEP: Saw pod success 05/17/23 08:14:23.569
    May 17 08:14:23.569: INFO: Pod "pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607" satisfied condition "Succeeded or Failed"
    May 17 08:14:23.570: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:14:23.574
    May 17 08:14:23.580: INFO: Waiting for pod pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607 to disappear
    May 17 08:14:23.581: INFO: Pod pod-projected-secrets-785f94ea-b742-4273-b5e4-dc865928a607 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 17 08:14:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8765" for this suite. 05/17/23 08:14:23.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:14:23.586
May 17 08:14:23.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename disruption 05/17/23 08:14:23.588
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:23.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:23.596
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 05/17/23 08:14:23.6
STEP: Updating PodDisruptionBudget status 05/17/23 08:14:25.604
STEP: Waiting for all pods to be running 05/17/23 08:14:25.609
May 17 08:14:25.610: INFO: running pods: 0 < 1
STEP: locating a running pod 05/17/23 08:14:27.612
STEP: Waiting for the pdb to be processed 05/17/23 08:14:27.618
STEP: Patching PodDisruptionBudget status 05/17/23 08:14:27.621
STEP: Waiting for the pdb to be processed 05/17/23 08:14:27.626
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 17 08:14:27.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1021" for this suite. 05/17/23 08:14:27.629
------------------------------
â€¢ [4.046 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:14:23.586
    May 17 08:14:23.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename disruption 05/17/23 08:14:23.588
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:23.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:23.596
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 05/17/23 08:14:23.6
    STEP: Updating PodDisruptionBudget status 05/17/23 08:14:25.604
    STEP: Waiting for all pods to be running 05/17/23 08:14:25.609
    May 17 08:14:25.610: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/17/23 08:14:27.612
    STEP: Waiting for the pdb to be processed 05/17/23 08:14:27.618
    STEP: Patching PodDisruptionBudget status 05/17/23 08:14:27.621
    STEP: Waiting for the pdb to be processed 05/17/23 08:14:27.626
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 17 08:14:27.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1021" for this suite. 05/17/23 08:14:27.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:14:27.633
May 17 08:14:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename daemonsets 05/17/23 08:14:27.633
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:27.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:27.64
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 05/17/23 08:14:27.648
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:14:27.651
May 17 08:14:27.652: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:14:27.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:14:27.654: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:14:28.656: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:14:28.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:14:28.657: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:14:29.657: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:14:29.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 08:14:29.658: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 05/17/23 08:14:29.66
STEP: DeleteCollection of the DaemonSets 05/17/23 08:14:29.661
STEP: Verify that ReplicaSets have been deleted 05/17/23 08:14:29.664
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
May 17 08:14:29.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1203611"},"items":null}

May 17 08:14:29.669: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1203611"},"items":[{"metadata":{"name":"daemon-set-5qnh8","generateName":"daemon-set-","namespace":"daemonsets-2365","uid":"d39a36c5-4eff-4c88-af03-dc31883c244b","resourceVersion":"1203596","creationTimestamp":"2023-05-17T08:14:27Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c732fa346d31de617700dcb96c25c75a2e815d474949fb75d0b10001a45d089e","cni.projectcalico.org/podIP":"192.168.36.113/32","cni.projectcalico.org/podIPs":"192.168.36.113/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2fef1a67-42b4-4546-888a-9e58fa67eff2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fef1a67-42b4-4546-888a-9e58fa67eff2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5b4pt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5b4pt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"}],"hostIP":"10.0.79.210","podIP":"192.168.36.113","podIPs":[{"ip":"192.168.36.113"}],"startTime":"2023-05-17T08:14:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T08:14:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://19cf93ef63241d3ae54560f0322edf192dd38606aef8195721ed9314e6c83109","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-j6xk7","generateName":"daemon-set-","namespace":"daemonsets-2365","uid":"4b7af1d3-13ae-47b4-a28e-ec563112864c","resourceVersion":"1203609","creationTimestamp":"2023-05-17T08:14:27Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"09f71429eaed76b0373d6ce3b084823263e0cb033d71c745febbdc67c23f78c6","cni.projectcalico.org/podIP":"192.168.169.172/32","cni.projectcalico.org/podIPs":"192.168.169.172/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2fef1a67-42b4-4546-888a-9e58fa67eff2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fef1a67-42b4-4546-888a-9e58fa67eff2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6dh6x","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6dh6x","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"}],"hostIP":"10.0.79.211","podIP":"192.168.169.172","podIPs":[{"ip":"192.168.169.172"}],"startTime":"2023-05-17T08:14:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T08:14:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://f503d81738a1f18a65c5e0f7d7b1b9300d81f7a8e4519c4b86d10fa1f04495bb","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:14:29.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2365" for this suite. 05/17/23 08:14:29.676
------------------------------
â€¢ [2.046 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:14:27.633
    May 17 08:14:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename daemonsets 05/17/23 08:14:27.633
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:27.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:27.64
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 05/17/23 08:14:27.648
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:14:27.651
    May 17 08:14:27.652: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:14:27.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:14:27.654: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:14:28.656: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:14:28.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:14:28.657: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:14:29.657: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:14:29.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 08:14:29.658: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 05/17/23 08:14:29.66
    STEP: DeleteCollection of the DaemonSets 05/17/23 08:14:29.661
    STEP: Verify that ReplicaSets have been deleted 05/17/23 08:14:29.664
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    May 17 08:14:29.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1203611"},"items":null}

    May 17 08:14:29.669: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1203611"},"items":[{"metadata":{"name":"daemon-set-5qnh8","generateName":"daemon-set-","namespace":"daemonsets-2365","uid":"d39a36c5-4eff-4c88-af03-dc31883c244b","resourceVersion":"1203596","creationTimestamp":"2023-05-17T08:14:27Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c732fa346d31de617700dcb96c25c75a2e815d474949fb75d0b10001a45d089e","cni.projectcalico.org/podIP":"192.168.36.113/32","cni.projectcalico.org/podIPs":"192.168.36.113/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2fef1a67-42b4-4546-888a-9e58fa67eff2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fef1a67-42b4-4546-888a-9e58fa67eff2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5b4pt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5b4pt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"}],"hostIP":"10.0.79.210","podIP":"192.168.36.113","podIPs":[{"ip":"192.168.36.113"}],"startTime":"2023-05-17T08:14:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T08:14:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://19cf93ef63241d3ae54560f0322edf192dd38606aef8195721ed9314e6c83109","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-j6xk7","generateName":"daemon-set-","namespace":"daemonsets-2365","uid":"4b7af1d3-13ae-47b4-a28e-ec563112864c","resourceVersion":"1203609","creationTimestamp":"2023-05-17T08:14:27Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"09f71429eaed76b0373d6ce3b084823263e0cb033d71c745febbdc67c23f78c6","cni.projectcalico.org/podIP":"192.168.169.172/32","cni.projectcalico.org/podIPs":"192.168.169.172/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2fef1a67-42b4-4546-888a-9e58fa67eff2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fef1a67-42b4-4546-888a-9e58fa67eff2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T08:14:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6dh6x","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6dh6x","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T08:14:27Z"}],"hostIP":"10.0.79.211","podIP":"192.168.169.172","podIPs":[{"ip":"192.168.169.172"}],"startTime":"2023-05-17T08:14:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T08:14:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://f503d81738a1f18a65c5e0f7d7b1b9300d81f7a8e4519c4b86d10fa1f04495bb","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:14:29.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2365" for this suite. 05/17/23 08:14:29.676
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:14:29.679
May 17 08:14:29.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 08:14:29.68
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:29.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:29.687
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9 in namespace container-probe-3565 05/17/23 08:14:29.689
May 17 08:14:29.692: INFO: Waiting up to 5m0s for pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9" in namespace "container-probe-3565" to be "not pending"
May 17 08:14:29.693: INFO: Pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.148001ms
May 17 08:14:31.696: INFO: Pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00441048s
May 17 08:14:31.696: INFO: Pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9" satisfied condition "not pending"
May 17 08:14:31.696: INFO: Started pod liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9 in namespace container-probe-3565
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:14:31.696
May 17 08:14:31.698: INFO: Initial restart count of pod liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9 is 0
STEP: deleting the pod 05/17/23 08:18:32.089
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 08:18:32.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3565" for this suite. 05/17/23 08:18:32.099
------------------------------
â€¢ [SLOW TEST] [242.422 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:14:29.679
    May 17 08:14:29.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 08:14:29.68
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:14:29.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:14:29.687
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9 in namespace container-probe-3565 05/17/23 08:14:29.689
    May 17 08:14:29.692: INFO: Waiting up to 5m0s for pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9" in namespace "container-probe-3565" to be "not pending"
    May 17 08:14:29.693: INFO: Pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.148001ms
    May 17 08:14:31.696: INFO: Pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00441048s
    May 17 08:14:31.696: INFO: Pod "liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9" satisfied condition "not pending"
    May 17 08:14:31.696: INFO: Started pod liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9 in namespace container-probe-3565
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:14:31.696
    May 17 08:14:31.698: INFO: Initial restart count of pod liveness-0c540019-a49f-4c87-a89b-f3c57ce835e9 is 0
    STEP: deleting the pod 05/17/23 08:18:32.089
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 08:18:32.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3565" for this suite. 05/17/23 08:18:32.099
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:18:32.102
May 17 08:18:32.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 08:18:32.102
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:32.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:32.11
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 05/17/23 08:18:32.115
STEP: submitting the pod to kubernetes 05/17/23 08:18:32.115
May 17 08:18:32.118: INFO: Waiting up to 5m0s for pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" in namespace "pods-8895" to be "running and ready"
May 17 08:18:32.120: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122": Phase="Pending", Reason="", readiness=false. Elapsed: 1.157126ms
May 17 08:18:32.120: INFO: The phase of Pod pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:18:34.123: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122": Phase="Running", Reason="", readiness=true. Elapsed: 2.004084764s
May 17 08:18:34.123: INFO: The phase of Pod pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122 is Running (Ready = true)
May 17 08:18:34.123: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/17/23 08:18:34.124
STEP: updating the pod 05/17/23 08:18:34.126
May 17 08:18:34.634: INFO: Successfully updated pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122"
May 17 08:18:34.634: INFO: Waiting up to 5m0s for pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" in namespace "pods-8895" to be "running"
May 17 08:18:34.636: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122": Phase="Running", Reason="", readiness=true. Elapsed: 1.409777ms
May 17 08:18:34.636: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/17/23 08:18:34.636
May 17 08:18:34.638: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 08:18:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8895" for this suite. 05/17/23 08:18:34.64
------------------------------
â€¢ [2.541 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:18:32.102
    May 17 08:18:32.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 08:18:32.102
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:32.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:32.11
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 05/17/23 08:18:32.115
    STEP: submitting the pod to kubernetes 05/17/23 08:18:32.115
    May 17 08:18:32.118: INFO: Waiting up to 5m0s for pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" in namespace "pods-8895" to be "running and ready"
    May 17 08:18:32.120: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122": Phase="Pending", Reason="", readiness=false. Elapsed: 1.157126ms
    May 17 08:18:32.120: INFO: The phase of Pod pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:18:34.123: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122": Phase="Running", Reason="", readiness=true. Elapsed: 2.004084764s
    May 17 08:18:34.123: INFO: The phase of Pod pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122 is Running (Ready = true)
    May 17 08:18:34.123: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/17/23 08:18:34.124
    STEP: updating the pod 05/17/23 08:18:34.126
    May 17 08:18:34.634: INFO: Successfully updated pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122"
    May 17 08:18:34.634: INFO: Waiting up to 5m0s for pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" in namespace "pods-8895" to be "running"
    May 17 08:18:34.636: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122": Phase="Running", Reason="", readiness=true. Elapsed: 1.409777ms
    May 17 08:18:34.636: INFO: Pod "pod-update-1a3b7128-fc60-48d7-b2c7-ebf4c0f9e122" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/17/23 08:18:34.636
    May 17 08:18:34.638: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 08:18:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8895" for this suite. 05/17/23 08:18:34.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:18:34.643
May 17 08:18:34.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:18:34.644
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:34.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:34.651
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-fcbf0831-a9b6-4a69-99d5-291333ffe7d9 05/17/23 08:18:34.653
STEP: Creating a pod to test consume secrets 05/17/23 08:18:34.655
May 17 08:18:34.659: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2" in namespace "projected-1687" to be "Succeeded or Failed"
May 17 08:18:34.661: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866732ms
May 17 08:18:36.664: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005493217s
May 17 08:18:38.664: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004908076s
STEP: Saw pod success 05/17/23 08:18:38.664
May 17 08:18:38.664: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2" satisfied condition "Succeeded or Failed"
May 17 08:18:38.665: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:18:38.675
May 17 08:18:38.682: INFO: Waiting for pod pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2 to disappear
May 17 08:18:38.683: INFO: Pod pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 17 08:18:38.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1687" for this suite. 05/17/23 08:18:38.685
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:18:34.643
    May 17 08:18:34.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:18:34.644
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:34.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:34.651
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-fcbf0831-a9b6-4a69-99d5-291333ffe7d9 05/17/23 08:18:34.653
    STEP: Creating a pod to test consume secrets 05/17/23 08:18:34.655
    May 17 08:18:34.659: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2" in namespace "projected-1687" to be "Succeeded or Failed"
    May 17 08:18:34.661: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866732ms
    May 17 08:18:36.664: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005493217s
    May 17 08:18:38.664: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004908076s
    STEP: Saw pod success 05/17/23 08:18:38.664
    May 17 08:18:38.664: INFO: Pod "pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2" satisfied condition "Succeeded or Failed"
    May 17 08:18:38.665: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:18:38.675
    May 17 08:18:38.682: INFO: Waiting for pod pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2 to disappear
    May 17 08:18:38.683: INFO: Pod pod-projected-secrets-b3622b4f-29bd-4e7c-89a2-85e5dea3d5e2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 17 08:18:38.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1687" for this suite. 05/17/23 08:18:38.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:18:38.687
May 17 08:18:38.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:18:38.688
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:38.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:38.696
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:18:38.703
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:18:39.122
STEP: Deploying the webhook pod 05/17/23 08:18:39.127
STEP: Wait for the deployment to be ready 05/17/23 08:18:39.133
May 17 08:18:39.136: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 08:18:41.141
STEP: Verifying the service has paired with the endpoint 05/17/23 08:18:41.147
May 17 08:18:42.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
May 17 08:18:42.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/17/23 08:18:42.657
STEP: Creating a custom resource that should be denied by the webhook 05/17/23 08:18:42.668
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/17/23 08:18:44.698
STEP: Updating the custom resource with disallowed data should be denied 05/17/23 08:18:44.703
STEP: Deleting the custom resource should be denied 05/17/23 08:18:44.708
STEP: Remove the offending key and value from the custom resource data 05/17/23 08:18:44.711
STEP: Deleting the updated custom resource should be successful 05/17/23 08:18:44.716
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:18:45.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7845" for this suite. 05/17/23 08:18:45.248
STEP: Destroying namespace "webhook-7845-markers" for this suite. 05/17/23 08:18:45.251
------------------------------
â€¢ [SLOW TEST] [6.568 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:18:38.687
    May 17 08:18:38.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:18:38.688
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:38.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:38.696
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:18:38.703
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:18:39.122
    STEP: Deploying the webhook pod 05/17/23 08:18:39.127
    STEP: Wait for the deployment to be ready 05/17/23 08:18:39.133
    May 17 08:18:39.136: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 08:18:41.141
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:18:41.147
    May 17 08:18:42.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    May 17 08:18:42.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/17/23 08:18:42.657
    STEP: Creating a custom resource that should be denied by the webhook 05/17/23 08:18:42.668
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/17/23 08:18:44.698
    STEP: Updating the custom resource with disallowed data should be denied 05/17/23 08:18:44.703
    STEP: Deleting the custom resource should be denied 05/17/23 08:18:44.708
    STEP: Remove the offending key and value from the custom resource data 05/17/23 08:18:44.711
    STEP: Deleting the updated custom resource should be successful 05/17/23 08:18:44.716
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:18:45.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7845" for this suite. 05/17/23 08:18:45.248
    STEP: Destroying namespace "webhook-7845-markers" for this suite. 05/17/23 08:18:45.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:18:45.256
May 17 08:18:45.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svc-latency 05/17/23 08:18:45.257
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:45.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:45.265
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May 17 08:18:45.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1865 05/17/23 08:18:45.267
I0517 08:18:45.271594      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1865, replica count: 1
I0517 08:18:46.322348      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0517 08:18:47.323424      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 08:18:47.429: INFO: Created: latency-svc-gzwvx
May 17 08:18:47.434: INFO: Got endpoints: latency-svc-gzwvx [10.418872ms]
May 17 08:18:47.441: INFO: Created: latency-svc-wb4ft
May 17 08:18:47.447: INFO: Got endpoints: latency-svc-wb4ft [12.734864ms]
May 17 08:18:47.447: INFO: Created: latency-svc-hxfzv
May 17 08:18:47.451: INFO: Got endpoints: latency-svc-hxfzv [17.429081ms]
May 17 08:18:47.452: INFO: Created: latency-svc-nzrsm
May 17 08:18:47.455: INFO: Got endpoints: latency-svc-nzrsm [21.58481ms]
May 17 08:18:47.457: INFO: Created: latency-svc-l5gjt
May 17 08:18:47.461: INFO: Got endpoints: latency-svc-l5gjt [27.744609ms]
May 17 08:18:47.463: INFO: Created: latency-svc-9jqkw
May 17 08:18:47.467: INFO: Got endpoints: latency-svc-9jqkw [33.081515ms]
May 17 08:18:47.468: INFO: Created: latency-svc-zdlkk
May 17 08:18:47.469: INFO: Got endpoints: latency-svc-zdlkk [35.639726ms]
May 17 08:18:47.473: INFO: Created: latency-svc-8n75v
May 17 08:18:47.477: INFO: Got endpoints: latency-svc-8n75v [43.506126ms]
May 17 08:18:47.478: INFO: Created: latency-svc-rmt9m
May 17 08:18:47.483: INFO: Got endpoints: latency-svc-rmt9m [48.747526ms]
May 17 08:18:47.483: INFO: Created: latency-svc-t4hsv
May 17 08:18:47.485: INFO: Got endpoints: latency-svc-t4hsv [51.04444ms]
May 17 08:18:47.488: INFO: Created: latency-svc-jdfzs
May 17 08:18:47.491: INFO: Got endpoints: latency-svc-jdfzs [56.902388ms]
May 17 08:18:47.495: INFO: Created: latency-svc-92524
May 17 08:18:47.496: INFO: Got endpoints: latency-svc-92524 [62.481094ms]
May 17 08:18:47.500: INFO: Created: latency-svc-prhsd
May 17 08:18:47.504: INFO: Got endpoints: latency-svc-prhsd [70.173247ms]
May 17 08:18:47.505: INFO: Created: latency-svc-n7nn8
May 17 08:18:47.507: INFO: Got endpoints: latency-svc-n7nn8 [73.528075ms]
May 17 08:18:47.511: INFO: Created: latency-svc-sbssn
May 17 08:18:47.515: INFO: Got endpoints: latency-svc-sbssn [81.167138ms]
May 17 08:18:47.516: INFO: Created: latency-svc-4jcnz
May 17 08:18:47.519: INFO: Got endpoints: latency-svc-4jcnz [85.690654ms]
May 17 08:18:47.521: INFO: Created: latency-svc-6nv92
May 17 08:18:47.524: INFO: Got endpoints: latency-svc-6nv92 [77.355182ms]
May 17 08:18:47.525: INFO: Created: latency-svc-76ljp
May 17 08:18:47.527: INFO: Got endpoints: latency-svc-76ljp [75.636462ms]
May 17 08:18:47.533: INFO: Created: latency-svc-x8vxx
May 17 08:18:47.544: INFO: Got endpoints: latency-svc-x8vxx [88.534544ms]
May 17 08:18:47.546: INFO: Created: latency-svc-8ctzb
May 17 08:18:47.549: INFO: Got endpoints: latency-svc-8ctzb [87.580143ms]
May 17 08:18:47.550: INFO: Created: latency-svc-6h7vj
May 17 08:18:47.554: INFO: Got endpoints: latency-svc-6h7vj [86.891426ms]
May 17 08:18:47.555: INFO: Created: latency-svc-4f4rx
May 17 08:18:47.558: INFO: Got endpoints: latency-svc-4f4rx [89.1555ms]
May 17 08:18:47.562: INFO: Created: latency-svc-mcs4t
May 17 08:18:47.564: INFO: Got endpoints: latency-svc-mcs4t [86.699254ms]
May 17 08:18:47.574: INFO: Created: latency-svc-2qtqd
May 17 08:18:47.579: INFO: Got endpoints: latency-svc-2qtqd [96.821784ms]
May 17 08:18:47.581: INFO: Created: latency-svc-c2t68
May 17 08:18:47.585: INFO: Got endpoints: latency-svc-c2t68 [100.180497ms]
May 17 08:18:47.586: INFO: Created: latency-svc-7kchq
May 17 08:18:47.589: INFO: Got endpoints: latency-svc-7kchq [98.29054ms]
May 17 08:18:47.590: INFO: Created: latency-svc-mp9rn
May 17 08:18:47.594: INFO: Got endpoints: latency-svc-mp9rn [97.400529ms]
May 17 08:18:47.595: INFO: Created: latency-svc-j66hj
May 17 08:18:47.596: INFO: Got endpoints: latency-svc-j66hj [92.482507ms]
May 17 08:18:47.600: INFO: Created: latency-svc-vz9fn
May 17 08:18:47.603: INFO: Got endpoints: latency-svc-vz9fn [95.885652ms]
May 17 08:18:47.604: INFO: Created: latency-svc-hvlc5
May 17 08:18:47.606: INFO: Got endpoints: latency-svc-hvlc5 [91.436235ms]
May 17 08:18:47.610: INFO: Created: latency-svc-5flb7
May 17 08:18:47.614: INFO: Got endpoints: latency-svc-5flb7 [94.459913ms]
May 17 08:18:47.615: INFO: Created: latency-svc-x8tl8
May 17 08:18:47.618: INFO: Got endpoints: latency-svc-x8tl8 [93.925368ms]
May 17 08:18:47.620: INFO: Created: latency-svc-2dl96
May 17 08:18:47.623: INFO: Got endpoints: latency-svc-2dl96 [95.77392ms]
May 17 08:18:47.623: INFO: Created: latency-svc-b5wb6
May 17 08:18:47.628: INFO: Created: latency-svc-k8vt2
May 17 08:18:47.633: INFO: Got endpoints: latency-svc-b5wb6 [88.834541ms]
May 17 08:18:47.633: INFO: Created: latency-svc-6ppdt
May 17 08:18:47.638: INFO: Created: latency-svc-lnszx
May 17 08:18:47.642: INFO: Created: latency-svc-4t6r5
May 17 08:18:47.660: INFO: Created: latency-svc-dhgdq
May 17 08:18:47.663: INFO: Created: latency-svc-77dhk
May 17 08:18:47.667: INFO: Created: latency-svc-stbtw
May 17 08:18:47.671: INFO: Created: latency-svc-thccd
May 17 08:18:47.675: INFO: Created: latency-svc-xwsx7
May 17 08:18:47.679: INFO: Created: latency-svc-ljm2n
May 17 08:18:47.683: INFO: Got endpoints: latency-svc-k8vt2 [134.010311ms]
May 17 08:18:47.684: INFO: Created: latency-svc-pxcrn
May 17 08:18:47.688: INFO: Created: latency-svc-h7qd9
May 17 08:18:47.693: INFO: Created: latency-svc-zs5jm
May 17 08:18:47.697: INFO: Created: latency-svc-x7vff
May 17 08:18:47.701: INFO: Created: latency-svc-bg4lb
May 17 08:18:47.706: INFO: Created: latency-svc-2x4kl
May 17 08:18:47.733: INFO: Got endpoints: latency-svc-6ppdt [178.972834ms]
May 17 08:18:47.740: INFO: Created: latency-svc-6f9vk
May 17 08:18:47.782: INFO: Got endpoints: latency-svc-lnszx [223.825624ms]
May 17 08:18:47.790: INFO: Created: latency-svc-qrwcn
May 17 08:18:47.832: INFO: Got endpoints: latency-svc-4t6r5 [268.424022ms]
May 17 08:18:47.842: INFO: Created: latency-svc-zwwhp
May 17 08:18:47.884: INFO: Got endpoints: latency-svc-dhgdq [304.959286ms]
May 17 08:18:47.892: INFO: Created: latency-svc-rprbx
May 17 08:18:47.932: INFO: Got endpoints: latency-svc-77dhk [347.465138ms]
May 17 08:18:47.940: INFO: Created: latency-svc-qwf9d
May 17 08:18:47.982: INFO: Got endpoints: latency-svc-stbtw [393.120231ms]
May 17 08:18:47.990: INFO: Created: latency-svc-kpg96
May 17 08:18:48.033: INFO: Got endpoints: latency-svc-thccd [439.553341ms]
May 17 08:18:48.041: INFO: Created: latency-svc-95bm4
May 17 08:18:48.083: INFO: Got endpoints: latency-svc-xwsx7 [487.062069ms]
May 17 08:18:48.091: INFO: Created: latency-svc-4scrb
May 17 08:18:48.134: INFO: Got endpoints: latency-svc-ljm2n [530.573203ms]
May 17 08:18:48.141: INFO: Created: latency-svc-sqmzw
May 17 08:18:48.187: INFO: Got endpoints: latency-svc-pxcrn [580.543524ms]
May 17 08:18:48.195: INFO: Created: latency-svc-vbn4v
May 17 08:18:48.234: INFO: Got endpoints: latency-svc-h7qd9 [620.39367ms]
May 17 08:18:48.242: INFO: Created: latency-svc-4mr9d
May 17 08:18:48.282: INFO: Got endpoints: latency-svc-zs5jm [664.364236ms]
May 17 08:18:48.294: INFO: Created: latency-svc-h2dhz
May 17 08:18:48.333: INFO: Got endpoints: latency-svc-x7vff [710.360756ms]
May 17 08:18:48.340: INFO: Created: latency-svc-66kwk
May 17 08:18:48.384: INFO: Got endpoints: latency-svc-bg4lb [751.254212ms]
May 17 08:18:48.391: INFO: Created: latency-svc-vj8sw
May 17 08:18:48.433: INFO: Got endpoints: latency-svc-2x4kl [749.758456ms]
May 17 08:18:48.441: INFO: Created: latency-svc-74hf4
May 17 08:18:48.482: INFO: Got endpoints: latency-svc-6f9vk [749.618938ms]
May 17 08:18:48.490: INFO: Created: latency-svc-4r5tj
May 17 08:18:48.534: INFO: Got endpoints: latency-svc-qrwcn [751.614197ms]
May 17 08:18:48.542: INFO: Created: latency-svc-dnx8d
May 17 08:18:48.583: INFO: Got endpoints: latency-svc-zwwhp [750.123889ms]
May 17 08:18:48.590: INFO: Created: latency-svc-rbtjt
May 17 08:18:48.633: INFO: Got endpoints: latency-svc-rprbx [748.286078ms]
May 17 08:18:48.640: INFO: Created: latency-svc-q4h5f
May 17 08:18:48.684: INFO: Got endpoints: latency-svc-qwf9d [751.277143ms]
May 17 08:18:48.691: INFO: Created: latency-svc-8qjpw
May 17 08:18:48.732: INFO: Got endpoints: latency-svc-kpg96 [750.156143ms]
May 17 08:18:48.740: INFO: Created: latency-svc-p9vv8
May 17 08:18:48.783: INFO: Got endpoints: latency-svc-95bm4 [749.772346ms]
May 17 08:18:48.792: INFO: Created: latency-svc-c2s8w
May 17 08:18:48.834: INFO: Got endpoints: latency-svc-4scrb [750.191743ms]
May 17 08:18:48.841: INFO: Created: latency-svc-gz4mb
May 17 08:18:48.882: INFO: Got endpoints: latency-svc-sqmzw [748.756827ms]
May 17 08:18:48.890: INFO: Created: latency-svc-cdxjf
May 17 08:18:48.933: INFO: Got endpoints: latency-svc-vbn4v [745.692044ms]
May 17 08:18:48.941: INFO: Created: latency-svc-wp5jn
May 17 08:18:48.982: INFO: Got endpoints: latency-svc-4mr9d [748.104241ms]
May 17 08:18:48.991: INFO: Created: latency-svc-vtjzb
May 17 08:18:49.034: INFO: Got endpoints: latency-svc-h2dhz [751.526715ms]
May 17 08:18:49.041: INFO: Created: latency-svc-kxzc4
May 17 08:18:49.083: INFO: Got endpoints: latency-svc-66kwk [749.911276ms]
May 17 08:18:49.091: INFO: Created: latency-svc-mz82l
May 17 08:18:49.133: INFO: Got endpoints: latency-svc-vj8sw [749.131487ms]
May 17 08:18:49.141: INFO: Created: latency-svc-5wtvh
May 17 08:18:49.183: INFO: Got endpoints: latency-svc-74hf4 [750.028005ms]
May 17 08:18:49.190: INFO: Created: latency-svc-26k2q
May 17 08:18:49.234: INFO: Got endpoints: latency-svc-4r5tj [751.277712ms]
May 17 08:18:49.240: INFO: Created: latency-svc-smtg5
May 17 08:18:49.283: INFO: Got endpoints: latency-svc-dnx8d [748.93463ms]
May 17 08:18:49.291: INFO: Created: latency-svc-59h8t
May 17 08:18:49.333: INFO: Got endpoints: latency-svc-rbtjt [750.157786ms]
May 17 08:18:49.340: INFO: Created: latency-svc-pc4rw
May 17 08:18:49.383: INFO: Got endpoints: latency-svc-q4h5f [750.111055ms]
May 17 08:18:49.390: INFO: Created: latency-svc-pv2q7
May 17 08:18:49.436: INFO: Got endpoints: latency-svc-8qjpw [752.061852ms]
May 17 08:18:49.444: INFO: Created: latency-svc-nqlpf
May 17 08:18:49.484: INFO: Got endpoints: latency-svc-p9vv8 [751.168506ms]
May 17 08:18:49.491: INFO: Created: latency-svc-f4rqt
May 17 08:18:49.533: INFO: Got endpoints: latency-svc-c2s8w [750.118997ms]
May 17 08:18:49.556: INFO: Created: latency-svc-fhshr
May 17 08:18:49.584: INFO: Got endpoints: latency-svc-gz4mb [750.276344ms]
May 17 08:18:49.591: INFO: Created: latency-svc-w699n
May 17 08:18:49.635: INFO: Got endpoints: latency-svc-cdxjf [752.229701ms]
May 17 08:18:49.643: INFO: Created: latency-svc-rsm7q
May 17 08:18:49.684: INFO: Got endpoints: latency-svc-wp5jn [751.509515ms]
May 17 08:18:49.693: INFO: Created: latency-svc-d7xnn
May 17 08:18:49.733: INFO: Got endpoints: latency-svc-vtjzb [750.629343ms]
May 17 08:18:49.741: INFO: Created: latency-svc-m77l7
May 17 08:18:49.784: INFO: Got endpoints: latency-svc-kxzc4 [750.119012ms]
May 17 08:18:49.791: INFO: Created: latency-svc-5cjqs
May 17 08:18:49.833: INFO: Got endpoints: latency-svc-mz82l [750.043186ms]
May 17 08:18:49.841: INFO: Created: latency-svc-8cnjm
May 17 08:18:49.882: INFO: Got endpoints: latency-svc-5wtvh [749.255019ms]
May 17 08:18:49.889: INFO: Created: latency-svc-kz8zx
May 17 08:18:49.933: INFO: Got endpoints: latency-svc-26k2q [750.070267ms]
May 17 08:18:49.940: INFO: Created: latency-svc-2hsz7
May 17 08:18:49.983: INFO: Got endpoints: latency-svc-smtg5 [748.80439ms]
May 17 08:18:49.991: INFO: Created: latency-svc-fpl6t
May 17 08:18:50.033: INFO: Got endpoints: latency-svc-59h8t [749.550095ms]
May 17 08:18:50.040: INFO: Created: latency-svc-tvnhx
May 17 08:18:50.083: INFO: Got endpoints: latency-svc-pc4rw [750.397483ms]
May 17 08:18:50.090: INFO: Created: latency-svc-chd47
May 17 08:18:50.134: INFO: Got endpoints: latency-svc-pv2q7 [751.264856ms]
May 17 08:18:50.142: INFO: Created: latency-svc-wcvz4
May 17 08:18:50.183: INFO: Got endpoints: latency-svc-nqlpf [747.353597ms]
May 17 08:18:50.190: INFO: Created: latency-svc-rg5bv
May 17 08:18:50.233: INFO: Got endpoints: latency-svc-f4rqt [749.83678ms]
May 17 08:18:50.240: INFO: Created: latency-svc-s5ls2
May 17 08:18:50.282: INFO: Got endpoints: latency-svc-fhshr [748.919645ms]
May 17 08:18:50.290: INFO: Created: latency-svc-mbdst
May 17 08:18:50.332: INFO: Got endpoints: latency-svc-w699n [748.258488ms]
May 17 08:18:50.340: INFO: Created: latency-svc-x5kmv
May 17 08:18:50.383: INFO: Got endpoints: latency-svc-rsm7q [748.512941ms]
May 17 08:18:50.390: INFO: Created: latency-svc-nmf6z
May 17 08:18:50.434: INFO: Got endpoints: latency-svc-d7xnn [749.598382ms]
May 17 08:18:50.442: INFO: Created: latency-svc-c2796
May 17 08:18:50.485: INFO: Got endpoints: latency-svc-m77l7 [752.41036ms]
May 17 08:18:50.493: INFO: Created: latency-svc-r782n
May 17 08:18:50.533: INFO: Got endpoints: latency-svc-5cjqs [748.98184ms]
May 17 08:18:50.541: INFO: Created: latency-svc-4xgw8
May 17 08:18:50.583: INFO: Got endpoints: latency-svc-8cnjm [749.465626ms]
May 17 08:18:50.591: INFO: Created: latency-svc-nl6t4
May 17 08:18:50.634: INFO: Got endpoints: latency-svc-kz8zx [751.467504ms]
May 17 08:18:50.641: INFO: Created: latency-svc-q7kxb
May 17 08:18:50.683: INFO: Got endpoints: latency-svc-2hsz7 [750.339531ms]
May 17 08:18:50.696: INFO: Created: latency-svc-2jmz6
May 17 08:18:50.733: INFO: Got endpoints: latency-svc-fpl6t [750.098575ms]
May 17 08:18:50.740: INFO: Created: latency-svc-lssnv
May 17 08:18:50.784: INFO: Got endpoints: latency-svc-tvnhx [751.888558ms]
May 17 08:18:50.792: INFO: Created: latency-svc-lbvcg
May 17 08:18:50.833: INFO: Got endpoints: latency-svc-chd47 [749.710152ms]
May 17 08:18:50.841: INFO: Created: latency-svc-kvv6t
May 17 08:18:50.882: INFO: Got endpoints: latency-svc-wcvz4 [748.340292ms]
May 17 08:18:50.890: INFO: Created: latency-svc-sph9l
May 17 08:18:50.934: INFO: Got endpoints: latency-svc-rg5bv [750.301885ms]
May 17 08:18:50.942: INFO: Created: latency-svc-65qlm
May 17 08:18:50.983: INFO: Got endpoints: latency-svc-s5ls2 [749.19319ms]
May 17 08:18:50.990: INFO: Created: latency-svc-m48j8
May 17 08:18:51.034: INFO: Got endpoints: latency-svc-mbdst [751.351042ms]
May 17 08:18:51.042: INFO: Created: latency-svc-cqjph
May 17 08:18:51.083: INFO: Got endpoints: latency-svc-x5kmv [750.637172ms]
May 17 08:18:51.091: INFO: Created: latency-svc-sx7w2
May 17 08:18:51.133: INFO: Got endpoints: latency-svc-nmf6z [749.549579ms]
May 17 08:18:51.140: INFO: Created: latency-svc-57bfj
May 17 08:18:51.184: INFO: Got endpoints: latency-svc-c2796 [749.767514ms]
May 17 08:18:51.192: INFO: Created: latency-svc-cx56k
May 17 08:18:51.233: INFO: Got endpoints: latency-svc-r782n [747.639598ms]
May 17 08:18:51.241: INFO: Created: latency-svc-94dbd
May 17 08:18:51.284: INFO: Got endpoints: latency-svc-4xgw8 [750.940012ms]
May 17 08:18:51.291: INFO: Created: latency-svc-8grs6
May 17 08:18:51.334: INFO: Got endpoints: latency-svc-nl6t4 [751.156614ms]
May 17 08:18:51.341: INFO: Created: latency-svc-rf2mm
May 17 08:18:51.384: INFO: Got endpoints: latency-svc-q7kxb [749.947953ms]
May 17 08:18:51.391: INFO: Created: latency-svc-k78rz
May 17 08:18:51.434: INFO: Got endpoints: latency-svc-2jmz6 [750.264259ms]
May 17 08:18:51.441: INFO: Created: latency-svc-6sqjn
May 17 08:18:51.483: INFO: Got endpoints: latency-svc-lssnv [750.638631ms]
May 17 08:18:51.492: INFO: Created: latency-svc-9nqb6
May 17 08:18:51.534: INFO: Got endpoints: latency-svc-lbvcg [749.103999ms]
May 17 08:18:51.541: INFO: Created: latency-svc-h4lj5
May 17 08:18:51.583: INFO: Got endpoints: latency-svc-kvv6t [750.041602ms]
May 17 08:18:51.595: INFO: Created: latency-svc-gw2n6
May 17 08:18:51.634: INFO: Got endpoints: latency-svc-sph9l [751.264527ms]
May 17 08:18:51.642: INFO: Created: latency-svc-gptfh
May 17 08:18:51.685: INFO: Got endpoints: latency-svc-65qlm [750.958499ms]
May 17 08:18:51.692: INFO: Created: latency-svc-s82lx
May 17 08:18:51.735: INFO: Got endpoints: latency-svc-m48j8 [752.557583ms]
May 17 08:18:51.743: INFO: Created: latency-svc-jlspg
May 17 08:18:51.783: INFO: Got endpoints: latency-svc-cqjph [749.571326ms]
May 17 08:18:51.791: INFO: Created: latency-svc-klbnk
May 17 08:18:51.833: INFO: Got endpoints: latency-svc-sx7w2 [749.603696ms]
May 17 08:18:51.841: INFO: Created: latency-svc-hnzgs
May 17 08:18:51.884: INFO: Got endpoints: latency-svc-57bfj [751.394275ms]
May 17 08:18:51.892: INFO: Created: latency-svc-bxvrh
May 17 08:18:51.933: INFO: Got endpoints: latency-svc-cx56k [749.559853ms]
May 17 08:18:51.946: INFO: Created: latency-svc-29wrr
May 17 08:18:51.983: INFO: Got endpoints: latency-svc-94dbd [749.781595ms]
May 17 08:18:51.991: INFO: Created: latency-svc-m9tld
May 17 08:18:52.033: INFO: Got endpoints: latency-svc-8grs6 [749.422609ms]
May 17 08:18:52.041: INFO: Created: latency-svc-c46xx
May 17 08:18:52.084: INFO: Got endpoints: latency-svc-rf2mm [750.615617ms]
May 17 08:18:52.093: INFO: Created: latency-svc-6zw65
May 17 08:18:52.134: INFO: Got endpoints: latency-svc-k78rz [749.833404ms]
May 17 08:18:52.142: INFO: Created: latency-svc-w2654
May 17 08:18:52.184: INFO: Got endpoints: latency-svc-6sqjn [750.375594ms]
May 17 08:18:52.192: INFO: Created: latency-svc-mf6ns
May 17 08:18:52.234: INFO: Got endpoints: latency-svc-9nqb6 [751.075971ms]
May 17 08:18:52.243: INFO: Created: latency-svc-dmgqm
May 17 08:18:52.284: INFO: Got endpoints: latency-svc-h4lj5 [750.477479ms]
May 17 08:18:52.292: INFO: Created: latency-svc-lll62
May 17 08:18:52.335: INFO: Got endpoints: latency-svc-gw2n6 [751.678493ms]
May 17 08:18:52.343: INFO: Created: latency-svc-xps7m
May 17 08:18:52.383: INFO: Got endpoints: latency-svc-gptfh [749.120522ms]
May 17 08:18:52.392: INFO: Created: latency-svc-5pjjh
May 17 08:18:52.434: INFO: Got endpoints: latency-svc-s82lx [749.866524ms]
May 17 08:18:52.444: INFO: Created: latency-svc-pprwn
May 17 08:18:52.484: INFO: Got endpoints: latency-svc-jlspg [748.320221ms]
May 17 08:18:52.492: INFO: Created: latency-svc-4cs7v
May 17 08:18:52.534: INFO: Got endpoints: latency-svc-klbnk [750.418076ms]
May 17 08:18:52.542: INFO: Created: latency-svc-8tb8k
May 17 08:18:52.583: INFO: Got endpoints: latency-svc-hnzgs [750.531772ms]
May 17 08:18:52.592: INFO: Created: latency-svc-ldmcf
May 17 08:18:52.634: INFO: Got endpoints: latency-svc-bxvrh [749.436971ms]
May 17 08:18:52.642: INFO: Created: latency-svc-2zf9r
May 17 08:18:52.684: INFO: Got endpoints: latency-svc-29wrr [750.741648ms]
May 17 08:18:52.707: INFO: Created: latency-svc-m4f66
May 17 08:18:52.735: INFO: Got endpoints: latency-svc-m9tld [751.556976ms]
May 17 08:18:52.742: INFO: Created: latency-svc-b766w
May 17 08:18:52.782: INFO: Got endpoints: latency-svc-c46xx [748.917481ms]
May 17 08:18:52.791: INFO: Created: latency-svc-f2hv2
May 17 08:18:52.834: INFO: Got endpoints: latency-svc-6zw65 [749.667564ms]
May 17 08:18:52.842: INFO: Created: latency-svc-qfh4n
May 17 08:18:52.883: INFO: Got endpoints: latency-svc-w2654 [748.842024ms]
May 17 08:18:52.894: INFO: Created: latency-svc-wv9b2
May 17 08:18:52.933: INFO: Got endpoints: latency-svc-mf6ns [749.070317ms]
May 17 08:18:52.941: INFO: Created: latency-svc-gk5g8
May 17 08:18:52.987: INFO: Got endpoints: latency-svc-dmgqm [752.681847ms]
May 17 08:18:52.995: INFO: Created: latency-svc-jg8db
May 17 08:18:53.035: INFO: Got endpoints: latency-svc-lll62 [750.600193ms]
May 17 08:18:53.044: INFO: Created: latency-svc-nt9w7
May 17 08:18:53.083: INFO: Got endpoints: latency-svc-xps7m [747.915373ms]
May 17 08:18:53.095: INFO: Created: latency-svc-m5x4d
May 17 08:18:53.134: INFO: Got endpoints: latency-svc-5pjjh [750.464676ms]
May 17 08:18:53.143: INFO: Created: latency-svc-vfsr5
May 17 08:18:53.183: INFO: Got endpoints: latency-svc-pprwn [748.462446ms]
May 17 08:18:53.191: INFO: Created: latency-svc-pjxhv
May 17 08:18:53.234: INFO: Got endpoints: latency-svc-4cs7v [750.08664ms]
May 17 08:18:53.242: INFO: Created: latency-svc-sv7g7
May 17 08:18:53.283: INFO: Got endpoints: latency-svc-8tb8k [749.524642ms]
May 17 08:18:53.291: INFO: Created: latency-svc-prdfn
May 17 08:18:53.333: INFO: Got endpoints: latency-svc-ldmcf [750.267917ms]
May 17 08:18:53.341: INFO: Created: latency-svc-gcjpt
May 17 08:18:53.384: INFO: Got endpoints: latency-svc-2zf9r [750.299062ms]
May 17 08:18:53.392: INFO: Created: latency-svc-sm8l8
May 17 08:18:53.433: INFO: Got endpoints: latency-svc-m4f66 [748.655208ms]
May 17 08:18:53.441: INFO: Created: latency-svc-nhmt7
May 17 08:18:53.484: INFO: Got endpoints: latency-svc-b766w [749.314887ms]
May 17 08:18:53.491: INFO: Created: latency-svc-zfz8m
May 17 08:18:53.534: INFO: Got endpoints: latency-svc-f2hv2 [751.064215ms]
May 17 08:18:53.541: INFO: Created: latency-svc-sb2hq
May 17 08:18:53.583: INFO: Got endpoints: latency-svc-qfh4n [749.208519ms]
May 17 08:18:53.591: INFO: Created: latency-svc-vv6nk
May 17 08:18:53.632: INFO: Got endpoints: latency-svc-wv9b2 [749.593727ms]
May 17 08:18:53.640: INFO: Created: latency-svc-8mrcf
May 17 08:18:53.683: INFO: Got endpoints: latency-svc-gk5g8 [749.884993ms]
May 17 08:18:53.691: INFO: Created: latency-svc-wpv2s
May 17 08:18:53.734: INFO: Got endpoints: latency-svc-jg8db [746.86181ms]
May 17 08:18:53.741: INFO: Created: latency-svc-rp7gl
May 17 08:18:53.782: INFO: Got endpoints: latency-svc-nt9w7 [747.473961ms]
May 17 08:18:53.790: INFO: Created: latency-svc-rqzpf
May 17 08:18:53.833: INFO: Got endpoints: latency-svc-m5x4d [750.181901ms]
May 17 08:18:53.840: INFO: Created: latency-svc-h47nj
May 17 08:18:53.883: INFO: Got endpoints: latency-svc-vfsr5 [749.879988ms]
May 17 08:18:53.891: INFO: Created: latency-svc-gjqsx
May 17 08:18:53.933: INFO: Got endpoints: latency-svc-pjxhv [750.026605ms]
May 17 08:18:53.941: INFO: Created: latency-svc-j2nh2
May 17 08:18:53.984: INFO: Got endpoints: latency-svc-sv7g7 [749.736922ms]
May 17 08:18:53.992: INFO: Created: latency-svc-xcxlf
May 17 08:18:54.034: INFO: Got endpoints: latency-svc-prdfn [750.521994ms]
May 17 08:18:54.042: INFO: Created: latency-svc-tnd8z
May 17 08:18:54.083: INFO: Got endpoints: latency-svc-gcjpt [749.955148ms]
May 17 08:18:54.091: INFO: Created: latency-svc-9hhbl
May 17 08:18:54.133: INFO: Got endpoints: latency-svc-sm8l8 [748.957189ms]
May 17 08:18:54.141: INFO: Created: latency-svc-6twwr
May 17 08:18:54.184: INFO: Got endpoints: latency-svc-nhmt7 [751.525732ms]
May 17 08:18:54.194: INFO: Created: latency-svc-qvzm8
May 17 08:18:54.233: INFO: Got endpoints: latency-svc-zfz8m [748.649867ms]
May 17 08:18:54.241: INFO: Created: latency-svc-55gh7
May 17 08:18:54.283: INFO: Got endpoints: latency-svc-sb2hq [749.88878ms]
May 17 08:18:54.291: INFO: Created: latency-svc-cfdrb
May 17 08:18:54.333: INFO: Got endpoints: latency-svc-vv6nk [749.914814ms]
May 17 08:18:54.341: INFO: Created: latency-svc-gfc7z
May 17 08:18:54.383: INFO: Got endpoints: latency-svc-8mrcf [750.425613ms]
May 17 08:18:54.390: INFO: Created: latency-svc-wmmsm
May 17 08:18:54.433: INFO: Got endpoints: latency-svc-wpv2s [749.956829ms]
May 17 08:18:54.444: INFO: Created: latency-svc-6nq4v
May 17 08:18:54.484: INFO: Got endpoints: latency-svc-rp7gl [749.58941ms]
May 17 08:18:54.492: INFO: Created: latency-svc-2dgnr
May 17 08:18:54.534: INFO: Got endpoints: latency-svc-rqzpf [752.087776ms]
May 17 08:18:54.542: INFO: Created: latency-svc-njq7j
May 17 08:18:54.583: INFO: Got endpoints: latency-svc-h47nj [750.530068ms]
May 17 08:18:54.591: INFO: Created: latency-svc-58zxl
May 17 08:18:54.633: INFO: Got endpoints: latency-svc-gjqsx [749.285477ms]
May 17 08:18:54.640: INFO: Created: latency-svc-nlt9j
May 17 08:18:54.684: INFO: Got endpoints: latency-svc-j2nh2 [751.065216ms]
May 17 08:18:54.692: INFO: Created: latency-svc-chgbx
May 17 08:18:54.734: INFO: Got endpoints: latency-svc-xcxlf [750.671462ms]
May 17 08:18:54.741: INFO: Created: latency-svc-dkkvw
May 17 08:18:54.783: INFO: Got endpoints: latency-svc-tnd8z [749.138508ms]
May 17 08:18:54.791: INFO: Created: latency-svc-jcktl
May 17 08:18:54.834: INFO: Got endpoints: latency-svc-9hhbl [750.435473ms]
May 17 08:18:54.841: INFO: Created: latency-svc-2cmsg
May 17 08:18:54.884: INFO: Got endpoints: latency-svc-6twwr [750.474363ms]
May 17 08:18:54.891: INFO: Created: latency-svc-zxxhj
May 17 08:18:54.933: INFO: Got endpoints: latency-svc-qvzm8 [748.568091ms]
May 17 08:18:54.941: INFO: Created: latency-svc-t24bv
May 17 08:18:54.983: INFO: Got endpoints: latency-svc-55gh7 [749.856484ms]
May 17 08:18:54.990: INFO: Created: latency-svc-qv9pr
May 17 08:18:55.033: INFO: Got endpoints: latency-svc-cfdrb [749.911631ms]
May 17 08:18:55.041: INFO: Created: latency-svc-k294f
May 17 08:18:55.083: INFO: Got endpoints: latency-svc-gfc7z [749.784989ms]
May 17 08:18:55.092: INFO: Created: latency-svc-bqjdm
May 17 08:18:55.134: INFO: Got endpoints: latency-svc-wmmsm [750.877496ms]
May 17 08:18:55.141: INFO: Created: latency-svc-44n5w
May 17 08:18:55.183: INFO: Got endpoints: latency-svc-6nq4v [749.55253ms]
May 17 08:18:55.190: INFO: Created: latency-svc-rvm4k
May 17 08:18:55.232: INFO: Got endpoints: latency-svc-2dgnr [748.661435ms]
May 17 08:18:55.240: INFO: Created: latency-svc-4n4rb
May 17 08:18:55.283: INFO: Got endpoints: latency-svc-njq7j [748.616836ms]
May 17 08:18:55.333: INFO: Got endpoints: latency-svc-58zxl [749.257169ms]
May 17 08:18:55.382: INFO: Got endpoints: latency-svc-nlt9j [749.474248ms]
May 17 08:18:55.432: INFO: Got endpoints: latency-svc-chgbx [748.271194ms]
May 17 08:18:55.483: INFO: Got endpoints: latency-svc-dkkvw [748.759583ms]
May 17 08:18:55.533: INFO: Got endpoints: latency-svc-jcktl [749.644549ms]
May 17 08:18:55.584: INFO: Got endpoints: latency-svc-2cmsg [750.43349ms]
May 17 08:18:55.632: INFO: Got endpoints: latency-svc-zxxhj [748.739111ms]
May 17 08:18:55.682: INFO: Got endpoints: latency-svc-t24bv [749.564135ms]
May 17 08:18:55.734: INFO: Got endpoints: latency-svc-qv9pr [751.538686ms]
May 17 08:18:55.783: INFO: Got endpoints: latency-svc-k294f [749.535234ms]
May 17 08:18:55.834: INFO: Got endpoints: latency-svc-bqjdm [750.562492ms]
May 17 08:18:55.883: INFO: Got endpoints: latency-svc-44n5w [749.04274ms]
May 17 08:18:55.933: INFO: Got endpoints: latency-svc-rvm4k [750.609817ms]
May 17 08:18:55.984: INFO: Got endpoints: latency-svc-4n4rb [751.256009ms]
May 17 08:18:55.984: INFO: Latencies: [12.734864ms 17.429081ms 21.58481ms 27.744609ms 33.081515ms 35.639726ms 43.506126ms 48.747526ms 51.04444ms 56.902388ms 62.481094ms 70.173247ms 73.528075ms 75.636462ms 77.355182ms 81.167138ms 85.690654ms 86.699254ms 86.891426ms 87.580143ms 88.534544ms 88.834541ms 89.1555ms 91.436235ms 92.482507ms 93.925368ms 94.459913ms 95.77392ms 95.885652ms 96.821784ms 97.400529ms 98.29054ms 100.180497ms 134.010311ms 178.972834ms 223.825624ms 268.424022ms 304.959286ms 347.465138ms 393.120231ms 439.553341ms 487.062069ms 530.573203ms 580.543524ms 620.39367ms 664.364236ms 710.360756ms 745.692044ms 746.86181ms 747.353597ms 747.473961ms 747.639598ms 747.915373ms 748.104241ms 748.258488ms 748.271194ms 748.286078ms 748.320221ms 748.340292ms 748.462446ms 748.512941ms 748.568091ms 748.616836ms 748.649867ms 748.655208ms 748.661435ms 748.739111ms 748.756827ms 748.759583ms 748.80439ms 748.842024ms 748.917481ms 748.919645ms 748.93463ms 748.957189ms 748.98184ms 749.04274ms 749.070317ms 749.103999ms 749.120522ms 749.131487ms 749.138508ms 749.19319ms 749.208519ms 749.255019ms 749.257169ms 749.285477ms 749.314887ms 749.422609ms 749.436971ms 749.465626ms 749.474248ms 749.524642ms 749.535234ms 749.549579ms 749.550095ms 749.55253ms 749.559853ms 749.564135ms 749.571326ms 749.58941ms 749.593727ms 749.598382ms 749.603696ms 749.618938ms 749.644549ms 749.667564ms 749.710152ms 749.736922ms 749.758456ms 749.767514ms 749.772346ms 749.781595ms 749.784989ms 749.833404ms 749.83678ms 749.856484ms 749.866524ms 749.879988ms 749.884993ms 749.88878ms 749.911276ms 749.911631ms 749.914814ms 749.947953ms 749.955148ms 749.956829ms 750.026605ms 750.028005ms 750.041602ms 750.043186ms 750.070267ms 750.08664ms 750.098575ms 750.111055ms 750.118997ms 750.119012ms 750.123889ms 750.156143ms 750.157786ms 750.181901ms 750.191743ms 750.264259ms 750.267917ms 750.276344ms 750.299062ms 750.301885ms 750.339531ms 750.375594ms 750.397483ms 750.418076ms 750.425613ms 750.43349ms 750.435473ms 750.464676ms 750.474363ms 750.477479ms 750.521994ms 750.530068ms 750.531772ms 750.562492ms 750.600193ms 750.609817ms 750.615617ms 750.629343ms 750.637172ms 750.638631ms 750.671462ms 750.741648ms 750.877496ms 750.940012ms 750.958499ms 751.064215ms 751.065216ms 751.075971ms 751.156614ms 751.168506ms 751.254212ms 751.256009ms 751.264527ms 751.264856ms 751.277143ms 751.277712ms 751.351042ms 751.394275ms 751.467504ms 751.509515ms 751.525732ms 751.526715ms 751.538686ms 751.556976ms 751.614197ms 751.678493ms 751.888558ms 752.061852ms 752.087776ms 752.229701ms 752.41036ms 752.557583ms 752.681847ms]
May 17 08:18:55.984: INFO: 50 %ile: 749.58941ms
May 17 08:18:55.984: INFO: 90 %ile: 751.264856ms
May 17 08:18:55.984: INFO: 99 %ile: 752.557583ms
May 17 08:18:55.984: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
May 17 08:18:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-1865" for this suite. 05/17/23 08:18:55.988
------------------------------
â€¢ [SLOW TEST] [10.735 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:18:45.256
    May 17 08:18:45.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svc-latency 05/17/23 08:18:45.257
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:45.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:45.265
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May 17 08:18:45.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1865 05/17/23 08:18:45.267
    I0517 08:18:45.271594      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1865, replica count: 1
    I0517 08:18:46.322348      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0517 08:18:47.323424      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 08:18:47.429: INFO: Created: latency-svc-gzwvx
    May 17 08:18:47.434: INFO: Got endpoints: latency-svc-gzwvx [10.418872ms]
    May 17 08:18:47.441: INFO: Created: latency-svc-wb4ft
    May 17 08:18:47.447: INFO: Got endpoints: latency-svc-wb4ft [12.734864ms]
    May 17 08:18:47.447: INFO: Created: latency-svc-hxfzv
    May 17 08:18:47.451: INFO: Got endpoints: latency-svc-hxfzv [17.429081ms]
    May 17 08:18:47.452: INFO: Created: latency-svc-nzrsm
    May 17 08:18:47.455: INFO: Got endpoints: latency-svc-nzrsm [21.58481ms]
    May 17 08:18:47.457: INFO: Created: latency-svc-l5gjt
    May 17 08:18:47.461: INFO: Got endpoints: latency-svc-l5gjt [27.744609ms]
    May 17 08:18:47.463: INFO: Created: latency-svc-9jqkw
    May 17 08:18:47.467: INFO: Got endpoints: latency-svc-9jqkw [33.081515ms]
    May 17 08:18:47.468: INFO: Created: latency-svc-zdlkk
    May 17 08:18:47.469: INFO: Got endpoints: latency-svc-zdlkk [35.639726ms]
    May 17 08:18:47.473: INFO: Created: latency-svc-8n75v
    May 17 08:18:47.477: INFO: Got endpoints: latency-svc-8n75v [43.506126ms]
    May 17 08:18:47.478: INFO: Created: latency-svc-rmt9m
    May 17 08:18:47.483: INFO: Got endpoints: latency-svc-rmt9m [48.747526ms]
    May 17 08:18:47.483: INFO: Created: latency-svc-t4hsv
    May 17 08:18:47.485: INFO: Got endpoints: latency-svc-t4hsv [51.04444ms]
    May 17 08:18:47.488: INFO: Created: latency-svc-jdfzs
    May 17 08:18:47.491: INFO: Got endpoints: latency-svc-jdfzs [56.902388ms]
    May 17 08:18:47.495: INFO: Created: latency-svc-92524
    May 17 08:18:47.496: INFO: Got endpoints: latency-svc-92524 [62.481094ms]
    May 17 08:18:47.500: INFO: Created: latency-svc-prhsd
    May 17 08:18:47.504: INFO: Got endpoints: latency-svc-prhsd [70.173247ms]
    May 17 08:18:47.505: INFO: Created: latency-svc-n7nn8
    May 17 08:18:47.507: INFO: Got endpoints: latency-svc-n7nn8 [73.528075ms]
    May 17 08:18:47.511: INFO: Created: latency-svc-sbssn
    May 17 08:18:47.515: INFO: Got endpoints: latency-svc-sbssn [81.167138ms]
    May 17 08:18:47.516: INFO: Created: latency-svc-4jcnz
    May 17 08:18:47.519: INFO: Got endpoints: latency-svc-4jcnz [85.690654ms]
    May 17 08:18:47.521: INFO: Created: latency-svc-6nv92
    May 17 08:18:47.524: INFO: Got endpoints: latency-svc-6nv92 [77.355182ms]
    May 17 08:18:47.525: INFO: Created: latency-svc-76ljp
    May 17 08:18:47.527: INFO: Got endpoints: latency-svc-76ljp [75.636462ms]
    May 17 08:18:47.533: INFO: Created: latency-svc-x8vxx
    May 17 08:18:47.544: INFO: Got endpoints: latency-svc-x8vxx [88.534544ms]
    May 17 08:18:47.546: INFO: Created: latency-svc-8ctzb
    May 17 08:18:47.549: INFO: Got endpoints: latency-svc-8ctzb [87.580143ms]
    May 17 08:18:47.550: INFO: Created: latency-svc-6h7vj
    May 17 08:18:47.554: INFO: Got endpoints: latency-svc-6h7vj [86.891426ms]
    May 17 08:18:47.555: INFO: Created: latency-svc-4f4rx
    May 17 08:18:47.558: INFO: Got endpoints: latency-svc-4f4rx [89.1555ms]
    May 17 08:18:47.562: INFO: Created: latency-svc-mcs4t
    May 17 08:18:47.564: INFO: Got endpoints: latency-svc-mcs4t [86.699254ms]
    May 17 08:18:47.574: INFO: Created: latency-svc-2qtqd
    May 17 08:18:47.579: INFO: Got endpoints: latency-svc-2qtqd [96.821784ms]
    May 17 08:18:47.581: INFO: Created: latency-svc-c2t68
    May 17 08:18:47.585: INFO: Got endpoints: latency-svc-c2t68 [100.180497ms]
    May 17 08:18:47.586: INFO: Created: latency-svc-7kchq
    May 17 08:18:47.589: INFO: Got endpoints: latency-svc-7kchq [98.29054ms]
    May 17 08:18:47.590: INFO: Created: latency-svc-mp9rn
    May 17 08:18:47.594: INFO: Got endpoints: latency-svc-mp9rn [97.400529ms]
    May 17 08:18:47.595: INFO: Created: latency-svc-j66hj
    May 17 08:18:47.596: INFO: Got endpoints: latency-svc-j66hj [92.482507ms]
    May 17 08:18:47.600: INFO: Created: latency-svc-vz9fn
    May 17 08:18:47.603: INFO: Got endpoints: latency-svc-vz9fn [95.885652ms]
    May 17 08:18:47.604: INFO: Created: latency-svc-hvlc5
    May 17 08:18:47.606: INFO: Got endpoints: latency-svc-hvlc5 [91.436235ms]
    May 17 08:18:47.610: INFO: Created: latency-svc-5flb7
    May 17 08:18:47.614: INFO: Got endpoints: latency-svc-5flb7 [94.459913ms]
    May 17 08:18:47.615: INFO: Created: latency-svc-x8tl8
    May 17 08:18:47.618: INFO: Got endpoints: latency-svc-x8tl8 [93.925368ms]
    May 17 08:18:47.620: INFO: Created: latency-svc-2dl96
    May 17 08:18:47.623: INFO: Got endpoints: latency-svc-2dl96 [95.77392ms]
    May 17 08:18:47.623: INFO: Created: latency-svc-b5wb6
    May 17 08:18:47.628: INFO: Created: latency-svc-k8vt2
    May 17 08:18:47.633: INFO: Got endpoints: latency-svc-b5wb6 [88.834541ms]
    May 17 08:18:47.633: INFO: Created: latency-svc-6ppdt
    May 17 08:18:47.638: INFO: Created: latency-svc-lnszx
    May 17 08:18:47.642: INFO: Created: latency-svc-4t6r5
    May 17 08:18:47.660: INFO: Created: latency-svc-dhgdq
    May 17 08:18:47.663: INFO: Created: latency-svc-77dhk
    May 17 08:18:47.667: INFO: Created: latency-svc-stbtw
    May 17 08:18:47.671: INFO: Created: latency-svc-thccd
    May 17 08:18:47.675: INFO: Created: latency-svc-xwsx7
    May 17 08:18:47.679: INFO: Created: latency-svc-ljm2n
    May 17 08:18:47.683: INFO: Got endpoints: latency-svc-k8vt2 [134.010311ms]
    May 17 08:18:47.684: INFO: Created: latency-svc-pxcrn
    May 17 08:18:47.688: INFO: Created: latency-svc-h7qd9
    May 17 08:18:47.693: INFO: Created: latency-svc-zs5jm
    May 17 08:18:47.697: INFO: Created: latency-svc-x7vff
    May 17 08:18:47.701: INFO: Created: latency-svc-bg4lb
    May 17 08:18:47.706: INFO: Created: latency-svc-2x4kl
    May 17 08:18:47.733: INFO: Got endpoints: latency-svc-6ppdt [178.972834ms]
    May 17 08:18:47.740: INFO: Created: latency-svc-6f9vk
    May 17 08:18:47.782: INFO: Got endpoints: latency-svc-lnszx [223.825624ms]
    May 17 08:18:47.790: INFO: Created: latency-svc-qrwcn
    May 17 08:18:47.832: INFO: Got endpoints: latency-svc-4t6r5 [268.424022ms]
    May 17 08:18:47.842: INFO: Created: latency-svc-zwwhp
    May 17 08:18:47.884: INFO: Got endpoints: latency-svc-dhgdq [304.959286ms]
    May 17 08:18:47.892: INFO: Created: latency-svc-rprbx
    May 17 08:18:47.932: INFO: Got endpoints: latency-svc-77dhk [347.465138ms]
    May 17 08:18:47.940: INFO: Created: latency-svc-qwf9d
    May 17 08:18:47.982: INFO: Got endpoints: latency-svc-stbtw [393.120231ms]
    May 17 08:18:47.990: INFO: Created: latency-svc-kpg96
    May 17 08:18:48.033: INFO: Got endpoints: latency-svc-thccd [439.553341ms]
    May 17 08:18:48.041: INFO: Created: latency-svc-95bm4
    May 17 08:18:48.083: INFO: Got endpoints: latency-svc-xwsx7 [487.062069ms]
    May 17 08:18:48.091: INFO: Created: latency-svc-4scrb
    May 17 08:18:48.134: INFO: Got endpoints: latency-svc-ljm2n [530.573203ms]
    May 17 08:18:48.141: INFO: Created: latency-svc-sqmzw
    May 17 08:18:48.187: INFO: Got endpoints: latency-svc-pxcrn [580.543524ms]
    May 17 08:18:48.195: INFO: Created: latency-svc-vbn4v
    May 17 08:18:48.234: INFO: Got endpoints: latency-svc-h7qd9 [620.39367ms]
    May 17 08:18:48.242: INFO: Created: latency-svc-4mr9d
    May 17 08:18:48.282: INFO: Got endpoints: latency-svc-zs5jm [664.364236ms]
    May 17 08:18:48.294: INFO: Created: latency-svc-h2dhz
    May 17 08:18:48.333: INFO: Got endpoints: latency-svc-x7vff [710.360756ms]
    May 17 08:18:48.340: INFO: Created: latency-svc-66kwk
    May 17 08:18:48.384: INFO: Got endpoints: latency-svc-bg4lb [751.254212ms]
    May 17 08:18:48.391: INFO: Created: latency-svc-vj8sw
    May 17 08:18:48.433: INFO: Got endpoints: latency-svc-2x4kl [749.758456ms]
    May 17 08:18:48.441: INFO: Created: latency-svc-74hf4
    May 17 08:18:48.482: INFO: Got endpoints: latency-svc-6f9vk [749.618938ms]
    May 17 08:18:48.490: INFO: Created: latency-svc-4r5tj
    May 17 08:18:48.534: INFO: Got endpoints: latency-svc-qrwcn [751.614197ms]
    May 17 08:18:48.542: INFO: Created: latency-svc-dnx8d
    May 17 08:18:48.583: INFO: Got endpoints: latency-svc-zwwhp [750.123889ms]
    May 17 08:18:48.590: INFO: Created: latency-svc-rbtjt
    May 17 08:18:48.633: INFO: Got endpoints: latency-svc-rprbx [748.286078ms]
    May 17 08:18:48.640: INFO: Created: latency-svc-q4h5f
    May 17 08:18:48.684: INFO: Got endpoints: latency-svc-qwf9d [751.277143ms]
    May 17 08:18:48.691: INFO: Created: latency-svc-8qjpw
    May 17 08:18:48.732: INFO: Got endpoints: latency-svc-kpg96 [750.156143ms]
    May 17 08:18:48.740: INFO: Created: latency-svc-p9vv8
    May 17 08:18:48.783: INFO: Got endpoints: latency-svc-95bm4 [749.772346ms]
    May 17 08:18:48.792: INFO: Created: latency-svc-c2s8w
    May 17 08:18:48.834: INFO: Got endpoints: latency-svc-4scrb [750.191743ms]
    May 17 08:18:48.841: INFO: Created: latency-svc-gz4mb
    May 17 08:18:48.882: INFO: Got endpoints: latency-svc-sqmzw [748.756827ms]
    May 17 08:18:48.890: INFO: Created: latency-svc-cdxjf
    May 17 08:18:48.933: INFO: Got endpoints: latency-svc-vbn4v [745.692044ms]
    May 17 08:18:48.941: INFO: Created: latency-svc-wp5jn
    May 17 08:18:48.982: INFO: Got endpoints: latency-svc-4mr9d [748.104241ms]
    May 17 08:18:48.991: INFO: Created: latency-svc-vtjzb
    May 17 08:18:49.034: INFO: Got endpoints: latency-svc-h2dhz [751.526715ms]
    May 17 08:18:49.041: INFO: Created: latency-svc-kxzc4
    May 17 08:18:49.083: INFO: Got endpoints: latency-svc-66kwk [749.911276ms]
    May 17 08:18:49.091: INFO: Created: latency-svc-mz82l
    May 17 08:18:49.133: INFO: Got endpoints: latency-svc-vj8sw [749.131487ms]
    May 17 08:18:49.141: INFO: Created: latency-svc-5wtvh
    May 17 08:18:49.183: INFO: Got endpoints: latency-svc-74hf4 [750.028005ms]
    May 17 08:18:49.190: INFO: Created: latency-svc-26k2q
    May 17 08:18:49.234: INFO: Got endpoints: latency-svc-4r5tj [751.277712ms]
    May 17 08:18:49.240: INFO: Created: latency-svc-smtg5
    May 17 08:18:49.283: INFO: Got endpoints: latency-svc-dnx8d [748.93463ms]
    May 17 08:18:49.291: INFO: Created: latency-svc-59h8t
    May 17 08:18:49.333: INFO: Got endpoints: latency-svc-rbtjt [750.157786ms]
    May 17 08:18:49.340: INFO: Created: latency-svc-pc4rw
    May 17 08:18:49.383: INFO: Got endpoints: latency-svc-q4h5f [750.111055ms]
    May 17 08:18:49.390: INFO: Created: latency-svc-pv2q7
    May 17 08:18:49.436: INFO: Got endpoints: latency-svc-8qjpw [752.061852ms]
    May 17 08:18:49.444: INFO: Created: latency-svc-nqlpf
    May 17 08:18:49.484: INFO: Got endpoints: latency-svc-p9vv8 [751.168506ms]
    May 17 08:18:49.491: INFO: Created: latency-svc-f4rqt
    May 17 08:18:49.533: INFO: Got endpoints: latency-svc-c2s8w [750.118997ms]
    May 17 08:18:49.556: INFO: Created: latency-svc-fhshr
    May 17 08:18:49.584: INFO: Got endpoints: latency-svc-gz4mb [750.276344ms]
    May 17 08:18:49.591: INFO: Created: latency-svc-w699n
    May 17 08:18:49.635: INFO: Got endpoints: latency-svc-cdxjf [752.229701ms]
    May 17 08:18:49.643: INFO: Created: latency-svc-rsm7q
    May 17 08:18:49.684: INFO: Got endpoints: latency-svc-wp5jn [751.509515ms]
    May 17 08:18:49.693: INFO: Created: latency-svc-d7xnn
    May 17 08:18:49.733: INFO: Got endpoints: latency-svc-vtjzb [750.629343ms]
    May 17 08:18:49.741: INFO: Created: latency-svc-m77l7
    May 17 08:18:49.784: INFO: Got endpoints: latency-svc-kxzc4 [750.119012ms]
    May 17 08:18:49.791: INFO: Created: latency-svc-5cjqs
    May 17 08:18:49.833: INFO: Got endpoints: latency-svc-mz82l [750.043186ms]
    May 17 08:18:49.841: INFO: Created: latency-svc-8cnjm
    May 17 08:18:49.882: INFO: Got endpoints: latency-svc-5wtvh [749.255019ms]
    May 17 08:18:49.889: INFO: Created: latency-svc-kz8zx
    May 17 08:18:49.933: INFO: Got endpoints: latency-svc-26k2q [750.070267ms]
    May 17 08:18:49.940: INFO: Created: latency-svc-2hsz7
    May 17 08:18:49.983: INFO: Got endpoints: latency-svc-smtg5 [748.80439ms]
    May 17 08:18:49.991: INFO: Created: latency-svc-fpl6t
    May 17 08:18:50.033: INFO: Got endpoints: latency-svc-59h8t [749.550095ms]
    May 17 08:18:50.040: INFO: Created: latency-svc-tvnhx
    May 17 08:18:50.083: INFO: Got endpoints: latency-svc-pc4rw [750.397483ms]
    May 17 08:18:50.090: INFO: Created: latency-svc-chd47
    May 17 08:18:50.134: INFO: Got endpoints: latency-svc-pv2q7 [751.264856ms]
    May 17 08:18:50.142: INFO: Created: latency-svc-wcvz4
    May 17 08:18:50.183: INFO: Got endpoints: latency-svc-nqlpf [747.353597ms]
    May 17 08:18:50.190: INFO: Created: latency-svc-rg5bv
    May 17 08:18:50.233: INFO: Got endpoints: latency-svc-f4rqt [749.83678ms]
    May 17 08:18:50.240: INFO: Created: latency-svc-s5ls2
    May 17 08:18:50.282: INFO: Got endpoints: latency-svc-fhshr [748.919645ms]
    May 17 08:18:50.290: INFO: Created: latency-svc-mbdst
    May 17 08:18:50.332: INFO: Got endpoints: latency-svc-w699n [748.258488ms]
    May 17 08:18:50.340: INFO: Created: latency-svc-x5kmv
    May 17 08:18:50.383: INFO: Got endpoints: latency-svc-rsm7q [748.512941ms]
    May 17 08:18:50.390: INFO: Created: latency-svc-nmf6z
    May 17 08:18:50.434: INFO: Got endpoints: latency-svc-d7xnn [749.598382ms]
    May 17 08:18:50.442: INFO: Created: latency-svc-c2796
    May 17 08:18:50.485: INFO: Got endpoints: latency-svc-m77l7 [752.41036ms]
    May 17 08:18:50.493: INFO: Created: latency-svc-r782n
    May 17 08:18:50.533: INFO: Got endpoints: latency-svc-5cjqs [748.98184ms]
    May 17 08:18:50.541: INFO: Created: latency-svc-4xgw8
    May 17 08:18:50.583: INFO: Got endpoints: latency-svc-8cnjm [749.465626ms]
    May 17 08:18:50.591: INFO: Created: latency-svc-nl6t4
    May 17 08:18:50.634: INFO: Got endpoints: latency-svc-kz8zx [751.467504ms]
    May 17 08:18:50.641: INFO: Created: latency-svc-q7kxb
    May 17 08:18:50.683: INFO: Got endpoints: latency-svc-2hsz7 [750.339531ms]
    May 17 08:18:50.696: INFO: Created: latency-svc-2jmz6
    May 17 08:18:50.733: INFO: Got endpoints: latency-svc-fpl6t [750.098575ms]
    May 17 08:18:50.740: INFO: Created: latency-svc-lssnv
    May 17 08:18:50.784: INFO: Got endpoints: latency-svc-tvnhx [751.888558ms]
    May 17 08:18:50.792: INFO: Created: latency-svc-lbvcg
    May 17 08:18:50.833: INFO: Got endpoints: latency-svc-chd47 [749.710152ms]
    May 17 08:18:50.841: INFO: Created: latency-svc-kvv6t
    May 17 08:18:50.882: INFO: Got endpoints: latency-svc-wcvz4 [748.340292ms]
    May 17 08:18:50.890: INFO: Created: latency-svc-sph9l
    May 17 08:18:50.934: INFO: Got endpoints: latency-svc-rg5bv [750.301885ms]
    May 17 08:18:50.942: INFO: Created: latency-svc-65qlm
    May 17 08:18:50.983: INFO: Got endpoints: latency-svc-s5ls2 [749.19319ms]
    May 17 08:18:50.990: INFO: Created: latency-svc-m48j8
    May 17 08:18:51.034: INFO: Got endpoints: latency-svc-mbdst [751.351042ms]
    May 17 08:18:51.042: INFO: Created: latency-svc-cqjph
    May 17 08:18:51.083: INFO: Got endpoints: latency-svc-x5kmv [750.637172ms]
    May 17 08:18:51.091: INFO: Created: latency-svc-sx7w2
    May 17 08:18:51.133: INFO: Got endpoints: latency-svc-nmf6z [749.549579ms]
    May 17 08:18:51.140: INFO: Created: latency-svc-57bfj
    May 17 08:18:51.184: INFO: Got endpoints: latency-svc-c2796 [749.767514ms]
    May 17 08:18:51.192: INFO: Created: latency-svc-cx56k
    May 17 08:18:51.233: INFO: Got endpoints: latency-svc-r782n [747.639598ms]
    May 17 08:18:51.241: INFO: Created: latency-svc-94dbd
    May 17 08:18:51.284: INFO: Got endpoints: latency-svc-4xgw8 [750.940012ms]
    May 17 08:18:51.291: INFO: Created: latency-svc-8grs6
    May 17 08:18:51.334: INFO: Got endpoints: latency-svc-nl6t4 [751.156614ms]
    May 17 08:18:51.341: INFO: Created: latency-svc-rf2mm
    May 17 08:18:51.384: INFO: Got endpoints: latency-svc-q7kxb [749.947953ms]
    May 17 08:18:51.391: INFO: Created: latency-svc-k78rz
    May 17 08:18:51.434: INFO: Got endpoints: latency-svc-2jmz6 [750.264259ms]
    May 17 08:18:51.441: INFO: Created: latency-svc-6sqjn
    May 17 08:18:51.483: INFO: Got endpoints: latency-svc-lssnv [750.638631ms]
    May 17 08:18:51.492: INFO: Created: latency-svc-9nqb6
    May 17 08:18:51.534: INFO: Got endpoints: latency-svc-lbvcg [749.103999ms]
    May 17 08:18:51.541: INFO: Created: latency-svc-h4lj5
    May 17 08:18:51.583: INFO: Got endpoints: latency-svc-kvv6t [750.041602ms]
    May 17 08:18:51.595: INFO: Created: latency-svc-gw2n6
    May 17 08:18:51.634: INFO: Got endpoints: latency-svc-sph9l [751.264527ms]
    May 17 08:18:51.642: INFO: Created: latency-svc-gptfh
    May 17 08:18:51.685: INFO: Got endpoints: latency-svc-65qlm [750.958499ms]
    May 17 08:18:51.692: INFO: Created: latency-svc-s82lx
    May 17 08:18:51.735: INFO: Got endpoints: latency-svc-m48j8 [752.557583ms]
    May 17 08:18:51.743: INFO: Created: latency-svc-jlspg
    May 17 08:18:51.783: INFO: Got endpoints: latency-svc-cqjph [749.571326ms]
    May 17 08:18:51.791: INFO: Created: latency-svc-klbnk
    May 17 08:18:51.833: INFO: Got endpoints: latency-svc-sx7w2 [749.603696ms]
    May 17 08:18:51.841: INFO: Created: latency-svc-hnzgs
    May 17 08:18:51.884: INFO: Got endpoints: latency-svc-57bfj [751.394275ms]
    May 17 08:18:51.892: INFO: Created: latency-svc-bxvrh
    May 17 08:18:51.933: INFO: Got endpoints: latency-svc-cx56k [749.559853ms]
    May 17 08:18:51.946: INFO: Created: latency-svc-29wrr
    May 17 08:18:51.983: INFO: Got endpoints: latency-svc-94dbd [749.781595ms]
    May 17 08:18:51.991: INFO: Created: latency-svc-m9tld
    May 17 08:18:52.033: INFO: Got endpoints: latency-svc-8grs6 [749.422609ms]
    May 17 08:18:52.041: INFO: Created: latency-svc-c46xx
    May 17 08:18:52.084: INFO: Got endpoints: latency-svc-rf2mm [750.615617ms]
    May 17 08:18:52.093: INFO: Created: latency-svc-6zw65
    May 17 08:18:52.134: INFO: Got endpoints: latency-svc-k78rz [749.833404ms]
    May 17 08:18:52.142: INFO: Created: latency-svc-w2654
    May 17 08:18:52.184: INFO: Got endpoints: latency-svc-6sqjn [750.375594ms]
    May 17 08:18:52.192: INFO: Created: latency-svc-mf6ns
    May 17 08:18:52.234: INFO: Got endpoints: latency-svc-9nqb6 [751.075971ms]
    May 17 08:18:52.243: INFO: Created: latency-svc-dmgqm
    May 17 08:18:52.284: INFO: Got endpoints: latency-svc-h4lj5 [750.477479ms]
    May 17 08:18:52.292: INFO: Created: latency-svc-lll62
    May 17 08:18:52.335: INFO: Got endpoints: latency-svc-gw2n6 [751.678493ms]
    May 17 08:18:52.343: INFO: Created: latency-svc-xps7m
    May 17 08:18:52.383: INFO: Got endpoints: latency-svc-gptfh [749.120522ms]
    May 17 08:18:52.392: INFO: Created: latency-svc-5pjjh
    May 17 08:18:52.434: INFO: Got endpoints: latency-svc-s82lx [749.866524ms]
    May 17 08:18:52.444: INFO: Created: latency-svc-pprwn
    May 17 08:18:52.484: INFO: Got endpoints: latency-svc-jlspg [748.320221ms]
    May 17 08:18:52.492: INFO: Created: latency-svc-4cs7v
    May 17 08:18:52.534: INFO: Got endpoints: latency-svc-klbnk [750.418076ms]
    May 17 08:18:52.542: INFO: Created: latency-svc-8tb8k
    May 17 08:18:52.583: INFO: Got endpoints: latency-svc-hnzgs [750.531772ms]
    May 17 08:18:52.592: INFO: Created: latency-svc-ldmcf
    May 17 08:18:52.634: INFO: Got endpoints: latency-svc-bxvrh [749.436971ms]
    May 17 08:18:52.642: INFO: Created: latency-svc-2zf9r
    May 17 08:18:52.684: INFO: Got endpoints: latency-svc-29wrr [750.741648ms]
    May 17 08:18:52.707: INFO: Created: latency-svc-m4f66
    May 17 08:18:52.735: INFO: Got endpoints: latency-svc-m9tld [751.556976ms]
    May 17 08:18:52.742: INFO: Created: latency-svc-b766w
    May 17 08:18:52.782: INFO: Got endpoints: latency-svc-c46xx [748.917481ms]
    May 17 08:18:52.791: INFO: Created: latency-svc-f2hv2
    May 17 08:18:52.834: INFO: Got endpoints: latency-svc-6zw65 [749.667564ms]
    May 17 08:18:52.842: INFO: Created: latency-svc-qfh4n
    May 17 08:18:52.883: INFO: Got endpoints: latency-svc-w2654 [748.842024ms]
    May 17 08:18:52.894: INFO: Created: latency-svc-wv9b2
    May 17 08:18:52.933: INFO: Got endpoints: latency-svc-mf6ns [749.070317ms]
    May 17 08:18:52.941: INFO: Created: latency-svc-gk5g8
    May 17 08:18:52.987: INFO: Got endpoints: latency-svc-dmgqm [752.681847ms]
    May 17 08:18:52.995: INFO: Created: latency-svc-jg8db
    May 17 08:18:53.035: INFO: Got endpoints: latency-svc-lll62 [750.600193ms]
    May 17 08:18:53.044: INFO: Created: latency-svc-nt9w7
    May 17 08:18:53.083: INFO: Got endpoints: latency-svc-xps7m [747.915373ms]
    May 17 08:18:53.095: INFO: Created: latency-svc-m5x4d
    May 17 08:18:53.134: INFO: Got endpoints: latency-svc-5pjjh [750.464676ms]
    May 17 08:18:53.143: INFO: Created: latency-svc-vfsr5
    May 17 08:18:53.183: INFO: Got endpoints: latency-svc-pprwn [748.462446ms]
    May 17 08:18:53.191: INFO: Created: latency-svc-pjxhv
    May 17 08:18:53.234: INFO: Got endpoints: latency-svc-4cs7v [750.08664ms]
    May 17 08:18:53.242: INFO: Created: latency-svc-sv7g7
    May 17 08:18:53.283: INFO: Got endpoints: latency-svc-8tb8k [749.524642ms]
    May 17 08:18:53.291: INFO: Created: latency-svc-prdfn
    May 17 08:18:53.333: INFO: Got endpoints: latency-svc-ldmcf [750.267917ms]
    May 17 08:18:53.341: INFO: Created: latency-svc-gcjpt
    May 17 08:18:53.384: INFO: Got endpoints: latency-svc-2zf9r [750.299062ms]
    May 17 08:18:53.392: INFO: Created: latency-svc-sm8l8
    May 17 08:18:53.433: INFO: Got endpoints: latency-svc-m4f66 [748.655208ms]
    May 17 08:18:53.441: INFO: Created: latency-svc-nhmt7
    May 17 08:18:53.484: INFO: Got endpoints: latency-svc-b766w [749.314887ms]
    May 17 08:18:53.491: INFO: Created: latency-svc-zfz8m
    May 17 08:18:53.534: INFO: Got endpoints: latency-svc-f2hv2 [751.064215ms]
    May 17 08:18:53.541: INFO: Created: latency-svc-sb2hq
    May 17 08:18:53.583: INFO: Got endpoints: latency-svc-qfh4n [749.208519ms]
    May 17 08:18:53.591: INFO: Created: latency-svc-vv6nk
    May 17 08:18:53.632: INFO: Got endpoints: latency-svc-wv9b2 [749.593727ms]
    May 17 08:18:53.640: INFO: Created: latency-svc-8mrcf
    May 17 08:18:53.683: INFO: Got endpoints: latency-svc-gk5g8 [749.884993ms]
    May 17 08:18:53.691: INFO: Created: latency-svc-wpv2s
    May 17 08:18:53.734: INFO: Got endpoints: latency-svc-jg8db [746.86181ms]
    May 17 08:18:53.741: INFO: Created: latency-svc-rp7gl
    May 17 08:18:53.782: INFO: Got endpoints: latency-svc-nt9w7 [747.473961ms]
    May 17 08:18:53.790: INFO: Created: latency-svc-rqzpf
    May 17 08:18:53.833: INFO: Got endpoints: latency-svc-m5x4d [750.181901ms]
    May 17 08:18:53.840: INFO: Created: latency-svc-h47nj
    May 17 08:18:53.883: INFO: Got endpoints: latency-svc-vfsr5 [749.879988ms]
    May 17 08:18:53.891: INFO: Created: latency-svc-gjqsx
    May 17 08:18:53.933: INFO: Got endpoints: latency-svc-pjxhv [750.026605ms]
    May 17 08:18:53.941: INFO: Created: latency-svc-j2nh2
    May 17 08:18:53.984: INFO: Got endpoints: latency-svc-sv7g7 [749.736922ms]
    May 17 08:18:53.992: INFO: Created: latency-svc-xcxlf
    May 17 08:18:54.034: INFO: Got endpoints: latency-svc-prdfn [750.521994ms]
    May 17 08:18:54.042: INFO: Created: latency-svc-tnd8z
    May 17 08:18:54.083: INFO: Got endpoints: latency-svc-gcjpt [749.955148ms]
    May 17 08:18:54.091: INFO: Created: latency-svc-9hhbl
    May 17 08:18:54.133: INFO: Got endpoints: latency-svc-sm8l8 [748.957189ms]
    May 17 08:18:54.141: INFO: Created: latency-svc-6twwr
    May 17 08:18:54.184: INFO: Got endpoints: latency-svc-nhmt7 [751.525732ms]
    May 17 08:18:54.194: INFO: Created: latency-svc-qvzm8
    May 17 08:18:54.233: INFO: Got endpoints: latency-svc-zfz8m [748.649867ms]
    May 17 08:18:54.241: INFO: Created: latency-svc-55gh7
    May 17 08:18:54.283: INFO: Got endpoints: latency-svc-sb2hq [749.88878ms]
    May 17 08:18:54.291: INFO: Created: latency-svc-cfdrb
    May 17 08:18:54.333: INFO: Got endpoints: latency-svc-vv6nk [749.914814ms]
    May 17 08:18:54.341: INFO: Created: latency-svc-gfc7z
    May 17 08:18:54.383: INFO: Got endpoints: latency-svc-8mrcf [750.425613ms]
    May 17 08:18:54.390: INFO: Created: latency-svc-wmmsm
    May 17 08:18:54.433: INFO: Got endpoints: latency-svc-wpv2s [749.956829ms]
    May 17 08:18:54.444: INFO: Created: latency-svc-6nq4v
    May 17 08:18:54.484: INFO: Got endpoints: latency-svc-rp7gl [749.58941ms]
    May 17 08:18:54.492: INFO: Created: latency-svc-2dgnr
    May 17 08:18:54.534: INFO: Got endpoints: latency-svc-rqzpf [752.087776ms]
    May 17 08:18:54.542: INFO: Created: latency-svc-njq7j
    May 17 08:18:54.583: INFO: Got endpoints: latency-svc-h47nj [750.530068ms]
    May 17 08:18:54.591: INFO: Created: latency-svc-58zxl
    May 17 08:18:54.633: INFO: Got endpoints: latency-svc-gjqsx [749.285477ms]
    May 17 08:18:54.640: INFO: Created: latency-svc-nlt9j
    May 17 08:18:54.684: INFO: Got endpoints: latency-svc-j2nh2 [751.065216ms]
    May 17 08:18:54.692: INFO: Created: latency-svc-chgbx
    May 17 08:18:54.734: INFO: Got endpoints: latency-svc-xcxlf [750.671462ms]
    May 17 08:18:54.741: INFO: Created: latency-svc-dkkvw
    May 17 08:18:54.783: INFO: Got endpoints: latency-svc-tnd8z [749.138508ms]
    May 17 08:18:54.791: INFO: Created: latency-svc-jcktl
    May 17 08:18:54.834: INFO: Got endpoints: latency-svc-9hhbl [750.435473ms]
    May 17 08:18:54.841: INFO: Created: latency-svc-2cmsg
    May 17 08:18:54.884: INFO: Got endpoints: latency-svc-6twwr [750.474363ms]
    May 17 08:18:54.891: INFO: Created: latency-svc-zxxhj
    May 17 08:18:54.933: INFO: Got endpoints: latency-svc-qvzm8 [748.568091ms]
    May 17 08:18:54.941: INFO: Created: latency-svc-t24bv
    May 17 08:18:54.983: INFO: Got endpoints: latency-svc-55gh7 [749.856484ms]
    May 17 08:18:54.990: INFO: Created: latency-svc-qv9pr
    May 17 08:18:55.033: INFO: Got endpoints: latency-svc-cfdrb [749.911631ms]
    May 17 08:18:55.041: INFO: Created: latency-svc-k294f
    May 17 08:18:55.083: INFO: Got endpoints: latency-svc-gfc7z [749.784989ms]
    May 17 08:18:55.092: INFO: Created: latency-svc-bqjdm
    May 17 08:18:55.134: INFO: Got endpoints: latency-svc-wmmsm [750.877496ms]
    May 17 08:18:55.141: INFO: Created: latency-svc-44n5w
    May 17 08:18:55.183: INFO: Got endpoints: latency-svc-6nq4v [749.55253ms]
    May 17 08:18:55.190: INFO: Created: latency-svc-rvm4k
    May 17 08:18:55.232: INFO: Got endpoints: latency-svc-2dgnr [748.661435ms]
    May 17 08:18:55.240: INFO: Created: latency-svc-4n4rb
    May 17 08:18:55.283: INFO: Got endpoints: latency-svc-njq7j [748.616836ms]
    May 17 08:18:55.333: INFO: Got endpoints: latency-svc-58zxl [749.257169ms]
    May 17 08:18:55.382: INFO: Got endpoints: latency-svc-nlt9j [749.474248ms]
    May 17 08:18:55.432: INFO: Got endpoints: latency-svc-chgbx [748.271194ms]
    May 17 08:18:55.483: INFO: Got endpoints: latency-svc-dkkvw [748.759583ms]
    May 17 08:18:55.533: INFO: Got endpoints: latency-svc-jcktl [749.644549ms]
    May 17 08:18:55.584: INFO: Got endpoints: latency-svc-2cmsg [750.43349ms]
    May 17 08:18:55.632: INFO: Got endpoints: latency-svc-zxxhj [748.739111ms]
    May 17 08:18:55.682: INFO: Got endpoints: latency-svc-t24bv [749.564135ms]
    May 17 08:18:55.734: INFO: Got endpoints: latency-svc-qv9pr [751.538686ms]
    May 17 08:18:55.783: INFO: Got endpoints: latency-svc-k294f [749.535234ms]
    May 17 08:18:55.834: INFO: Got endpoints: latency-svc-bqjdm [750.562492ms]
    May 17 08:18:55.883: INFO: Got endpoints: latency-svc-44n5w [749.04274ms]
    May 17 08:18:55.933: INFO: Got endpoints: latency-svc-rvm4k [750.609817ms]
    May 17 08:18:55.984: INFO: Got endpoints: latency-svc-4n4rb [751.256009ms]
    May 17 08:18:55.984: INFO: Latencies: [12.734864ms 17.429081ms 21.58481ms 27.744609ms 33.081515ms 35.639726ms 43.506126ms 48.747526ms 51.04444ms 56.902388ms 62.481094ms 70.173247ms 73.528075ms 75.636462ms 77.355182ms 81.167138ms 85.690654ms 86.699254ms 86.891426ms 87.580143ms 88.534544ms 88.834541ms 89.1555ms 91.436235ms 92.482507ms 93.925368ms 94.459913ms 95.77392ms 95.885652ms 96.821784ms 97.400529ms 98.29054ms 100.180497ms 134.010311ms 178.972834ms 223.825624ms 268.424022ms 304.959286ms 347.465138ms 393.120231ms 439.553341ms 487.062069ms 530.573203ms 580.543524ms 620.39367ms 664.364236ms 710.360756ms 745.692044ms 746.86181ms 747.353597ms 747.473961ms 747.639598ms 747.915373ms 748.104241ms 748.258488ms 748.271194ms 748.286078ms 748.320221ms 748.340292ms 748.462446ms 748.512941ms 748.568091ms 748.616836ms 748.649867ms 748.655208ms 748.661435ms 748.739111ms 748.756827ms 748.759583ms 748.80439ms 748.842024ms 748.917481ms 748.919645ms 748.93463ms 748.957189ms 748.98184ms 749.04274ms 749.070317ms 749.103999ms 749.120522ms 749.131487ms 749.138508ms 749.19319ms 749.208519ms 749.255019ms 749.257169ms 749.285477ms 749.314887ms 749.422609ms 749.436971ms 749.465626ms 749.474248ms 749.524642ms 749.535234ms 749.549579ms 749.550095ms 749.55253ms 749.559853ms 749.564135ms 749.571326ms 749.58941ms 749.593727ms 749.598382ms 749.603696ms 749.618938ms 749.644549ms 749.667564ms 749.710152ms 749.736922ms 749.758456ms 749.767514ms 749.772346ms 749.781595ms 749.784989ms 749.833404ms 749.83678ms 749.856484ms 749.866524ms 749.879988ms 749.884993ms 749.88878ms 749.911276ms 749.911631ms 749.914814ms 749.947953ms 749.955148ms 749.956829ms 750.026605ms 750.028005ms 750.041602ms 750.043186ms 750.070267ms 750.08664ms 750.098575ms 750.111055ms 750.118997ms 750.119012ms 750.123889ms 750.156143ms 750.157786ms 750.181901ms 750.191743ms 750.264259ms 750.267917ms 750.276344ms 750.299062ms 750.301885ms 750.339531ms 750.375594ms 750.397483ms 750.418076ms 750.425613ms 750.43349ms 750.435473ms 750.464676ms 750.474363ms 750.477479ms 750.521994ms 750.530068ms 750.531772ms 750.562492ms 750.600193ms 750.609817ms 750.615617ms 750.629343ms 750.637172ms 750.638631ms 750.671462ms 750.741648ms 750.877496ms 750.940012ms 750.958499ms 751.064215ms 751.065216ms 751.075971ms 751.156614ms 751.168506ms 751.254212ms 751.256009ms 751.264527ms 751.264856ms 751.277143ms 751.277712ms 751.351042ms 751.394275ms 751.467504ms 751.509515ms 751.525732ms 751.526715ms 751.538686ms 751.556976ms 751.614197ms 751.678493ms 751.888558ms 752.061852ms 752.087776ms 752.229701ms 752.41036ms 752.557583ms 752.681847ms]
    May 17 08:18:55.984: INFO: 50 %ile: 749.58941ms
    May 17 08:18:55.984: INFO: 90 %ile: 751.264856ms
    May 17 08:18:55.984: INFO: 99 %ile: 752.557583ms
    May 17 08:18:55.984: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    May 17 08:18:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-1865" for this suite. 05/17/23 08:18:55.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:18:55.992
May 17 08:18:55.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:18:55.993
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:55.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:56
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:18:56.008
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:18:56.482
STEP: Deploying the webhook pod 05/17/23 08:18:56.486
STEP: Wait for the deployment to be ready 05/17/23 08:18:56.492
May 17 08:18:56.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 08:18:58.501
STEP: Verifying the service has paired with the endpoint 05/17/23 08:18:58.509
May 17 08:18:59.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 08:18:59.511
STEP: create a pod that should be denied by the webhook 05/17/23 08:18:59.521
STEP: create a pod that causes the webhook to hang 05/17/23 08:18:59.529
STEP: create a configmap that should be denied by the webhook 05/17/23 08:19:09.532
STEP: create a configmap that should be admitted by the webhook 05/17/23 08:19:09.537
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 08:19:09.542
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 08:19:09.546
STEP: create a namespace that bypass the webhook 05/17/23 08:19:09.548
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/17/23 08:19:09.552
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:19:09.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1131" for this suite. 05/17/23 08:19:09.579
STEP: Destroying namespace "webhook-1131-markers" for this suite. 05/17/23 08:19:09.583
------------------------------
â€¢ [SLOW TEST] [13.593 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:18:55.992
    May 17 08:18:55.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:18:55.993
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:18:55.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:18:56
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:18:56.008
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:18:56.482
    STEP: Deploying the webhook pod 05/17/23 08:18:56.486
    STEP: Wait for the deployment to be ready 05/17/23 08:18:56.492
    May 17 08:18:56.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 08:18:58.501
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:18:58.509
    May 17 08:18:59.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 08:18:59.511
    STEP: create a pod that should be denied by the webhook 05/17/23 08:18:59.521
    STEP: create a pod that causes the webhook to hang 05/17/23 08:18:59.529
    STEP: create a configmap that should be denied by the webhook 05/17/23 08:19:09.532
    STEP: create a configmap that should be admitted by the webhook 05/17/23 08:19:09.537
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 08:19:09.542
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 08:19:09.546
    STEP: create a namespace that bypass the webhook 05/17/23 08:19:09.548
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/17/23 08:19:09.552
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:19:09.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1131" for this suite. 05/17/23 08:19:09.579
    STEP: Destroying namespace "webhook-1131-markers" for this suite. 05/17/23 08:19:09.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:19:09.585
May 17 08:19:09.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 08:19:09.586
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:09.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:09.594
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May 17 08:19:09.596: INFO: Creating deployment "test-recreate-deployment"
May 17 08:19:09.598: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 17 08:19:09.602: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 17 08:19:11.605: INFO: Waiting deployment "test-recreate-deployment" to complete
May 17 08:19:11.607: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 17 08:19:11.611: INFO: Updating deployment test-recreate-deployment
May 17 08:19:11.611: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 08:19:11.648: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7528  43ab636b-881c-4204-8c36-284073d17836 1206129 2 2023-05-17 08:19:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a729c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 08:19:11 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-17 08:19:11 +0000 UTC,LastTransitionTime:2023-05-17 08:19:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 17 08:19:11.649: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7528  e01fa6b6-2cfc-4d58-b169-caf1f85488e4 1206128 1 2023-05-17 08:19:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 43ab636b-881c-4204-8c36-284073d17836 0xc004a72e90 0xc004a72e91}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43ab636b-881c-4204-8c36-284073d17836\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a72f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 08:19:11.649: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 17 08:19:11.649: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7528  de0e626a-5608-4102-8bc4-bc5e101e6349 1206117 2 2023-05-17 08:19:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 43ab636b-881c-4204-8c36-284073d17836 0xc004a72d77 0xc004a72d78}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43ab636b-881c-4204-8c36-284073d17836\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a72e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 08:19:11.651: INFO: Pod "test-recreate-deployment-cff6dc657-ps75h" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-ps75h test-recreate-deployment-cff6dc657- deployment-7528  36d3e305-dfc9-45e4-ab85-250e44c1f00f 1206127 0 2023-05-17 08:19:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 e01fa6b6-2cfc-4d58-b169-caf1f85488e4 0xc0049bcf40 0xc0049bcf41}] [] [{kube-controller-manager Update v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e01fa6b6-2cfc-4d58-b169-caf1f85488e4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7k6g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7k6g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 08:19:11.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7528" for this suite. 05/17/23 08:19:11.652
------------------------------
â€¢ [2.070 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:19:09.585
    May 17 08:19:09.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 08:19:09.586
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:09.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:09.594
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May 17 08:19:09.596: INFO: Creating deployment "test-recreate-deployment"
    May 17 08:19:09.598: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May 17 08:19:09.602: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    May 17 08:19:11.605: INFO: Waiting deployment "test-recreate-deployment" to complete
    May 17 08:19:11.607: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May 17 08:19:11.611: INFO: Updating deployment test-recreate-deployment
    May 17 08:19:11.611: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 08:19:11.648: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7528  43ab636b-881c-4204-8c36-284073d17836 1206129 2 2023-05-17 08:19:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a729c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 08:19:11 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-17 08:19:11 +0000 UTC,LastTransitionTime:2023-05-17 08:19:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May 17 08:19:11.649: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7528  e01fa6b6-2cfc-4d58-b169-caf1f85488e4 1206128 1 2023-05-17 08:19:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 43ab636b-881c-4204-8c36-284073d17836 0xc004a72e90 0xc004a72e91}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43ab636b-881c-4204-8c36-284073d17836\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a72f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 08:19:11.649: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May 17 08:19:11.649: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7528  de0e626a-5608-4102-8bc4-bc5e101e6349 1206117 2 2023-05-17 08:19:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 43ab636b-881c-4204-8c36-284073d17836 0xc004a72d77 0xc004a72d78}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43ab636b-881c-4204-8c36-284073d17836\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a72e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 08:19:11.651: INFO: Pod "test-recreate-deployment-cff6dc657-ps75h" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-ps75h test-recreate-deployment-cff6dc657- deployment-7528  36d3e305-dfc9-45e4-ab85-250e44c1f00f 1206127 0 2023-05-17 08:19:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 e01fa6b6-2cfc-4d58-b169-caf1f85488e4 0xc0049bcf40 0xc0049bcf41}] [] [{kube-controller-manager Update v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e01fa6b6-2cfc-4d58-b169-caf1f85488e4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7k6g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7k6g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:,StartTime:2023-05-17 08:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 08:19:11.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7528" for this suite. 05/17/23 08:19:11.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:19:11.655
May 17 08:19:11.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename daemonsets 05/17/23 08:19:11.656
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:11.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:11.666
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 05/17/23 08:19:11.674
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:19:11.677
May 17 08:19:11.679: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:19:11.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:19:11.680: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:19:12.683: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:19:12.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:19:12.685: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:19:13.683: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:19:13.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 08:19:13.684: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/17/23 08:19:13.686
May 17 08:19:13.695: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:19:13.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:19:13.698: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:19:14.703: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:19:14.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:19:14.704: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:19:15.702: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:19:15.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 08:19:15.703: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/17/23 08:19:15.703
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:19:15.706
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4796, will wait for the garbage collector to delete the pods 05/17/23 08:19:15.706
May 17 08:19:15.761: INFO: Deleting DaemonSet.extensions daemon-set took: 3.36277ms
May 17 08:19:15.863: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.146499ms
May 17 08:19:18.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:19:18.264: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 08:19:18.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1206259"},"items":null}

May 17 08:19:18.266: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1206259"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:19:18.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4796" for this suite. 05/17/23 08:19:18.272
------------------------------
â€¢ [SLOW TEST] [6.619 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:19:11.655
    May 17 08:19:11.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename daemonsets 05/17/23 08:19:11.656
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:11.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:11.666
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 05/17/23 08:19:11.674
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:19:11.677
    May 17 08:19:11.679: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:19:11.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:19:11.680: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:19:12.683: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:19:12.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:19:12.685: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:19:13.683: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:19:13.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 08:19:13.684: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/17/23 08:19:13.686
    May 17 08:19:13.695: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:19:13.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:19:13.698: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:19:14.703: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:19:14.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:19:14.704: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:19:15.702: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:19:15.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 08:19:15.703: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/17/23 08:19:15.703
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:19:15.706
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4796, will wait for the garbage collector to delete the pods 05/17/23 08:19:15.706
    May 17 08:19:15.761: INFO: Deleting DaemonSet.extensions daemon-set took: 3.36277ms
    May 17 08:19:15.863: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.146499ms
    May 17 08:19:18.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:19:18.264: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 08:19:18.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1206259"},"items":null}

    May 17 08:19:18.266: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1206259"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:19:18.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4796" for this suite. 05/17/23 08:19:18.272
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:19:18.274
May 17 08:19:18.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:19:18.275
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:18.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:18.282
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 05/17/23 08:19:18.283
May 17 08:19:18.284: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-416 proxy --unix-socket=/tmp/kubectl-proxy-unix1745818205/test'
STEP: retrieving proxy /api/ output 05/17/23 08:19:18.323
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:19:18.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-416" for this suite. 05/17/23 08:19:18.326
------------------------------
â€¢ [0.054 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:19:18.274
    May 17 08:19:18.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:19:18.275
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:18.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:18.282
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 05/17/23 08:19:18.283
    May 17 08:19:18.284: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-416 proxy --unix-socket=/tmp/kubectl-proxy-unix1745818205/test'
    STEP: retrieving proxy /api/ output 05/17/23 08:19:18.323
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:19:18.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-416" for this suite. 05/17/23 08:19:18.326
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:19:18.328
May 17 08:19:18.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:19:18.329
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:18.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:18.337
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 05/17/23 08:19:35.341
STEP: Creating a ResourceQuota 05/17/23 08:19:40.344
STEP: Ensuring resource quota status is calculated 05/17/23 08:19:40.347
STEP: Creating a ConfigMap 05/17/23 08:19:42.349
STEP: Ensuring resource quota status captures configMap creation 05/17/23 08:19:42.356
STEP: Deleting a ConfigMap 05/17/23 08:19:44.359
STEP: Ensuring resource quota status released usage 05/17/23 08:19:44.362
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:19:46.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8591" for this suite. 05/17/23 08:19:46.366
------------------------------
â€¢ [SLOW TEST] [28.041 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:19:18.328
    May 17 08:19:18.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:19:18.329
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:18.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:18.337
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 05/17/23 08:19:35.341
    STEP: Creating a ResourceQuota 05/17/23 08:19:40.344
    STEP: Ensuring resource quota status is calculated 05/17/23 08:19:40.347
    STEP: Creating a ConfigMap 05/17/23 08:19:42.349
    STEP: Ensuring resource quota status captures configMap creation 05/17/23 08:19:42.356
    STEP: Deleting a ConfigMap 05/17/23 08:19:44.359
    STEP: Ensuring resource quota status released usage 05/17/23 08:19:44.362
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:19:46.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8591" for this suite. 05/17/23 08:19:46.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:19:46.37
May 17 08:19:46.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename cronjob 05/17/23 08:19:46.37
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:46.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:46.377
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/17/23 08:19:46.379
STEP: creating 05/17/23 08:19:46.379
STEP: getting 05/17/23 08:19:46.382
STEP: listing 05/17/23 08:19:46.383
STEP: watching 05/17/23 08:19:46.384
May 17 08:19:46.384: INFO: starting watch
STEP: cluster-wide listing 05/17/23 08:19:46.385
STEP: cluster-wide watching 05/17/23 08:19:46.386
May 17 08:19:46.386: INFO: starting watch
STEP: patching 05/17/23 08:19:46.387
STEP: updating 05/17/23 08:19:46.39
May 17 08:19:46.394: INFO: waiting for watch events with expected annotations
May 17 08:19:46.394: INFO: saw patched and updated annotations
STEP: patching /status 05/17/23 08:19:46.394
STEP: updating /status 05/17/23 08:19:46.397
STEP: get /status 05/17/23 08:19:46.4
STEP: deleting 05/17/23 08:19:46.402
STEP: deleting a collection 05/17/23 08:19:46.407
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 17 08:19:46.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2068" for this suite. 05/17/23 08:19:46.414
------------------------------
â€¢ [0.046 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:19:46.37
    May 17 08:19:46.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename cronjob 05/17/23 08:19:46.37
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:46.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:46.377
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/17/23 08:19:46.379
    STEP: creating 05/17/23 08:19:46.379
    STEP: getting 05/17/23 08:19:46.382
    STEP: listing 05/17/23 08:19:46.383
    STEP: watching 05/17/23 08:19:46.384
    May 17 08:19:46.384: INFO: starting watch
    STEP: cluster-wide listing 05/17/23 08:19:46.385
    STEP: cluster-wide watching 05/17/23 08:19:46.386
    May 17 08:19:46.386: INFO: starting watch
    STEP: patching 05/17/23 08:19:46.387
    STEP: updating 05/17/23 08:19:46.39
    May 17 08:19:46.394: INFO: waiting for watch events with expected annotations
    May 17 08:19:46.394: INFO: saw patched and updated annotations
    STEP: patching /status 05/17/23 08:19:46.394
    STEP: updating /status 05/17/23 08:19:46.397
    STEP: get /status 05/17/23 08:19:46.4
    STEP: deleting 05/17/23 08:19:46.402
    STEP: deleting a collection 05/17/23 08:19:46.407
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 17 08:19:46.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2068" for this suite. 05/17/23 08:19:46.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:19:46.417
May 17 08:19:46.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:19:46.417
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:46.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:46.423
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-4502/secret-test-e70801fd-4e04-441b-8797-66154b45bcfb 05/17/23 08:19:46.425
STEP: Creating a pod to test consume secrets 05/17/23 08:19:46.427
May 17 08:19:46.431: INFO: Waiting up to 5m0s for pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e" in namespace "secrets-4502" to be "Succeeded or Failed"
May 17 08:19:46.432: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.177847ms
May 17 08:19:48.434: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003467764s
May 17 08:19:50.436: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004850432s
STEP: Saw pod success 05/17/23 08:19:50.436
May 17 08:19:50.436: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e" satisfied condition "Succeeded or Failed"
May 17 08:19:50.437: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-608dec57-8f11-4193-a187-43747e97073e container env-test: <nil>
STEP: delete the pod 05/17/23 08:19:50.441
May 17 08:19:50.447: INFO: Waiting for pod pod-configmaps-608dec57-8f11-4193-a187-43747e97073e to disappear
May 17 08:19:50.449: INFO: Pod pod-configmaps-608dec57-8f11-4193-a187-43747e97073e no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:19:50.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4502" for this suite. 05/17/23 08:19:50.45
------------------------------
â€¢ [4.037 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:19:46.417
    May 17 08:19:46.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:19:46.417
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:46.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:46.423
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-4502/secret-test-e70801fd-4e04-441b-8797-66154b45bcfb 05/17/23 08:19:46.425
    STEP: Creating a pod to test consume secrets 05/17/23 08:19:46.427
    May 17 08:19:46.431: INFO: Waiting up to 5m0s for pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e" in namespace "secrets-4502" to be "Succeeded or Failed"
    May 17 08:19:46.432: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.177847ms
    May 17 08:19:48.434: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003467764s
    May 17 08:19:50.436: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004850432s
    STEP: Saw pod success 05/17/23 08:19:50.436
    May 17 08:19:50.436: INFO: Pod "pod-configmaps-608dec57-8f11-4193-a187-43747e97073e" satisfied condition "Succeeded or Failed"
    May 17 08:19:50.437: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-608dec57-8f11-4193-a187-43747e97073e container env-test: <nil>
    STEP: delete the pod 05/17/23 08:19:50.441
    May 17 08:19:50.447: INFO: Waiting for pod pod-configmaps-608dec57-8f11-4193-a187-43747e97073e to disappear
    May 17 08:19:50.449: INFO: Pod pod-configmaps-608dec57-8f11-4193-a187-43747e97073e no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:19:50.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4502" for this suite. 05/17/23 08:19:50.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:19:50.454
May 17 08:19:50.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:19:50.454
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:50.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:50.461
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 05/17/23 08:19:50.462
STEP: Creating a ResourceQuota 05/17/23 08:19:55.464
STEP: Ensuring resource quota status is calculated 05/17/23 08:19:55.467
STEP: Creating a Service 05/17/23 08:19:57.47
STEP: Creating a NodePort Service 05/17/23 08:19:57.48
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/17/23 08:19:57.491
STEP: Ensuring resource quota status captures service creation 05/17/23 08:19:57.502
STEP: Deleting Services 05/17/23 08:19:59.504
STEP: Ensuring resource quota status released usage 05/17/23 08:19:59.519
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:20:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7033" for this suite. 05/17/23 08:20:01.523
------------------------------
â€¢ [SLOW TEST] [11.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:19:50.454
    May 17 08:19:50.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:19:50.454
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:19:50.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:19:50.461
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 05/17/23 08:19:50.462
    STEP: Creating a ResourceQuota 05/17/23 08:19:55.464
    STEP: Ensuring resource quota status is calculated 05/17/23 08:19:55.467
    STEP: Creating a Service 05/17/23 08:19:57.47
    STEP: Creating a NodePort Service 05/17/23 08:19:57.48
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/17/23 08:19:57.491
    STEP: Ensuring resource quota status captures service creation 05/17/23 08:19:57.502
    STEP: Deleting Services 05/17/23 08:19:59.504
    STEP: Ensuring resource quota status released usage 05/17/23 08:19:59.519
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:20:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7033" for this suite. 05/17/23 08:20:01.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:20:01.527
May 17 08:20:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replication-controller 05/17/23 08:20:01.527
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:20:01.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:20:01.535
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
May 17 08:20:01.537: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/17/23 08:20:02.542
STEP: Checking rc "condition-test" has the desired failure condition set 05/17/23 08:20:02.545
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/17/23 08:20:03.548
May 17 08:20:03.552: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/17/23 08:20:03.552
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 17 08:20:04.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-377" for this suite. 05/17/23 08:20:04.559
------------------------------
â€¢ [3.035 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:20:01.527
    May 17 08:20:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replication-controller 05/17/23 08:20:01.527
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:20:01.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:20:01.535
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    May 17 08:20:01.537: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/17/23 08:20:02.542
    STEP: Checking rc "condition-test" has the desired failure condition set 05/17/23 08:20:02.545
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/17/23 08:20:03.548
    May 17 08:20:03.552: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/17/23 08:20:03.552
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 17 08:20:04.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-377" for this suite. 05/17/23 08:20:04.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:20:04.563
May 17 08:20:04.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 08:20:04.564
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:20:04.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:20:04.57
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 08:21:04.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3658" for this suite. 05/17/23 08:21:04.58
------------------------------
â€¢ [SLOW TEST] [60.020 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:20:04.563
    May 17 08:20:04.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 08:20:04.564
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:20:04.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:20:04.57
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:04.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3658" for this suite. 05/17/23 08:21:04.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:04.584
May 17 08:21:04.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:21:04.585
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:04.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:04.592
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 05/17/23 08:21:04.594
May 17 08:21:04.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 create -f -'
May 17 08:21:05.178: INFO: stderr: ""
May 17 08:21:05.178: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:21:05.178
May 17 08:21:05.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 08:21:05.228: INFO: stderr: ""
May 17 08:21:05.228: INFO: stdout: "update-demo-nautilus-hpwx7 update-demo-nautilus-t7mvx "
May 17 08:21:05.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-hpwx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:21:05.279: INFO: stderr: ""
May 17 08:21:05.279: INFO: stdout: ""
May 17 08:21:05.279: INFO: update-demo-nautilus-hpwx7 is created but not running
May 17 08:21:10.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 08:21:10.334: INFO: stderr: ""
May 17 08:21:10.334: INFO: stdout: "update-demo-nautilus-hpwx7 update-demo-nautilus-t7mvx "
May 17 08:21:10.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-hpwx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:21:10.383: INFO: stderr: ""
May 17 08:21:10.383: INFO: stdout: "true"
May 17 08:21:10.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-hpwx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 08:21:10.430: INFO: stderr: ""
May 17 08:21:10.430: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 17 08:21:10.430: INFO: validating pod update-demo-nautilus-hpwx7
May 17 08:21:10.433: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 08:21:10.433: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 08:21:10.433: INFO: update-demo-nautilus-hpwx7 is verified up and running
May 17 08:21:10.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-t7mvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 08:21:10.480: INFO: stderr: ""
May 17 08:21:10.480: INFO: stdout: "true"
May 17 08:21:10.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-t7mvx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 08:21:10.527: INFO: stderr: ""
May 17 08:21:10.527: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 17 08:21:10.527: INFO: validating pod update-demo-nautilus-t7mvx
May 17 08:21:10.530: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 08:21:10.530: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 08:21:10.530: INFO: update-demo-nautilus-t7mvx is verified up and running
STEP: using delete to clean up resources 05/17/23 08:21:10.53
May 17 08:21:10.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 delete --grace-period=0 --force -f -'
May 17 08:21:10.580: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:21:10.580: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 17 08:21:10.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get rc,svc -l name=update-demo --no-headers'
May 17 08:21:10.639: INFO: stderr: "No resources found in kubectl-4600 namespace.\n"
May 17 08:21:10.639: INFO: stdout: ""
May 17 08:21:10.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 08:21:10.692: INFO: stderr: ""
May 17 08:21:10.692: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:21:10.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4600" for this suite. 05/17/23 08:21:10.694
------------------------------
â€¢ [SLOW TEST] [6.113 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:04.584
    May 17 08:21:04.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:21:04.585
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:04.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:04.592
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 05/17/23 08:21:04.594
    May 17 08:21:04.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 create -f -'
    May 17 08:21:05.178: INFO: stderr: ""
    May 17 08:21:05.178: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 08:21:05.178
    May 17 08:21:05.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 08:21:05.228: INFO: stderr: ""
    May 17 08:21:05.228: INFO: stdout: "update-demo-nautilus-hpwx7 update-demo-nautilus-t7mvx "
    May 17 08:21:05.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-hpwx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:21:05.279: INFO: stderr: ""
    May 17 08:21:05.279: INFO: stdout: ""
    May 17 08:21:05.279: INFO: update-demo-nautilus-hpwx7 is created but not running
    May 17 08:21:10.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 08:21:10.334: INFO: stderr: ""
    May 17 08:21:10.334: INFO: stdout: "update-demo-nautilus-hpwx7 update-demo-nautilus-t7mvx "
    May 17 08:21:10.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-hpwx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:21:10.383: INFO: stderr: ""
    May 17 08:21:10.383: INFO: stdout: "true"
    May 17 08:21:10.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-hpwx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 08:21:10.430: INFO: stderr: ""
    May 17 08:21:10.430: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 17 08:21:10.430: INFO: validating pod update-demo-nautilus-hpwx7
    May 17 08:21:10.433: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 08:21:10.433: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 08:21:10.433: INFO: update-demo-nautilus-hpwx7 is verified up and running
    May 17 08:21:10.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-t7mvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 08:21:10.480: INFO: stderr: ""
    May 17 08:21:10.480: INFO: stdout: "true"
    May 17 08:21:10.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods update-demo-nautilus-t7mvx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 08:21:10.527: INFO: stderr: ""
    May 17 08:21:10.527: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 17 08:21:10.527: INFO: validating pod update-demo-nautilus-t7mvx
    May 17 08:21:10.530: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 08:21:10.530: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 08:21:10.530: INFO: update-demo-nautilus-t7mvx is verified up and running
    STEP: using delete to clean up resources 05/17/23 08:21:10.53
    May 17 08:21:10.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 delete --grace-period=0 --force -f -'
    May 17 08:21:10.580: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:21:10.580: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May 17 08:21:10.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get rc,svc -l name=update-demo --no-headers'
    May 17 08:21:10.639: INFO: stderr: "No resources found in kubectl-4600 namespace.\n"
    May 17 08:21:10.639: INFO: stdout: ""
    May 17 08:21:10.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4600 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 17 08:21:10.692: INFO: stderr: ""
    May 17 08:21:10.692: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:10.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4600" for this suite. 05/17/23 08:21:10.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:10.698
May 17 08:21:10.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 08:21:10.698
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:10.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:10.707
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/17/23 08:21:10.708
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/17/23 08:21:10.708
STEP: creating a pod to probe DNS 05/17/23 08:21:10.708
STEP: submitting the pod to kubernetes 05/17/23 08:21:10.709
May 17 08:21:10.713: INFO: Waiting up to 15m0s for pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051" in namespace "dns-2569" to be "running"
May 17 08:21:10.714: INFO: Pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200784ms
May 17 08:21:12.717: INFO: Pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051": Phase="Running", Reason="", readiness=true. Elapsed: 2.003865021s
May 17 08:21:12.717: INFO: Pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051" satisfied condition "running"
STEP: retrieving the pod 05/17/23 08:21:12.717
STEP: looking for the results for each expected name from probers 05/17/23 08:21:12.718
May 17 08:21:12.725: INFO: DNS probes using dns-2569/dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051 succeeded

STEP: deleting the pod 05/17/23 08:21:12.725
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 08:21:12.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2569" for this suite. 05/17/23 08:21:12.733
------------------------------
â€¢ [2.038 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:10.698
    May 17 08:21:10.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 08:21:10.698
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:10.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:10.707
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/17/23 08:21:10.708
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/17/23 08:21:10.708
    STEP: creating a pod to probe DNS 05/17/23 08:21:10.708
    STEP: submitting the pod to kubernetes 05/17/23 08:21:10.709
    May 17 08:21:10.713: INFO: Waiting up to 15m0s for pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051" in namespace "dns-2569" to be "running"
    May 17 08:21:10.714: INFO: Pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200784ms
    May 17 08:21:12.717: INFO: Pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051": Phase="Running", Reason="", readiness=true. Elapsed: 2.003865021s
    May 17 08:21:12.717: INFO: Pod "dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 08:21:12.717
    STEP: looking for the results for each expected name from probers 05/17/23 08:21:12.718
    May 17 08:21:12.725: INFO: DNS probes using dns-2569/dns-test-adc8b5b8-4a98-484e-a4da-e344eab44051 succeeded

    STEP: deleting the pod 05/17/23 08:21:12.725
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:12.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2569" for this suite. 05/17/23 08:21:12.733
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:12.736
May 17 08:21:12.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:21:12.736
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:12.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:12.744
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:21:12.745
May 17 08:21:12.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d" in namespace "projected-1063" to be "Succeeded or Failed"
May 17 08:21:12.751: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.094615ms
May 17 08:21:14.753: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003289634s
May 17 08:21:16.753: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003791596s
STEP: Saw pod success 05/17/23 08:21:16.753
May 17 08:21:16.753: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d" satisfied condition "Succeeded or Failed"
May 17 08:21:16.755: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d container client-container: <nil>
STEP: delete the pod 05/17/23 08:21:16.758
May 17 08:21:16.764: INFO: Waiting for pod downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d to disappear
May 17 08:21:16.765: INFO: Pod downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:21:16.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1063" for this suite. 05/17/23 08:21:16.766
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:12.736
    May 17 08:21:12.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:21:12.736
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:12.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:12.744
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:21:12.745
    May 17 08:21:12.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d" in namespace "projected-1063" to be "Succeeded or Failed"
    May 17 08:21:12.751: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.094615ms
    May 17 08:21:14.753: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003289634s
    May 17 08:21:16.753: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003791596s
    STEP: Saw pod success 05/17/23 08:21:16.753
    May 17 08:21:16.753: INFO: Pod "downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d" satisfied condition "Succeeded or Failed"
    May 17 08:21:16.755: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d container client-container: <nil>
    STEP: delete the pod 05/17/23 08:21:16.758
    May 17 08:21:16.764: INFO: Waiting for pod downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d to disappear
    May 17 08:21:16.765: INFO: Pod downwardapi-volume-88a9656f-f84d-42e2-a59d-906ebf0c8d7d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:16.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1063" for this suite. 05/17/23 08:21:16.766
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:16.769
May 17 08:21:16.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:21:16.77
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:16.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:16.777
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:21:16.779
May 17 08:21:16.782: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa" in namespace "projected-2927" to be "Succeeded or Failed"
May 17 08:21:16.784: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.165795ms
May 17 08:21:18.786: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003997054s
May 17 08:21:20.787: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004255239s
STEP: Saw pod success 05/17/23 08:21:20.787
May 17 08:21:20.787: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa" satisfied condition "Succeeded or Failed"
May 17 08:21:20.788: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa container client-container: <nil>
STEP: delete the pod 05/17/23 08:21:20.791
May 17 08:21:20.797: INFO: Waiting for pod downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa to disappear
May 17 08:21:20.799: INFO: Pod downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:21:20.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2927" for this suite. 05/17/23 08:21:20.8
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:16.769
    May 17 08:21:16.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:21:16.77
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:16.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:16.777
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:21:16.779
    May 17 08:21:16.782: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa" in namespace "projected-2927" to be "Succeeded or Failed"
    May 17 08:21:16.784: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.165795ms
    May 17 08:21:18.786: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003997054s
    May 17 08:21:20.787: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004255239s
    STEP: Saw pod success 05/17/23 08:21:20.787
    May 17 08:21:20.787: INFO: Pod "downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa" satisfied condition "Succeeded or Failed"
    May 17 08:21:20.788: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa container client-container: <nil>
    STEP: delete the pod 05/17/23 08:21:20.791
    May 17 08:21:20.797: INFO: Waiting for pod downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa to disappear
    May 17 08:21:20.799: INFO: Pod downwardapi-volume-e8de60b6-d057-4e60-ae3e-e2be04d166aa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:20.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2927" for this suite. 05/17/23 08:21:20.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:20.804
May 17 08:21:20.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 08:21:20.805
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:20.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:20.812
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/17/23 08:21:20.814
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_tcp@PTR;sleep 1; done
 05/17/23 08:21:20.821
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_tcp@PTR;sleep 1; done
 05/17/23 08:21:20.821
STEP: creating a pod to probe DNS 05/17/23 08:21:20.821
STEP: submitting the pod to kubernetes 05/17/23 08:21:20.821
May 17 08:21:20.826: INFO: Waiting up to 15m0s for pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc" in namespace "dns-6308" to be "running"
May 17 08:21:20.827: INFO: Pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.291975ms
May 17 08:21:22.830: INFO: Pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004031092s
May 17 08:21:22.830: INFO: Pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc" satisfied condition "running"
STEP: retrieving the pod 05/17/23 08:21:22.83
STEP: looking for the results for each expected name from probers 05/17/23 08:21:22.832
May 17 08:21:22.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.835: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.837: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.838: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.845: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.847: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.848: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.849: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:22.854: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

May 17 08:21:27.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.860: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.863: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:27.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

May 17 08:21:32.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.862: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.872: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:32.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

May 17 08:21:37.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.862: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.869: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.870: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.872: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.873: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:37.878: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

May 17 08:21:42.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.860: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.862: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.863: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.872: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:42.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

May 17 08:21:47.857: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.862: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.872: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
May 17 08:21:47.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

May 17 08:21:52.880: INFO: DNS probes using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc succeeded

STEP: deleting the pod 05/17/23 08:21:52.88
STEP: deleting the test service 05/17/23 08:21:52.887
STEP: deleting the test headless service 05/17/23 08:21:52.896
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 08:21:52.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6308" for this suite. 05/17/23 08:21:52.902
------------------------------
â€¢ [SLOW TEST] [32.100 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:20.804
    May 17 08:21:20.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 08:21:20.805
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:20.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:20.812
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/17/23 08:21:20.814
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_tcp@PTR;sleep 1; done
     05/17/23 08:21:20.821
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6308.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6308.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6308.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.226.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.226.13_tcp@PTR;sleep 1; done
     05/17/23 08:21:20.821
    STEP: creating a pod to probe DNS 05/17/23 08:21:20.821
    STEP: submitting the pod to kubernetes 05/17/23 08:21:20.821
    May 17 08:21:20.826: INFO: Waiting up to 15m0s for pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc" in namespace "dns-6308" to be "running"
    May 17 08:21:20.827: INFO: Pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.291975ms
    May 17 08:21:22.830: INFO: Pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004031092s
    May 17 08:21:22.830: INFO: Pod "dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 08:21:22.83
    STEP: looking for the results for each expected name from probers 05/17/23 08:21:22.832
    May 17 08:21:22.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.835: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.837: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.838: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.845: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.847: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.848: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.849: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:22.854: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

    May 17 08:21:27.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.860: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.863: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:27.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

    May 17 08:21:32.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.862: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.872: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:32.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

    May 17 08:21:37.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.862: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.869: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.870: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.872: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.873: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:37.878: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

    May 17 08:21:42.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.860: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.862: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.863: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.872: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:42.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

    May 17 08:21:47.857: INFO: Unable to read wheezy_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.862: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.870: INFO: Unable to read jessie_udp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.872: INFO: Unable to read jessie_tcp@dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local from pod dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc: the server could not find the requested resource (get pods dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc)
    May 17 08:21:47.880: INFO: Lookups using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc failed for: [wheezy_udp@dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@dns-test-service.dns-6308.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_udp@dns-test-service.dns-6308.svc.cluster.local jessie_tcp@dns-test-service.dns-6308.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6308.svc.cluster.local]

    May 17 08:21:52.880: INFO: DNS probes using dns-6308/dns-test-c619108d-a60c-4d15-ae3f-6a7b4148e4cc succeeded

    STEP: deleting the pod 05/17/23 08:21:52.88
    STEP: deleting the test service 05/17/23 08:21:52.887
    STEP: deleting the test headless service 05/17/23 08:21:52.896
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:52.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6308" for this suite. 05/17/23 08:21:52.902
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:52.905
May 17 08:21:52.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 08:21:52.906
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:52.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:52.914
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May 17 08:21:52.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:21:56.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3778" for this suite. 05/17/23 08:21:56.005
------------------------------
â€¢ [3.103 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:52.905
    May 17 08:21:52.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 08:21:52.906
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:52.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:52.914
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May 17 08:21:52.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:56.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3778" for this suite. 05/17/23 08:21:56.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:56.008
May 17 08:21:56.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 08:21:56.008
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:56.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:56.016
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 05/17/23 08:21:56.019
STEP: watching for the Service to be added 05/17/23 08:21:56.024
May 17 08:21:56.025: INFO: Found Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 17 08:21:56.025: INFO: Service test-service-ql8wb created
STEP: Getting /status 05/17/23 08:21:56.025
May 17 08:21:56.027: INFO: Service test-service-ql8wb has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/17/23 08:21:56.027
STEP: watching for the Service to be patched 05/17/23 08:21:56.03
May 17 08:21:56.031: INFO: observed Service test-service-ql8wb in namespace services-5022 with annotations: map[] & LoadBalancer: {[]}
May 17 08:21:56.031: INFO: Found Service test-service-ql8wb in namespace services-5022 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 17 08:21:56.031: INFO: Service test-service-ql8wb has service status patched
STEP: updating the ServiceStatus 05/17/23 08:21:56.031
May 17 08:21:56.035: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/17/23 08:21:56.035
May 17 08:21:56.036: INFO: Observed Service test-service-ql8wb in namespace services-5022 with annotations: map[] & Conditions: {[]}
May 17 08:21:56.036: INFO: Observed event: &Service{ObjectMeta:{test-service-ql8wb  services-5022  1d552128-5bc8-4cad-8ab1-639fe4eaca9b 1207004 0 2023-05-17 08:21:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-17 08:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-17 08:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.102.30.148,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.102.30.148],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 17 08:21:56.036: INFO: Found Service test-service-ql8wb in namespace services-5022 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 08:21:56.036: INFO: Service test-service-ql8wb has service status updated
STEP: patching the service 05/17/23 08:21:56.036
STEP: watching for the Service to be patched 05/17/23 08:21:56.039
May 17 08:21:56.040: INFO: observed Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true]
May 17 08:21:56.040: INFO: observed Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true]
May 17 08:21:56.040: INFO: observed Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true]
May 17 08:21:56.040: INFO: Found Service test-service-ql8wb in namespace services-5022 with labels: map[test-service:patched test-service-static:true]
May 17 08:21:56.040: INFO: Service test-service-ql8wb patched
STEP: deleting the service 05/17/23 08:21:56.04
STEP: watching for the Service to be deleted 05/17/23 08:21:56.045
May 17 08:21:56.046: INFO: Observed event: ADDED
May 17 08:21:56.046: INFO: Observed event: MODIFIED
May 17 08:21:56.046: INFO: Observed event: MODIFIED
May 17 08:21:56.046: INFO: Observed event: MODIFIED
May 17 08:21:56.046: INFO: Found Service test-service-ql8wb in namespace services-5022 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 17 08:21:56.046: INFO: Service test-service-ql8wb deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 08:21:56.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5022" for this suite. 05/17/23 08:21:56.048
------------------------------
â€¢ [0.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:56.008
    May 17 08:21:56.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 08:21:56.008
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:56.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:56.016
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 05/17/23 08:21:56.019
    STEP: watching for the Service to be added 05/17/23 08:21:56.024
    May 17 08:21:56.025: INFO: Found Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May 17 08:21:56.025: INFO: Service test-service-ql8wb created
    STEP: Getting /status 05/17/23 08:21:56.025
    May 17 08:21:56.027: INFO: Service test-service-ql8wb has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/17/23 08:21:56.027
    STEP: watching for the Service to be patched 05/17/23 08:21:56.03
    May 17 08:21:56.031: INFO: observed Service test-service-ql8wb in namespace services-5022 with annotations: map[] & LoadBalancer: {[]}
    May 17 08:21:56.031: INFO: Found Service test-service-ql8wb in namespace services-5022 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May 17 08:21:56.031: INFO: Service test-service-ql8wb has service status patched
    STEP: updating the ServiceStatus 05/17/23 08:21:56.031
    May 17 08:21:56.035: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/17/23 08:21:56.035
    May 17 08:21:56.036: INFO: Observed Service test-service-ql8wb in namespace services-5022 with annotations: map[] & Conditions: {[]}
    May 17 08:21:56.036: INFO: Observed event: &Service{ObjectMeta:{test-service-ql8wb  services-5022  1d552128-5bc8-4cad-8ab1-639fe4eaca9b 1207004 0 2023-05-17 08:21:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-17 08:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-17 08:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.102.30.148,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.102.30.148],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May 17 08:21:56.036: INFO: Found Service test-service-ql8wb in namespace services-5022 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 08:21:56.036: INFO: Service test-service-ql8wb has service status updated
    STEP: patching the service 05/17/23 08:21:56.036
    STEP: watching for the Service to be patched 05/17/23 08:21:56.039
    May 17 08:21:56.040: INFO: observed Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true]
    May 17 08:21:56.040: INFO: observed Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true]
    May 17 08:21:56.040: INFO: observed Service test-service-ql8wb in namespace services-5022 with labels: map[test-service-static:true]
    May 17 08:21:56.040: INFO: Found Service test-service-ql8wb in namespace services-5022 with labels: map[test-service:patched test-service-static:true]
    May 17 08:21:56.040: INFO: Service test-service-ql8wb patched
    STEP: deleting the service 05/17/23 08:21:56.04
    STEP: watching for the Service to be deleted 05/17/23 08:21:56.045
    May 17 08:21:56.046: INFO: Observed event: ADDED
    May 17 08:21:56.046: INFO: Observed event: MODIFIED
    May 17 08:21:56.046: INFO: Observed event: MODIFIED
    May 17 08:21:56.046: INFO: Observed event: MODIFIED
    May 17 08:21:56.046: INFO: Found Service test-service-ql8wb in namespace services-5022 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May 17 08:21:56.046: INFO: Service test-service-ql8wb deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:56.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5022" for this suite. 05/17/23 08:21:56.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:56.051
May 17 08:21:56.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename podtemplate 05/17/23 08:21:56.052
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:56.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:56.06
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/17/23 08:21:56.061
May 17 08:21:56.064: INFO: created test-podtemplate-1
May 17 08:21:56.068: INFO: created test-podtemplate-2
May 17 08:21:56.071: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/17/23 08:21:56.071
STEP: delete collection of pod templates 05/17/23 08:21:56.073
May 17 08:21:56.073: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/17/23 08:21:56.079
May 17 08:21:56.079: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May 17 08:21:56.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5559" for this suite. 05/17/23 08:21:56.082
------------------------------
â€¢ [0.033 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:56.051
    May 17 08:21:56.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename podtemplate 05/17/23 08:21:56.052
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:56.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:56.06
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/17/23 08:21:56.061
    May 17 08:21:56.064: INFO: created test-podtemplate-1
    May 17 08:21:56.068: INFO: created test-podtemplate-2
    May 17 08:21:56.071: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/17/23 08:21:56.071
    STEP: delete collection of pod templates 05/17/23 08:21:56.073
    May 17 08:21:56.073: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/17/23 08:21:56.079
    May 17 08:21:56.079: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:56.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5559" for this suite. 05/17/23 08:21:56.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:56.085
May 17 08:21:56.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename proxy 05/17/23 08:21:56.086
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:56.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:56.094
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May 17 08:21:56.095: INFO: Creating pod...
May 17 08:21:56.099: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2397" to be "running"
May 17 08:21:56.100: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.636753ms
May 17 08:21:58.103: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004088829s
May 17 08:21:58.103: INFO: Pod "agnhost" satisfied condition "running"
May 17 08:21:58.103: INFO: Creating service...
May 17 08:21:58.109: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/DELETE
May 17 08:21:58.111: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 08:21:58.111: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/GET
May 17 08:21:58.113: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 17 08:21:58.113: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/HEAD
May 17 08:21:58.114: INFO: http.Client request:HEAD | StatusCode:200
May 17 08:21:58.114: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/OPTIONS
May 17 08:21:58.115: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 08:21:58.115: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/PATCH
May 17 08:21:58.117: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 08:21:58.117: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/POST
May 17 08:21:58.118: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 08:21:58.118: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/PUT
May 17 08:21:58.119: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 17 08:21:58.119: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/DELETE
May 17 08:21:58.121: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 08:21:58.121: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/GET
May 17 08:21:58.123: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 17 08:21:58.123: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/HEAD
May 17 08:21:58.124: INFO: http.Client request:HEAD | StatusCode:200
May 17 08:21:58.124: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/OPTIONS
May 17 08:21:58.126: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 08:21:58.126: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/PATCH
May 17 08:21:58.128: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 08:21:58.128: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/POST
May 17 08:21:58.129: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 08:21:58.129: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/PUT
May 17 08:21:58.131: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May 17 08:21:58.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2397" for this suite. 05/17/23 08:21:58.133
------------------------------
â€¢ [2.051 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:56.085
    May 17 08:21:56.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename proxy 05/17/23 08:21:56.086
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:56.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:56.094
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May 17 08:21:56.095: INFO: Creating pod...
    May 17 08:21:56.099: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2397" to be "running"
    May 17 08:21:56.100: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.636753ms
    May 17 08:21:58.103: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004088829s
    May 17 08:21:58.103: INFO: Pod "agnhost" satisfied condition "running"
    May 17 08:21:58.103: INFO: Creating service...
    May 17 08:21:58.109: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/DELETE
    May 17 08:21:58.111: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 08:21:58.111: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/GET
    May 17 08:21:58.113: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May 17 08:21:58.113: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/HEAD
    May 17 08:21:58.114: INFO: http.Client request:HEAD | StatusCode:200
    May 17 08:21:58.114: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/OPTIONS
    May 17 08:21:58.115: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 08:21:58.115: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/PATCH
    May 17 08:21:58.117: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 08:21:58.117: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/POST
    May 17 08:21:58.118: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 08:21:58.118: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/pods/agnhost/proxy/some/path/with/PUT
    May 17 08:21:58.119: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 17 08:21:58.119: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/DELETE
    May 17 08:21:58.121: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 08:21:58.121: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/GET
    May 17 08:21:58.123: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May 17 08:21:58.123: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/HEAD
    May 17 08:21:58.124: INFO: http.Client request:HEAD | StatusCode:200
    May 17 08:21:58.124: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/OPTIONS
    May 17 08:21:58.126: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 08:21:58.126: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/PATCH
    May 17 08:21:58.128: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 08:21:58.128: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/POST
    May 17 08:21:58.129: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 08:21:58.129: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2397/services/test-service/proxy/some/path/with/PUT
    May 17 08:21:58.131: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May 17 08:21:58.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2397" for this suite. 05/17/23 08:21:58.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:21:58.137
May 17 08:21:58.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:21:58.137
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:58.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:58.145
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 05/17/23 08:21:58.147
May 17 08:21:58.151: INFO: Waiting up to 5m0s for pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7" in namespace "downward-api-6281" to be "running and ready"
May 17 08:21:58.153: INFO: Pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.359034ms
May 17 08:21:58.153: INFO: The phase of Pod labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:22:00.155: INFO: Pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7": Phase="Running", Reason="", readiness=true. Elapsed: 2.003726137s
May 17 08:22:00.155: INFO: The phase of Pod labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7 is Running (Ready = true)
May 17 08:22:00.155: INFO: Pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7" satisfied condition "running and ready"
May 17 08:22:00.666: INFO: Successfully updated pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:22:04.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6281" for this suite. 05/17/23 08:22:04.68
------------------------------
â€¢ [SLOW TEST] [6.546 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:21:58.137
    May 17 08:21:58.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:21:58.137
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:21:58.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:21:58.145
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 05/17/23 08:21:58.147
    May 17 08:21:58.151: INFO: Waiting up to 5m0s for pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7" in namespace "downward-api-6281" to be "running and ready"
    May 17 08:21:58.153: INFO: Pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.359034ms
    May 17 08:21:58.153: INFO: The phase of Pod labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:22:00.155: INFO: Pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7": Phase="Running", Reason="", readiness=true. Elapsed: 2.003726137s
    May 17 08:22:00.155: INFO: The phase of Pod labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7 is Running (Ready = true)
    May 17 08:22:00.155: INFO: Pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7" satisfied condition "running and ready"
    May 17 08:22:00.666: INFO: Successfully updated pod "labelsupdate2afa4e28-02aa-4826-bef1-e7ae538baaa7"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:22:04.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6281" for this suite. 05/17/23 08:22:04.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:22:04.683
May 17 08:22:04.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 08:22:04.684
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:22:04.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:22:04.692
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4946 05/17/23 08:22:04.694
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 05/17/23 08:22:04.696
STEP: Creating stateful set ss in namespace statefulset-4946 05/17/23 08:22:04.698
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4946 05/17/23 08:22:04.701
May 17 08:22:04.702: INFO: Found 0 stateful pods, waiting for 1
May 17 08:22:14.705: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/17/23 08:22:14.705
May 17 08:22:14.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:22:14.809: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:22:14.809: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:22:14.809: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:22:14.810: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 17 08:22:24.816: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:22:24.816: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:22:24.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999785s
May 17 08:22:25.828: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997909756s
May 17 08:22:26.830: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995042919s
May 17 08:22:27.835: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991472718s
May 17 08:22:28.837: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988342354s
May 17 08:22:29.840: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985481274s
May 17 08:22:30.842: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983227685s
May 17 08:22:31.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981100006s
May 17 08:22:32.846: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978874096s
May 17 08:22:33.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 976.606136ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4946 05/17/23 08:22:34.85
May 17 08:22:34.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 08:22:34.947: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 08:22:34.947: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 08:22:34.947: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 08:22:34.948: INFO: Found 1 stateful pods, waiting for 3
May 17 08:22:44.953: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 08:22:44.953: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 08:22:44.953: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/17/23 08:22:44.953
STEP: Scale down will halt with unhealthy stateful pod 05/17/23 08:22:44.953
May 17 08:22:44.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:22:45.048: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:22:45.048: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:22:45.048: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:22:45.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:22:45.138: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:22:45.138: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:22:45.138: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:22:45.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:22:45.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:22:45.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:22:45.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:22:45.236: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:22:45.237: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 17 08:22:55.244: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:22:55.244: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:22:55.244: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:22:55.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999784s
May 17 08:22:56.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998318891s
May 17 08:22:57.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99568288s
May 17 08:22:58.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992701877s
May 17 08:22:59.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990259147s
May 17 08:23:00.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98766898s
May 17 08:23:01.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985220261s
May 17 08:23:02.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982540407s
May 17 08:23:03.271: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.98051933s
May 17 08:23:04.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 978.003903ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4946 05/17/23 08:23:05.273
May 17 08:23:05.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 08:23:05.368: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 08:23:05.368: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 08:23:05.368: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 08:23:05.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 08:23:05.457: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 08:23:05.457: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 08:23:05.457: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 08:23:05.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 08:23:05.550: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 08:23:05.550: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 08:23:05.550: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 08:23:05.550: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/17/23 08:23:15.558
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 08:23:15.558: INFO: Deleting all statefulset in ns statefulset-4946
May 17 08:23:15.560: INFO: Scaling statefulset ss to 0
May 17 08:23:15.565: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:23:15.566: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 08:23:15.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4946" for this suite. 05/17/23 08:23:15.574
------------------------------
â€¢ [SLOW TEST] [70.893 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:22:04.683
    May 17 08:22:04.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 08:22:04.684
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:22:04.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:22:04.692
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4946 05/17/23 08:22:04.694
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/17/23 08:22:04.696
    STEP: Creating stateful set ss in namespace statefulset-4946 05/17/23 08:22:04.698
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4946 05/17/23 08:22:04.701
    May 17 08:22:04.702: INFO: Found 0 stateful pods, waiting for 1
    May 17 08:22:14.705: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/17/23 08:22:14.705
    May 17 08:22:14.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:22:14.809: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:22:14.809: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:22:14.809: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:22:14.810: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May 17 08:22:24.816: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:22:24.816: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:22:24.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999785s
    May 17 08:22:25.828: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997909756s
    May 17 08:22:26.830: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995042919s
    May 17 08:22:27.835: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991472718s
    May 17 08:22:28.837: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988342354s
    May 17 08:22:29.840: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985481274s
    May 17 08:22:30.842: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983227685s
    May 17 08:22:31.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981100006s
    May 17 08:22:32.846: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978874096s
    May 17 08:22:33.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 976.606136ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4946 05/17/23 08:22:34.85
    May 17 08:22:34.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 08:22:34.947: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 08:22:34.947: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 08:22:34.947: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 08:22:34.948: INFO: Found 1 stateful pods, waiting for 3
    May 17 08:22:44.953: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 08:22:44.953: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 08:22:44.953: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/17/23 08:22:44.953
    STEP: Scale down will halt with unhealthy stateful pod 05/17/23 08:22:44.953
    May 17 08:22:44.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:22:45.048: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:22:45.048: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:22:45.048: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:22:45.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:22:45.138: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:22:45.138: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:22:45.138: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:22:45.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:22:45.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:22:45.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:22:45.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:22:45.236: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:22:45.237: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May 17 08:22:55.244: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:22:55.244: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:22:55.244: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:22:55.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999784s
    May 17 08:22:56.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998318891s
    May 17 08:22:57.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99568288s
    May 17 08:22:58.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992701877s
    May 17 08:22:59.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990259147s
    May 17 08:23:00.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98766898s
    May 17 08:23:01.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985220261s
    May 17 08:23:02.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982540407s
    May 17 08:23:03.271: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.98051933s
    May 17 08:23:04.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 978.003903ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4946 05/17/23 08:23:05.273
    May 17 08:23:05.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 08:23:05.368: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 08:23:05.368: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 08:23:05.368: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 08:23:05.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 08:23:05.457: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 08:23:05.457: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 08:23:05.457: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 08:23:05.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-4946 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 08:23:05.550: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 08:23:05.550: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 08:23:05.550: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 08:23:05.550: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/17/23 08:23:15.558
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 08:23:15.558: INFO: Deleting all statefulset in ns statefulset-4946
    May 17 08:23:15.560: INFO: Scaling statefulset ss to 0
    May 17 08:23:15.565: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:23:15.566: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:23:15.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4946" for this suite. 05/17/23 08:23:15.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:23:15.578
May 17 08:23:15.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename namespaces 05/17/23 08:23:15.578
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:23:15.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:23:15.587
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-6z5jn" 05/17/23 08:23:15.588
May 17 08:23:15.594: INFO: Namespace "e2e-ns-6z5jn-7475" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-6z5jn-7475" 05/17/23 08:23:15.594
May 17 08:23:15.598: INFO: Namespace "e2e-ns-6z5jn-7475" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-6z5jn-7475" 05/17/23 08:23:15.598
May 17 08:23:15.602: INFO: Namespace "e2e-ns-6z5jn-7475" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:23:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1945" for this suite. 05/17/23 08:23:15.604
STEP: Destroying namespace "e2e-ns-6z5jn-7475" for this suite. 05/17/23 08:23:15.606
------------------------------
â€¢ [0.031 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:23:15.578
    May 17 08:23:15.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename namespaces 05/17/23 08:23:15.578
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:23:15.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:23:15.587
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-6z5jn" 05/17/23 08:23:15.588
    May 17 08:23:15.594: INFO: Namespace "e2e-ns-6z5jn-7475" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-6z5jn-7475" 05/17/23 08:23:15.594
    May 17 08:23:15.598: INFO: Namespace "e2e-ns-6z5jn-7475" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-6z5jn-7475" 05/17/23 08:23:15.598
    May 17 08:23:15.602: INFO: Namespace "e2e-ns-6z5jn-7475" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:23:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1945" for this suite. 05/17/23 08:23:15.604
    STEP: Destroying namespace "e2e-ns-6z5jn-7475" for this suite. 05/17/23 08:23:15.606
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:23:15.609
May 17 08:23:15.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename cronjob 05/17/23 08:23:15.61
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:23:15.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:23:15.619
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/17/23 08:23:15.62
STEP: Ensuring more than one job is running at a time 05/17/23 08:23:15.623
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/17/23 08:25:01.626
STEP: Removing cronjob 05/17/23 08:25:01.628
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 17 08:25:01.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9927" for this suite. 05/17/23 08:25:01.632
------------------------------
â€¢ [SLOW TEST] [106.027 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:23:15.609
    May 17 08:23:15.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename cronjob 05/17/23 08:23:15.61
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:23:15.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:23:15.619
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/17/23 08:23:15.62
    STEP: Ensuring more than one job is running at a time 05/17/23 08:23:15.623
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/17/23 08:25:01.626
    STEP: Removing cronjob 05/17/23 08:25:01.628
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 17 08:25:01.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9927" for this suite. 05/17/23 08:25:01.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:25:01.636
May 17 08:25:01.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:25:01.636
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:25:01.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:25:01.645
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1255c625-ab04-46dc-9efe-0d4763cd81b2 05/17/23 08:25:01.649
STEP: Creating the pod 05/17/23 08:25:01.652
May 17 08:25:01.655: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d" in namespace "projected-2523" to be "running and ready"
May 17 08:25:01.658: INFO: Pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631293ms
May 17 08:25:01.658: INFO: The phase of Pod pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d is Pending, waiting for it to be Running (with Ready = true)
May 17 08:25:03.661: INFO: Pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005765134s
May 17 08:25:03.661: INFO: The phase of Pod pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d is Running (Ready = true)
May 17 08:25:03.661: INFO: Pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-1255c625-ab04-46dc-9efe-0d4763cd81b2 05/17/23 08:25:03.672
STEP: waiting to observe update in volume 05/17/23 08:25:03.675
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 08:26:13.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2523" for this suite. 05/17/23 08:26:13.878
------------------------------
â€¢ [SLOW TEST] [72.246 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:25:01.636
    May 17 08:25:01.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:25:01.636
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:25:01.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:25:01.645
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-1255c625-ab04-46dc-9efe-0d4763cd81b2 05/17/23 08:25:01.649
    STEP: Creating the pod 05/17/23 08:25:01.652
    May 17 08:25:01.655: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d" in namespace "projected-2523" to be "running and ready"
    May 17 08:25:01.658: INFO: Pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631293ms
    May 17 08:25:01.658: INFO: The phase of Pod pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:25:03.661: INFO: Pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005765134s
    May 17 08:25:03.661: INFO: The phase of Pod pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d is Running (Ready = true)
    May 17 08:25:03.661: INFO: Pod "pod-projected-configmaps-2c9f2878-8503-409b-b867-018bc605d74d" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-1255c625-ab04-46dc-9efe-0d4763cd81b2 05/17/23 08:25:03.672
    STEP: waiting to observe update in volume 05/17/23 08:25:03.675
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:13.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2523" for this suite. 05/17/23 08:26:13.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:13.882
May 17 08:26:13.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:26:13.883
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:13.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:13.896
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
May 17 08:26:13.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 08:26:15.269
May 17 08:26:15.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 create -f -'
May 17 08:26:15.751: INFO: stderr: ""
May 17 08:26:15.751: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 17 08:26:15.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 delete e2e-test-crd-publish-openapi-1803-crds test-cr'
May 17 08:26:15.811: INFO: stderr: ""
May 17 08:26:15.811: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 17 08:26:15.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 apply -f -'
May 17 08:26:15.992: INFO: stderr: ""
May 17 08:26:15.992: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 17 08:26:15.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 delete e2e-test-crd-publish-openapi-1803-crds test-cr'
May 17 08:26:16.060: INFO: stderr: ""
May 17 08:26:16.060: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/17/23 08:26:16.06
May 17 08:26:16.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 explain e2e-test-crd-publish-openapi-1803-crds'
May 17 08:26:16.233: INFO: stderr: ""
May 17 08:26:16.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1803-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:26:17.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1314" for this suite. 05/17/23 08:26:17.575
------------------------------
â€¢ [3.697 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:13.882
    May 17 08:26:13.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:26:13.883
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:13.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:13.896
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    May 17 08:26:13.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 08:26:15.269
    May 17 08:26:15.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 create -f -'
    May 17 08:26:15.751: INFO: stderr: ""
    May 17 08:26:15.751: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May 17 08:26:15.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 delete e2e-test-crd-publish-openapi-1803-crds test-cr'
    May 17 08:26:15.811: INFO: stderr: ""
    May 17 08:26:15.811: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May 17 08:26:15.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 apply -f -'
    May 17 08:26:15.992: INFO: stderr: ""
    May 17 08:26:15.992: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May 17 08:26:15.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 --namespace=crd-publish-openapi-1314 delete e2e-test-crd-publish-openapi-1803-crds test-cr'
    May 17 08:26:16.060: INFO: stderr: ""
    May 17 08:26:16.060: INFO: stdout: "e2e-test-crd-publish-openapi-1803-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/17/23 08:26:16.06
    May 17 08:26:16.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=crd-publish-openapi-1314 explain e2e-test-crd-publish-openapi-1803-crds'
    May 17 08:26:16.233: INFO: stderr: ""
    May 17 08:26:16.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1803-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:17.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1314" for this suite. 05/17/23 08:26:17.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:17.58
May 17 08:26:17.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename disruption 05/17/23 08:26:17.581
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:17.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:17.588
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 05/17/23 08:26:17.592
STEP: Waiting for all pods to be running 05/17/23 08:26:19.606
May 17 08:26:19.608: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 17 08:26:21.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6740" for this suite. 05/17/23 08:26:21.615
------------------------------
â€¢ [4.039 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:17.58
    May 17 08:26:17.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename disruption 05/17/23 08:26:17.581
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:17.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:17.588
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 05/17/23 08:26:17.592
    STEP: Waiting for all pods to be running 05/17/23 08:26:19.606
    May 17 08:26:19.608: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:21.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6740" for this suite. 05/17/23 08:26:21.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:21.619
May 17 08:26:21.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:26:21.62
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:21.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:21.628
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 05/17/23 08:26:21.629
May 17 08:26:21.633: INFO: Waiting up to 5m0s for pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f" in namespace "emptydir-2948" to be "Succeeded or Failed"
May 17 08:26:21.634: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347994ms
May 17 08:26:23.637: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00420245s
May 17 08:26:25.638: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004916904s
STEP: Saw pod success 05/17/23 08:26:25.638
May 17 08:26:25.638: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f" satisfied condition "Succeeded or Failed"
May 17 08:26:25.640: INFO: Trying to get logs from node k8s-node1 pod pod-6f794068-656b-4ba1-a83a-7c9d01783a9f container test-container: <nil>
STEP: delete the pod 05/17/23 08:26:25.643
May 17 08:26:25.650: INFO: Waiting for pod pod-6f794068-656b-4ba1-a83a-7c9d01783a9f to disappear
May 17 08:26:25.652: INFO: Pod pod-6f794068-656b-4ba1-a83a-7c9d01783a9f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:26:25.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2948" for this suite. 05/17/23 08:26:25.654
------------------------------
â€¢ [4.039 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:21.619
    May 17 08:26:21.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:26:21.62
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:21.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:21.628
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 05/17/23 08:26:21.629
    May 17 08:26:21.633: INFO: Waiting up to 5m0s for pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f" in namespace "emptydir-2948" to be "Succeeded or Failed"
    May 17 08:26:21.634: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347994ms
    May 17 08:26:23.637: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00420245s
    May 17 08:26:25.638: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004916904s
    STEP: Saw pod success 05/17/23 08:26:25.638
    May 17 08:26:25.638: INFO: Pod "pod-6f794068-656b-4ba1-a83a-7c9d01783a9f" satisfied condition "Succeeded or Failed"
    May 17 08:26:25.640: INFO: Trying to get logs from node k8s-node1 pod pod-6f794068-656b-4ba1-a83a-7c9d01783a9f container test-container: <nil>
    STEP: delete the pod 05/17/23 08:26:25.643
    May 17 08:26:25.650: INFO: Waiting for pod pod-6f794068-656b-4ba1-a83a-7c9d01783a9f to disappear
    May 17 08:26:25.652: INFO: Pod pod-6f794068-656b-4ba1-a83a-7c9d01783a9f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:25.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2948" for this suite. 05/17/23 08:26:25.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:25.658
May 17 08:26:25.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:26:25.659
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:25.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:25.667
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 08:26:25.669
May 17 08:26:25.673: INFO: Waiting up to 5m0s for pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19" in namespace "emptydir-6059" to be "Succeeded or Failed"
May 17 08:26:25.675: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452648ms
May 17 08:26:27.677: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003982818s
May 17 08:26:29.679: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005393543s
STEP: Saw pod success 05/17/23 08:26:29.679
May 17 08:26:29.679: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19" satisfied condition "Succeeded or Failed"
May 17 08:26:29.681: INFO: Trying to get logs from node k8s-node1 pod pod-227cb1d1-3801-42c9-a5f3-189101455e19 container test-container: <nil>
STEP: delete the pod 05/17/23 08:26:29.684
May 17 08:26:29.691: INFO: Waiting for pod pod-227cb1d1-3801-42c9-a5f3-189101455e19 to disappear
May 17 08:26:29.692: INFO: Pod pod-227cb1d1-3801-42c9-a5f3-189101455e19 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:26:29.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6059" for this suite. 05/17/23 08:26:29.694
------------------------------
â€¢ [4.039 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:25.658
    May 17 08:26:25.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:26:25.659
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:25.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:25.667
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 08:26:25.669
    May 17 08:26:25.673: INFO: Waiting up to 5m0s for pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19" in namespace "emptydir-6059" to be "Succeeded or Failed"
    May 17 08:26:25.675: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452648ms
    May 17 08:26:27.677: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003982818s
    May 17 08:26:29.679: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005393543s
    STEP: Saw pod success 05/17/23 08:26:29.679
    May 17 08:26:29.679: INFO: Pod "pod-227cb1d1-3801-42c9-a5f3-189101455e19" satisfied condition "Succeeded or Failed"
    May 17 08:26:29.681: INFO: Trying to get logs from node k8s-node1 pod pod-227cb1d1-3801-42c9-a5f3-189101455e19 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:26:29.684
    May 17 08:26:29.691: INFO: Waiting for pod pod-227cb1d1-3801-42c9-a5f3-189101455e19 to disappear
    May 17 08:26:29.692: INFO: Pod pod-227cb1d1-3801-42c9-a5f3-189101455e19 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:29.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6059" for this suite. 05/17/23 08:26:29.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:29.698
May 17 08:26:29.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 08:26:29.699
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:29.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:29.707
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3642 05/17/23 08:26:29.709
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 08:26:29.715
STEP: creating service externalsvc in namespace services-3642 05/17/23 08:26:29.717
STEP: creating replication controller externalsvc in namespace services-3642 05/17/23 08:26:29.724
I0517 08:26:29.728107      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3642, replica count: 2
I0517 08:26:32.779306      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/17/23 08:26:32.781
May 17 08:26:32.790: INFO: Creating new exec pod
May 17 08:26:32.793: INFO: Waiting up to 5m0s for pod "execpod42t5c" in namespace "services-3642" to be "running"
May 17 08:26:32.794: INFO: Pod "execpod42t5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.198405ms
May 17 08:26:34.797: INFO: Pod "execpod42t5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003958637s
May 17 08:26:34.797: INFO: Pod "execpod42t5c" satisfied condition "running"
May 17 08:26:34.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3642 exec execpod42t5c -- /bin/sh -x -c nslookup clusterip-service.services-3642.svc.cluster.local'
May 17 08:26:34.924: INFO: stderr: "+ nslookup clusterip-service.services-3642.svc.cluster.local\n"
May 17 08:26:34.924: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3642.svc.cluster.local\tcanonical name = externalsvc.services-3642.svc.cluster.local.\nName:\texternalsvc.services-3642.svc.cluster.local\nAddress: 10.107.3.109\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3642, will wait for the garbage collector to delete the pods 05/17/23 08:26:34.924
May 17 08:26:34.981: INFO: Deleting ReplicationController externalsvc took: 3.552377ms
May 17 08:26:35.081: INFO: Terminating ReplicationController externalsvc pods took: 100.179748ms
May 17 08:26:37.090: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 08:26:37.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3642" for this suite. 05/17/23 08:26:37.098
------------------------------
â€¢ [SLOW TEST] [7.403 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:29.698
    May 17 08:26:29.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 08:26:29.699
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:29.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:29.707
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3642 05/17/23 08:26:29.709
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 08:26:29.715
    STEP: creating service externalsvc in namespace services-3642 05/17/23 08:26:29.717
    STEP: creating replication controller externalsvc in namespace services-3642 05/17/23 08:26:29.724
    I0517 08:26:29.728107      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3642, replica count: 2
    I0517 08:26:32.779306      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/17/23 08:26:32.781
    May 17 08:26:32.790: INFO: Creating new exec pod
    May 17 08:26:32.793: INFO: Waiting up to 5m0s for pod "execpod42t5c" in namespace "services-3642" to be "running"
    May 17 08:26:32.794: INFO: Pod "execpod42t5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.198405ms
    May 17 08:26:34.797: INFO: Pod "execpod42t5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003958637s
    May 17 08:26:34.797: INFO: Pod "execpod42t5c" satisfied condition "running"
    May 17 08:26:34.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-3642 exec execpod42t5c -- /bin/sh -x -c nslookup clusterip-service.services-3642.svc.cluster.local'
    May 17 08:26:34.924: INFO: stderr: "+ nslookup clusterip-service.services-3642.svc.cluster.local\n"
    May 17 08:26:34.924: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3642.svc.cluster.local\tcanonical name = externalsvc.services-3642.svc.cluster.local.\nName:\texternalsvc.services-3642.svc.cluster.local\nAddress: 10.107.3.109\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3642, will wait for the garbage collector to delete the pods 05/17/23 08:26:34.924
    May 17 08:26:34.981: INFO: Deleting ReplicationController externalsvc took: 3.552377ms
    May 17 08:26:35.081: INFO: Terminating ReplicationController externalsvc pods took: 100.179748ms
    May 17 08:26:37.090: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:37.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3642" for this suite. 05/17/23 08:26:37.098
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:37.1
May 17 08:26:37.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 08:26:37.101
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:37.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:37.109
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7138 05/17/23 08:26:37.111
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 05/17/23 08:26:37.113
STEP: Creating pod with conflicting port in namespace statefulset-7138 05/17/23 08:26:37.115
STEP: Waiting until pod test-pod will start running in namespace statefulset-7138 05/17/23 08:26:37.119
May 17 08:26:37.119: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7138" to be "running"
May 17 08:26:37.120: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.246213ms
May 17 08:26:39.124: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004563097s
May 17 08:26:39.124: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-7138 05/17/23 08:26:39.124
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7138 05/17/23 08:26:39.127
May 17 08:26:39.134: INFO: Observed stateful pod in namespace: statefulset-7138, name: ss-0, uid: 2ec31844-318f-43a0-a3f1-b65e44c4ce91, status phase: Pending. Waiting for statefulset controller to delete.
May 17 08:26:39.141: INFO: Observed stateful pod in namespace: statefulset-7138, name: ss-0, uid: 2ec31844-318f-43a0-a3f1-b65e44c4ce91, status phase: Failed. Waiting for statefulset controller to delete.
May 17 08:26:39.145: INFO: Observed stateful pod in namespace: statefulset-7138, name: ss-0, uid: 2ec31844-318f-43a0-a3f1-b65e44c4ce91, status phase: Failed. Waiting for statefulset controller to delete.
May 17 08:26:39.146: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7138
STEP: Removing pod with conflicting port in namespace statefulset-7138 05/17/23 08:26:39.146
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7138 and will be in running state 05/17/23 08:26:39.152
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 08:26:41.159: INFO: Deleting all statefulset in ns statefulset-7138
May 17 08:26:41.160: INFO: Scaling statefulset ss to 0
May 17 08:26:51.171: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:26:51.173: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 08:26:51.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7138" for this suite. 05/17/23 08:26:51.181
------------------------------
â€¢ [SLOW TEST] [14.083 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:37.1
    May 17 08:26:37.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 08:26:37.101
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:37.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:37.109
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7138 05/17/23 08:26:37.111
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 05/17/23 08:26:37.113
    STEP: Creating pod with conflicting port in namespace statefulset-7138 05/17/23 08:26:37.115
    STEP: Waiting until pod test-pod will start running in namespace statefulset-7138 05/17/23 08:26:37.119
    May 17 08:26:37.119: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7138" to be "running"
    May 17 08:26:37.120: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.246213ms
    May 17 08:26:39.124: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004563097s
    May 17 08:26:39.124: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-7138 05/17/23 08:26:39.124
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7138 05/17/23 08:26:39.127
    May 17 08:26:39.134: INFO: Observed stateful pod in namespace: statefulset-7138, name: ss-0, uid: 2ec31844-318f-43a0-a3f1-b65e44c4ce91, status phase: Pending. Waiting for statefulset controller to delete.
    May 17 08:26:39.141: INFO: Observed stateful pod in namespace: statefulset-7138, name: ss-0, uid: 2ec31844-318f-43a0-a3f1-b65e44c4ce91, status phase: Failed. Waiting for statefulset controller to delete.
    May 17 08:26:39.145: INFO: Observed stateful pod in namespace: statefulset-7138, name: ss-0, uid: 2ec31844-318f-43a0-a3f1-b65e44c4ce91, status phase: Failed. Waiting for statefulset controller to delete.
    May 17 08:26:39.146: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7138
    STEP: Removing pod with conflicting port in namespace statefulset-7138 05/17/23 08:26:39.146
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7138 and will be in running state 05/17/23 08:26:39.152
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 08:26:41.159: INFO: Deleting all statefulset in ns statefulset-7138
    May 17 08:26:41.160: INFO: Scaling statefulset ss to 0
    May 17 08:26:51.171: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:26:51.173: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:51.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7138" for this suite. 05/17/23 08:26:51.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:51.184
May 17 08:26:51.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:26:51.185
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:51.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:51.192
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 05/17/23 08:26:51.193
May 17 08:26:51.197: INFO: Waiting up to 5m0s for pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34" in namespace "downward-api-8031" to be "Succeeded or Failed"
May 17 08:26:51.199: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34": Phase="Pending", Reason="", readiness=false. Elapsed: 1.44861ms
May 17 08:26:53.202: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004627134s
May 17 08:26:55.202: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004673208s
STEP: Saw pod success 05/17/23 08:26:55.202
May 17 08:26:55.202: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34" satisfied condition "Succeeded or Failed"
May 17 08:26:55.204: INFO: Trying to get logs from node k8s-node1 pod downward-api-39192957-8836-4814-9c69-73d97e5c6f34 container dapi-container: <nil>
STEP: delete the pod 05/17/23 08:26:55.208
May 17 08:26:55.213: INFO: Waiting for pod downward-api-39192957-8836-4814-9c69-73d97e5c6f34 to disappear
May 17 08:26:55.214: INFO: Pod downward-api-39192957-8836-4814-9c69-73d97e5c6f34 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 17 08:26:55.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8031" for this suite. 05/17/23 08:26:55.216
------------------------------
â€¢ [4.034 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:51.184
    May 17 08:26:51.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:26:51.185
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:51.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:51.192
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 05/17/23 08:26:51.193
    May 17 08:26:51.197: INFO: Waiting up to 5m0s for pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34" in namespace "downward-api-8031" to be "Succeeded or Failed"
    May 17 08:26:51.199: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34": Phase="Pending", Reason="", readiness=false. Elapsed: 1.44861ms
    May 17 08:26:53.202: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004627134s
    May 17 08:26:55.202: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004673208s
    STEP: Saw pod success 05/17/23 08:26:55.202
    May 17 08:26:55.202: INFO: Pod "downward-api-39192957-8836-4814-9c69-73d97e5c6f34" satisfied condition "Succeeded or Failed"
    May 17 08:26:55.204: INFO: Trying to get logs from node k8s-node1 pod downward-api-39192957-8836-4814-9c69-73d97e5c6f34 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 08:26:55.208
    May 17 08:26:55.213: INFO: Waiting for pod downward-api-39192957-8836-4814-9c69-73d97e5c6f34 to disappear
    May 17 08:26:55.214: INFO: Pod downward-api-39192957-8836-4814-9c69-73d97e5c6f34 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 17 08:26:55.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8031" for this suite. 05/17/23 08:26:55.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:26:55.218
May 17 08:26:55.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename job 05/17/23 08:26:55.219
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:55.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:55.226
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 05/17/23 08:26:55.227
STEP: Ensuring active pods == parallelism 05/17/23 08:26:55.23
STEP: Orphaning one of the Job's Pods 05/17/23 08:26:57.233
May 17 08:26:57.742: INFO: Successfully updated pod "adopt-release-rqzfj"
STEP: Checking that the Job readopts the Pod 05/17/23 08:26:57.742
May 17 08:26:57.742: INFO: Waiting up to 15m0s for pod "adopt-release-rqzfj" in namespace "job-7429" to be "adopted"
May 17 08:26:57.743: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 1.257494ms
May 17 08:26:59.746: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 2.004180792s
May 17 08:26:59.746: INFO: Pod "adopt-release-rqzfj" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/17/23 08:26:59.746
May 17 08:27:00.253: INFO: Successfully updated pod "adopt-release-rqzfj"
STEP: Checking that the Job releases the Pod 05/17/23 08:27:00.253
May 17 08:27:00.253: INFO: Waiting up to 15m0s for pod "adopt-release-rqzfj" in namespace "job-7429" to be "released"
May 17 08:27:00.254: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 1.180152ms
May 17 08:27:02.257: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 2.003402617s
May 17 08:27:02.257: INFO: Pod "adopt-release-rqzfj" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 17 08:27:02.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7429" for this suite. 05/17/23 08:27:02.259
------------------------------
â€¢ [SLOW TEST] [7.044 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:26:55.218
    May 17 08:26:55.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename job 05/17/23 08:26:55.219
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:26:55.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:26:55.226
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 05/17/23 08:26:55.227
    STEP: Ensuring active pods == parallelism 05/17/23 08:26:55.23
    STEP: Orphaning one of the Job's Pods 05/17/23 08:26:57.233
    May 17 08:26:57.742: INFO: Successfully updated pod "adopt-release-rqzfj"
    STEP: Checking that the Job readopts the Pod 05/17/23 08:26:57.742
    May 17 08:26:57.742: INFO: Waiting up to 15m0s for pod "adopt-release-rqzfj" in namespace "job-7429" to be "adopted"
    May 17 08:26:57.743: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 1.257494ms
    May 17 08:26:59.746: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 2.004180792s
    May 17 08:26:59.746: INFO: Pod "adopt-release-rqzfj" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/17/23 08:26:59.746
    May 17 08:27:00.253: INFO: Successfully updated pod "adopt-release-rqzfj"
    STEP: Checking that the Job releases the Pod 05/17/23 08:27:00.253
    May 17 08:27:00.253: INFO: Waiting up to 15m0s for pod "adopt-release-rqzfj" in namespace "job-7429" to be "released"
    May 17 08:27:00.254: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 1.180152ms
    May 17 08:27:02.257: INFO: Pod "adopt-release-rqzfj": Phase="Running", Reason="", readiness=true. Elapsed: 2.003402617s
    May 17 08:27:02.257: INFO: Pod "adopt-release-rqzfj" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 17 08:27:02.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7429" for this suite. 05/17/23 08:27:02.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:27:02.262
May 17 08:27:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 08:27:02.263
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:02.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:02.272
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May 17 08:27:02.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:27:03.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3675" for this suite. 05/17/23 08:27:03.287
------------------------------
â€¢ [1.027 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:27:02.262
    May 17 08:27:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 08:27:02.263
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:02.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:02.272
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May 17 08:27:02.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:27:03.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3675" for this suite. 05/17/23 08:27:03.287
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:27:03.29
May 17 08:27:03.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename watch 05/17/23 08:27:03.29
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:03.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:03.297
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/17/23 08:27:03.298
STEP: creating a watch on configmaps with label B 05/17/23 08:27:03.299
STEP: creating a watch on configmaps with label A or B 05/17/23 08:27:03.3
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/17/23 08:27:03.3
May 17 08:27:03.303: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208444 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:27:03.303: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208444 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/17/23 08:27:03.303
May 17 08:27:03.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208445 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:27:03.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208445 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/17/23 08:27:03.306
May 17 08:27:03.310: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208446 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:27:03.310: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208446 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/17/23 08:27:03.31
May 17 08:27:03.312: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208447 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:27:03.312: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208447 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/17/23 08:27:03.312
May 17 08:27:03.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208448 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:27:03.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208448 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/17/23 08:27:13.315
May 17 08:27:13.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208495 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:27:13.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208495 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 17 08:27:23.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6724" for this suite. 05/17/23 08:27:23.322
------------------------------
â€¢ [SLOW TEST] [20.036 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:27:03.29
    May 17 08:27:03.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename watch 05/17/23 08:27:03.29
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:03.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:03.297
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/17/23 08:27:03.298
    STEP: creating a watch on configmaps with label B 05/17/23 08:27:03.299
    STEP: creating a watch on configmaps with label A or B 05/17/23 08:27:03.3
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/17/23 08:27:03.3
    May 17 08:27:03.303: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208444 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:27:03.303: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208444 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/17/23 08:27:03.303
    May 17 08:27:03.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208445 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:27:03.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208445 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/17/23 08:27:03.306
    May 17 08:27:03.310: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208446 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:27:03.310: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208446 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/17/23 08:27:03.31
    May 17 08:27:03.312: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208447 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:27:03.312: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6724  5e586552-1a47-4d84-a6e3-73102fe0f5be 1208447 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/17/23 08:27:03.312
    May 17 08:27:03.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208448 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:27:03.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208448 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/17/23 08:27:13.315
    May 17 08:27:13.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208495 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:27:13.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6724  d5a4165e-95b5-4099-a6e8-58979d7cb26f 1208495 0 2023-05-17 08:27:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 08:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 17 08:27:23.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6724" for this suite. 05/17/23 08:27:23.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:27:23.326
May 17 08:27:23.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename endpointslice 05/17/23 08:27:23.327
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:23.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:23.334
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 05/17/23 08:27:28.363
STEP: referencing matching pods with named port 05/17/23 08:27:33.367
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/17/23 08:27:38.372
STEP: recreating EndpointSlices after they've been deleted 05/17/23 08:27:43.377
May 17 08:27:43.384: INFO: EndpointSlice for Service endpointslice-9641/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 17 08:27:53.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9641" for this suite. 05/17/23 08:27:53.392
------------------------------
â€¢ [SLOW TEST] [30.068 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:27:23.326
    May 17 08:27:23.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename endpointslice 05/17/23 08:27:23.327
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:23.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:23.334
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 05/17/23 08:27:28.363
    STEP: referencing matching pods with named port 05/17/23 08:27:33.367
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/17/23 08:27:38.372
    STEP: recreating EndpointSlices after they've been deleted 05/17/23 08:27:43.377
    May 17 08:27:43.384: INFO: EndpointSlice for Service endpointslice-9641/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 17 08:27:53.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9641" for this suite. 05/17/23 08:27:53.392
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:27:53.395
May 17 08:27:53.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:27:53.395
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:53.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:53.402
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 05/17/23 08:27:53.404
STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:27:53.406
STEP: Creating a ResourceQuota with not best effort scope 05/17/23 08:27:55.409
STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:27:55.411
STEP: Creating a best-effort pod 05/17/23 08:27:57.413
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/17/23 08:27:57.421
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/17/23 08:27:59.424
STEP: Deleting the pod 05/17/23 08:28:01.427
STEP: Ensuring resource quota status released the pod usage 05/17/23 08:28:01.432
STEP: Creating a not best-effort pod 05/17/23 08:28:03.435
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/17/23 08:28:03.443
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/17/23 08:28:05.446
STEP: Deleting the pod 05/17/23 08:28:07.448
STEP: Ensuring resource quota status released the pod usage 05/17/23 08:28:07.455
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:28:09.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4437" for this suite. 05/17/23 08:28:09.46
------------------------------
â€¢ [SLOW TEST] [16.068 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:27:53.395
    May 17 08:27:53.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:27:53.395
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:27:53.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:27:53.402
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 05/17/23 08:27:53.404
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:27:53.406
    STEP: Creating a ResourceQuota with not best effort scope 05/17/23 08:27:55.409
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 08:27:55.411
    STEP: Creating a best-effort pod 05/17/23 08:27:57.413
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/17/23 08:27:57.421
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/17/23 08:27:59.424
    STEP: Deleting the pod 05/17/23 08:28:01.427
    STEP: Ensuring resource quota status released the pod usage 05/17/23 08:28:01.432
    STEP: Creating a not best-effort pod 05/17/23 08:28:03.435
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/17/23 08:28:03.443
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/17/23 08:28:05.446
    STEP: Deleting the pod 05/17/23 08:28:07.448
    STEP: Ensuring resource quota status released the pod usage 05/17/23 08:28:07.455
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:09.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4437" for this suite. 05/17/23 08:28:09.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:09.463
May 17 08:28:09.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename events 05/17/23 08:28:09.464
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:09.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:09.471
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/17/23 08:28:09.473
STEP: get a list of Events with a label in the current namespace 05/17/23 08:28:09.48
STEP: delete a list of events 05/17/23 08:28:09.481
May 17 08:28:09.481: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/17/23 08:28:09.488
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May 17 08:28:09.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7131" for this suite. 05/17/23 08:28:09.491
------------------------------
â€¢ [0.030 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:09.463
    May 17 08:28:09.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename events 05/17/23 08:28:09.464
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:09.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:09.471
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/17/23 08:28:09.473
    STEP: get a list of Events with a label in the current namespace 05/17/23 08:28:09.48
    STEP: delete a list of events 05/17/23 08:28:09.481
    May 17 08:28:09.481: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/17/23 08:28:09.488
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:09.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7131" for this suite. 05/17/23 08:28:09.491
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:09.493
May 17 08:28:09.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:28:09.494
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:09.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:09.501
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 05/17/23 08:28:09.503
May 17 08:28:09.503: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-3996 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/17/23 08:28:09.542
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:28:09.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3996" for this suite. 05/17/23 08:28:09.55
------------------------------
â€¢ [0.060 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:09.493
    May 17 08:28:09.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:28:09.494
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:09.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:09.501
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 05/17/23 08:28:09.503
    May 17 08:28:09.503: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-3996 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/17/23 08:28:09.542
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:09.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3996" for this suite. 05/17/23 08:28:09.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:09.555
May 17 08:28:09.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 08:28:09.555
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:09.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:09.564
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/17/23 08:28:09.567
STEP: waiting for Deployment to be created 05/17/23 08:28:09.569
STEP: waiting for all Replicas to be Ready 05/17/23 08:28:09.57
May 17 08:28:09.571: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:09.571: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:09.582: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:09.582: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:09.587: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:09.587: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:09.604: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:09.604: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 08:28:10.749: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 17 08:28:10.749: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 17 08:28:11.153: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/17/23 08:28:11.153
W0517 08:28:11.156811      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 17 08:28:11.157: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/17/23 08:28:11.157
May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.164: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.164: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.172: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.172: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:11.179: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:11.179: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:11.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:11.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:12.761: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:12.761: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:12.781: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
STEP: listing Deployments 05/17/23 08:28:12.781
May 17 08:28:12.783: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/17/23 08:28:12.783
May 17 08:28:12.791: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/17/23 08:28:12.791
May 17 08:28:12.796: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:12.798: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:12.805: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:12.811: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:12.815: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:13.772: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:13.780: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:13.785: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:13.792: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 17 08:28:15.166: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/17/23 08:28:15.176
STEP: fetching the DeploymentStatus 05/17/23 08:28:15.181
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 3
STEP: deleting the Deployment 05/17/23 08:28:15.184
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
May 17 08:28:15.188: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 08:28:15.190: INFO: Log out all the ReplicaSets if there is no deployment created
May 17 08:28:15.191: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8935  078bd750-f663-4f95-a9fe-8b202235ed96 1208927 2 2023-05-17 08:28:12 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 076f76e1-3ed4-4aa3-87de-408818e96dfd 0xc00347d507 0xc00347d508}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"076f76e1-3ed4-4aa3-87de-408818e96dfd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00347d590 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May 17 08:28:15.193: INFO: pod: "test-deployment-7b7876f9d6-684gv":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-684gv test-deployment-7b7876f9d6- deployment-8935  3ef06e9f-9f17-4063-96a2-77d2eb5c1826 1208926 0 2023-05-17 08:28:13 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:3842a30e4a15a0d6d89c0cbc24c3682c3bd9a412e2265e5806ac61332f1f818d cni.projectcalico.org/podIP:192.168.169.162/32 cni.projectcalico.org/podIPs:192.168.169.162/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 078bd750-f663-4f95-a9fe-8b202235ed96 0xc00347da47 0xc00347da48}] [] [{kube-controller-manager Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"078bd750-f663-4f95-a9fe-8b202235ed96\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:28:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c27nr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c27nr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.162,StartTime:2023-05-17 08:28:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c8204fca4a0261091db6a91fabd2cb6bd99328a974fdad031b0e03e6ddebf5ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 08:28:15.193: INFO: pod: "test-deployment-7b7876f9d6-hvhf9":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-hvhf9 test-deployment-7b7876f9d6- deployment-8935  05ee0f02-de11-4772-ae84-3584ecf4020f 1208872 0 2023-05-17 08:28:12 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:c58fd740f0cf9963e9842c9791f3128a4d73d50d1cb53b728abe50ece87bb11b cni.projectcalico.org/podIP:192.168.36.99/32 cni.projectcalico.org/podIPs:192.168.36.99/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 078bd750-f663-4f95-a9fe-8b202235ed96 0xc00347dc87 0xc00347dc88}] [] [{kube-controller-manager Update v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"078bd750-f663-4f95-a9fe-8b202235ed96\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rrgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rrgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.99,StartTime:2023-05-17 08:28:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://af89f8fb641547ab2a97680031dc42143c507d89cf30e11fee1cadcc40333836,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 08:28:15.193: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8935  59a1569a-97de-4a09-a1ee-5e3066de0380 1208935 4 2023-05-17 08:28:11 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 076f76e1-3ed4-4aa3-87de-408818e96dfd 0xc00347d5f7 0xc00347d5f8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"076f76e1-3ed4-4aa3-87de-408818e96dfd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00347d680 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 17 08:28:15.195: INFO: pod: "test-deployment-7df74c55ff-nm7zg":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-nm7zg test-deployment-7df74c55ff- deployment-8935  8a75afc0-1473-443c-a46e-ed23aab80bd4 1208895 0 2023-05-17 08:28:12 +0000 UTC 2023-05-17 08:28:14 +0000 UTC 0xc000d6d428 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:5d4d3ea02e332a13f12110709a34d3f7a488c17ed496334ccf0c88ff57109c19 cni.projectcalico.org/podIP:192.168.169.189/32 cni.projectcalico.org/podIPs:192.168.169.189/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 59a1569a-97de-4a09-a1ee-5e3066de0380 0xc000d6d457 0xc000d6d458}] [] [{kube-controller-manager Update v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59a1569a-97de-4a09-a1ee-5e3066de0380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:28:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blstx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blstx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.189,StartTime:2023-05-17 08:28:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://b577d5eb1e7b5a805dce678497f96acdedb95e0bdb7f16a605707ae9c4f9c6b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 08:28:15.195: INFO: pod: "test-deployment-7df74c55ff-vhwl4":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-vhwl4 test-deployment-7df74c55ff- deployment-8935  31b30ff3-c410-4f5a-b361-018f5002c4b3 1208930 0 2023-05-17 08:28:11 +0000 UTC 2023-05-17 08:28:16 +0000 UTC 0xc000d6d668 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:8a7245dae8bd22b27d055f1ea7aa614065c40e13486afe2ce89132dca789ff05 cni.projectcalico.org/podIP:192.168.36.125/32 cni.projectcalico.org/podIPs:192.168.36.125/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 59a1569a-97de-4a09-a1ee-5e3066de0380 0xc000d6d697 0xc000d6d698}] [] [{calico Update v1 2023-05-17 08:28:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 08:28:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59a1569a-97de-4a09-a1ee-5e3066de0380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjglc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjglc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.125,StartTime:2023-05-17 08:28:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://43f15980da5a3bc8ecb93070527cd806b7a6f04e56cda22bdc76cca1603df29a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 08:28:15.195: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8935  90505c48-13f3-4ed5-a99c-30369fb809ef 1208820 3 2023-05-17 08:28:09 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 076f76e1-3ed4-4aa3-87de-408818e96dfd 0xc00347d6e7 0xc00347d6e8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"076f76e1-3ed4-4aa3-87de-408818e96dfd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00347d780 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 08:28:15.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8935" for this suite. 05/17/23 08:28:15.199
------------------------------
â€¢ [SLOW TEST] [5.650 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:09.555
    May 17 08:28:09.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 08:28:09.555
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:09.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:09.564
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/17/23 08:28:09.567
    STEP: waiting for Deployment to be created 05/17/23 08:28:09.569
    STEP: waiting for all Replicas to be Ready 05/17/23 08:28:09.57
    May 17 08:28:09.571: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:09.571: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:09.582: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:09.582: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:09.587: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:09.587: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:09.604: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:09.604: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 08:28:10.749: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May 17 08:28:10.749: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May 17 08:28:11.153: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/17/23 08:28:11.153
    W0517 08:28:11.156811      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 17 08:28:11.157: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/17/23 08:28:11.157
    May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.158: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 0
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.159: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.164: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.164: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.172: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.172: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:11.179: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:11.179: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:11.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:11.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:12.761: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:12.761: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:12.781: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    STEP: listing Deployments 05/17/23 08:28:12.781
    May 17 08:28:12.783: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/17/23 08:28:12.783
    May 17 08:28:12.791: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/17/23 08:28:12.791
    May 17 08:28:12.796: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:12.798: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:12.805: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:12.811: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:12.815: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:13.772: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:13.780: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:13.785: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:13.792: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 08:28:15.166: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/17/23 08:28:15.176
    STEP: fetching the DeploymentStatus 05/17/23 08:28:15.181
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 1
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 2
    May 17 08:28:15.184: INFO: observed Deployment test-deployment in namespace deployment-8935 with ReadyReplicas 3
    STEP: deleting the Deployment 05/17/23 08:28:15.184
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    May 17 08:28:15.188: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 08:28:15.190: INFO: Log out all the ReplicaSets if there is no deployment created
    May 17 08:28:15.191: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8935  078bd750-f663-4f95-a9fe-8b202235ed96 1208927 2 2023-05-17 08:28:12 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 076f76e1-3ed4-4aa3-87de-408818e96dfd 0xc00347d507 0xc00347d508}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"076f76e1-3ed4-4aa3-87de-408818e96dfd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00347d590 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    May 17 08:28:15.193: INFO: pod: "test-deployment-7b7876f9d6-684gv":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-684gv test-deployment-7b7876f9d6- deployment-8935  3ef06e9f-9f17-4063-96a2-77d2eb5c1826 1208926 0 2023-05-17 08:28:13 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:3842a30e4a15a0d6d89c0cbc24c3682c3bd9a412e2265e5806ac61332f1f818d cni.projectcalico.org/podIP:192.168.169.162/32 cni.projectcalico.org/podIPs:192.168.169.162/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 078bd750-f663-4f95-a9fe-8b202235ed96 0xc00347da47 0xc00347da48}] [] [{kube-controller-manager Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"078bd750-f663-4f95-a9fe-8b202235ed96\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:28:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c27nr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c27nr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.162,StartTime:2023-05-17 08:28:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c8204fca4a0261091db6a91fabd2cb6bd99328a974fdad031b0e03e6ddebf5ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 08:28:15.193: INFO: pod: "test-deployment-7b7876f9d6-hvhf9":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-hvhf9 test-deployment-7b7876f9d6- deployment-8935  05ee0f02-de11-4772-ae84-3584ecf4020f 1208872 0 2023-05-17 08:28:12 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:c58fd740f0cf9963e9842c9791f3128a4d73d50d1cb53b728abe50ece87bb11b cni.projectcalico.org/podIP:192.168.36.99/32 cni.projectcalico.org/podIPs:192.168.36.99/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 078bd750-f663-4f95-a9fe-8b202235ed96 0xc00347dc87 0xc00347dc88}] [] [{kube-controller-manager Update v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"078bd750-f663-4f95-a9fe-8b202235ed96\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rrgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rrgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.99,StartTime:2023-05-17 08:28:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://af89f8fb641547ab2a97680031dc42143c507d89cf30e11fee1cadcc40333836,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 08:28:15.193: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8935  59a1569a-97de-4a09-a1ee-5e3066de0380 1208935 4 2023-05-17 08:28:11 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 076f76e1-3ed4-4aa3-87de-408818e96dfd 0xc00347d5f7 0xc00347d5f8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"076f76e1-3ed4-4aa3-87de-408818e96dfd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:28:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00347d680 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May 17 08:28:15.195: INFO: pod: "test-deployment-7df74c55ff-nm7zg":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-nm7zg test-deployment-7df74c55ff- deployment-8935  8a75afc0-1473-443c-a46e-ed23aab80bd4 1208895 0 2023-05-17 08:28:12 +0000 UTC 2023-05-17 08:28:14 +0000 UTC 0xc000d6d428 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:5d4d3ea02e332a13f12110709a34d3f7a488c17ed496334ccf0c88ff57109c19 cni.projectcalico.org/podIP:192.168.169.189/32 cni.projectcalico.org/podIPs:192.168.169.189/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 59a1569a-97de-4a09-a1ee-5e3066de0380 0xc000d6d457 0xc000d6d458}] [] [{kube-controller-manager Update v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59a1569a-97de-4a09-a1ee-5e3066de0380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:28:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.169.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blstx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blstx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.211,PodIP:192.168.169.189,StartTime:2023-05-17 08:28:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://b577d5eb1e7b5a805dce678497f96acdedb95e0bdb7f16a605707ae9c4f9c6b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.169.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 08:28:15.195: INFO: pod: "test-deployment-7df74c55ff-vhwl4":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-vhwl4 test-deployment-7df74c55ff- deployment-8935  31b30ff3-c410-4f5a-b361-018f5002c4b3 1208930 0 2023-05-17 08:28:11 +0000 UTC 2023-05-17 08:28:16 +0000 UTC 0xc000d6d668 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:8a7245dae8bd22b27d055f1ea7aa614065c40e13486afe2ce89132dca789ff05 cni.projectcalico.org/podIP:192.168.36.125/32 cni.projectcalico.org/podIPs:192.168.36.125/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 59a1569a-97de-4a09-a1ee-5e3066de0380 0xc000d6d697 0xc000d6d698}] [] [{calico Update v1 2023-05-17 08:28:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 08:28:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59a1569a-97de-4a09-a1ee-5e3066de0380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjglc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjglc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:28:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.125,StartTime:2023-05-17 08:28:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:28:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://43f15980da5a3bc8ecb93070527cd806b7a6f04e56cda22bdc76cca1603df29a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 08:28:15.195: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8935  90505c48-13f3-4ed5-a99c-30369fb809ef 1208820 3 2023-05-17 08:28:09 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 076f76e1-3ed4-4aa3-87de-408818e96dfd 0xc00347d6e7 0xc00347d6e8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"076f76e1-3ed4-4aa3-87de-408818e96dfd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:28:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00347d780 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:15.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8935" for this suite. 05/17/23 08:28:15.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:15.205
May 17 08:28:15.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:28:15.205
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:15.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:15.212
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:28:15.214
May 17 08:28:15.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7255 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 17 08:28:15.271: INFO: stderr: ""
May 17 08:28:15.271: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/17/23 08:28:15.271
May 17 08:28:15.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7255 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
May 17 08:28:15.731: INFO: stderr: ""
May 17 08:28:15.731: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:28:15.731
May 17 08:28:15.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7255 delete pods e2e-test-httpd-pod'
May 17 08:28:17.770: INFO: stderr: ""
May 17 08:28:17.770: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:28:17.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7255" for this suite. 05/17/23 08:28:17.773
------------------------------
â€¢ [2.571 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:15.205
    May 17 08:28:15.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:28:15.205
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:15.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:15.212
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:28:15.214
    May 17 08:28:15.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7255 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May 17 08:28:15.271: INFO: stderr: ""
    May 17 08:28:15.271: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/17/23 08:28:15.271
    May 17 08:28:15.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7255 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    May 17 08:28:15.731: INFO: stderr: ""
    May 17 08:28:15.731: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:28:15.731
    May 17 08:28:15.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7255 delete pods e2e-test-httpd-pod'
    May 17 08:28:17.770: INFO: stderr: ""
    May 17 08:28:17.770: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:17.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7255" for this suite. 05/17/23 08:28:17.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:17.777
May 17 08:28:17.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:28:17.777
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:17.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:17.786
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-025e39bb-d8bd-4994-861c-61f40137962a 05/17/23 08:28:17.788
STEP: Creating a pod to test consume configMaps 05/17/23 08:28:17.79
May 17 08:28:17.794: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814" in namespace "projected-1779" to be "Succeeded or Failed"
May 17 08:28:17.795: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814": Phase="Pending", Reason="", readiness=false. Elapsed: 1.508028ms
May 17 08:28:19.799: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005137752s
May 17 08:28:21.799: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005539039s
STEP: Saw pod success 05/17/23 08:28:21.799
May 17 08:28:21.799: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814" satisfied condition "Succeeded or Failed"
May 17 08:28:21.801: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 08:28:21.804
May 17 08:28:21.811: INFO: Waiting for pod pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814 to disappear
May 17 08:28:21.812: INFO: Pod pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 08:28:21.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1779" for this suite. 05/17/23 08:28:21.814
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:17.777
    May 17 08:28:17.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:28:17.777
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:17.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:17.786
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-025e39bb-d8bd-4994-861c-61f40137962a 05/17/23 08:28:17.788
    STEP: Creating a pod to test consume configMaps 05/17/23 08:28:17.79
    May 17 08:28:17.794: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814" in namespace "projected-1779" to be "Succeeded or Failed"
    May 17 08:28:17.795: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814": Phase="Pending", Reason="", readiness=false. Elapsed: 1.508028ms
    May 17 08:28:19.799: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005137752s
    May 17 08:28:21.799: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005539039s
    STEP: Saw pod success 05/17/23 08:28:21.799
    May 17 08:28:21.799: INFO: Pod "pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814" satisfied condition "Succeeded or Failed"
    May 17 08:28:21.801: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 08:28:21.804
    May 17 08:28:21.811: INFO: Waiting for pod pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814 to disappear
    May 17 08:28:21.812: INFO: Pod pod-projected-configmaps-68a01648-c277-4f18-b805-6c87d67f2814 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:21.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1779" for this suite. 05/17/23 08:28:21.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:21.817
May 17 08:28:21.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:28:21.818
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:21.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:21.827
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-16d11962-06ce-4b45-be21-3f06d43f1a77 05/17/23 08:28:21.83
STEP: Creating configMap with name cm-test-opt-upd-e49db050-a35d-4971-b5b5-3dacbb41113c 05/17/23 08:28:21.833
STEP: Creating the pod 05/17/23 08:28:21.835
May 17 08:28:21.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a" in namespace "projected-3012" to be "running and ready"
May 17 08:28:21.841: INFO: Pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.300897ms
May 17 08:28:21.841: INFO: The phase of Pod pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a is Pending, waiting for it to be Running (with Ready = true)
May 17 08:28:23.844: INFO: Pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004902117s
May 17 08:28:23.844: INFO: The phase of Pod pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a is Running (Ready = true)
May 17 08:28:23.844: INFO: Pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-16d11962-06ce-4b45-be21-3f06d43f1a77 05/17/23 08:28:23.855
STEP: Updating configmap cm-test-opt-upd-e49db050-a35d-4971-b5b5-3dacbb41113c 05/17/23 08:28:23.858
STEP: Creating configMap with name cm-test-opt-create-619e0c3b-6bf4-4642-a31a-49516f838ed5 05/17/23 08:28:23.861
STEP: waiting to observe update in volume 05/17/23 08:28:23.863
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 08:28:25.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3012" for this suite. 05/17/23 08:28:25.878
------------------------------
â€¢ [4.064 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:21.817
    May 17 08:28:21.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:28:21.818
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:21.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:21.827
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-16d11962-06ce-4b45-be21-3f06d43f1a77 05/17/23 08:28:21.83
    STEP: Creating configMap with name cm-test-opt-upd-e49db050-a35d-4971-b5b5-3dacbb41113c 05/17/23 08:28:21.833
    STEP: Creating the pod 05/17/23 08:28:21.835
    May 17 08:28:21.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a" in namespace "projected-3012" to be "running and ready"
    May 17 08:28:21.841: INFO: Pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.300897ms
    May 17 08:28:21.841: INFO: The phase of Pod pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:28:23.844: INFO: Pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004902117s
    May 17 08:28:23.844: INFO: The phase of Pod pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a is Running (Ready = true)
    May 17 08:28:23.844: INFO: Pod "pod-projected-configmaps-a723b734-0764-468d-9974-adca378b059a" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-16d11962-06ce-4b45-be21-3f06d43f1a77 05/17/23 08:28:23.855
    STEP: Updating configmap cm-test-opt-upd-e49db050-a35d-4971-b5b5-3dacbb41113c 05/17/23 08:28:23.858
    STEP: Creating configMap with name cm-test-opt-create-619e0c3b-6bf4-4642-a31a-49516f838ed5 05/17/23 08:28:23.861
    STEP: waiting to observe update in volume 05/17/23 08:28:23.863
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:25.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3012" for this suite. 05/17/23 08:28:25.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:25.882
May 17 08:28:25.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 08:28:25.882
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:25.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:25.89
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-5900/configmap-test-7c7e3a71-6523-471e-aabc-8676697f81dd 05/17/23 08:28:25.891
STEP: Creating a pod to test consume configMaps 05/17/23 08:28:25.894
May 17 08:28:25.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705" in namespace "configmap-5900" to be "Succeeded or Failed"
May 17 08:28:25.899: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705": Phase="Pending", Reason="", readiness=false. Elapsed: 1.418846ms
May 17 08:28:27.901: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003502641s
May 17 08:28:29.902: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004358548s
STEP: Saw pod success 05/17/23 08:28:29.902
May 17 08:28:29.902: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705" satisfied condition "Succeeded or Failed"
May 17 08:28:29.904: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705 container env-test: <nil>
STEP: delete the pod 05/17/23 08:28:29.907
May 17 08:28:29.915: INFO: Waiting for pod pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705 to disappear
May 17 08:28:29.916: INFO: Pod pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 08:28:29.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5900" for this suite. 05/17/23 08:28:29.918
------------------------------
â€¢ [4.039 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:25.882
    May 17 08:28:25.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 08:28:25.882
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:25.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:25.89
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-5900/configmap-test-7c7e3a71-6523-471e-aabc-8676697f81dd 05/17/23 08:28:25.891
    STEP: Creating a pod to test consume configMaps 05/17/23 08:28:25.894
    May 17 08:28:25.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705" in namespace "configmap-5900" to be "Succeeded or Failed"
    May 17 08:28:25.899: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705": Phase="Pending", Reason="", readiness=false. Elapsed: 1.418846ms
    May 17 08:28:27.901: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003502641s
    May 17 08:28:29.902: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004358548s
    STEP: Saw pod success 05/17/23 08:28:29.902
    May 17 08:28:29.902: INFO: Pod "pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705" satisfied condition "Succeeded or Failed"
    May 17 08:28:29.904: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705 container env-test: <nil>
    STEP: delete the pod 05/17/23 08:28:29.907
    May 17 08:28:29.915: INFO: Waiting for pod pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705 to disappear
    May 17 08:28:29.916: INFO: Pod pod-configmaps-457b5f79-cd91-423d-9614-4c8c4e200705 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:29.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5900" for this suite. 05/17/23 08:28:29.918
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:29.921
May 17 08:28:29.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pod-network-test 05/17/23 08:28:29.921
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:29.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:29.931
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-8642 05/17/23 08:28:29.932
STEP: creating a selector 05/17/23 08:28:29.932
STEP: Creating the service pods in kubernetes 05/17/23 08:28:29.932
May 17 08:28:29.932: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 08:28:29.942: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8642" to be "running and ready"
May 17 08:28:29.943: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531535ms
May 17 08:28:29.944: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:28:31.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004198705s
May 17 08:28:31.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:28:33.947: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005301718s
May 17 08:28:33.947: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:28:35.948: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00563212s
May 17 08:28:35.948: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:28:37.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004230034s
May 17 08:28:37.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:28:39.947: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005211913s
May 17 08:28:39.947: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:28:41.947: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004573293s
May 17 08:28:41.947: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 08:28:41.947: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 08:28:41.948: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8642" to be "running and ready"
May 17 08:28:41.949: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.295119ms
May 17 08:28:41.949: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 08:28:41.949: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 08:28:41.95
May 17 08:28:41.954: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8642" to be "running"
May 17 08:28:41.956: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28197ms
May 17 08:28:43.959: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004740013s
May 17 08:28:43.959: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 08:28:43.961: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 17 08:28:43.961: INFO: Breadth first check of 192.168.36.117 on host 10.0.79.210...
May 17 08:28:43.962: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.108:9080/dial?request=hostname&protocol=http&host=192.168.36.117&port=8083&tries=1'] Namespace:pod-network-test-8642 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:28:43.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:28:43.963: INFO: ExecWithOptions: Clientset creation
May 17 08:28:43.963: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8642/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.108%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.36.117%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 08:28:44.010: INFO: Waiting for responses: map[]
May 17 08:28:44.010: INFO: reached 192.168.36.117 after 0/1 tries
May 17 08:28:44.010: INFO: Breadth first check of 192.168.169.188 on host 10.0.79.211...
May 17 08:28:44.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.108:9080/dial?request=hostname&protocol=http&host=192.168.169.188&port=8083&tries=1'] Namespace:pod-network-test-8642 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:28:44.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:28:44.013: INFO: ExecWithOptions: Clientset creation
May 17 08:28:44.013: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8642/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.108%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.169.188%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 08:28:44.065: INFO: Waiting for responses: map[]
May 17 08:28:44.065: INFO: reached 192.168.169.188 after 0/1 tries
May 17 08:28:44.065: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 17 08:28:44.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8642" for this suite. 05/17/23 08:28:44.068
------------------------------
â€¢ [SLOW TEST] [14.151 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:29.921
    May 17 08:28:29.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 08:28:29.921
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:29.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:29.931
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-8642 05/17/23 08:28:29.932
    STEP: creating a selector 05/17/23 08:28:29.932
    STEP: Creating the service pods in kubernetes 05/17/23 08:28:29.932
    May 17 08:28:29.932: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 08:28:29.942: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8642" to be "running and ready"
    May 17 08:28:29.943: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531535ms
    May 17 08:28:29.944: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:28:31.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004198705s
    May 17 08:28:31.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:28:33.947: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005301718s
    May 17 08:28:33.947: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:28:35.948: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00563212s
    May 17 08:28:35.948: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:28:37.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004230034s
    May 17 08:28:37.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:28:39.947: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005211913s
    May 17 08:28:39.947: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:28:41.947: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004573293s
    May 17 08:28:41.947: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 08:28:41.947: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 08:28:41.948: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8642" to be "running and ready"
    May 17 08:28:41.949: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.295119ms
    May 17 08:28:41.949: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 08:28:41.949: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 08:28:41.95
    May 17 08:28:41.954: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8642" to be "running"
    May 17 08:28:41.956: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28197ms
    May 17 08:28:43.959: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004740013s
    May 17 08:28:43.959: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 08:28:43.961: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    May 17 08:28:43.961: INFO: Breadth first check of 192.168.36.117 on host 10.0.79.210...
    May 17 08:28:43.962: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.108:9080/dial?request=hostname&protocol=http&host=192.168.36.117&port=8083&tries=1'] Namespace:pod-network-test-8642 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:28:43.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:28:43.963: INFO: ExecWithOptions: Clientset creation
    May 17 08:28:43.963: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8642/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.108%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.36.117%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 08:28:44.010: INFO: Waiting for responses: map[]
    May 17 08:28:44.010: INFO: reached 192.168.36.117 after 0/1 tries
    May 17 08:28:44.010: INFO: Breadth first check of 192.168.169.188 on host 10.0.79.211...
    May 17 08:28:44.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.36.108:9080/dial?request=hostname&protocol=http&host=192.168.169.188&port=8083&tries=1'] Namespace:pod-network-test-8642 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:28:44.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:28:44.013: INFO: ExecWithOptions: Clientset creation
    May 17 08:28:44.013: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8642/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.36.108%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.169.188%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 08:28:44.065: INFO: Waiting for responses: map[]
    May 17 08:28:44.065: INFO: reached 192.168.169.188 after 0/1 tries
    May 17 08:28:44.065: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:44.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8642" for this suite. 05/17/23 08:28:44.068
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:44.072
May 17 08:28:44.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:28:44.073
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:44.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:44.082
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 05/17/23 08:28:44.083
May 17 08:28:44.083: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 17 08:28:44.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
May 17 08:28:44.246: INFO: stderr: ""
May 17 08:28:44.246: INFO: stdout: "service/agnhost-replica created\n"
May 17 08:28:44.246: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 17 08:28:44.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
May 17 08:28:44.398: INFO: stderr: ""
May 17 08:28:44.398: INFO: stdout: "service/agnhost-primary created\n"
May 17 08:28:44.398: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 17 08:28:44.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
May 17 08:28:44.544: INFO: stderr: ""
May 17 08:28:44.544: INFO: stdout: "service/frontend created\n"
May 17 08:28:44.544: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 17 08:28:44.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
May 17 08:28:44.698: INFO: stderr: ""
May 17 08:28:44.698: INFO: stdout: "deployment.apps/frontend created\n"
May 17 08:28:44.698: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 17 08:28:44.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
May 17 08:28:44.845: INFO: stderr: ""
May 17 08:28:44.845: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 17 08:28:44.845: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 17 08:28:44.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
May 17 08:28:44.993: INFO: stderr: ""
May 17 08:28:44.994: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/17/23 08:28:44.994
May 17 08:28:44.994: INFO: Waiting for all frontend pods to be Running.
May 17 08:28:50.046: INFO: Waiting for frontend to serve content.
May 17 08:28:55.053: INFO: Trying to add a new entry to the guestbook.
May 17 08:28:55.060: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/17/23 08:28:55.063
May 17 08:28:55.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
May 17 08:28:55.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:28:55.121: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 08:28:55.121
May 17 08:28:55.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
May 17 08:28:55.183: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:28:55.183: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 08:28:55.183
May 17 08:28:55.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
May 17 08:28:55.240: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:28:55.240: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 08:28:55.24
May 17 08:28:55.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
May 17 08:28:55.293: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:28:55.293: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 08:28:55.293
May 17 08:28:55.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
May 17 08:28:55.348: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:28:55.348: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 08:28:55.348
May 17 08:28:55.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
May 17 08:28:55.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:28:55.403: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:28:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8422" for this suite. 05/17/23 08:28:55.405
------------------------------
â€¢ [SLOW TEST] [11.339 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:44.072
    May 17 08:28:44.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:28:44.073
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:44.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:44.082
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 05/17/23 08:28:44.083
    May 17 08:28:44.083: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May 17 08:28:44.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
    May 17 08:28:44.246: INFO: stderr: ""
    May 17 08:28:44.246: INFO: stdout: "service/agnhost-replica created\n"
    May 17 08:28:44.246: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May 17 08:28:44.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
    May 17 08:28:44.398: INFO: stderr: ""
    May 17 08:28:44.398: INFO: stdout: "service/agnhost-primary created\n"
    May 17 08:28:44.398: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May 17 08:28:44.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
    May 17 08:28:44.544: INFO: stderr: ""
    May 17 08:28:44.544: INFO: stdout: "service/frontend created\n"
    May 17 08:28:44.544: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May 17 08:28:44.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
    May 17 08:28:44.698: INFO: stderr: ""
    May 17 08:28:44.698: INFO: stdout: "deployment.apps/frontend created\n"
    May 17 08:28:44.698: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May 17 08:28:44.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
    May 17 08:28:44.845: INFO: stderr: ""
    May 17 08:28:44.845: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May 17 08:28:44.845: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May 17 08:28:44.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 create -f -'
    May 17 08:28:44.993: INFO: stderr: ""
    May 17 08:28:44.994: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/17/23 08:28:44.994
    May 17 08:28:44.994: INFO: Waiting for all frontend pods to be Running.
    May 17 08:28:50.046: INFO: Waiting for frontend to serve content.
    May 17 08:28:55.053: INFO: Trying to add a new entry to the guestbook.
    May 17 08:28:55.060: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/17/23 08:28:55.063
    May 17 08:28:55.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
    May 17 08:28:55.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:28:55.121: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 08:28:55.121
    May 17 08:28:55.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
    May 17 08:28:55.183: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:28:55.183: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 08:28:55.183
    May 17 08:28:55.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
    May 17 08:28:55.240: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:28:55.240: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 08:28:55.24
    May 17 08:28:55.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
    May 17 08:28:55.293: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:28:55.293: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 08:28:55.293
    May 17 08:28:55.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
    May 17 08:28:55.348: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:28:55.348: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 08:28:55.348
    May 17 08:28:55.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-8422 delete --grace-period=0 --force -f -'
    May 17 08:28:55.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:28:55.403: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8422" for this suite. 05/17/23 08:28:55.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:55.411
May 17 08:28:55.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:28:55.412
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:55.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:55.419
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-f8c560a1-12e0-4334-8956-799b094d6d44 05/17/23 08:28:55.421
STEP: Creating a pod to test consume secrets 05/17/23 08:28:55.423
May 17 08:28:55.427: INFO: Waiting up to 5m0s for pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0" in namespace "secrets-6384" to be "Succeeded or Failed"
May 17 08:28:55.429: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541524ms
May 17 08:28:57.431: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403376s
May 17 08:28:59.433: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005424924s
STEP: Saw pod success 05/17/23 08:28:59.433
May 17 08:28:59.433: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0" satisfied condition "Succeeded or Failed"
May 17 08:28:59.435: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:28:59.438
May 17 08:28:59.444: INFO: Waiting for pod pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0 to disappear
May 17 08:28:59.447: INFO: Pod pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:28:59.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6384" for this suite. 05/17/23 08:28:59.449
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:55.411
    May 17 08:28:55.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:28:55.412
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:55.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:55.419
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-f8c560a1-12e0-4334-8956-799b094d6d44 05/17/23 08:28:55.421
    STEP: Creating a pod to test consume secrets 05/17/23 08:28:55.423
    May 17 08:28:55.427: INFO: Waiting up to 5m0s for pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0" in namespace "secrets-6384" to be "Succeeded or Failed"
    May 17 08:28:55.429: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541524ms
    May 17 08:28:57.431: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403376s
    May 17 08:28:59.433: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005424924s
    STEP: Saw pod success 05/17/23 08:28:59.433
    May 17 08:28:59.433: INFO: Pod "pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0" satisfied condition "Succeeded or Failed"
    May 17 08:28:59.435: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:28:59.438
    May 17 08:28:59.444: INFO: Waiting for pod pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0 to disappear
    May 17 08:28:59.447: INFO: Pod pod-secrets-4677265a-f801-45b3-a599-71fa02db5bc0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:28:59.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6384" for this suite. 05/17/23 08:28:59.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:28:59.456
May 17 08:28:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replicaset 05/17/23 08:28:59.457
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:59.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:59.464
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/17/23 08:28:59.467
STEP: Verify that the required pods have come up. 05/17/23 08:28:59.469
May 17 08:28:59.471: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 08:29:04.474: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 08:29:04.474
STEP: Getting /status 05/17/23 08:29:04.474
May 17 08:29:04.475: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/17/23 08:29:04.475
May 17 08:29:04.480: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/17/23 08:29:04.48
May 17 08:29:04.481: INFO: Observed &ReplicaSet event: ADDED
May 17 08:29:04.481: INFO: Observed &ReplicaSet event: MODIFIED
May 17 08:29:04.481: INFO: Observed &ReplicaSet event: MODIFIED
May 17 08:29:04.481: INFO: Observed &ReplicaSet event: MODIFIED
May 17 08:29:04.481: INFO: Found replicaset test-rs in namespace replicaset-8344 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 08:29:04.481: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/17/23 08:29:04.481
May 17 08:29:04.481: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 17 08:29:04.486: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/17/23 08:29:04.486
May 17 08:29:04.487: INFO: Observed &ReplicaSet event: ADDED
May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
May 17 08:29:04.487: INFO: Observed replicaset test-rs in namespace replicaset-8344 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
May 17 08:29:04.487: INFO: Found replicaset test-rs in namespace replicaset-8344 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May 17 08:29:04.487: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 17 08:29:04.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8344" for this suite. 05/17/23 08:29:04.489
------------------------------
â€¢ [SLOW TEST] [5.036 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:28:59.456
    May 17 08:28:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replicaset 05/17/23 08:28:59.457
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:28:59.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:28:59.464
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/17/23 08:28:59.467
    STEP: Verify that the required pods have come up. 05/17/23 08:28:59.469
    May 17 08:28:59.471: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 08:29:04.474: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 08:29:04.474
    STEP: Getting /status 05/17/23 08:29:04.474
    May 17 08:29:04.475: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/17/23 08:29:04.475
    May 17 08:29:04.480: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/17/23 08:29:04.48
    May 17 08:29:04.481: INFO: Observed &ReplicaSet event: ADDED
    May 17 08:29:04.481: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 08:29:04.481: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 08:29:04.481: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 08:29:04.481: INFO: Found replicaset test-rs in namespace replicaset-8344 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 08:29:04.481: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/17/23 08:29:04.481
    May 17 08:29:04.481: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 17 08:29:04.486: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/17/23 08:29:04.486
    May 17 08:29:04.487: INFO: Observed &ReplicaSet event: ADDED
    May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 08:29:04.487: INFO: Observed replicaset test-rs in namespace replicaset-8344 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 08:29:04.487: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 08:29:04.487: INFO: Found replicaset test-rs in namespace replicaset-8344 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May 17 08:29:04.487: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:29:04.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8344" for this suite. 05/17/23 08:29:04.489
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:29:04.492
May 17 08:29:04.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename proxy 05/17/23 08:29:04.493
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:04.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:04.501
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May 17 08:29:04.503: INFO: Creating pod...
May 17 08:29:04.507: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8362" to be "running"
May 17 08:29:04.508: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.145493ms
May 17 08:29:06.510: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003873411s
May 17 08:29:06.510: INFO: Pod "agnhost" satisfied condition "running"
May 17 08:29:06.510: INFO: Creating service...
May 17 08:29:06.518: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=DELETE
May 17 08:29:06.521: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 08:29:06.521: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=OPTIONS
May 17 08:29:06.523: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 08:29:06.523: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=PATCH
May 17 08:29:06.524: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 08:29:06.524: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=POST
May 17 08:29:06.526: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 08:29:06.526: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=PUT
May 17 08:29:06.527: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 17 08:29:06.527: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=DELETE
May 17 08:29:06.529: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 08:29:06.529: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=OPTIONS
May 17 08:29:06.531: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 08:29:06.531: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=PATCH
May 17 08:29:06.533: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 08:29:06.533: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=POST
May 17 08:29:06.535: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 08:29:06.535: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=PUT
May 17 08:29:06.537: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 17 08:29:06.537: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=GET
May 17 08:29:06.538: INFO: http.Client request:GET StatusCode:301
May 17 08:29:06.538: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=GET
May 17 08:29:06.539: INFO: http.Client request:GET StatusCode:301
May 17 08:29:06.539: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=HEAD
May 17 08:29:06.540: INFO: http.Client request:HEAD StatusCode:301
May 17 08:29:06.540: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=HEAD
May 17 08:29:06.542: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May 17 08:29:06.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8362" for this suite. 05/17/23 08:29:06.544
------------------------------
â€¢ [2.054 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:29:04.492
    May 17 08:29:04.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename proxy 05/17/23 08:29:04.493
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:04.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:04.501
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May 17 08:29:04.503: INFO: Creating pod...
    May 17 08:29:04.507: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8362" to be "running"
    May 17 08:29:04.508: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.145493ms
    May 17 08:29:06.510: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003873411s
    May 17 08:29:06.510: INFO: Pod "agnhost" satisfied condition "running"
    May 17 08:29:06.510: INFO: Creating service...
    May 17 08:29:06.518: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=DELETE
    May 17 08:29:06.521: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 08:29:06.521: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=OPTIONS
    May 17 08:29:06.523: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 08:29:06.523: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=PATCH
    May 17 08:29:06.524: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 08:29:06.524: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=POST
    May 17 08:29:06.526: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 08:29:06.526: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=PUT
    May 17 08:29:06.527: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 17 08:29:06.527: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=DELETE
    May 17 08:29:06.529: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 08:29:06.529: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May 17 08:29:06.531: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 08:29:06.531: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=PATCH
    May 17 08:29:06.533: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 08:29:06.533: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=POST
    May 17 08:29:06.535: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 08:29:06.535: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=PUT
    May 17 08:29:06.537: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 17 08:29:06.537: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=GET
    May 17 08:29:06.538: INFO: http.Client request:GET StatusCode:301
    May 17 08:29:06.538: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=GET
    May 17 08:29:06.539: INFO: http.Client request:GET StatusCode:301
    May 17 08:29:06.539: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/pods/agnhost/proxy?method=HEAD
    May 17 08:29:06.540: INFO: http.Client request:HEAD StatusCode:301
    May 17 08:29:06.540: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8362/services/e2e-proxy-test-service/proxy?method=HEAD
    May 17 08:29:06.542: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May 17 08:29:06.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8362" for this suite. 05/17/23 08:29:06.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:29:06.548
May 17 08:29:06.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename events 05/17/23 08:29:06.548
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:06.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:06.556
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/17/23 08:29:06.558
May 17 08:29:06.560: INFO: created test-event-1
May 17 08:29:06.563: INFO: created test-event-2
May 17 08:29:06.565: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/17/23 08:29:06.565
STEP: delete collection of events 05/17/23 08:29:06.566
May 17 08:29:06.566: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/17/23 08:29:06.574
May 17 08:29:06.574: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May 17 08:29:06.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6282" for this suite. 05/17/23 08:29:06.577
------------------------------
â€¢ [0.032 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:29:06.548
    May 17 08:29:06.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename events 05/17/23 08:29:06.548
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:06.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:06.556
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/17/23 08:29:06.558
    May 17 08:29:06.560: INFO: created test-event-1
    May 17 08:29:06.563: INFO: created test-event-2
    May 17 08:29:06.565: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/17/23 08:29:06.565
    STEP: delete collection of events 05/17/23 08:29:06.566
    May 17 08:29:06.566: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/17/23 08:29:06.574
    May 17 08:29:06.574: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May 17 08:29:06.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6282" for this suite. 05/17/23 08:29:06.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:29:06.58
May 17 08:29:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:29:06.581
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:06.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:06.588
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 05/17/23 08:29:06.59
May 17 08:29:06.595: INFO: Waiting up to 5m0s for pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa" in namespace "projected-5385" to be "running and ready"
May 17 08:29:06.596: INFO: Pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.352947ms
May 17 08:29:06.596: INFO: The phase of Pod labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa is Pending, waiting for it to be Running (with Ready = true)
May 17 08:29:08.599: INFO: Pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004348417s
May 17 08:29:08.599: INFO: The phase of Pod labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa is Running (Ready = true)
May 17 08:29:08.599: INFO: Pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa" satisfied condition "running and ready"
May 17 08:29:09.114: INFO: Successfully updated pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:29:13.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5385" for this suite. 05/17/23 08:29:13.131
------------------------------
â€¢ [SLOW TEST] [6.555 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:29:06.58
    May 17 08:29:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:29:06.581
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:06.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:06.588
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 05/17/23 08:29:06.59
    May 17 08:29:06.595: INFO: Waiting up to 5m0s for pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa" in namespace "projected-5385" to be "running and ready"
    May 17 08:29:06.596: INFO: Pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.352947ms
    May 17 08:29:06.596: INFO: The phase of Pod labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:29:08.599: INFO: Pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004348417s
    May 17 08:29:08.599: INFO: The phase of Pod labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa is Running (Ready = true)
    May 17 08:29:08.599: INFO: Pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa" satisfied condition "running and ready"
    May 17 08:29:09.114: INFO: Successfully updated pod "labelsupdatea511aa56-68ea-44fd-a29d-3fca018038aa"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:29:13.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5385" for this suite. 05/17/23 08:29:13.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:29:13.137
May 17 08:29:13.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename containers 05/17/23 08:29:13.137
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:13.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:13.146
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
May 17 08:29:13.151: INFO: Waiting up to 5m0s for pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9" in namespace "containers-4596" to be "running"
May 17 08:29:13.153: INFO: Pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34405ms
May 17 08:29:15.155: INFO: Pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003609877s
May 17 08:29:15.155: INFO: Pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 17 08:29:15.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4596" for this suite. 05/17/23 08:29:15.16
------------------------------
â€¢ [2.027 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:29:13.137
    May 17 08:29:13.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename containers 05/17/23 08:29:13.137
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:13.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:13.146
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    May 17 08:29:13.151: INFO: Waiting up to 5m0s for pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9" in namespace "containers-4596" to be "running"
    May 17 08:29:13.153: INFO: Pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34405ms
    May 17 08:29:15.155: INFO: Pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003609877s
    May 17 08:29:15.155: INFO: Pod "client-containers-494c409d-76dd-4d2f-b0ad-30d11e1b60a9" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 17 08:29:15.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4596" for this suite. 05/17/23 08:29:15.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:29:15.164
May 17 08:29:15.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:29:15.165
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:15.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:15.172
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 05/17/23 08:29:15.174
May 17 08:29:15.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-859 api-versions'
May 17 08:29:15.226: INFO: stderr: ""
May 17 08:29:15.226: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:29:15.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-859" for this suite. 05/17/23 08:29:15.227
------------------------------
â€¢ [0.067 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:29:15.164
    May 17 08:29:15.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:29:15.165
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:15.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:15.172
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 05/17/23 08:29:15.174
    May 17 08:29:15.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-859 api-versions'
    May 17 08:29:15.226: INFO: stderr: ""
    May 17 08:29:15.226: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:29:15.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-859" for this suite. 05/17/23 08:29:15.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:29:15.231
May 17 08:29:15.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename taint-multiple-pods 05/17/23 08:29:15.232
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:15.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:15.239
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
May 17 08:29:15.240: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 08:30:15.252: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
May 17 08:30:15.254: INFO: Starting informer...
STEP: Starting pods... 05/17/23 08:30:15.254
May 17 08:30:15.464: INFO: Pod1 is running on k8s-node1. Tainting Node
May 17 08:30:15.670: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7190" to be "running"
May 17 08:30:15.671: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.172674ms
May 17 08:30:17.674: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003748142s
May 17 08:30:17.674: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May 17 08:30:17.674: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7190" to be "running"
May 17 08:30:17.675: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.39101ms
May 17 08:30:17.675: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May 17 08:30:17.675: INFO: Pod2 is running on k8s-node1. Tainting Node
STEP: Trying to apply a taint on the Node 05/17/23 08:30:17.675
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 08:30:17.684
STEP: Waiting for Pod1 and Pod2 to be deleted 05/17/23 08:30:17.685
May 17 08:30:23.018: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 17 08:30:43.057: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 08:30:43.065
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:30:43.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7190" for this suite. 05/17/23 08:30:43.068
------------------------------
â€¢ [SLOW TEST] [87.840 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:29:15.231
    May 17 08:29:15.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename taint-multiple-pods 05/17/23 08:29:15.232
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:29:15.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:29:15.239
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    May 17 08:29:15.240: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 08:30:15.252: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    May 17 08:30:15.254: INFO: Starting informer...
    STEP: Starting pods... 05/17/23 08:30:15.254
    May 17 08:30:15.464: INFO: Pod1 is running on k8s-node1. Tainting Node
    May 17 08:30:15.670: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7190" to be "running"
    May 17 08:30:15.671: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.172674ms
    May 17 08:30:17.674: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003748142s
    May 17 08:30:17.674: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May 17 08:30:17.674: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7190" to be "running"
    May 17 08:30:17.675: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.39101ms
    May 17 08:30:17.675: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May 17 08:30:17.675: INFO: Pod2 is running on k8s-node1. Tainting Node
    STEP: Trying to apply a taint on the Node 05/17/23 08:30:17.675
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 08:30:17.684
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/17/23 08:30:17.685
    May 17 08:30:23.018: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May 17 08:30:43.057: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 08:30:43.065
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:30:43.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7190" for this suite. 05/17/23 08:30:43.068
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:30:43.072
May 17 08:30:43.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:30:43.072
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:43.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:43.079
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 05/17/23 08:30:43.08
May 17 08:30:43.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7845 create -f -'
May 17 08:30:43.226: INFO: stderr: ""
May 17 08:30:43.226: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/17/23 08:30:43.226
May 17 08:30:43.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7845 diff -f -'
May 17 08:30:43.376: INFO: rc: 1
May 17 08:30:43.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7845 delete -f -'
May 17 08:30:43.432: INFO: stderr: ""
May 17 08:30:43.432: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:30:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7845" for this suite. 05/17/23 08:30:43.434
------------------------------
â€¢ [0.365 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:30:43.072
    May 17 08:30:43.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:30:43.072
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:43.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:43.079
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 05/17/23 08:30:43.08
    May 17 08:30:43.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7845 create -f -'
    May 17 08:30:43.226: INFO: stderr: ""
    May 17 08:30:43.226: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/17/23 08:30:43.226
    May 17 08:30:43.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7845 diff -f -'
    May 17 08:30:43.376: INFO: rc: 1
    May 17 08:30:43.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7845 delete -f -'
    May 17 08:30:43.432: INFO: stderr: ""
    May 17 08:30:43.432: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:30:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7845" for this suite. 05/17/23 08:30:43.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:30:43.437
May 17 08:30:43.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-runtime 05/17/23 08:30:43.438
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:43.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:43.446
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 05/17/23 08:30:43.447
STEP: wait for the container to reach Succeeded 05/17/23 08:30:43.451
STEP: get the container status 05/17/23 08:30:47.461
STEP: the container should be terminated 05/17/23 08:30:47.463
STEP: the termination message should be set 05/17/23 08:30:47.463
May 17 08:30:47.463: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/17/23 08:30:47.463
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 17 08:30:47.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4792" for this suite. 05/17/23 08:30:47.472
------------------------------
â€¢ [4.037 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:30:43.437
    May 17 08:30:43.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-runtime 05/17/23 08:30:43.438
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:43.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:43.446
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 05/17/23 08:30:43.447
    STEP: wait for the container to reach Succeeded 05/17/23 08:30:43.451
    STEP: get the container status 05/17/23 08:30:47.461
    STEP: the container should be terminated 05/17/23 08:30:47.463
    STEP: the termination message should be set 05/17/23 08:30:47.463
    May 17 08:30:47.463: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/17/23 08:30:47.463
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 17 08:30:47.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4792" for this suite. 05/17/23 08:30:47.472
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:30:47.474
May 17 08:30:47.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:30:47.475
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:47.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:47.483
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-2432bf4a-3cbd-41bc-a656-f2d9fe9538bc 05/17/23 08:30:47.484
STEP: Creating a pod to test consume secrets 05/17/23 08:30:47.486
May 17 08:30:47.490: INFO: Waiting up to 5m0s for pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66" in namespace "secrets-93" to be "Succeeded or Failed"
May 17 08:30:47.491: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.320476ms
May 17 08:30:49.493: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003412977s
May 17 08:30:51.495: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005371164s
STEP: Saw pod success 05/17/23 08:30:51.495
May 17 08:30:51.495: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66" satisfied condition "Succeeded or Failed"
May 17 08:30:51.497: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:30:51.507
May 17 08:30:51.513: INFO: Waiting for pod pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66 to disappear
May 17 08:30:51.515: INFO: Pod pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:30:51.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-93" for this suite. 05/17/23 08:30:51.517
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:30:47.474
    May 17 08:30:47.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:30:47.475
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:47.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:47.483
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-2432bf4a-3cbd-41bc-a656-f2d9fe9538bc 05/17/23 08:30:47.484
    STEP: Creating a pod to test consume secrets 05/17/23 08:30:47.486
    May 17 08:30:47.490: INFO: Waiting up to 5m0s for pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66" in namespace "secrets-93" to be "Succeeded or Failed"
    May 17 08:30:47.491: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.320476ms
    May 17 08:30:49.493: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003412977s
    May 17 08:30:51.495: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005371164s
    STEP: Saw pod success 05/17/23 08:30:51.495
    May 17 08:30:51.495: INFO: Pod "pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66" satisfied condition "Succeeded or Failed"
    May 17 08:30:51.497: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:30:51.507
    May 17 08:30:51.513: INFO: Waiting for pod pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66 to disappear
    May 17 08:30:51.515: INFO: Pod pod-secrets-e88175bc-56db-4fb8-81cd-6f53e31deb66 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:30:51.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-93" for this suite. 05/17/23 08:30:51.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:30:51.52
May 17 08:30:51.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 08:30:51.521
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:51.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:51.528
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8586.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8586.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/17/23 08:30:51.53
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8586.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8586.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/17/23 08:30:51.53
STEP: creating a pod to probe /etc/hosts 05/17/23 08:30:51.53
STEP: submitting the pod to kubernetes 05/17/23 08:30:51.53
May 17 08:30:51.534: INFO: Waiting up to 15m0s for pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3" in namespace "dns-8586" to be "running"
May 17 08:30:51.536: INFO: Pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.207308ms
May 17 08:30:53.539: INFO: Pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004230019s
May 17 08:30:53.539: INFO: Pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3" satisfied condition "running"
STEP: retrieving the pod 05/17/23 08:30:53.539
STEP: looking for the results for each expected name from probers 05/17/23 08:30:53.54
May 17 08:30:53.547: INFO: DNS probes using dns-8586/dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3 succeeded

STEP: deleting the pod 05/17/23 08:30:53.547
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 08:30:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8586" for this suite. 05/17/23 08:30:53.555
------------------------------
â€¢ [2.038 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:30:51.52
    May 17 08:30:51.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 08:30:51.521
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:51.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:51.528
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8586.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8586.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/17/23 08:30:51.53
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8586.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8586.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/17/23 08:30:51.53
    STEP: creating a pod to probe /etc/hosts 05/17/23 08:30:51.53
    STEP: submitting the pod to kubernetes 05/17/23 08:30:51.53
    May 17 08:30:51.534: INFO: Waiting up to 15m0s for pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3" in namespace "dns-8586" to be "running"
    May 17 08:30:51.536: INFO: Pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.207308ms
    May 17 08:30:53.539: INFO: Pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004230019s
    May 17 08:30:53.539: INFO: Pod "dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 08:30:53.539
    STEP: looking for the results for each expected name from probers 05/17/23 08:30:53.54
    May 17 08:30:53.547: INFO: DNS probes using dns-8586/dns-test-598ee3a1-b419-4319-af1d-7382abe1bae3 succeeded

    STEP: deleting the pod 05/17/23 08:30:53.547
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 08:30:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8586" for this suite. 05/17/23 08:30:53.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:30:53.559
May 17 08:30:53.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename security-context-test 05/17/23 08:30:53.559
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:53.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:53.566
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
May 17 08:30:53.572: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7" in namespace "security-context-test-1596" to be "Succeeded or Failed"
May 17 08:30:53.573: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19885ms
May 17 08:30:55.575: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003240871s
May 17 08:30:57.576: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003812387s
May 17 08:30:57.576: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 17 08:30:57.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1596" for this suite. 05/17/23 08:30:57.578
------------------------------
â€¢ [4.023 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:30:53.559
    May 17 08:30:53.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename security-context-test 05/17/23 08:30:53.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:53.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:53.566
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    May 17 08:30:53.572: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7" in namespace "security-context-test-1596" to be "Succeeded or Failed"
    May 17 08:30:53.573: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19885ms
    May 17 08:30:55.575: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003240871s
    May 17 08:30:57.576: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003812387s
    May 17 08:30:57.576: INFO: Pod "busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 17 08:30:57.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1596" for this suite. 05/17/23 08:30:57.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:30:57.583
May 17 08:30:57.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 08:30:57.583
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:57.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:57.591
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 05/17/23 08:30:57.594
STEP: waiting for available Endpoint 05/17/23 08:30:57.596
STEP: listing all Endpoints 05/17/23 08:30:57.597
STEP: updating the Endpoint 05/17/23 08:30:57.598
STEP: fetching the Endpoint 05/17/23 08:30:57.602
STEP: patching the Endpoint 05/17/23 08:30:57.603
STEP: fetching the Endpoint 05/17/23 08:30:57.607
STEP: deleting the Endpoint by Collection 05/17/23 08:30:57.608
STEP: waiting for Endpoint deletion 05/17/23 08:30:57.611
STEP: fetching the Endpoint 05/17/23 08:30:57.612
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 08:30:57.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1873" for this suite. 05/17/23 08:30:57.616
------------------------------
â€¢ [0.036 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:30:57.583
    May 17 08:30:57.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 08:30:57.583
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:57.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:57.591
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 05/17/23 08:30:57.594
    STEP: waiting for available Endpoint 05/17/23 08:30:57.596
    STEP: listing all Endpoints 05/17/23 08:30:57.597
    STEP: updating the Endpoint 05/17/23 08:30:57.598
    STEP: fetching the Endpoint 05/17/23 08:30:57.602
    STEP: patching the Endpoint 05/17/23 08:30:57.603
    STEP: fetching the Endpoint 05/17/23 08:30:57.607
    STEP: deleting the Endpoint by Collection 05/17/23 08:30:57.608
    STEP: waiting for Endpoint deletion 05/17/23 08:30:57.611
    STEP: fetching the Endpoint 05/17/23 08:30:57.612
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 08:30:57.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1873" for this suite. 05/17/23 08:30:57.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:30:57.62
May 17 08:30:57.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-pred 05/17/23 08:30:57.621
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:57.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:57.628
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 17 08:30:57.629: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 08:30:57.633: INFO: Waiting for terminating namespaces to be deleted...
May 17 08:30:57.634: INFO: 
Logging pods the apiserver thinks is on node k8s-node1 before test
May 17 08:30:57.637: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.637: INFO: 	Container calico-node ready: true, restart count 0
May 17 08:30:57.637: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.637: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 08:30:57.637: INFO: busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7 from security-context-test-1596 started at 2023-05-17 08:30:53 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.637: INFO: 	Container busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7 ready: false, restart count 0
May 17 08:30:57.637: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 08:30:57.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 08:30:57.637: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 08:30:57.637: INFO: 
Logging pods the apiserver thinks is on node k8s-node2 before test
May 17 08:30:57.640: INFO: calico-kube-controllers-57b57c56f-v49s5 from kube-system started at 2023-05-17 08:30:17 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.640: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 08:30:57.640: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.640: INFO: 	Container calico-node ready: true, restart count 0
May 17 08:30:57.640: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.640: INFO: 	Container coredns ready: true, restart count 0
May 17 08:30:57.640: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.640: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 08:30:57.640: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
May 17 08:30:57.640: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 08:30:57.640: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 08:30:57.640: INFO: 	Container e2e ready: true, restart count 0
May 17 08:30:57.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 08:30:57.640: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
May 17 08:30:57.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 08:30:57.640: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 08:30:57.64
May 17 08:30:57.644: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-834" to be "running"
May 17 08:30:57.645: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.345926ms
May 17 08:30:59.648: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004215665s
May 17 08:30:59.648: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 08:30:59.65
STEP: Trying to apply a random label on the found node. 05/17/23 08:30:59.657
STEP: verifying the node has the label kubernetes.io/e2e-4b1419df-9467-4eb5-9415-61c1a42a53b4 95 05/17/23 08:30:59.663
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/17/23 08:30:59.664
May 17 08:30:59.668: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-834" to be "not pending"
May 17 08:30:59.669: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.022449ms
May 17 08:31:01.672: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.003801975s
May 17 08:31:01.672: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.79.210 on the node which pod4 resides and expect not scheduled 05/17/23 08:31:01.672
May 17 08:31:01.675: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-834" to be "not pending"
May 17 08:31:01.677: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.456458ms
May 17 08:31:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00438709s
May 17 08:31:05.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004170403s
May 17 08:31:07.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004277767s
May 17 08:31:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004497418s
May 17 08:31:11.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005388905s
May 17 08:31:13.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004772744s
May 17 08:31:15.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003734025s
May 17 08:31:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004332878s
May 17 08:31:19.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005346315s
May 17 08:31:21.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005371629s
May 17 08:31:23.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00525488s
May 17 08:31:25.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005205216s
May 17 08:31:27.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.003386017s
May 17 08:31:29.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004279651s
May 17 08:31:31.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004310121s
May 17 08:31:33.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004640367s
May 17 08:31:35.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.003962629s
May 17 08:31:37.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.003998303s
May 17 08:31:39.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.003785745s
May 17 08:31:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004495225s
May 17 08:31:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.003773395s
May 17 08:31:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005182462s
May 17 08:31:47.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004044309s
May 17 08:31:49.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003881633s
May 17 08:31:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004263095s
May 17 08:31:53.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004457322s
May 17 08:31:55.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005709748s
May 17 08:31:57.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004502551s
May 17 08:31:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004258503s
May 17 08:32:01.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005112382s
May 17 08:32:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004428329s
May 17 08:32:05.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00550238s
May 17 08:32:07.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004426466s
May 17 08:32:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004473949s
May 17 08:32:11.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005425535s
May 17 08:32:13.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.003758039s
May 17 08:32:15.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003888489s
May 17 08:32:17.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004028382s
May 17 08:32:19.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005028767s
May 17 08:32:21.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005139981s
May 17 08:32:23.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00429714s
May 17 08:32:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005080814s
May 17 08:32:27.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00478959s
May 17 08:32:29.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004710604s
May 17 08:32:31.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004899346s
May 17 08:32:33.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004455434s
May 17 08:32:35.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005856504s
May 17 08:32:37.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004130981s
May 17 08:32:39.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005417161s
May 17 08:32:41.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005324482s
May 17 08:32:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004049199s
May 17 08:32:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004945885s
May 17 08:32:47.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.003461851s
May 17 08:32:49.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005175177s
May 17 08:32:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005055549s
May 17 08:32:53.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004195945s
May 17 08:32:55.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0047856s
May 17 08:32:57.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00367866s
May 17 08:32:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004407293s
May 17 08:33:01.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005298279s
May 17 08:33:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.004191727s
May 17 08:33:05.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005224889s
May 17 08:33:07.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.003903722s
May 17 08:33:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005030132s
May 17 08:33:11.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004933241s
May 17 08:33:13.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.003585795s
May 17 08:33:15.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004575859s
May 17 08:33:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004617578s
May 17 08:33:19.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005497056s
May 17 08:33:21.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.0036344s
May 17 08:33:23.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005635605s
May 17 08:33:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004559623s
May 17 08:33:27.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.003593878s
May 17 08:33:29.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004184776s
May 17 08:33:31.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005262074s
May 17 08:33:33.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.00388148s
May 17 08:33:35.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.00507187s
May 17 08:33:37.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.003779175s
May 17 08:33:39.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004732238s
May 17 08:33:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005138927s
May 17 08:33:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004157957s
May 17 08:33:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.004786084s
May 17 08:33:47.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.003975247s
May 17 08:33:49.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.004888494s
May 17 08:33:51.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.003805004s
May 17 08:33:53.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.003728201s
May 17 08:33:55.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00467401s
May 17 08:33:57.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.003655338s
May 17 08:33:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004367423s
May 17 08:34:01.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.003813032s
May 17 08:34:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004974758s
May 17 08:34:05.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005073077s
May 17 08:34:07.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00509026s
May 17 08:34:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004789061s
May 17 08:34:11.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004953435s
May 17 08:34:13.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.003719552s
May 17 08:34:15.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004438236s
May 17 08:34:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.004655844s
May 17 08:34:19.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004935706s
May 17 08:34:21.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005013772s
May 17 08:34:23.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00414995s
May 17 08:34:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005058094s
May 17 08:34:27.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004310484s
May 17 08:34:29.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004405249s
May 17 08:34:31.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005619478s
May 17 08:34:33.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.00526921s
May 17 08:34:35.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005736802s
May 17 08:34:37.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004380378s
May 17 08:34:39.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00560971s
May 17 08:34:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004865303s
May 17 08:34:43.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.00451664s
May 17 08:34:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004432659s
May 17 08:34:47.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004229049s
May 17 08:34:49.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005017784s
May 17 08:34:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005060828s
May 17 08:34:53.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.003975713s
May 17 08:34:55.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005465134s
May 17 08:34:57.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004633646s
May 17 08:34:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004466233s
May 17 08:35:01.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005336491s
May 17 08:35:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004955066s
May 17 08:35:05.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005147483s
May 17 08:35:07.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.003971804s
May 17 08:35:09.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.003628072s
May 17 08:35:11.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.00449824s
May 17 08:35:13.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004359129s
May 17 08:35:15.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005600677s
May 17 08:35:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004415729s
May 17 08:35:19.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005465006s
May 17 08:35:21.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005503749s
May 17 08:35:23.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004738743s
May 17 08:35:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004912472s
May 17 08:35:27.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00431991s
May 17 08:35:29.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005446953s
May 17 08:35:31.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005587308s
May 17 08:35:33.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.003625633s
May 17 08:35:35.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004729715s
May 17 08:35:37.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004788321s
May 17 08:35:39.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004966022s
May 17 08:35:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005011489s
May 17 08:35:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.003990649s
May 17 08:35:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005134329s
May 17 08:35:47.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004303843s
May 17 08:35:49.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005378418s
May 17 08:35:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004457091s
May 17 08:35:53.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.003577886s
May 17 08:35:55.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004688936s
May 17 08:35:57.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.003740776s
May 17 08:35:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00463916s
May 17 08:36:01.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004048547s
May 17 08:36:01.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005268818s
STEP: removing the label kubernetes.io/e2e-4b1419df-9467-4eb5-9415-61c1a42a53b4 off the node k8s-node1 05/17/23 08:36:01.681
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4b1419df-9467-4eb5-9415-61c1a42a53b4 05/17/23 08:36:01.688
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:36:01.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-834" for this suite. 05/17/23 08:36:01.692
------------------------------
â€¢ [SLOW TEST] [304.075 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:30:57.62
    May 17 08:30:57.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-pred 05/17/23 08:30:57.621
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:30:57.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:30:57.628
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 17 08:30:57.629: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 08:30:57.633: INFO: Waiting for terminating namespaces to be deleted...
    May 17 08:30:57.634: INFO: 
    Logging pods the apiserver thinks is on node k8s-node1 before test
    May 17 08:30:57.637: INFO: calico-node-vdnbq from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.637: INFO: 	Container calico-node ready: true, restart count 0
    May 17 08:30:57.637: INFO: kube-proxy-t87gs from kube-system started at 2023-05-08 09:52:27 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.637: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 08:30:57.637: INFO: busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7 from security-context-test-1596 started at 2023-05-17 08:30:53 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.637: INFO: 	Container busybox-readonly-false-ae790ab2-82b4-4b2e-adcb-7247473f05d7 ready: false, restart count 0
    May 17 08:30:57.637: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-lj27d from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 08:30:57.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 08:30:57.637: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 08:30:57.637: INFO: 
    Logging pods the apiserver thinks is on node k8s-node2 before test
    May 17 08:30:57.640: INFO: calico-kube-controllers-57b57c56f-v49s5 from kube-system started at 2023-05-17 08:30:17 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.640: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May 17 08:30:57.640: INFO: calico-node-cwcgg from kube-system started at 2023-05-08 10:41:50 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.640: INFO: 	Container calico-node ready: true, restart count 0
    May 17 08:30:57.640: INFO: coredns-5bbd96d687-7h7tp from kube-system started at 2023-05-17 07:39:23 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.640: INFO: 	Container coredns ready: true, restart count 0
    May 17 08:30:57.640: INFO: kube-proxy-stkj5 from kube-system started at 2023-05-08 09:52:47 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.640: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 08:30:57.640: INFO: sonobuoy from sonobuoy started at 2023-05-17 07:37:40 +0000 UTC (1 container statuses recorded)
    May 17 08:30:57.640: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 08:30:57.640: INFO: sonobuoy-e2e-job-874306d8c9804b9b from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 08:30:57.640: INFO: 	Container e2e ready: true, restart count 0
    May 17 08:30:57.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 08:30:57.640: INFO: sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-cvm78 from sonobuoy started at 2023-05-17 07:37:41 +0000 UTC (2 container statuses recorded)
    May 17 08:30:57.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 08:30:57.640: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 08:30:57.64
    May 17 08:30:57.644: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-834" to be "running"
    May 17 08:30:57.645: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.345926ms
    May 17 08:30:59.648: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004215665s
    May 17 08:30:59.648: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 08:30:59.65
    STEP: Trying to apply a random label on the found node. 05/17/23 08:30:59.657
    STEP: verifying the node has the label kubernetes.io/e2e-4b1419df-9467-4eb5-9415-61c1a42a53b4 95 05/17/23 08:30:59.663
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/17/23 08:30:59.664
    May 17 08:30:59.668: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-834" to be "not pending"
    May 17 08:30:59.669: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.022449ms
    May 17 08:31:01.672: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.003801975s
    May 17 08:31:01.672: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.79.210 on the node which pod4 resides and expect not scheduled 05/17/23 08:31:01.672
    May 17 08:31:01.675: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-834" to be "not pending"
    May 17 08:31:01.677: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.456458ms
    May 17 08:31:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00438709s
    May 17 08:31:05.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004170403s
    May 17 08:31:07.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004277767s
    May 17 08:31:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004497418s
    May 17 08:31:11.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005388905s
    May 17 08:31:13.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004772744s
    May 17 08:31:15.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003734025s
    May 17 08:31:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004332878s
    May 17 08:31:19.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005346315s
    May 17 08:31:21.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005371629s
    May 17 08:31:23.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00525488s
    May 17 08:31:25.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005205216s
    May 17 08:31:27.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.003386017s
    May 17 08:31:29.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004279651s
    May 17 08:31:31.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004310121s
    May 17 08:31:33.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004640367s
    May 17 08:31:35.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.003962629s
    May 17 08:31:37.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.003998303s
    May 17 08:31:39.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.003785745s
    May 17 08:31:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004495225s
    May 17 08:31:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.003773395s
    May 17 08:31:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005182462s
    May 17 08:31:47.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004044309s
    May 17 08:31:49.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003881633s
    May 17 08:31:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004263095s
    May 17 08:31:53.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004457322s
    May 17 08:31:55.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005709748s
    May 17 08:31:57.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004502551s
    May 17 08:31:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004258503s
    May 17 08:32:01.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005112382s
    May 17 08:32:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004428329s
    May 17 08:32:05.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00550238s
    May 17 08:32:07.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004426466s
    May 17 08:32:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004473949s
    May 17 08:32:11.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005425535s
    May 17 08:32:13.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.003758039s
    May 17 08:32:15.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003888489s
    May 17 08:32:17.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004028382s
    May 17 08:32:19.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005028767s
    May 17 08:32:21.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005139981s
    May 17 08:32:23.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00429714s
    May 17 08:32:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005080814s
    May 17 08:32:27.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00478959s
    May 17 08:32:29.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004710604s
    May 17 08:32:31.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004899346s
    May 17 08:32:33.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004455434s
    May 17 08:32:35.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005856504s
    May 17 08:32:37.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004130981s
    May 17 08:32:39.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005417161s
    May 17 08:32:41.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005324482s
    May 17 08:32:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004049199s
    May 17 08:32:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004945885s
    May 17 08:32:47.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.003461851s
    May 17 08:32:49.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005175177s
    May 17 08:32:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005055549s
    May 17 08:32:53.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004195945s
    May 17 08:32:55.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0047856s
    May 17 08:32:57.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00367866s
    May 17 08:32:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004407293s
    May 17 08:33:01.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005298279s
    May 17 08:33:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.004191727s
    May 17 08:33:05.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005224889s
    May 17 08:33:07.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.003903722s
    May 17 08:33:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005030132s
    May 17 08:33:11.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004933241s
    May 17 08:33:13.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.003585795s
    May 17 08:33:15.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004575859s
    May 17 08:33:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004617578s
    May 17 08:33:19.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005497056s
    May 17 08:33:21.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.0036344s
    May 17 08:33:23.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005635605s
    May 17 08:33:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004559623s
    May 17 08:33:27.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.003593878s
    May 17 08:33:29.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004184776s
    May 17 08:33:31.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005262074s
    May 17 08:33:33.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.00388148s
    May 17 08:33:35.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.00507187s
    May 17 08:33:37.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.003779175s
    May 17 08:33:39.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004732238s
    May 17 08:33:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005138927s
    May 17 08:33:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004157957s
    May 17 08:33:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.004786084s
    May 17 08:33:47.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.003975247s
    May 17 08:33:49.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.004888494s
    May 17 08:33:51.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.003805004s
    May 17 08:33:53.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.003728201s
    May 17 08:33:55.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00467401s
    May 17 08:33:57.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.003655338s
    May 17 08:33:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004367423s
    May 17 08:34:01.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.003813032s
    May 17 08:34:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004974758s
    May 17 08:34:05.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005073077s
    May 17 08:34:07.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00509026s
    May 17 08:34:09.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004789061s
    May 17 08:34:11.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004953435s
    May 17 08:34:13.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.003719552s
    May 17 08:34:15.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004438236s
    May 17 08:34:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.004655844s
    May 17 08:34:19.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004935706s
    May 17 08:34:21.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005013772s
    May 17 08:34:23.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00414995s
    May 17 08:34:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005058094s
    May 17 08:34:27.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004310484s
    May 17 08:34:29.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004405249s
    May 17 08:34:31.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005619478s
    May 17 08:34:33.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.00526921s
    May 17 08:34:35.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005736802s
    May 17 08:34:37.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004380378s
    May 17 08:34:39.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00560971s
    May 17 08:34:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004865303s
    May 17 08:34:43.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.00451664s
    May 17 08:34:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004432659s
    May 17 08:34:47.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004229049s
    May 17 08:34:49.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005017784s
    May 17 08:34:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005060828s
    May 17 08:34:53.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.003975713s
    May 17 08:34:55.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005465134s
    May 17 08:34:57.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004633646s
    May 17 08:34:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004466233s
    May 17 08:35:01.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005336491s
    May 17 08:35:03.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004955066s
    May 17 08:35:05.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005147483s
    May 17 08:35:07.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.003971804s
    May 17 08:35:09.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.003628072s
    May 17 08:35:11.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.00449824s
    May 17 08:35:13.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004359129s
    May 17 08:35:15.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005600677s
    May 17 08:35:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004415729s
    May 17 08:35:19.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005465006s
    May 17 08:35:21.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005503749s
    May 17 08:35:23.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004738743s
    May 17 08:35:25.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004912472s
    May 17 08:35:27.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00431991s
    May 17 08:35:29.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005446953s
    May 17 08:35:31.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005587308s
    May 17 08:35:33.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.003625633s
    May 17 08:35:35.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004729715s
    May 17 08:35:37.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004788321s
    May 17 08:35:39.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004966022s
    May 17 08:35:41.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005011489s
    May 17 08:35:43.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.003990649s
    May 17 08:35:45.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005134329s
    May 17 08:35:47.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004303843s
    May 17 08:35:49.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005378418s
    May 17 08:35:51.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004457091s
    May 17 08:35:53.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.003577886s
    May 17 08:35:55.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004688936s
    May 17 08:35:57.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.003740776s
    May 17 08:35:59.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00463916s
    May 17 08:36:01.679: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004048547s
    May 17 08:36:01.681: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005268818s
    STEP: removing the label kubernetes.io/e2e-4b1419df-9467-4eb5-9415-61c1a42a53b4 off the node k8s-node1 05/17/23 08:36:01.681
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-4b1419df-9467-4eb5-9415-61c1a42a53b4 05/17/23 08:36:01.688
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:01.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-834" for this suite. 05/17/23 08:36:01.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:01.696
May 17 08:36:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 08:36:01.696
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:01.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:01.703
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/17/23 08:36:01.706
May 17 08:36:01.706: INFO: Creating simple deployment test-deployment-gr5s7
May 17 08:36:01.712: INFO: deployment "test-deployment-gr5s7" doesn't have the required revision set
STEP: Getting /status 05/17/23 08:36:03.72
May 17 08:36:03.722: INFO: Deployment test-deployment-gr5s7 has Conditions: [{Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 05/17/23 08:36:03.723
May 17 08:36:03.727: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 8, 36, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 8, 36, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 8, 36, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 8, 36, 1, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gr5s7-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/17/23 08:36:03.727
May 17 08:36:03.728: INFO: Observed &Deployment event: ADDED
May 17 08:36:03.728: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gr5s7-54bc444df" is progressing.}
May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
May 17 08:36:03.729: INFO: Found Deployment test-deployment-gr5s7 in namespace deployment-8851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 08:36:03.729: INFO: Deployment test-deployment-gr5s7 has an updated status
STEP: patching the Statefulset Status 05/17/23 08:36:03.729
May 17 08:36:03.729: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 17 08:36:03.733: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/17/23 08:36:03.733
May 17 08:36:03.734: INFO: Observed &Deployment event: ADDED
May 17 08:36:03.734: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
May 17 08:36:03.734: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.734: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
May 17 08:36:03.734: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gr5s7-54bc444df" is progressing.}
May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
May 17 08:36:03.735: INFO: Found deployment test-deployment-gr5s7 in namespace deployment-8851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May 17 08:36:03.735: INFO: Deployment test-deployment-gr5s7 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 08:36:03.737: INFO: Deployment "test-deployment-gr5s7":
&Deployment{ObjectMeta:{test-deployment-gr5s7  deployment-8851  a74c4c14-76d8-491e-a774-50c099d80000 1210756 1 2023-05-17 08:36:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-17 08:36:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-17 08:36:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-17 08:36:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b880e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gr5s7-54bc444df",LastUpdateTime:2023-05-17 08:36:03 +0000 UTC,LastTransitionTime:2023-05-17 08:36:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 08:36:03.738: INFO: New ReplicaSet "test-deployment-gr5s7-54bc444df" of Deployment "test-deployment-gr5s7":
&ReplicaSet{ObjectMeta:{test-deployment-gr5s7-54bc444df  deployment-8851  0bc4775a-fa7f-4093-8939-b3197d8eef47 1210751 1 2023-05-17 08:36:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gr5s7 a74c4c14-76d8-491e-a774-50c099d80000 0xc004b884c7 0xc004b884c8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:36:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a74c4c14-76d8-491e-a774-50c099d80000\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:36:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b88578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 08:36:03.740: INFO: Pod "test-deployment-gr5s7-54bc444df-dwgfc" is available:
&Pod{ObjectMeta:{test-deployment-gr5s7-54bc444df-dwgfc test-deployment-gr5s7-54bc444df- deployment-8851  ecec8a3c-0da8-4aa0-b16b-5286981074c6 1210750 0 2023-05-17 08:36:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:5c5e4d05f2116f4190e7c09c882e0f7f908b0a03ea188f58ba5a6e45672de3ef cni.projectcalico.org/podIP:192.168.36.78/32 cni.projectcalico.org/podIPs:192.168.36.78/32] [{apps/v1 ReplicaSet test-deployment-gr5s7-54bc444df 0bc4775a-fa7f-4093-8939-b3197d8eef47 0xc0032d8ea7 0xc0032d8ea8}] [] [{kube-controller-manager Update v1 2023-05-17 08:36:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bc4775a-fa7f-4093-8939-b3197d8eef47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:36:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:36:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-278qk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-278qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.78,StartTime:2023-05-17 08:36:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:36:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c586589b23ee4dc3c0ceec4be6cd8deb8d336e35600ae9399052d516dc2fad49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 08:36:03.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8851" for this suite. 05/17/23 08:36:03.741
------------------------------
â€¢ [2.048 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:01.696
    May 17 08:36:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 08:36:01.696
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:01.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:01.703
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/17/23 08:36:01.706
    May 17 08:36:01.706: INFO: Creating simple deployment test-deployment-gr5s7
    May 17 08:36:01.712: INFO: deployment "test-deployment-gr5s7" doesn't have the required revision set
    STEP: Getting /status 05/17/23 08:36:03.72
    May 17 08:36:03.722: INFO: Deployment test-deployment-gr5s7 has Conditions: [{Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 05/17/23 08:36:03.723
    May 17 08:36:03.727: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 8, 36, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 8, 36, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 8, 36, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 8, 36, 1, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gr5s7-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/17/23 08:36:03.727
    May 17 08:36:03.728: INFO: Observed &Deployment event: ADDED
    May 17 08:36:03.728: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
    May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gr5s7-54bc444df" is progressing.}
    May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
    May 17 08:36:03.729: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 08:36:03.729: INFO: Observed Deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
    May 17 08:36:03.729: INFO: Found Deployment test-deployment-gr5s7 in namespace deployment-8851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 08:36:03.729: INFO: Deployment test-deployment-gr5s7 has an updated status
    STEP: patching the Statefulset Status 05/17/23 08:36:03.729
    May 17 08:36:03.729: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 17 08:36:03.733: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/17/23 08:36:03.733
    May 17 08:36:03.734: INFO: Observed &Deployment event: ADDED
    May 17 08:36:03.734: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
    May 17 08:36:03.734: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.734: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gr5s7-54bc444df"}
    May 17 08:36:03.734: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:01 +0000 UTC 2023-05-17 08:36:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gr5s7-54bc444df" is progressing.}
    May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
    May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 08:36:02 +0000 UTC 2023-05-17 08:36:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gr5s7-54bc444df" has successfully progressed.}
    May 17 08:36:03.735: INFO: Observed deployment test-deployment-gr5s7 in namespace deployment-8851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 08:36:03.735: INFO: Observed &Deployment event: MODIFIED
    May 17 08:36:03.735: INFO: Found deployment test-deployment-gr5s7 in namespace deployment-8851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May 17 08:36:03.735: INFO: Deployment test-deployment-gr5s7 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 08:36:03.737: INFO: Deployment "test-deployment-gr5s7":
    &Deployment{ObjectMeta:{test-deployment-gr5s7  deployment-8851  a74c4c14-76d8-491e-a774-50c099d80000 1210756 1 2023-05-17 08:36:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-17 08:36:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-17 08:36:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-17 08:36:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b880e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gr5s7-54bc444df",LastUpdateTime:2023-05-17 08:36:03 +0000 UTC,LastTransitionTime:2023-05-17 08:36:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 08:36:03.738: INFO: New ReplicaSet "test-deployment-gr5s7-54bc444df" of Deployment "test-deployment-gr5s7":
    &ReplicaSet{ObjectMeta:{test-deployment-gr5s7-54bc444df  deployment-8851  0bc4775a-fa7f-4093-8939-b3197d8eef47 1210751 1 2023-05-17 08:36:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gr5s7 a74c4c14-76d8-491e-a774-50c099d80000 0xc004b884c7 0xc004b884c8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:36:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a74c4c14-76d8-491e-a774-50c099d80000\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:36:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b88578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 08:36:03.740: INFO: Pod "test-deployment-gr5s7-54bc444df-dwgfc" is available:
    &Pod{ObjectMeta:{test-deployment-gr5s7-54bc444df-dwgfc test-deployment-gr5s7-54bc444df- deployment-8851  ecec8a3c-0da8-4aa0-b16b-5286981074c6 1210750 0 2023-05-17 08:36:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:5c5e4d05f2116f4190e7c09c882e0f7f908b0a03ea188f58ba5a6e45672de3ef cni.projectcalico.org/podIP:192.168.36.78/32 cni.projectcalico.org/podIPs:192.168.36.78/32] [{apps/v1 ReplicaSet test-deployment-gr5s7-54bc444df 0bc4775a-fa7f-4093-8939-b3197d8eef47 0xc0032d8ea7 0xc0032d8ea8}] [] [{kube-controller-manager Update v1 2023-05-17 08:36:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bc4775a-fa7f-4093-8939-b3197d8eef47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-17 08:36:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-17 08:36:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-278qk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-278qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:36:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.78,StartTime:2023-05-17 08:36:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:36:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://c586589b23ee4dc3c0ceec4be6cd8deb8d336e35600ae9399052d516dc2fad49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:03.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8851" for this suite. 05/17/23 08:36:03.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:03.744
May 17 08:36:03.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 08:36:03.745
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:03.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:03.753
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9597 05/17/23 08:36:03.754
STEP: creating service affinity-nodeport-transition in namespace services-9597 05/17/23 08:36:03.754
STEP: creating replication controller affinity-nodeport-transition in namespace services-9597 05/17/23 08:36:03.762
I0517 08:36:03.765374      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9597, replica count: 3
I0517 08:36:06.816527      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 08:36:06.821: INFO: Creating new exec pod
May 17 08:36:06.823: INFO: Waiting up to 5m0s for pod "execpod-affinity8grx9" in namespace "services-9597" to be "running"
May 17 08:36:06.824: INFO: Pod "execpod-affinity8grx9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.244905ms
May 17 08:36:08.827: INFO: Pod "execpod-affinity8grx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003635499s
May 17 08:36:08.827: INFO: Pod "execpod-affinity8grx9" satisfied condition "running"
May 17 08:36:09.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
May 17 08:36:09.925: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 17 08:36:09.925: INFO: stdout: ""
May 17 08:36:09.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 10.98.35.163 80'
May 17 08:36:10.020: INFO: stderr: "+ nc -v -z -w 2 10.98.35.163 80\nConnection to 10.98.35.163 80 port [tcp/http] succeeded!\n"
May 17 08:36:10.020: INFO: stdout: ""
May 17 08:36:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 32084'
May 17 08:36:10.119: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 32084\nConnection to 10.0.79.210 32084 port [tcp/*] succeeded!\n"
May 17 08:36:10.119: INFO: stdout: ""
May 17 08:36:10.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 32084'
May 17 08:36:10.214: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 32084\nConnection to 10.0.79.211 32084 port [tcp/*] succeeded!\n"
May 17 08:36:10.214: INFO: stdout: ""
May 17 08:36:10.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.79.210:32084/ ; done'
May 17 08:36:10.364: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n"
May 17 08:36:10.364: INFO: stdout: "\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-bnmjn\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n"
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-bnmjn
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
May 17 08:36:10.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.79.210:32084/ ; done'
May 17 08:36:10.509: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n"
May 17 08:36:10.509: INFO: stdout: "\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts"
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
May 17 08:36:10.509: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9597, will wait for the garbage collector to delete the pods 05/17/23 08:36:10.517
May 17 08:36:10.574: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.144647ms
May 17 08:36:10.675: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.01198ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 08:36:12.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9597" for this suite. 05/17/23 08:36:12.692
------------------------------
â€¢ [SLOW TEST] [8.950 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:03.744
    May 17 08:36:03.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 08:36:03.745
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:03.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:03.753
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9597 05/17/23 08:36:03.754
    STEP: creating service affinity-nodeport-transition in namespace services-9597 05/17/23 08:36:03.754
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9597 05/17/23 08:36:03.762
    I0517 08:36:03.765374      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9597, replica count: 3
    I0517 08:36:06.816527      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 08:36:06.821: INFO: Creating new exec pod
    May 17 08:36:06.823: INFO: Waiting up to 5m0s for pod "execpod-affinity8grx9" in namespace "services-9597" to be "running"
    May 17 08:36:06.824: INFO: Pod "execpod-affinity8grx9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.244905ms
    May 17 08:36:08.827: INFO: Pod "execpod-affinity8grx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003635499s
    May 17 08:36:08.827: INFO: Pod "execpod-affinity8grx9" satisfied condition "running"
    May 17 08:36:09.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    May 17 08:36:09.925: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May 17 08:36:09.925: INFO: stdout: ""
    May 17 08:36:09.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 10.98.35.163 80'
    May 17 08:36:10.020: INFO: stderr: "+ nc -v -z -w 2 10.98.35.163 80\nConnection to 10.98.35.163 80 port [tcp/http] succeeded!\n"
    May 17 08:36:10.020: INFO: stdout: ""
    May 17 08:36:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 32084'
    May 17 08:36:10.119: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 32084\nConnection to 10.0.79.210 32084 port [tcp/*] succeeded!\n"
    May 17 08:36:10.119: INFO: stdout: ""
    May 17 08:36:10.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 32084'
    May 17 08:36:10.214: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 32084\nConnection to 10.0.79.211 32084 port [tcp/*] succeeded!\n"
    May 17 08:36:10.214: INFO: stdout: ""
    May 17 08:36:10.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.79.210:32084/ ; done'
    May 17 08:36:10.364: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n"
    May 17 08:36:10.364: INFO: stdout: "\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-bnmjn\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-5bc7n\naffinity-nodeport-transition-5bc7n"
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-bnmjn
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.364: INFO: Received response from host: affinity-nodeport-transition-5bc7n
    May 17 08:36:10.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-9597 exec execpod-affinity8grx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.79.210:32084/ ; done'
    May 17 08:36:10.509: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:32084/\n"
    May 17 08:36:10.509: INFO: stdout: "\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts\naffinity-nodeport-transition-lxpts"
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Received response from host: affinity-nodeport-transition-lxpts
    May 17 08:36:10.509: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9597, will wait for the garbage collector to delete the pods 05/17/23 08:36:10.517
    May 17 08:36:10.574: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.144647ms
    May 17 08:36:10.675: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.01198ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:12.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9597" for this suite. 05/17/23 08:36:12.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:12.695
May 17 08:36:12.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:36:12.696
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:12.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:12.706
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 08:36:12.708
May 17 08:36:12.713: INFO: Waiting up to 5m0s for pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0" in namespace "emptydir-1456" to be "Succeeded or Failed"
May 17 08:36:12.714: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.329167ms
May 17 08:36:14.716: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003866399s
May 17 08:36:16.716: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003455777s
STEP: Saw pod success 05/17/23 08:36:16.716
May 17 08:36:16.716: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0" satisfied condition "Succeeded or Failed"
May 17 08:36:16.718: INFO: Trying to get logs from node k8s-node1 pod pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0 container test-container: <nil>
STEP: delete the pod 05/17/23 08:36:16.727
May 17 08:36:16.733: INFO: Waiting for pod pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0 to disappear
May 17 08:36:16.734: INFO: Pod pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:36:16.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1456" for this suite. 05/17/23 08:36:16.736
------------------------------
â€¢ [4.044 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:12.695
    May 17 08:36:12.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:36:12.696
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:12.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:12.706
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 08:36:12.708
    May 17 08:36:12.713: INFO: Waiting up to 5m0s for pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0" in namespace "emptydir-1456" to be "Succeeded or Failed"
    May 17 08:36:12.714: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.329167ms
    May 17 08:36:14.716: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003866399s
    May 17 08:36:16.716: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003455777s
    STEP: Saw pod success 05/17/23 08:36:16.716
    May 17 08:36:16.716: INFO: Pod "pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0" satisfied condition "Succeeded or Failed"
    May 17 08:36:16.718: INFO: Trying to get logs from node k8s-node1 pod pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:36:16.727
    May 17 08:36:16.733: INFO: Waiting for pod pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0 to disappear
    May 17 08:36:16.734: INFO: Pod pod-2bc272b3-eaca-4f5f-b366-75b1f2a80ef0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:16.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1456" for this suite. 05/17/23 08:36:16.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:16.74
May 17 08:36:16.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:36:16.741
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:16.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:16.749
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
May 17 08:36:16.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 create -f -'
May 17 08:36:16.892: INFO: stderr: ""
May 17 08:36:16.892: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 17 08:36:16.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 create -f -'
May 17 08:36:17.041: INFO: stderr: ""
May 17 08:36:17.041: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/17/23 08:36:17.041
May 17 08:36:18.045: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:36:18.045: INFO: Found 1 / 1
May 17 08:36:18.045: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 17 08:36:18.046: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:36:18.046: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 08:36:18.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe pod agnhost-primary-5z6pv'
May 17 08:36:18.107: INFO: stderr: ""
May 17 08:36:18.107: INFO: stdout: "Name:             agnhost-primary-5z6pv\nNamespace:        kubectl-7095\nPriority:         0\nService Account:  default\nNode:             k8s-node1/10.0.79.210\nStart Time:       Wed, 17 May 2023 08:36:16 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: a2e7cb5a9e09e16ecdb96b59e8a6e9883050f5ac1cb7562756488a18a14da809\n                  cni.projectcalico.org/podIP: 192.168.36.121/32\n                  cni.projectcalico.org/podIPs: 192.168.36.121/32\nStatus:           Running\nIP:               192.168.36.121\nIPs:\n  IP:           192.168.36.121\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4c0d63477f651e90820de4426f4e9e4bf1ddf0860a403f5c9d29f2c14d2a73b6\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 17 May 2023 08:36:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qw2ng (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qw2ng:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7095/agnhost-primary-5z6pv to k8s-node1\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 17 08:36:18.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe rc agnhost-primary'
May 17 08:36:18.165: INFO: stderr: ""
May 17 08:36:18.165: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7095\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-5z6pv\n"
May 17 08:36:18.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe service agnhost-primary'
May 17 08:36:18.223: INFO: stderr: ""
May 17 08:36:18.223: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7095\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.94.226\nIPs:               10.100.94.226\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.36.121:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 17 08:36:18.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe node k8s-master'
May 17 08:36:18.292: INFO: stderr: ""
May 17 08:36:18.292: INFO: stdout: "Name:               k8s-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.79.209/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.235.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 08 May 2023 09:42:35 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-master\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 17 May 2023 08:36:13 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 08 May 2023 10:41:53 +0000   Mon, 08 May 2023 10:41:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 09:42:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 09:42:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 09:42:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 10:21:05 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.79.209\n  Hostname:    k8s-master\nCapacity:\n  cpu:                8\n  ephemeral-storage:  209590304Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16221664Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  193158423847\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16119264Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 7deb1fe2d78d4280b703c56798959e4b\n  System UUID:                8b814d6a-d5c4-45a8-8b9e-c700974f1381\n  Boot ID:                    d3e47108-ae67-4108-bfb5-745d25b58c1c\n  Kernel Version:             5.10.0-1.el7.jd_801.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-8gm55                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 coredns-5bbd96d687-b42pv                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     8d\n  kube-system                 etcd-k8s-master                                            100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         8d\n  kube-system                 kube-apiserver-k8s-master                                  250m (3%)     0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 kube-controller-manager-k8s-master                         200m (2%)     0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 kube-proxy-7qltd                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 kube-scheduler-k8s-master                                  100m (1%)     0 (0%)      0 (0%)           0 (0%)         8d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-nw97f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (12%)     0 (0%)\n  memory             170Mi (1%)  170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
May 17 08:36:18.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe namespace kubectl-7095'
May 17 08:36:18.345: INFO: stderr: ""
May 17 08:36:18.345: INFO: stdout: "Name:         kubectl-7095\nLabels:       e2e-framework=kubectl\n              e2e-run=ae2668ca-7e49-4bda-8445-8d9b7c33c609\n              kubernetes.io/metadata.name=kubectl-7095\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:36:18.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7095" for this suite. 05/17/23 08:36:18.347
------------------------------
â€¢ [1.610 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:16.74
    May 17 08:36:16.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:36:16.741
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:16.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:16.749
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    May 17 08:36:16.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 create -f -'
    May 17 08:36:16.892: INFO: stderr: ""
    May 17 08:36:16.892: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May 17 08:36:16.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 create -f -'
    May 17 08:36:17.041: INFO: stderr: ""
    May 17 08:36:17.041: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/17/23 08:36:17.041
    May 17 08:36:18.045: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:36:18.045: INFO: Found 1 / 1
    May 17 08:36:18.045: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May 17 08:36:18.046: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:36:18.046: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 17 08:36:18.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe pod agnhost-primary-5z6pv'
    May 17 08:36:18.107: INFO: stderr: ""
    May 17 08:36:18.107: INFO: stdout: "Name:             agnhost-primary-5z6pv\nNamespace:        kubectl-7095\nPriority:         0\nService Account:  default\nNode:             k8s-node1/10.0.79.210\nStart Time:       Wed, 17 May 2023 08:36:16 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: a2e7cb5a9e09e16ecdb96b59e8a6e9883050f5ac1cb7562756488a18a14da809\n                  cni.projectcalico.org/podIP: 192.168.36.121/32\n                  cni.projectcalico.org/podIPs: 192.168.36.121/32\nStatus:           Running\nIP:               192.168.36.121\nIPs:\n  IP:           192.168.36.121\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4c0d63477f651e90820de4426f4e9e4bf1ddf0860a403f5c9d29f2c14d2a73b6\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 17 May 2023 08:36:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qw2ng (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qw2ng:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7095/agnhost-primary-5z6pv to k8s-node1\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    May 17 08:36:18.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe rc agnhost-primary'
    May 17 08:36:18.165: INFO: stderr: ""
    May 17 08:36:18.165: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7095\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-5z6pv\n"
    May 17 08:36:18.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe service agnhost-primary'
    May 17 08:36:18.223: INFO: stderr: ""
    May 17 08:36:18.223: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7095\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.94.226\nIPs:               10.100.94.226\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.36.121:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May 17 08:36:18.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe node k8s-master'
    May 17 08:36:18.292: INFO: stderr: ""
    May 17 08:36:18.292: INFO: stdout: "Name:               k8s-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.79.209/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.235.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 08 May 2023 09:42:35 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-master\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 17 May 2023 08:36:13 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 08 May 2023 10:41:53 +0000   Mon, 08 May 2023 10:41:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 09:42:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 09:42:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 09:42:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 17 May 2023 08:33:11 +0000   Mon, 08 May 2023 10:21:05 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.79.209\n  Hostname:    k8s-master\nCapacity:\n  cpu:                8\n  ephemeral-storage:  209590304Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16221664Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  193158423847\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16119264Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 7deb1fe2d78d4280b703c56798959e4b\n  System UUID:                8b814d6a-d5c4-45a8-8b9e-c700974f1381\n  Boot ID:                    d3e47108-ae67-4108-bfb5-745d25b58c1c\n  Kernel Version:             5.10.0-1.el7.jd_801.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-8gm55                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 coredns-5bbd96d687-b42pv                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     8d\n  kube-system                 etcd-k8s-master                                            100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         8d\n  kube-system                 kube-apiserver-k8s-master                                  250m (3%)     0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 kube-controller-manager-k8s-master                         200m (2%)     0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 kube-proxy-7qltd                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d\n  kube-system                 kube-scheduler-k8s-master                                  100m (1%)     0 (0%)      0 (0%)           0 (0%)         8d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c59468c375484f0c-nw97f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (12%)     0 (0%)\n  memory             170Mi (1%)  170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    May 17 08:36:18.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-7095 describe namespace kubectl-7095'
    May 17 08:36:18.345: INFO: stderr: ""
    May 17 08:36:18.345: INFO: stdout: "Name:         kubectl-7095\nLabels:       e2e-framework=kubectl\n              e2e-run=ae2668ca-7e49-4bda-8445-8d9b7c33c609\n              kubernetes.io/metadata.name=kubectl-7095\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:18.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7095" for this suite. 05/17/23 08:36:18.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:18.351
May 17 08:36:18.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename subpath 05/17/23 08:36:18.352
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:18.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:18.359
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 08:36:18.361
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-288z 05/17/23 08:36:18.365
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 08:36:18.365
May 17 08:36:18.369: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-288z" in namespace "subpath-7779" to be "Succeeded or Failed"
May 17 08:36:18.370: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.309399ms
May 17 08:36:20.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 2.0045562s
May 17 08:36:22.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 4.004714152s
May 17 08:36:24.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 6.005048044s
May 17 08:36:26.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 8.005288417s
May 17 08:36:28.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 10.00409206s
May 17 08:36:30.375: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 12.005847845s
May 17 08:36:32.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 14.004500101s
May 17 08:36:34.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 16.005293656s
May 17 08:36:36.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 18.003993232s
May 17 08:36:38.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 20.004093926s
May 17 08:36:40.375: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=false. Elapsed: 22.005582606s
May 17 08:36:42.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0041743s
STEP: Saw pod success 05/17/23 08:36:42.373
May 17 08:36:42.373: INFO: Pod "pod-subpath-test-configmap-288z" satisfied condition "Succeeded or Failed"
May 17 08:36:42.375: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-configmap-288z container test-container-subpath-configmap-288z: <nil>
STEP: delete the pod 05/17/23 08:36:42.379
May 17 08:36:42.386: INFO: Waiting for pod pod-subpath-test-configmap-288z to disappear
May 17 08:36:42.387: INFO: Pod pod-subpath-test-configmap-288z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-288z 05/17/23 08:36:42.387
May 17 08:36:42.387: INFO: Deleting pod "pod-subpath-test-configmap-288z" in namespace "subpath-7779"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 17 08:36:42.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7779" for this suite. 05/17/23 08:36:42.391
------------------------------
â€¢ [SLOW TEST] [24.044 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:18.351
    May 17 08:36:18.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename subpath 05/17/23 08:36:18.352
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:18.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:18.359
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 08:36:18.361
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-288z 05/17/23 08:36:18.365
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 08:36:18.365
    May 17 08:36:18.369: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-288z" in namespace "subpath-7779" to be "Succeeded or Failed"
    May 17 08:36:18.370: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.309399ms
    May 17 08:36:20.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 2.0045562s
    May 17 08:36:22.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 4.004714152s
    May 17 08:36:24.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 6.005048044s
    May 17 08:36:26.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 8.005288417s
    May 17 08:36:28.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 10.00409206s
    May 17 08:36:30.375: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 12.005847845s
    May 17 08:36:32.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 14.004500101s
    May 17 08:36:34.374: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 16.005293656s
    May 17 08:36:36.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 18.003993232s
    May 17 08:36:38.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=true. Elapsed: 20.004093926s
    May 17 08:36:40.375: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Running", Reason="", readiness=false. Elapsed: 22.005582606s
    May 17 08:36:42.373: INFO: Pod "pod-subpath-test-configmap-288z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0041743s
    STEP: Saw pod success 05/17/23 08:36:42.373
    May 17 08:36:42.373: INFO: Pod "pod-subpath-test-configmap-288z" satisfied condition "Succeeded or Failed"
    May 17 08:36:42.375: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-configmap-288z container test-container-subpath-configmap-288z: <nil>
    STEP: delete the pod 05/17/23 08:36:42.379
    May 17 08:36:42.386: INFO: Waiting for pod pod-subpath-test-configmap-288z to disappear
    May 17 08:36:42.387: INFO: Pod pod-subpath-test-configmap-288z no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-288z 05/17/23 08:36:42.387
    May 17 08:36:42.387: INFO: Deleting pod "pod-subpath-test-configmap-288z" in namespace "subpath-7779"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:42.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7779" for this suite. 05/17/23 08:36:42.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:42.395
May 17 08:36:42.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:36:42.396
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:42.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:42.405
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-4b55361c-b4cb-44a1-98ae-8e4d9c664783 05/17/23 08:36:42.407
STEP: Creating a pod to test consume secrets 05/17/23 08:36:42.409
May 17 08:36:42.414: INFO: Waiting up to 5m0s for pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50" in namespace "secrets-9606" to be "Succeeded or Failed"
May 17 08:36:42.415: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50": Phase="Pending", Reason="", readiness=false. Elapsed: 1.329519ms
May 17 08:36:44.420: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005591964s
May 17 08:36:46.419: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005061977s
STEP: Saw pod success 05/17/23 08:36:46.419
May 17 08:36:46.419: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50" satisfied condition "Succeeded or Failed"
May 17 08:36:46.421: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:36:46.424
May 17 08:36:46.430: INFO: Waiting for pod pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50 to disappear
May 17 08:36:46.431: INFO: Pod pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:36:46.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9606" for this suite. 05/17/23 08:36:46.434
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:42.395
    May 17 08:36:42.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:36:42.396
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:42.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:42.405
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-4b55361c-b4cb-44a1-98ae-8e4d9c664783 05/17/23 08:36:42.407
    STEP: Creating a pod to test consume secrets 05/17/23 08:36:42.409
    May 17 08:36:42.414: INFO: Waiting up to 5m0s for pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50" in namespace "secrets-9606" to be "Succeeded or Failed"
    May 17 08:36:42.415: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50": Phase="Pending", Reason="", readiness=false. Elapsed: 1.329519ms
    May 17 08:36:44.420: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005591964s
    May 17 08:36:46.419: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005061977s
    STEP: Saw pod success 05/17/23 08:36:46.419
    May 17 08:36:46.419: INFO: Pod "pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50" satisfied condition "Succeeded or Failed"
    May 17 08:36:46.421: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:36:46.424
    May 17 08:36:46.430: INFO: Waiting for pod pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50 to disappear
    May 17 08:36:46.431: INFO: Pod pod-secrets-8e7e0f79-a22c-4cd5-b366-38e1bc56bf50 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:46.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9606" for this suite. 05/17/23 08:36:46.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:46.437
May 17 08:36:46.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replication-controller 05/17/23 08:36:46.438
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:46.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:46.446
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 05/17/23 08:36:46.449
STEP: waiting for RC to be added 05/17/23 08:36:46.451
STEP: waiting for available Replicas 05/17/23 08:36:46.451
STEP: patching ReplicationController 05/17/23 08:36:47.701
STEP: waiting for RC to be modified 05/17/23 08:36:47.706
STEP: patching ReplicationController status 05/17/23 08:36:47.706
STEP: waiting for RC to be modified 05/17/23 08:36:47.709
STEP: waiting for available Replicas 05/17/23 08:36:47.709
STEP: fetching ReplicationController status 05/17/23 08:36:47.712
STEP: patching ReplicationController scale 05/17/23 08:36:47.714
STEP: waiting for RC to be modified 05/17/23 08:36:47.718
STEP: waiting for ReplicationController's scale to be the max amount 05/17/23 08:36:47.718
STEP: fetching ReplicationController; ensuring that it's patched 05/17/23 08:36:49.019
STEP: updating ReplicationController status 05/17/23 08:36:49.021
STEP: waiting for RC to be modified 05/17/23 08:36:49.024
STEP: listing all ReplicationControllers 05/17/23 08:36:49.025
STEP: checking that ReplicationController has expected values 05/17/23 08:36:49.026
STEP: deleting ReplicationControllers by collection 05/17/23 08:36:49.026
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/17/23 08:36:49.03
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 17 08:36:49.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1247" for this suite. 05/17/23 08:36:49.087
------------------------------
â€¢ [2.652 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:46.437
    May 17 08:36:46.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replication-controller 05/17/23 08:36:46.438
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:46.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:46.446
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 05/17/23 08:36:46.449
    STEP: waiting for RC to be added 05/17/23 08:36:46.451
    STEP: waiting for available Replicas 05/17/23 08:36:46.451
    STEP: patching ReplicationController 05/17/23 08:36:47.701
    STEP: waiting for RC to be modified 05/17/23 08:36:47.706
    STEP: patching ReplicationController status 05/17/23 08:36:47.706
    STEP: waiting for RC to be modified 05/17/23 08:36:47.709
    STEP: waiting for available Replicas 05/17/23 08:36:47.709
    STEP: fetching ReplicationController status 05/17/23 08:36:47.712
    STEP: patching ReplicationController scale 05/17/23 08:36:47.714
    STEP: waiting for RC to be modified 05/17/23 08:36:47.718
    STEP: waiting for ReplicationController's scale to be the max amount 05/17/23 08:36:47.718
    STEP: fetching ReplicationController; ensuring that it's patched 05/17/23 08:36:49.019
    STEP: updating ReplicationController status 05/17/23 08:36:49.021
    STEP: waiting for RC to be modified 05/17/23 08:36:49.024
    STEP: listing all ReplicationControllers 05/17/23 08:36:49.025
    STEP: checking that ReplicationController has expected values 05/17/23 08:36:49.026
    STEP: deleting ReplicationControllers by collection 05/17/23 08:36:49.026
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/17/23 08:36:49.03
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:49.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1247" for this suite. 05/17/23 08:36:49.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:49.092
May 17 08:36:49.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:36:49.092
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:49.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:49.1
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-vxf5m" 05/17/23 08:36:49.103
May 17 08:36:49.106: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard cpu limit of 500m
May 17 08:36:49.106: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-vxf5m" /status 05/17/23 08:36:49.106
STEP: Confirm /status for "e2e-rq-status-vxf5m" resourceQuota via watch 05/17/23 08:36:49.111
May 17 08:36:49.112: INFO: observed resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList(nil)
May 17 08:36:49.112: INFO: Found resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May 17 08:36:49.112: INFO: ResourceQuota "e2e-rq-status-vxf5m" /status was updated
STEP: Patching hard spec values for cpu & memory 05/17/23 08:36:49.113
May 17 08:36:49.115: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard cpu limit of 1
May 17 08:36:49.115: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-vxf5m" /status 05/17/23 08:36:49.115
STEP: Confirm /status for "e2e-rq-status-vxf5m" resourceQuota via watch 05/17/23 08:36:49.118
May 17 08:36:49.118: INFO: observed resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May 17 08:36:49.118: INFO: Found resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
May 17 08:36:49.118: INFO: ResourceQuota "e2e-rq-status-vxf5m" /status was patched
STEP: Get "e2e-rq-status-vxf5m" /status 05/17/23 08:36:49.118
May 17 08:36:49.120: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard cpu of 1
May 17 08:36:49.120: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-vxf5m" /status before checking Spec is unchanged 05/17/23 08:36:49.121
May 17 08:36:49.124: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard cpu of 2
May 17 08:36:49.124: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard memory of 2Gi
May 17 08:36:49.124: INFO: Found resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
May 17 08:36:54.128: INFO: ResourceQuota "e2e-rq-status-vxf5m" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:36:54.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8015" for this suite. 05/17/23 08:36:54.131
------------------------------
â€¢ [SLOW TEST] [5.042 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:49.092
    May 17 08:36:49.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:36:49.092
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:49.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:49.1
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-vxf5m" 05/17/23 08:36:49.103
    May 17 08:36:49.106: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard cpu limit of 500m
    May 17 08:36:49.106: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-vxf5m" /status 05/17/23 08:36:49.106
    STEP: Confirm /status for "e2e-rq-status-vxf5m" resourceQuota via watch 05/17/23 08:36:49.111
    May 17 08:36:49.112: INFO: observed resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList(nil)
    May 17 08:36:49.112: INFO: Found resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May 17 08:36:49.112: INFO: ResourceQuota "e2e-rq-status-vxf5m" /status was updated
    STEP: Patching hard spec values for cpu & memory 05/17/23 08:36:49.113
    May 17 08:36:49.115: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard cpu limit of 1
    May 17 08:36:49.115: INFO: Resource quota "e2e-rq-status-vxf5m" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-vxf5m" /status 05/17/23 08:36:49.115
    STEP: Confirm /status for "e2e-rq-status-vxf5m" resourceQuota via watch 05/17/23 08:36:49.118
    May 17 08:36:49.118: INFO: observed resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May 17 08:36:49.118: INFO: Found resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    May 17 08:36:49.118: INFO: ResourceQuota "e2e-rq-status-vxf5m" /status was patched
    STEP: Get "e2e-rq-status-vxf5m" /status 05/17/23 08:36:49.118
    May 17 08:36:49.120: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard cpu of 1
    May 17 08:36:49.120: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-vxf5m" /status before checking Spec is unchanged 05/17/23 08:36:49.121
    May 17 08:36:49.124: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard cpu of 2
    May 17 08:36:49.124: INFO: Resourcequota "e2e-rq-status-vxf5m" reports status: hard memory of 2Gi
    May 17 08:36:49.124: INFO: Found resourceQuota "e2e-rq-status-vxf5m" in namespace "resourcequota-8015" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    May 17 08:36:54.128: INFO: ResourceQuota "e2e-rq-status-vxf5m" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:54.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8015" for this suite. 05/17/23 08:36:54.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:54.135
May 17 08:36:54.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:36:54.135
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:54.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:54.143
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 08:36:54.145
May 17 08:36:54.148: INFO: Waiting up to 5m0s for pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e" in namespace "emptydir-630" to be "Succeeded or Failed"
May 17 08:36:54.149: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298369ms
May 17 08:36:56.153: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004871744s
May 17 08:36:58.152: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004223628s
STEP: Saw pod success 05/17/23 08:36:58.152
May 17 08:36:58.152: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e" satisfied condition "Succeeded or Failed"
May 17 08:36:58.154: INFO: Trying to get logs from node k8s-node1 pod pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e container test-container: <nil>
STEP: delete the pod 05/17/23 08:36:58.157
May 17 08:36:58.164: INFO: Waiting for pod pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e to disappear
May 17 08:36:58.165: INFO: Pod pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:36:58.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-630" for this suite. 05/17/23 08:36:58.167
------------------------------
â€¢ [4.035 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:54.135
    May 17 08:36:54.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:36:54.135
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:54.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:54.143
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 08:36:54.145
    May 17 08:36:54.148: INFO: Waiting up to 5m0s for pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e" in namespace "emptydir-630" to be "Succeeded or Failed"
    May 17 08:36:54.149: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298369ms
    May 17 08:36:56.153: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004871744s
    May 17 08:36:58.152: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004223628s
    STEP: Saw pod success 05/17/23 08:36:58.152
    May 17 08:36:58.152: INFO: Pod "pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e" satisfied condition "Succeeded or Failed"
    May 17 08:36:58.154: INFO: Trying to get logs from node k8s-node1 pod pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e container test-container: <nil>
    STEP: delete the pod 05/17/23 08:36:58.157
    May 17 08:36:58.164: INFO: Waiting for pod pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e to disappear
    May 17 08:36:58.165: INFO: Pod pod-dc99bfc9-6502-4d88-8eda-d8ec8503707e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:36:58.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-630" for this suite. 05/17/23 08:36:58.167
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:36:58.17
May 17 08:36:58.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename hostport 05/17/23 08:36:58.17
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:58.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:58.179
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/17/23 08:36:58.183
May 17 08:36:58.188: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-897" to be "running and ready"
May 17 08:36:58.189: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.180588ms
May 17 08:36:58.189: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:37:00.191: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003737351s
May 17 08:37:00.191: INFO: The phase of Pod pod1 is Running (Ready = true)
May 17 08:37:00.191: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.79.211 on the node which pod1 resides and expect scheduled 05/17/23 08:37:00.191
May 17 08:37:00.195: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-897" to be "running and ready"
May 17 08:37:00.196: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33378ms
May 17 08:37:00.196: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:37:02.199: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004694221s
May 17 08:37:02.199: INFO: The phase of Pod pod2 is Running (Ready = true)
May 17 08:37:02.199: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.79.211 but use UDP protocol on the node which pod2 resides 05/17/23 08:37:02.199
May 17 08:37:02.203: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-897" to be "running and ready"
May 17 08:37:02.204: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.426286ms
May 17 08:37:02.204: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:37:04.207: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004035427s
May 17 08:37:04.207: INFO: The phase of Pod pod3 is Running (Ready = true)
May 17 08:37:04.207: INFO: Pod "pod3" satisfied condition "running and ready"
May 17 08:37:04.211: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-897" to be "running and ready"
May 17 08:37:04.212: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397121ms
May 17 08:37:04.212: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 17 08:37:06.216: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004862473s
May 17 08:37:06.216: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May 17 08:37:06.216: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/17/23 08:37:06.218
May 17 08:37:06.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.79.211 http://127.0.0.1:54323/hostname] Namespace:hostport-897 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:37:06.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:37:06.218: INFO: ExecWithOptions: Clientset creation
May 17 08:37:06.218: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-897/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.79.211+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.79.211, port: 54323 05/17/23 08:37:06.287
May 17 08:37:06.287: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.79.211:54323/hostname] Namespace:hostport-897 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:37:06.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:37:06.287: INFO: ExecWithOptions: Clientset creation
May 17 08:37:06.287: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-897/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.79.211%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.79.211, port: 54323 UDP 05/17/23 08:37:06.335
May 17 08:37:06.335: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.79.211 54323] Namespace:hostport-897 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:37:06.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:37:06.336: INFO: ExecWithOptions: Clientset creation
May 17 08:37:06.336: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-897/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.79.211+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
May 17 08:37:11.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-897" for this suite. 05/17/23 08:37:11.386
------------------------------
â€¢ [SLOW TEST] [13.220 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:36:58.17
    May 17 08:36:58.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename hostport 05/17/23 08:36:58.17
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:36:58.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:36:58.179
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/17/23 08:36:58.183
    May 17 08:36:58.188: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-897" to be "running and ready"
    May 17 08:36:58.189: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.180588ms
    May 17 08:36:58.189: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:37:00.191: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003737351s
    May 17 08:37:00.191: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 17 08:37:00.191: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.79.211 on the node which pod1 resides and expect scheduled 05/17/23 08:37:00.191
    May 17 08:37:00.195: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-897" to be "running and ready"
    May 17 08:37:00.196: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33378ms
    May 17 08:37:00.196: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:37:02.199: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004694221s
    May 17 08:37:02.199: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 17 08:37:02.199: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.79.211 but use UDP protocol on the node which pod2 resides 05/17/23 08:37:02.199
    May 17 08:37:02.203: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-897" to be "running and ready"
    May 17 08:37:02.204: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.426286ms
    May 17 08:37:02.204: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:37:04.207: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004035427s
    May 17 08:37:04.207: INFO: The phase of Pod pod3 is Running (Ready = true)
    May 17 08:37:04.207: INFO: Pod "pod3" satisfied condition "running and ready"
    May 17 08:37:04.211: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-897" to be "running and ready"
    May 17 08:37:04.212: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397121ms
    May 17 08:37:04.212: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:37:06.216: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004862473s
    May 17 08:37:06.216: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May 17 08:37:06.216: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/17/23 08:37:06.218
    May 17 08:37:06.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.79.211 http://127.0.0.1:54323/hostname] Namespace:hostport-897 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:37:06.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:37:06.218: INFO: ExecWithOptions: Clientset creation
    May 17 08:37:06.218: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-897/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.79.211+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.79.211, port: 54323 05/17/23 08:37:06.287
    May 17 08:37:06.287: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.79.211:54323/hostname] Namespace:hostport-897 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:37:06.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:37:06.287: INFO: ExecWithOptions: Clientset creation
    May 17 08:37:06.287: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-897/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.79.211%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.79.211, port: 54323 UDP 05/17/23 08:37:06.335
    May 17 08:37:06.335: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.79.211 54323] Namespace:hostport-897 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:37:06.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:37:06.336: INFO: ExecWithOptions: Clientset creation
    May 17 08:37:06.336: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-897/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.79.211+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    May 17 08:37:11.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-897" for this suite. 05/17/23 08:37:11.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:37:11.391
May 17 08:37:11.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename cronjob 05/17/23 08:37:11.391
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:37:11.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:37:11.4
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/17/23 08:37:11.402
STEP: Ensuring no jobs are scheduled 05/17/23 08:37:11.406
STEP: Ensuring no job exists by listing jobs explicitly 05/17/23 08:42:11.411
STEP: Removing cronjob 05/17/23 08:42:11.412
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 17 08:42:11.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6594" for this suite. 05/17/23 08:42:11.416
------------------------------
â€¢ [SLOW TEST] [300.028 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:37:11.391
    May 17 08:37:11.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename cronjob 05/17/23 08:37:11.391
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:37:11.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:37:11.4
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/17/23 08:37:11.402
    STEP: Ensuring no jobs are scheduled 05/17/23 08:37:11.406
    STEP: Ensuring no job exists by listing jobs explicitly 05/17/23 08:42:11.411
    STEP: Removing cronjob 05/17/23 08:42:11.412
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 17 08:42:11.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6594" for this suite. 05/17/23 08:42:11.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:42:11.419
May 17 08:42:11.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:42:11.42
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:11.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:11.428
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 05/17/23 08:42:11.43
May 17 08:42:11.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9843 create -f -'
May 17 08:42:11.579: INFO: stderr: ""
May 17 08:42:11.579: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/17/23 08:42:11.579
May 17 08:42:12.581: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:42:12.581: INFO: Found 1 / 1
May 17 08:42:12.581: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/17/23 08:42:12.581
May 17 08:42:12.582: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:42:12.582: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 08:42:12.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9843 patch pod agnhost-primary-zf2m7 -p {"metadata":{"annotations":{"x":"y"}}}'
May 17 08:42:12.639: INFO: stderr: ""
May 17 08:42:12.639: INFO: stdout: "pod/agnhost-primary-zf2m7 patched\n"
STEP: checking annotations 05/17/23 08:42:12.639
May 17 08:42:12.640: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 08:42:12.640: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:42:12.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9843" for this suite. 05/17/23 08:42:12.642
------------------------------
â€¢ [1.225 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:42:11.419
    May 17 08:42:11.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:42:11.42
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:11.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:11.428
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 05/17/23 08:42:11.43
    May 17 08:42:11.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9843 create -f -'
    May 17 08:42:11.579: INFO: stderr: ""
    May 17 08:42:11.579: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/17/23 08:42:11.579
    May 17 08:42:12.581: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:42:12.581: INFO: Found 1 / 1
    May 17 08:42:12.581: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/17/23 08:42:12.581
    May 17 08:42:12.582: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:42:12.582: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 17 08:42:12.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-9843 patch pod agnhost-primary-zf2m7 -p {"metadata":{"annotations":{"x":"y"}}}'
    May 17 08:42:12.639: INFO: stderr: ""
    May 17 08:42:12.639: INFO: stdout: "pod/agnhost-primary-zf2m7 patched\n"
    STEP: checking annotations 05/17/23 08:42:12.639
    May 17 08:42:12.640: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 08:42:12.640: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:42:12.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9843" for this suite. 05/17/23 08:42:12.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:42:12.645
May 17 08:42:12.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename conformance-tests 05/17/23 08:42:12.646
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:12.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:12.654
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/17/23 08:42:12.655
May 17 08:42:12.655: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
May 17 08:42:12.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-1837" for this suite. 05/17/23 08:42:12.659
------------------------------
â€¢ [0.016 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:42:12.645
    May 17 08:42:12.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename conformance-tests 05/17/23 08:42:12.646
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:12.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:12.654
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/17/23 08:42:12.655
    May 17 08:42:12.655: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    May 17 08:42:12.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-1837" for this suite. 05/17/23 08:42:12.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:42:12.662
May 17 08:42:12.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 08:42:12.663
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:12.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:12.67
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-8062 05/17/23 08:42:12.671
STEP: creating service affinity-clusterip-transition in namespace services-8062 05/17/23 08:42:12.671
STEP: creating replication controller affinity-clusterip-transition in namespace services-8062 05/17/23 08:42:12.678
I0517 08:42:12.681456      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8062, replica count: 3
I0517 08:42:15.732500      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 08:42:15.735: INFO: Creating new exec pod
May 17 08:42:15.739: INFO: Waiting up to 5m0s for pod "execpod-affinitytndrx" in namespace "services-8062" to be "running"
May 17 08:42:15.740: INFO: Pod "execpod-affinitytndrx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461722ms
May 17 08:42:17.742: INFO: Pod "execpod-affinitytndrx": Phase="Running", Reason="", readiness=true. Elapsed: 2.003550267s
May 17 08:42:17.743: INFO: Pod "execpod-affinitytndrx" satisfied condition "running"
May 17 08:42:18.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
May 17 08:42:18.847: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 17 08:42:18.847: INFO: stdout: ""
May 17 08:42:18.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c nc -v -z -w 2 10.107.37.8 80'
May 17 08:42:18.941: INFO: stderr: "+ nc -v -z -w 2 10.107.37.8 80\nConnection to 10.107.37.8 80 port [tcp/http] succeeded!\n"
May 17 08:42:18.941: INFO: stdout: ""
May 17 08:42:18.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.37.8:80/ ; done'
May 17 08:42:19.096: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n"
May 17 08:42:19.096: INFO: stdout: "\naffinity-clusterip-transition-z9q4g\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-wkcmd"
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-z9q4g
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
May 17 08:42:19.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.37.8:80/ ; done'
May 17 08:42:19.248: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n"
May 17 08:42:19.248: INFO: stdout: "\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc"
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
May 17 08:42:19.248: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8062, will wait for the garbage collector to delete the pods 05/17/23 08:42:19.255
May 17 08:42:19.311: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.434019ms
May 17 08:42:19.411: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.167395ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 08:42:21.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8062" for this suite. 05/17/23 08:42:21.321
------------------------------
â€¢ [SLOW TEST] [8.661 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:42:12.662
    May 17 08:42:12.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 08:42:12.663
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:12.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:12.67
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-8062 05/17/23 08:42:12.671
    STEP: creating service affinity-clusterip-transition in namespace services-8062 05/17/23 08:42:12.671
    STEP: creating replication controller affinity-clusterip-transition in namespace services-8062 05/17/23 08:42:12.678
    I0517 08:42:12.681456      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8062, replica count: 3
    I0517 08:42:15.732500      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 08:42:15.735: INFO: Creating new exec pod
    May 17 08:42:15.739: INFO: Waiting up to 5m0s for pod "execpod-affinitytndrx" in namespace "services-8062" to be "running"
    May 17 08:42:15.740: INFO: Pod "execpod-affinitytndrx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461722ms
    May 17 08:42:17.742: INFO: Pod "execpod-affinitytndrx": Phase="Running", Reason="", readiness=true. Elapsed: 2.003550267s
    May 17 08:42:17.743: INFO: Pod "execpod-affinitytndrx" satisfied condition "running"
    May 17 08:42:18.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    May 17 08:42:18.847: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May 17 08:42:18.847: INFO: stdout: ""
    May 17 08:42:18.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c nc -v -z -w 2 10.107.37.8 80'
    May 17 08:42:18.941: INFO: stderr: "+ nc -v -z -w 2 10.107.37.8 80\nConnection to 10.107.37.8 80 port [tcp/http] succeeded!\n"
    May 17 08:42:18.941: INFO: stdout: ""
    May 17 08:42:18.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.37.8:80/ ; done'
    May 17 08:42:19.096: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n"
    May 17 08:42:19.096: INFO: stdout: "\naffinity-clusterip-transition-z9q4g\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-wkcmd\naffinity-clusterip-transition-wkcmd"
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-z9q4g
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
    May 17 08:42:19.096: INFO: Received response from host: affinity-clusterip-transition-wkcmd
    May 17 08:42:19.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-8062 exec execpod-affinitytndrx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.37.8:80/ ; done'
    May 17 08:42:19.248: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.37.8:80/\n"
    May 17 08:42:19.248: INFO: stdout: "\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc\naffinity-clusterip-transition-t2xlc"
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Received response from host: affinity-clusterip-transition-t2xlc
    May 17 08:42:19.248: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8062, will wait for the garbage collector to delete the pods 05/17/23 08:42:19.255
    May 17 08:42:19.311: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.434019ms
    May 17 08:42:19.411: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.167395ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 08:42:21.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8062" for this suite. 05/17/23 08:42:21.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:42:21.324
May 17 08:42:21.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 08:42:21.325
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:21.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:21.333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/17/23 08:42:21.334
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local;sleep 1; done
 05/17/23 08:42:21.336
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local;sleep 1; done
 05/17/23 08:42:21.336
STEP: creating a pod to probe DNS 05/17/23 08:42:21.336
STEP: submitting the pod to kubernetes 05/17/23 08:42:21.336
May 17 08:42:21.342: INFO: Waiting up to 15m0s for pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923" in namespace "dns-5796" to be "running"
May 17 08:42:21.343: INFO: Pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923": Phase="Pending", Reason="", readiness=false. Elapsed: 1.264713ms
May 17 08:42:23.346: INFO: Pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923": Phase="Running", Reason="", readiness=true. Elapsed: 2.003934258s
May 17 08:42:23.346: INFO: Pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923" satisfied condition "running"
STEP: retrieving the pod 05/17/23 08:42:23.346
STEP: looking for the results for each expected name from probers 05/17/23 08:42:23.348
May 17 08:42:23.350: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.351: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.353: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.354: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.356: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.357: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.358: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.359: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:23.359: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

May 17 08:42:28.363: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.365: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.366: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.369: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.370: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.372: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.373: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:28.373: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

May 17 08:42:33.362: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.364: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.368: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.369: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.371: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.372: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:33.372: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

May 17 08:42:38.363: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.365: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.366: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.369: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.370: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.371: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.373: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:38.373: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

May 17 08:42:43.362: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.364: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.366: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.368: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.370: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.371: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.373: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.374: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:43.374: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

May 17 08:42:48.362: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.364: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.368: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.370: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.371: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.372: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
May 17 08:42:48.372: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

May 17 08:42:53.372: INFO: DNS probes using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 succeeded

STEP: deleting the pod 05/17/23 08:42:53.372
STEP: deleting the test headless service 05/17/23 08:42:53.378
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 08:42:53.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5796" for this suite. 05/17/23 08:42:53.386
------------------------------
â€¢ [SLOW TEST] [32.065 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:42:21.324
    May 17 08:42:21.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 08:42:21.325
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:21.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:21.333
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/17/23 08:42:21.334
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local;sleep 1; done
     05/17/23 08:42:21.336
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5796.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local;sleep 1; done
     05/17/23 08:42:21.336
    STEP: creating a pod to probe DNS 05/17/23 08:42:21.336
    STEP: submitting the pod to kubernetes 05/17/23 08:42:21.336
    May 17 08:42:21.342: INFO: Waiting up to 15m0s for pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923" in namespace "dns-5796" to be "running"
    May 17 08:42:21.343: INFO: Pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923": Phase="Pending", Reason="", readiness=false. Elapsed: 1.264713ms
    May 17 08:42:23.346: INFO: Pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923": Phase="Running", Reason="", readiness=true. Elapsed: 2.003934258s
    May 17 08:42:23.346: INFO: Pod "dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 08:42:23.346
    STEP: looking for the results for each expected name from probers 05/17/23 08:42:23.348
    May 17 08:42:23.350: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.351: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.353: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.354: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.356: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.357: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.358: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.359: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:23.359: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

    May 17 08:42:28.363: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.365: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.366: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.369: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.370: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.372: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.373: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:28.373: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

    May 17 08:42:33.362: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.364: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.368: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.369: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.371: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.372: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:33.372: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

    May 17 08:42:38.363: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.365: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.366: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.369: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.370: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.371: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.373: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:38.373: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

    May 17 08:42:43.362: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.364: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.366: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.368: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.370: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.371: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.373: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.374: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:43.374: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

    May 17 08:42:48.362: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.364: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.368: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.370: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.371: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.372: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local from pod dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923: the server could not find the requested resource (get pods dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923)
    May 17 08:42:48.372: INFO: Lookups using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5796.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5796.svc.cluster.local jessie_udp@dns-test-service-2.dns-5796.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5796.svc.cluster.local]

    May 17 08:42:53.372: INFO: DNS probes using dns-5796/dns-test-d6b4c293-4b2f-4a5b-90cd-292ebf344923 succeeded

    STEP: deleting the pod 05/17/23 08:42:53.372
    STEP: deleting the test headless service 05/17/23 08:42:53.378
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 08:42:53.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5796" for this suite. 05/17/23 08:42:53.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:42:53.389
May 17 08:42:53.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 08:42:53.39
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:53.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:53.396
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 05/17/23 08:42:53.397
May 17 08:42:53.400: INFO: Waiting up to 5m0s for pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16" in namespace "var-expansion-9107" to be "Succeeded or Failed"
May 17 08:42:53.402: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.279164ms
May 17 08:42:55.404: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003372627s
May 17 08:42:57.407: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006332183s
STEP: Saw pod success 05/17/23 08:42:57.407
May 17 08:42:57.407: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16" satisfied condition "Succeeded or Failed"
May 17 08:42:57.408: INFO: Trying to get logs from node k8s-node1 pod var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16 container dapi-container: <nil>
STEP: delete the pod 05/17/23 08:42:57.418
May 17 08:42:57.423: INFO: Waiting for pod var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16 to disappear
May 17 08:42:57.424: INFO: Pod var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 08:42:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9107" for this suite. 05/17/23 08:42:57.426
------------------------------
â€¢ [4.040 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:42:53.389
    May 17 08:42:53.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 08:42:53.39
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:53.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:53.396
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 05/17/23 08:42:53.397
    May 17 08:42:53.400: INFO: Waiting up to 5m0s for pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16" in namespace "var-expansion-9107" to be "Succeeded or Failed"
    May 17 08:42:53.402: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.279164ms
    May 17 08:42:55.404: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003372627s
    May 17 08:42:57.407: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006332183s
    STEP: Saw pod success 05/17/23 08:42:57.407
    May 17 08:42:57.407: INFO: Pod "var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16" satisfied condition "Succeeded or Failed"
    May 17 08:42:57.408: INFO: Trying to get logs from node k8s-node1 pod var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 08:42:57.418
    May 17 08:42:57.423: INFO: Waiting for pod var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16 to disappear
    May 17 08:42:57.424: INFO: Pod var-expansion-7a89dd15-5f1a-4e10-b5d7-ea148220bb16 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 08:42:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9107" for this suite. 05/17/23 08:42:57.426
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:42:57.429
May 17 08:42:57.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename disruption 05/17/23 08:42:57.43
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:57.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:57.437
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 05/17/23 08:42:57.439
STEP: Waiting for the pdb to be processed 05/17/23 08:42:57.441
STEP: First trying to evict a pod which shouldn't be evictable 05/17/23 08:42:59.447
STEP: Waiting for all pods to be running 05/17/23 08:42:59.448
May 17 08:42:59.449: INFO: pods: 0 < 3
STEP: locating a running pod 05/17/23 08:43:01.452
STEP: Updating the pdb to allow a pod to be evicted 05/17/23 08:43:01.456
STEP: Waiting for the pdb to be processed 05/17/23 08:43:01.461
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 08:43:03.464
STEP: Waiting for all pods to be running 05/17/23 08:43:03.465
STEP: Waiting for the pdb to observed all healthy pods 05/17/23 08:43:03.466
STEP: Patching the pdb to disallow a pod to be evicted 05/17/23 08:43:03.477
STEP: Waiting for the pdb to be processed 05/17/23 08:43:03.483
STEP: Waiting for all pods to be running 05/17/23 08:43:05.487
STEP: locating a running pod 05/17/23 08:43:05.489
STEP: Deleting the pdb to allow a pod to be evicted 05/17/23 08:43:05.493
STEP: Waiting for the pdb to be deleted 05/17/23 08:43:05.495
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 08:43:05.496
STEP: Waiting for all pods to be running 05/17/23 08:43:05.496
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 17 08:43:05.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7782" for this suite. 05/17/23 08:43:05.506
------------------------------
â€¢ [SLOW TEST] [8.080 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:42:57.429
    May 17 08:42:57.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename disruption 05/17/23 08:42:57.43
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:42:57.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:42:57.437
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 05/17/23 08:42:57.439
    STEP: Waiting for the pdb to be processed 05/17/23 08:42:57.441
    STEP: First trying to evict a pod which shouldn't be evictable 05/17/23 08:42:59.447
    STEP: Waiting for all pods to be running 05/17/23 08:42:59.448
    May 17 08:42:59.449: INFO: pods: 0 < 3
    STEP: locating a running pod 05/17/23 08:43:01.452
    STEP: Updating the pdb to allow a pod to be evicted 05/17/23 08:43:01.456
    STEP: Waiting for the pdb to be processed 05/17/23 08:43:01.461
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 08:43:03.464
    STEP: Waiting for all pods to be running 05/17/23 08:43:03.465
    STEP: Waiting for the pdb to observed all healthy pods 05/17/23 08:43:03.466
    STEP: Patching the pdb to disallow a pod to be evicted 05/17/23 08:43:03.477
    STEP: Waiting for the pdb to be processed 05/17/23 08:43:03.483
    STEP: Waiting for all pods to be running 05/17/23 08:43:05.487
    STEP: locating a running pod 05/17/23 08:43:05.489
    STEP: Deleting the pdb to allow a pod to be evicted 05/17/23 08:43:05.493
    STEP: Waiting for the pdb to be deleted 05/17/23 08:43:05.495
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 08:43:05.496
    STEP: Waiting for all pods to be running 05/17/23 08:43:05.496
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 17 08:43:05.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7782" for this suite. 05/17/23 08:43:05.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:43:05.51
May 17 08:43:05.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:43:05.51
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:05.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:05.518
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 05/17/23 08:43:05.519
May 17 08:43:05.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 create -f -'
May 17 08:43:05.665: INFO: stderr: ""
May 17 08:43:05.665: INFO: stdout: "pod/pause created\n"
May 17 08:43:05.665: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 17 08:43:05.665: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5855" to be "running and ready"
May 17 08:43:05.667: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837063ms
May 17 08:43:05.667: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-node1' to be 'Running' but was 'Pending'
May 17 08:43:07.670: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004513856s
May 17 08:43:07.670: INFO: Pod "pause" satisfied condition "running and ready"
May 17 08:43:07.670: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 05/17/23 08:43:07.67
May 17 08:43:07.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 label pods pause testing-label=testing-label-value'
May 17 08:43:07.727: INFO: stderr: ""
May 17 08:43:07.727: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/17/23 08:43:07.727
May 17 08:43:07.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get pod pause -L testing-label'
May 17 08:43:07.776: INFO: stderr: ""
May 17 08:43:07.776: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/17/23 08:43:07.776
May 17 08:43:07.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 label pods pause testing-label-'
May 17 08:43:07.831: INFO: stderr: ""
May 17 08:43:07.831: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/17/23 08:43:07.831
May 17 08:43:07.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get pod pause -L testing-label'
May 17 08:43:07.881: INFO: stderr: ""
May 17 08:43:07.881: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 05/17/23 08:43:07.881
May 17 08:43:07.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 delete --grace-period=0 --force -f -'
May 17 08:43:07.937: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 08:43:07.937: INFO: stdout: "pod \"pause\" force deleted\n"
May 17 08:43:07.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get rc,svc -l name=pause --no-headers'
May 17 08:43:07.989: INFO: stderr: "No resources found in kubectl-5855 namespace.\n"
May 17 08:43:07.989: INFO: stdout: ""
May 17 08:43:07.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 08:43:08.042: INFO: stderr: ""
May 17 08:43:08.042: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:43:08.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5855" for this suite. 05/17/23 08:43:08.045
------------------------------
â€¢ [2.538 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:43:05.51
    May 17 08:43:05.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:43:05.51
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:05.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:05.518
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 05/17/23 08:43:05.519
    May 17 08:43:05.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 create -f -'
    May 17 08:43:05.665: INFO: stderr: ""
    May 17 08:43:05.665: INFO: stdout: "pod/pause created\n"
    May 17 08:43:05.665: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May 17 08:43:05.665: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5855" to be "running and ready"
    May 17 08:43:05.667: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837063ms
    May 17 08:43:05.667: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-node1' to be 'Running' but was 'Pending'
    May 17 08:43:07.670: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004513856s
    May 17 08:43:07.670: INFO: Pod "pause" satisfied condition "running and ready"
    May 17 08:43:07.670: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 05/17/23 08:43:07.67
    May 17 08:43:07.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 label pods pause testing-label=testing-label-value'
    May 17 08:43:07.727: INFO: stderr: ""
    May 17 08:43:07.727: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/17/23 08:43:07.727
    May 17 08:43:07.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get pod pause -L testing-label'
    May 17 08:43:07.776: INFO: stderr: ""
    May 17 08:43:07.776: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/17/23 08:43:07.776
    May 17 08:43:07.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 label pods pause testing-label-'
    May 17 08:43:07.831: INFO: stderr: ""
    May 17 08:43:07.831: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/17/23 08:43:07.831
    May 17 08:43:07.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get pod pause -L testing-label'
    May 17 08:43:07.881: INFO: stderr: ""
    May 17 08:43:07.881: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 05/17/23 08:43:07.881
    May 17 08:43:07.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 delete --grace-period=0 --force -f -'
    May 17 08:43:07.937: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 08:43:07.937: INFO: stdout: "pod \"pause\" force deleted\n"
    May 17 08:43:07.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get rc,svc -l name=pause --no-headers'
    May 17 08:43:07.989: INFO: stderr: "No resources found in kubectl-5855 namespace.\n"
    May 17 08:43:07.989: INFO: stdout: ""
    May 17 08:43:07.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-5855 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 17 08:43:08.042: INFO: stderr: ""
    May 17 08:43:08.042: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:43:08.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5855" for this suite. 05/17/23 08:43:08.045
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:43:08.048
May 17 08:43:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:43:08.049
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:08.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:08.057
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:43:08.059
May 17 08:43:08.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9" in namespace "projected-4435" to be "Succeeded or Failed"
May 17 08:43:08.064: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395901ms
May 17 08:43:10.068: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004947924s
May 17 08:43:12.068: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005136158s
STEP: Saw pod success 05/17/23 08:43:12.068
May 17 08:43:12.068: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9" satisfied condition "Succeeded or Failed"
May 17 08:43:12.069: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9 container client-container: <nil>
STEP: delete the pod 05/17/23 08:43:12.072
May 17 08:43:12.079: INFO: Waiting for pod downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9 to disappear
May 17 08:43:12.080: INFO: Pod downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:43:12.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4435" for this suite. 05/17/23 08:43:12.082
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:43:08.048
    May 17 08:43:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:43:08.049
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:08.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:08.057
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:43:08.059
    May 17 08:43:08.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9" in namespace "projected-4435" to be "Succeeded or Failed"
    May 17 08:43:08.064: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395901ms
    May 17 08:43:10.068: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004947924s
    May 17 08:43:12.068: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005136158s
    STEP: Saw pod success 05/17/23 08:43:12.068
    May 17 08:43:12.068: INFO: Pod "downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9" satisfied condition "Succeeded or Failed"
    May 17 08:43:12.069: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:43:12.072
    May 17 08:43:12.079: INFO: Waiting for pod downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9 to disappear
    May 17 08:43:12.080: INFO: Pod downwardapi-volume-51769357-30c3-43b8-92f1-e8b6c92b80e9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:43:12.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4435" for this suite. 05/17/23 08:43:12.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:43:12.085
May 17 08:43:12.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename disruption 05/17/23 08:43:12.085
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:12.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:12.092
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 05/17/23 08:43:12.093
STEP: Waiting for the pdb to be processed 05/17/23 08:43:12.096
STEP: updating the pdb 05/17/23 08:43:14.1
STEP: Waiting for the pdb to be processed 05/17/23 08:43:14.105
STEP: patching the pdb 05/17/23 08:43:16.108
STEP: Waiting for the pdb to be processed 05/17/23 08:43:16.113
STEP: Waiting for the pdb to be deleted 05/17/23 08:43:18.12
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 17 08:43:18.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-538" for this suite. 05/17/23 08:43:18.123
------------------------------
â€¢ [SLOW TEST] [6.041 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:43:12.085
    May 17 08:43:12.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename disruption 05/17/23 08:43:12.085
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:12.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:12.092
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 05/17/23 08:43:12.093
    STEP: Waiting for the pdb to be processed 05/17/23 08:43:12.096
    STEP: updating the pdb 05/17/23 08:43:14.1
    STEP: Waiting for the pdb to be processed 05/17/23 08:43:14.105
    STEP: patching the pdb 05/17/23 08:43:16.108
    STEP: Waiting for the pdb to be processed 05/17/23 08:43:16.113
    STEP: Waiting for the pdb to be deleted 05/17/23 08:43:18.12
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 17 08:43:18.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-538" for this suite. 05/17/23 08:43:18.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:43:18.126
May 17 08:43:18.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 08:43:18.127
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:18.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:18.134
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 05/17/23 08:43:18.136
STEP: waiting for pod running 05/17/23 08:43:18.139
May 17 08:43:18.139: INFO: Waiting up to 2m0s for pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" in namespace "var-expansion-830" to be "running"
May 17 08:43:18.141: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.195977ms
May 17 08:43:20.144: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004692452s
May 17 08:43:20.144: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" satisfied condition "running"
STEP: creating a file in subpath 05/17/23 08:43:20.144
May 17 08:43:20.146: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-830 PodName:var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:43:20.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:43:20.146: INFO: ExecWithOptions: Clientset creation
May 17 08:43:20.146: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-830/pods/var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/17/23 08:43:20.187
May 17 08:43:20.190: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-830 PodName:var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:43:20.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:43:20.190: INFO: ExecWithOptions: Clientset creation
May 17 08:43:20.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-830/pods/var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/17/23 08:43:20.228
May 17 08:43:20.736: INFO: Successfully updated pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e"
STEP: waiting for annotated pod running 05/17/23 08:43:20.736
May 17 08:43:20.736: INFO: Waiting up to 2m0s for pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" in namespace "var-expansion-830" to be "running"
May 17 08:43:20.739: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.300989ms
May 17 08:43:20.739: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" satisfied condition "running"
STEP: deleting the pod gracefully 05/17/23 08:43:20.739
May 17 08:43:20.739: INFO: Deleting pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" in namespace "var-expansion-830"
May 17 08:43:20.742: INFO: Wait up to 5m0s for pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 08:43:54.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-830" for this suite. 05/17/23 08:43:54.75
------------------------------
â€¢ [SLOW TEST] [36.627 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:43:18.126
    May 17 08:43:18.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 08:43:18.127
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:18.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:18.134
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 05/17/23 08:43:18.136
    STEP: waiting for pod running 05/17/23 08:43:18.139
    May 17 08:43:18.139: INFO: Waiting up to 2m0s for pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" in namespace "var-expansion-830" to be "running"
    May 17 08:43:18.141: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.195977ms
    May 17 08:43:20.144: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004692452s
    May 17 08:43:20.144: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" satisfied condition "running"
    STEP: creating a file in subpath 05/17/23 08:43:20.144
    May 17 08:43:20.146: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-830 PodName:var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:43:20.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:43:20.146: INFO: ExecWithOptions: Clientset creation
    May 17 08:43:20.146: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-830/pods/var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/17/23 08:43:20.187
    May 17 08:43:20.190: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-830 PodName:var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:43:20.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:43:20.190: INFO: ExecWithOptions: Clientset creation
    May 17 08:43:20.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-830/pods/var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/17/23 08:43:20.228
    May 17 08:43:20.736: INFO: Successfully updated pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e"
    STEP: waiting for annotated pod running 05/17/23 08:43:20.736
    May 17 08:43:20.736: INFO: Waiting up to 2m0s for pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" in namespace "var-expansion-830" to be "running"
    May 17 08:43:20.739: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.300989ms
    May 17 08:43:20.739: INFO: Pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" satisfied condition "running"
    STEP: deleting the pod gracefully 05/17/23 08:43:20.739
    May 17 08:43:20.739: INFO: Deleting pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" in namespace "var-expansion-830"
    May 17 08:43:20.742: INFO: Wait up to 5m0s for pod "var-expansion-2ac80a03-7299-4951-81aa-ba9627a80d0e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 08:43:54.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-830" for this suite. 05/17/23 08:43:54.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:43:54.753
May 17 08:43:54.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:43:54.754
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:54.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:54.761
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:43:54.763
May 17 08:43:54.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171" in namespace "downward-api-5449" to be "Succeeded or Failed"
May 17 08:43:54.768: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171": Phase="Pending", Reason="", readiness=false. Elapsed: 1.467879ms
May 17 08:43:56.770: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003736654s
May 17 08:43:58.771: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003866398s
STEP: Saw pod success 05/17/23 08:43:58.771
May 17 08:43:58.771: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171" satisfied condition "Succeeded or Failed"
May 17 08:43:58.772: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171 container client-container: <nil>
STEP: delete the pod 05/17/23 08:43:58.775
May 17 08:43:58.781: INFO: Waiting for pod downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171 to disappear
May 17 08:43:58.783: INFO: Pod downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:43:58.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5449" for this suite. 05/17/23 08:43:58.784
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:43:54.753
    May 17 08:43:54.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:43:54.754
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:54.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:54.761
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:43:54.763
    May 17 08:43:54.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171" in namespace "downward-api-5449" to be "Succeeded or Failed"
    May 17 08:43:54.768: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171": Phase="Pending", Reason="", readiness=false. Elapsed: 1.467879ms
    May 17 08:43:56.770: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003736654s
    May 17 08:43:58.771: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003866398s
    STEP: Saw pod success 05/17/23 08:43:58.771
    May 17 08:43:58.771: INFO: Pod "downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171" satisfied condition "Succeeded or Failed"
    May 17 08:43:58.772: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:43:58.775
    May 17 08:43:58.781: INFO: Waiting for pod downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171 to disappear
    May 17 08:43:58.783: INFO: Pod downwardapi-volume-7eb7bf74-848a-48ec-b244-0211f403f171 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:43:58.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5449" for this suite. 05/17/23 08:43:58.784
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:43:58.787
May 17 08:43:58.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sysctl 05/17/23 08:43:58.788
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:58.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:58.795
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/17/23 08:43:58.796
STEP: Watching for error events or started pod 05/17/23 08:43:58.8
STEP: Waiting for pod completion 05/17/23 08:44:00.803
May 17 08:44:00.803: INFO: Waiting up to 3m0s for pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349" in namespace "sysctl-7091" to be "completed"
May 17 08:44:00.804: INFO: Pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349": Phase="Pending", Reason="", readiness=false. Elapsed: 1.435747ms
May 17 08:44:02.807: INFO: Pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004142312s
May 17 08:44:02.807: INFO: Pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/17/23 08:44:02.809
STEP: Getting logs from the pod 05/17/23 08:44:02.809
STEP: Checking that the sysctl is actually updated 05/17/23 08:44:02.812
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 17 08:44:02.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7091" for this suite. 05/17/23 08:44:02.814
------------------------------
â€¢ [4.029 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:43:58.787
    May 17 08:43:58.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sysctl 05/17/23 08:43:58.788
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:43:58.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:43:58.795
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/17/23 08:43:58.796
    STEP: Watching for error events or started pod 05/17/23 08:43:58.8
    STEP: Waiting for pod completion 05/17/23 08:44:00.803
    May 17 08:44:00.803: INFO: Waiting up to 3m0s for pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349" in namespace "sysctl-7091" to be "completed"
    May 17 08:44:00.804: INFO: Pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349": Phase="Pending", Reason="", readiness=false. Elapsed: 1.435747ms
    May 17 08:44:02.807: INFO: Pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004142312s
    May 17 08:44:02.807: INFO: Pod "sysctl-305ff7b6-d9b4-4fb5-9b87-f7993e689349" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/17/23 08:44:02.809
    STEP: Getting logs from the pod 05/17/23 08:44:02.809
    STEP: Checking that the sysctl is actually updated 05/17/23 08:44:02.812
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:02.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7091" for this suite. 05/17/23 08:44:02.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:02.817
May 17 08:44:02.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename containers 05/17/23 08:44:02.818
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:02.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:02.825
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 05/17/23 08:44:02.827
May 17 08:44:02.830: INFO: Waiting up to 5m0s for pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7" in namespace "containers-8186" to be "Succeeded or Failed"
May 17 08:44:02.831: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.160579ms
May 17 08:44:04.833: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003556278s
May 17 08:44:06.834: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004463262s
STEP: Saw pod success 05/17/23 08:44:06.834
May 17 08:44:06.834: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7" satisfied condition "Succeeded or Failed"
May 17 08:44:06.836: INFO: Trying to get logs from node k8s-node1 pod client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 08:44:06.839
May 17 08:44:06.845: INFO: Waiting for pod client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7 to disappear
May 17 08:44:06.846: INFO: Pod client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 17 08:44:06.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8186" for this suite. 05/17/23 08:44:06.848
------------------------------
â€¢ [4.033 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:02.817
    May 17 08:44:02.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename containers 05/17/23 08:44:02.818
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:02.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:02.825
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 05/17/23 08:44:02.827
    May 17 08:44:02.830: INFO: Waiting up to 5m0s for pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7" in namespace "containers-8186" to be "Succeeded or Failed"
    May 17 08:44:02.831: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.160579ms
    May 17 08:44:04.833: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003556278s
    May 17 08:44:06.834: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004463262s
    STEP: Saw pod success 05/17/23 08:44:06.834
    May 17 08:44:06.834: INFO: Pod "client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7" satisfied condition "Succeeded or Failed"
    May 17 08:44:06.836: INFO: Trying to get logs from node k8s-node1 pod client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 08:44:06.839
    May 17 08:44:06.845: INFO: Waiting for pod client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7 to disappear
    May 17 08:44:06.846: INFO: Pod client-containers-bf6ba8db-c89a-4cee-a811-4ec7c6fcf0c7 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:06.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8186" for this suite. 05/17/23 08:44:06.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:06.851
May 17 08:44:06.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:44:06.852
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:06.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:06.86
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-ae4fe809-69da-4885-81ab-0563f4e4bd85 05/17/23 08:44:06.861
STEP: Creating a pod to test consume secrets 05/17/23 08:44:06.864
May 17 08:44:06.867: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a" in namespace "projected-6529" to be "Succeeded or Failed"
May 17 08:44:06.868: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.121511ms
May 17 08:44:08.870: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003308737s
May 17 08:44:10.871: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003994346s
STEP: Saw pod success 05/17/23 08:44:10.871
May 17 08:44:10.871: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a" satisfied condition "Succeeded or Failed"
May 17 08:44:10.872: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:44:10.875
May 17 08:44:10.881: INFO: Waiting for pod pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a to disappear
May 17 08:44:10.883: INFO: Pod pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 17 08:44:10.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6529" for this suite. 05/17/23 08:44:10.884
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:06.851
    May 17 08:44:06.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:44:06.852
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:06.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:06.86
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-ae4fe809-69da-4885-81ab-0563f4e4bd85 05/17/23 08:44:06.861
    STEP: Creating a pod to test consume secrets 05/17/23 08:44:06.864
    May 17 08:44:06.867: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a" in namespace "projected-6529" to be "Succeeded or Failed"
    May 17 08:44:06.868: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.121511ms
    May 17 08:44:08.870: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003308737s
    May 17 08:44:10.871: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003994346s
    STEP: Saw pod success 05/17/23 08:44:10.871
    May 17 08:44:10.871: INFO: Pod "pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a" satisfied condition "Succeeded or Failed"
    May 17 08:44:10.872: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:44:10.875
    May 17 08:44:10.881: INFO: Waiting for pod pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a to disappear
    May 17 08:44:10.883: INFO: Pod pod-projected-secrets-1ab88f05-ebb2-4234-adfb-7df8a556529a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:10.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6529" for this suite. 05/17/23 08:44:10.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:10.888
May 17 08:44:10.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:44:10.889
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:10.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:10.896
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 05/17/23 08:44:10.897
STEP: Creating a ResourceQuota 05/17/23 08:44:15.899
STEP: Ensuring resource quota status is calculated 05/17/23 08:44:15.903
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:44:17.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2153" for this suite. 05/17/23 08:44:17.908
------------------------------
â€¢ [SLOW TEST] [7.023 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:10.888
    May 17 08:44:10.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:44:10.889
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:10.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:10.896
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 05/17/23 08:44:10.897
    STEP: Creating a ResourceQuota 05/17/23 08:44:15.899
    STEP: Ensuring resource quota status is calculated 05/17/23 08:44:15.903
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:17.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2153" for this suite. 05/17/23 08:44:17.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:17.912
May 17 08:44:17.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:44:17.912
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:17.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:17.921
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:44:17.923
May 17 08:44:17.928: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b" in namespace "downward-api-3187" to be "Succeeded or Failed"
May 17 08:44:17.929: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.433956ms
May 17 08:44:19.932: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004281649s
May 17 08:44:21.932: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004494508s
STEP: Saw pod success 05/17/23 08:44:21.932
May 17 08:44:21.932: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b" satisfied condition "Succeeded or Failed"
May 17 08:44:21.934: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b container client-container: <nil>
STEP: delete the pod 05/17/23 08:44:21.937
May 17 08:44:21.943: INFO: Waiting for pod downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b to disappear
May 17 08:44:21.944: INFO: Pod downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:44:21.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3187" for this suite. 05/17/23 08:44:21.946
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:17.912
    May 17 08:44:17.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:44:17.912
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:17.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:17.921
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:44:17.923
    May 17 08:44:17.928: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b" in namespace "downward-api-3187" to be "Succeeded or Failed"
    May 17 08:44:17.929: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.433956ms
    May 17 08:44:19.932: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004281649s
    May 17 08:44:21.932: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004494508s
    STEP: Saw pod success 05/17/23 08:44:21.932
    May 17 08:44:21.932: INFO: Pod "downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b" satisfied condition "Succeeded or Failed"
    May 17 08:44:21.934: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b container client-container: <nil>
    STEP: delete the pod 05/17/23 08:44:21.937
    May 17 08:44:21.943: INFO: Waiting for pod downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b to disappear
    May 17 08:44:21.944: INFO: Pod downwardapi-volume-66806b54-e434-49e9-a52b-2094c88ed32b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:21.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3187" for this suite. 05/17/23 08:44:21.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:21.949
May 17 08:44:21.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:44:21.95
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:21.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:21.958
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-65qvf"  05/17/23 08:44:21.96
May 17 08:44:21.962: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-65qvf"  05/17/23 08:44:21.962
May 17 08:44:21.967: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 17 08:44:21.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2927" for this suite. 05/17/23 08:44:21.969
------------------------------
â€¢ [0.023 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:21.949
    May 17 08:44:21.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:44:21.95
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:21.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:21.958
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-65qvf"  05/17/23 08:44:21.96
    May 17 08:44:21.962: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-65qvf"  05/17/23 08:44:21.962
    May 17 08:44:21.967: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:21.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2927" for this suite. 05/17/23 08:44:21.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:21.973
May 17 08:44:21.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 08:44:21.974
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:21.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:21.981
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/17/23 08:44:21.984
May 17 08:44:21.989: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1693" to be "running and ready"
May 17 08:44:21.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31112ms
May 17 08:44:21.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 08:44:23.993: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004597187s
May 17 08:44:23.993: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 08:44:23.993: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 05/17/23 08:44:23.995
May 17 08:44:23.999: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1693" to be "running and ready"
May 17 08:44:24.000: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.22026ms
May 17 08:44:24.000: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 08:44:26.003: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003886118s
May 17 08:44:26.003: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May 17 08:44:26.003: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/17/23 08:44:26.004
STEP: delete the pod with lifecycle hook 05/17/23 08:44:26.014
May 17 08:44:26.018: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 08:44:26.019: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 08:44:28.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 08:44:28.021: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 08:44:30.020: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 08:44:30.022: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 17 08:44:30.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1693" for this suite. 05/17/23 08:44:30.025
------------------------------
â€¢ [SLOW TEST] [8.054 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:21.973
    May 17 08:44:21.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 08:44:21.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:21.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:21.981
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 08:44:21.984
    May 17 08:44:21.989: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1693" to be "running and ready"
    May 17 08:44:21.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31112ms
    May 17 08:44:21.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:44:23.993: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004597187s
    May 17 08:44:23.993: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 08:44:23.993: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 05/17/23 08:44:23.995
    May 17 08:44:23.999: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1693" to be "running and ready"
    May 17 08:44:24.000: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.22026ms
    May 17 08:44:24.000: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:44:26.003: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003886118s
    May 17 08:44:26.003: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May 17 08:44:26.003: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/17/23 08:44:26.004
    STEP: delete the pod with lifecycle hook 05/17/23 08:44:26.014
    May 17 08:44:26.018: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May 17 08:44:26.019: INFO: Pod pod-with-poststart-exec-hook still exists
    May 17 08:44:28.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May 17 08:44:28.021: INFO: Pod pod-with-poststart-exec-hook still exists
    May 17 08:44:30.020: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May 17 08:44:30.022: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:30.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1693" for this suite. 05/17/23 08:44:30.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:30.028
May 17 08:44:30.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:44:30.029
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:30.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:30.037
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:44:30.044
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:44:30.303
STEP: Deploying the webhook pod 05/17/23 08:44:30.307
STEP: Wait for the deployment to be ready 05/17/23 08:44:30.314
May 17 08:44:30.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:44:32.322
STEP: Verifying the service has paired with the endpoint 05/17/23 08:44:32.329
May 17 08:44:33.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 05/17/23 08:44:33.36
STEP: Creating a configMap that should be mutated 05/17/23 08:44:33.368
STEP: Deleting the collection of validation webhooks 05/17/23 08:44:33.384
STEP: Creating a configMap that should not be mutated 05/17/23 08:44:33.402
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:44:33.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8903" for this suite. 05/17/23 08:44:33.424
STEP: Destroying namespace "webhook-8903-markers" for this suite. 05/17/23 08:44:33.427
------------------------------
â€¢ [3.402 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:30.028
    May 17 08:44:30.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:44:30.029
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:30.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:30.037
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:44:30.044
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:44:30.303
    STEP: Deploying the webhook pod 05/17/23 08:44:30.307
    STEP: Wait for the deployment to be ready 05/17/23 08:44:30.314
    May 17 08:44:30.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:44:32.322
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:44:32.329
    May 17 08:44:33.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 05/17/23 08:44:33.36
    STEP: Creating a configMap that should be mutated 05/17/23 08:44:33.368
    STEP: Deleting the collection of validation webhooks 05/17/23 08:44:33.384
    STEP: Creating a configMap that should not be mutated 05/17/23 08:44:33.402
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:33.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8903" for this suite. 05/17/23 08:44:33.424
    STEP: Destroying namespace "webhook-8903-markers" for this suite. 05/17/23 08:44:33.427
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:33.43
May 17 08:44:33.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pod-network-test 05/17/23 08:44:33.431
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:33.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:33.437
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4895 05/17/23 08:44:33.439
STEP: creating a selector 05/17/23 08:44:33.439
STEP: Creating the service pods in kubernetes 05/17/23 08:44:33.439
May 17 08:44:33.439: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 08:44:33.449: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4895" to be "running and ready"
May 17 08:44:33.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.116047ms
May 17 08:44:33.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:44:35.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004827235s
May 17 08:44:35.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:37.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.003915402s
May 17 08:44:37.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:39.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004171993s
May 17 08:44:39.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:41.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004209523s
May 17 08:44:41.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:43.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004134489s
May 17 08:44:43.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:45.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004842569s
May 17 08:44:45.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:47.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.003924457s
May 17 08:44:47.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:49.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00377804s
May 17 08:44:49.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:51.452: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.003581107s
May 17 08:44:51.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:53.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.003761232s
May 17 08:44:53.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 08:44:55.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004767846s
May 17 08:44:55.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 08:44:55.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 08:44:55.455: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4895" to be "running and ready"
May 17 08:44:55.457: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.327799ms
May 17 08:44:55.457: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 08:44:55.457: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 08:44:55.458
May 17 08:44:55.464: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4895" to be "running"
May 17 08:44:55.467: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039023ms
May 17 08:44:57.470: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006559174s
May 17 08:44:57.471: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 08:44:57.472: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4895" to be "running"
May 17 08:44:57.474: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.235179ms
May 17 08:44:57.474: INFO: Pod "host-test-container-pod" satisfied condition "running"
May 17 08:44:57.475: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 17 08:44:57.475: INFO: Going to poll 192.168.36.93 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 17 08:44:57.476: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.36.93 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4895 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:44:57.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:44:57.476: INFO: ExecWithOptions: Clientset creation
May 17 08:44:57.476: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4895/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.36.93+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 08:44:58.513: INFO: Found all 1 expected endpoints: [netserver-0]
May 17 08:44:58.513: INFO: Going to poll 192.168.169.157 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 17 08:44:58.515: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.169.157 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4895 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:44:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:44:58.515: INFO: ExecWithOptions: Clientset creation
May 17 08:44:58.515: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4895/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.169.157+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 08:44:59.552: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 17 08:44:59.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4895" for this suite. 05/17/23 08:44:59.555
------------------------------
â€¢ [SLOW TEST] [26.128 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:33.43
    May 17 08:44:33.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 08:44:33.431
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:33.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:33.437
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4895 05/17/23 08:44:33.439
    STEP: creating a selector 05/17/23 08:44:33.439
    STEP: Creating the service pods in kubernetes 05/17/23 08:44:33.439
    May 17 08:44:33.439: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 08:44:33.449: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4895" to be "running and ready"
    May 17 08:44:33.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.116047ms
    May 17 08:44:33.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:44:35.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004827235s
    May 17 08:44:35.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:37.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.003915402s
    May 17 08:44:37.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:39.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004171993s
    May 17 08:44:39.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:41.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004209523s
    May 17 08:44:41.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:43.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004134489s
    May 17 08:44:43.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:45.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004842569s
    May 17 08:44:45.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:47.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.003924457s
    May 17 08:44:47.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:49.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00377804s
    May 17 08:44:49.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:51.452: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.003581107s
    May 17 08:44:51.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:53.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.003761232s
    May 17 08:44:53.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 08:44:55.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004767846s
    May 17 08:44:55.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 08:44:55.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 08:44:55.455: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4895" to be "running and ready"
    May 17 08:44:55.457: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.327799ms
    May 17 08:44:55.457: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 08:44:55.457: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 08:44:55.458
    May 17 08:44:55.464: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4895" to be "running"
    May 17 08:44:55.467: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039023ms
    May 17 08:44:57.470: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006559174s
    May 17 08:44:57.471: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 08:44:57.472: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4895" to be "running"
    May 17 08:44:57.474: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.235179ms
    May 17 08:44:57.474: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May 17 08:44:57.475: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    May 17 08:44:57.475: INFO: Going to poll 192.168.36.93 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    May 17 08:44:57.476: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.36.93 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4895 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:44:57.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:44:57.476: INFO: ExecWithOptions: Clientset creation
    May 17 08:44:57.476: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4895/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.36.93+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 08:44:58.513: INFO: Found all 1 expected endpoints: [netserver-0]
    May 17 08:44:58.513: INFO: Going to poll 192.168.169.157 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    May 17 08:44:58.515: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.169.157 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4895 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:44:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:44:58.515: INFO: ExecWithOptions: Clientset creation
    May 17 08:44:58.515: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4895/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.169.157+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 08:44:59.552: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 17 08:44:59.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4895" for this suite. 05/17/23 08:44:59.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:44:59.558
May 17 08:44:59.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename namespaces 05/17/23 08:44:59.559
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:59.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:59.568
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 05/17/23 08:44:59.569
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:59.574
STEP: Creating a pod in the namespace 05/17/23 08:44:59.576
STEP: Waiting for the pod to have running status 05/17/23 08:44:59.579
May 17 08:44:59.579: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-529" to be "running"
May 17 08:44:59.581: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288102ms
May 17 08:45:01.584: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004379853s
May 17 08:45:01.584: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/17/23 08:45:01.584
STEP: Waiting for the namespace to be removed. 05/17/23 08:45:01.587
STEP: Recreating the namespace 05/17/23 08:45:12.59
STEP: Verifying there are no pods in the namespace 05/17/23 08:45:12.596
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:45:12.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-168" for this suite. 05/17/23 08:45:12.599
STEP: Destroying namespace "nsdeletetest-529" for this suite. 05/17/23 08:45:12.602
May 17 08:45:12.603: INFO: Namespace nsdeletetest-529 was already deleted
STEP: Destroying namespace "nsdeletetest-2204" for this suite. 05/17/23 08:45:12.603
------------------------------
â€¢ [SLOW TEST] [13.048 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:44:59.558
    May 17 08:44:59.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename namespaces 05/17/23 08:44:59.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:59.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:44:59.568
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 05/17/23 08:44:59.569
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:44:59.574
    STEP: Creating a pod in the namespace 05/17/23 08:44:59.576
    STEP: Waiting for the pod to have running status 05/17/23 08:44:59.579
    May 17 08:44:59.579: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-529" to be "running"
    May 17 08:44:59.581: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288102ms
    May 17 08:45:01.584: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004379853s
    May 17 08:45:01.584: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/17/23 08:45:01.584
    STEP: Waiting for the namespace to be removed. 05/17/23 08:45:01.587
    STEP: Recreating the namespace 05/17/23 08:45:12.59
    STEP: Verifying there are no pods in the namespace 05/17/23 08:45:12.596
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:45:12.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-168" for this suite. 05/17/23 08:45:12.599
    STEP: Destroying namespace "nsdeletetest-529" for this suite. 05/17/23 08:45:12.602
    May 17 08:45:12.603: INFO: Namespace nsdeletetest-529 was already deleted
    STEP: Destroying namespace "nsdeletetest-2204" for this suite. 05/17/23 08:45:12.603
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:45:12.607
May 17 08:45:12.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename watch 05/17/23 08:45:12.608
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:12.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:12.614
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/17/23 08:45:12.616
STEP: modifying the configmap once 05/17/23 08:45:12.618
STEP: modifying the configmap a second time 05/17/23 08:45:12.622
STEP: deleting the configmap 05/17/23 08:45:12.625
STEP: creating a watch on configmaps from the resource version returned by the first update 05/17/23 08:45:12.627
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/17/23 08:45:12.627
May 17 08:45:12.627: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4166  65602da9-82d5-4249-a47f-5fb6adb34d52 1213177 0 2023-05-17 08:45:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 08:45:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 08:45:12.627: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4166  65602da9-82d5-4249-a47f-5fb6adb34d52 1213178 0 2023-05-17 08:45:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 08:45:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 17 08:45:12.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4166" for this suite. 05/17/23 08:45:12.629
------------------------------
â€¢ [0.024 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:45:12.607
    May 17 08:45:12.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename watch 05/17/23 08:45:12.608
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:12.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:12.614
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/17/23 08:45:12.616
    STEP: modifying the configmap once 05/17/23 08:45:12.618
    STEP: modifying the configmap a second time 05/17/23 08:45:12.622
    STEP: deleting the configmap 05/17/23 08:45:12.625
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/17/23 08:45:12.627
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/17/23 08:45:12.627
    May 17 08:45:12.627: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4166  65602da9-82d5-4249-a47f-5fb6adb34d52 1213177 0 2023-05-17 08:45:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 08:45:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 08:45:12.627: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4166  65602da9-82d5-4249-a47f-5fb6adb34d52 1213178 0 2023-05-17 08:45:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 08:45:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 17 08:45:12.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4166" for this suite. 05/17/23 08:45:12.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:45:12.632
May 17 08:45:12.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:45:12.633
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:12.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:12.64
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:45:12.647
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:45:13.011
STEP: Deploying the webhook pod 05/17/23 08:45:13.015
STEP: Wait for the deployment to be ready 05/17/23 08:45:13.021
May 17 08:45:13.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:45:15.031
STEP: Verifying the service has paired with the endpoint 05/17/23 08:45:15.038
May 17 08:45:16.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
May 17 08:45:16.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3762-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 08:45:16.547
STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 08:45:16.558
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:45:19.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9288" for this suite. 05/17/23 08:45:19.127
STEP: Destroying namespace "webhook-9288-markers" for this suite. 05/17/23 08:45:19.129
------------------------------
â€¢ [SLOW TEST] [6.501 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:45:12.632
    May 17 08:45:12.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:45:12.633
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:12.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:12.64
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:45:12.647
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:45:13.011
    STEP: Deploying the webhook pod 05/17/23 08:45:13.015
    STEP: Wait for the deployment to be ready 05/17/23 08:45:13.021
    May 17 08:45:13.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:45:15.031
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:45:15.038
    May 17 08:45:16.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    May 17 08:45:16.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3762-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 08:45:16.547
    STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 08:45:16.558
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:45:19.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9288" for this suite. 05/17/23 08:45:19.127
    STEP: Destroying namespace "webhook-9288-markers" for this suite. 05/17/23 08:45:19.129
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:45:19.133
May 17 08:45:19.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:45:19.133
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:19.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:19.141
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 05/17/23 08:45:19.142
May 17 08:45:19.147: INFO: Waiting up to 5m0s for pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef" in namespace "downward-api-3364" to be "running and ready"
May 17 08:45:19.148: INFO: Pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2168ms
May 17 08:45:19.148: INFO: The phase of Pod annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef is Pending, waiting for it to be Running (with Ready = true)
May 17 08:45:21.151: INFO: Pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.004197131s
May 17 08:45:21.151: INFO: The phase of Pod annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef is Running (Ready = true)
May 17 08:45:21.151: INFO: Pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef" satisfied condition "running and ready"
May 17 08:45:21.666: INFO: Successfully updated pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:45:25.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3364" for this suite. 05/17/23 08:45:25.683
------------------------------
â€¢ [SLOW TEST] [6.554 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:45:19.133
    May 17 08:45:19.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:45:19.133
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:19.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:19.141
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 05/17/23 08:45:19.142
    May 17 08:45:19.147: INFO: Waiting up to 5m0s for pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef" in namespace "downward-api-3364" to be "running and ready"
    May 17 08:45:19.148: INFO: Pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2168ms
    May 17 08:45:19.148: INFO: The phase of Pod annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:45:21.151: INFO: Pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.004197131s
    May 17 08:45:21.151: INFO: The phase of Pod annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef is Running (Ready = true)
    May 17 08:45:21.151: INFO: Pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef" satisfied condition "running and ready"
    May 17 08:45:21.666: INFO: Successfully updated pod "annotationupdate56d9cd68-8ca0-47d6-b928-147a8055f8ef"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:45:25.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3364" for this suite. 05/17/23 08:45:25.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:45:25.688
May 17 08:45:25.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sched-preemption 05/17/23 08:45:25.688
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:25.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:25.697
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 17 08:45:25.706: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 08:46:25.721: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 05/17/23 08:46:25.722
May 17 08:46:25.732: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 17 08:46:25.737: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 17 08:46:25.746: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 17 08:46:25.749: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/17/23 08:46:25.749
May 17 08:46:25.749: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7091" to be "running"
May 17 08:46:25.751: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949196ms
May 17 08:46:27.754: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004250276s
May 17 08:46:27.754: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May 17 08:46:27.754: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7091" to be "running"
May 17 08:46:27.755: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.445233ms
May 17 08:46:27.755: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May 17 08:46:27.755: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7091" to be "running"
May 17 08:46:27.756: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.157556ms
May 17 08:46:27.756: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May 17 08:46:27.756: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7091" to be "running"
May 17 08:46:27.757: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.135612ms
May 17 08:46:27.757: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/17/23 08:46:27.757
May 17 08:46:27.760: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7091" to be "running"
May 17 08:46:27.761: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.214534ms
May 17 08:46:29.764: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003912244s
May 17 08:46:31.765: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00445864s
May 17 08:46:33.765: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004552982s
May 17 08:46:33.765: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:46:33.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7091" for this suite. 05/17/23 08:46:33.792
------------------------------
â€¢ [SLOW TEST] [68.107 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:45:25.688
    May 17 08:45:25.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 08:45:25.688
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:45:25.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:45:25.697
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 17 08:45:25.706: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 08:46:25.721: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 05/17/23 08:46:25.722
    May 17 08:46:25.732: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May 17 08:46:25.737: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May 17 08:46:25.746: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May 17 08:46:25.749: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/17/23 08:46:25.749
    May 17 08:46:25.749: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7091" to be "running"
    May 17 08:46:25.751: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949196ms
    May 17 08:46:27.754: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004250276s
    May 17 08:46:27.754: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May 17 08:46:27.754: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7091" to be "running"
    May 17 08:46:27.755: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.445233ms
    May 17 08:46:27.755: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May 17 08:46:27.755: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7091" to be "running"
    May 17 08:46:27.756: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.157556ms
    May 17 08:46:27.756: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May 17 08:46:27.756: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7091" to be "running"
    May 17 08:46:27.757: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.135612ms
    May 17 08:46:27.757: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/17/23 08:46:27.757
    May 17 08:46:27.760: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7091" to be "running"
    May 17 08:46:27.761: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.214534ms
    May 17 08:46:29.764: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003912244s
    May 17 08:46:31.765: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00445864s
    May 17 08:46:33.765: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004552982s
    May 17 08:46:33.765: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:46:33.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7091" for this suite. 05/17/23 08:46:33.792
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:46:33.795
May 17 08:46:33.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:46:33.796
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:33.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:33.804
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 05/17/23 08:46:33.805
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/17/23 08:46:33.807
STEP: patching the secret 05/17/23 08:46:33.808
STEP: deleting the secret using a LabelSelector 05/17/23 08:46:33.811
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/17/23 08:46:33.815
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:46:33.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3868" for this suite. 05/17/23 08:46:33.817
------------------------------
â€¢ [0.025 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:46:33.795
    May 17 08:46:33.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:46:33.796
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:33.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:33.804
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 05/17/23 08:46:33.805
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/17/23 08:46:33.807
    STEP: patching the secret 05/17/23 08:46:33.808
    STEP: deleting the secret using a LabelSelector 05/17/23 08:46:33.811
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/17/23 08:46:33.815
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:46:33.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3868" for this suite. 05/17/23 08:46:33.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:46:33.82
May 17 08:46:33.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 08:46:33.821
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:33.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:33.827
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 05/17/23 08:46:33.829
STEP: submitting the pod to kubernetes 05/17/23 08:46:33.829
May 17 08:46:33.833: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" in namespace "pods-7137" to be "running and ready"
May 17 08:46:33.834: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.142851ms
May 17 08:46:33.834: INFO: The phase of Pod pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:46:35.837: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=true. Elapsed: 2.004139134s
May 17 08:46:35.837: INFO: The phase of Pod pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10 is Running (Ready = true)
May 17 08:46:35.837: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/17/23 08:46:35.839
STEP: updating the pod 05/17/23 08:46:35.84
May 17 08:46:36.347: INFO: Successfully updated pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10"
May 17 08:46:36.348: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" in namespace "pods-7137" to be "terminated with reason DeadlineExceeded"
May 17 08:46:36.350: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=true. Elapsed: 2.69551ms
May 17 08:46:38.353: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=true. Elapsed: 2.005660625s
May 17 08:46:40.352: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=false. Elapsed: 4.004953356s
May 17 08:46:42.353: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005017168s
May 17 08:46:42.353: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 08:46:42.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7137" for this suite. 05/17/23 08:46:42.354
------------------------------
â€¢ [SLOW TEST] [8.538 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:46:33.82
    May 17 08:46:33.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 08:46:33.821
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:33.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:33.827
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 05/17/23 08:46:33.829
    STEP: submitting the pod to kubernetes 05/17/23 08:46:33.829
    May 17 08:46:33.833: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" in namespace "pods-7137" to be "running and ready"
    May 17 08:46:33.834: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.142851ms
    May 17 08:46:33.834: INFO: The phase of Pod pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:46:35.837: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=true. Elapsed: 2.004139134s
    May 17 08:46:35.837: INFO: The phase of Pod pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10 is Running (Ready = true)
    May 17 08:46:35.837: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/17/23 08:46:35.839
    STEP: updating the pod 05/17/23 08:46:35.84
    May 17 08:46:36.347: INFO: Successfully updated pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10"
    May 17 08:46:36.348: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" in namespace "pods-7137" to be "terminated with reason DeadlineExceeded"
    May 17 08:46:36.350: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=true. Elapsed: 2.69551ms
    May 17 08:46:38.353: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=true. Elapsed: 2.005660625s
    May 17 08:46:40.352: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Running", Reason="", readiness=false. Elapsed: 4.004953356s
    May 17 08:46:42.353: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005017168s
    May 17 08:46:42.353: INFO: Pod "pod-update-activedeadlineseconds-33fee388-bb36-434b-90bb-2ea94d9b4f10" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 08:46:42.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7137" for this suite. 05/17/23 08:46:42.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:46:42.359
May 17 08:46:42.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:46:42.359
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:42.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:42.367
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:46:42.375
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:46:42.652
STEP: Deploying the webhook pod 05/17/23 08:46:42.657
STEP: Wait for the deployment to be ready 05/17/23 08:46:42.664
May 17 08:46:42.667: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:46:44.673
STEP: Verifying the service has paired with the endpoint 05/17/23 08:46:44.68
May 17 08:46:45.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/17/23 08:46:45.682
STEP: create a configmap that should be updated by the webhook 05/17/23 08:46:45.692
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:46:45.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-299" for this suite. 05/17/23 08:46:45.72
STEP: Destroying namespace "webhook-299-markers" for this suite. 05/17/23 08:46:45.725
------------------------------
â€¢ [3.369 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:46:42.359
    May 17 08:46:42.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:46:42.359
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:42.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:42.367
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:46:42.375
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:46:42.652
    STEP: Deploying the webhook pod 05/17/23 08:46:42.657
    STEP: Wait for the deployment to be ready 05/17/23 08:46:42.664
    May 17 08:46:42.667: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:46:44.673
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:46:44.68
    May 17 08:46:45.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/17/23 08:46:45.682
    STEP: create a configmap that should be updated by the webhook 05/17/23 08:46:45.692
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:46:45.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-299" for this suite. 05/17/23 08:46:45.72
    STEP: Destroying namespace "webhook-299-markers" for this suite. 05/17/23 08:46:45.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:46:45.728
May 17 08:46:45.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 08:46:45.729
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:45.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:45.736
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb in namespace container-probe-4462 05/17/23 08:46:45.737
May 17 08:46:45.743: INFO: Waiting up to 5m0s for pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb" in namespace "container-probe-4462" to be "not pending"
May 17 08:46:45.745: INFO: Pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.211114ms
May 17 08:46:47.747: INFO: Pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003653057s
May 17 08:46:47.747: INFO: Pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb" satisfied condition "not pending"
May 17 08:46:47.747: INFO: Started pod test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb in namespace container-probe-4462
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:46:47.747
May 17 08:46:47.748: INFO: Initial restart count of pod test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb is 0
STEP: deleting the pod 05/17/23 08:50:48.129
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 08:50:48.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4462" for this suite. 05/17/23 08:50:48.138
------------------------------
â€¢ [SLOW TEST] [242.413 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:46:45.728
    May 17 08:46:45.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 08:46:45.729
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:46:45.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:46:45.736
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb in namespace container-probe-4462 05/17/23 08:46:45.737
    May 17 08:46:45.743: INFO: Waiting up to 5m0s for pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb" in namespace "container-probe-4462" to be "not pending"
    May 17 08:46:45.745: INFO: Pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.211114ms
    May 17 08:46:47.747: INFO: Pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003653057s
    May 17 08:46:47.747: INFO: Pod "test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb" satisfied condition "not pending"
    May 17 08:46:47.747: INFO: Started pod test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb in namespace container-probe-4462
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:46:47.747
    May 17 08:46:47.748: INFO: Initial restart count of pod test-webserver-8fb7e59c-6a0f-4115-a644-5cc706e213fb is 0
    STEP: deleting the pod 05/17/23 08:50:48.129
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 08:50:48.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4462" for this suite. 05/17/23 08:50:48.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:50:48.141
May 17 08:50:48.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-runtime 05/17/23 08:50:48.142
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:48.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:48.149
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 05/17/23 08:50:48.15
STEP: wait for the container to reach Failed 05/17/23 08:50:48.154
STEP: get the container status 05/17/23 08:50:51.162
STEP: the container should be terminated 05/17/23 08:50:51.164
STEP: the termination message should be set 05/17/23 08:50:51.164
May 17 08:50:51.164: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/17/23 08:50:51.164
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 17 08:50:51.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9296" for this suite. 05/17/23 08:50:51.174
------------------------------
â€¢ [3.035 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:50:48.141
    May 17 08:50:48.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-runtime 05/17/23 08:50:48.142
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:48.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:48.149
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 05/17/23 08:50:48.15
    STEP: wait for the container to reach Failed 05/17/23 08:50:48.154
    STEP: get the container status 05/17/23 08:50:51.162
    STEP: the container should be terminated 05/17/23 08:50:51.164
    STEP: the termination message should be set 05/17/23 08:50:51.164
    May 17 08:50:51.164: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/17/23 08:50:51.164
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 17 08:50:51.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9296" for this suite. 05/17/23 08:50:51.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:50:51.177
May 17 08:50:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-webhook 05/17/23 08:50:51.178
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:51.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:51.185
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/17/23 08:50:51.186
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 08:50:51.363
STEP: Deploying the custom resource conversion webhook pod 05/17/23 08:50:51.368
STEP: Wait for the deployment to be ready 05/17/23 08:50:51.373
May 17 08:50:51.377: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:50:53.382
STEP: Verifying the service has paired with the endpoint 05/17/23 08:50:53.389
May 17 08:50:54.390: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May 17 08:50:54.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Creating a v1 custom resource 05/17/23 08:50:56.948
STEP: Create a v2 custom resource 05/17/23 08:50:56.958
STEP: List CRs in v1 05/17/23 08:50:56.992
STEP: List CRs in v2 05/17/23 08:50:56.995
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:50:57.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4717" for this suite. 05/17/23 08:50:57.523
------------------------------
â€¢ [SLOW TEST] [6.349 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:50:51.177
    May 17 08:50:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-webhook 05/17/23 08:50:51.178
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:51.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:51.185
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/17/23 08:50:51.186
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 08:50:51.363
    STEP: Deploying the custom resource conversion webhook pod 05/17/23 08:50:51.368
    STEP: Wait for the deployment to be ready 05/17/23 08:50:51.373
    May 17 08:50:51.377: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:50:53.382
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:50:53.389
    May 17 08:50:54.390: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May 17 08:50:54.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Creating a v1 custom resource 05/17/23 08:50:56.948
    STEP: Create a v2 custom resource 05/17/23 08:50:56.958
    STEP: List CRs in v1 05/17/23 08:50:56.992
    STEP: List CRs in v2 05/17/23 08:50:56.995
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:50:57.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4717" for this suite. 05/17/23 08:50:57.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:50:57.526
May 17 08:50:57.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename gc 05/17/23 08:50:57.527
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:57.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:57.534
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/17/23 08:50:57.536
STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 08:50:57.538
STEP: delete the deployment 05/17/23 08:50:58.044
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/17/23 08:50:58.046
STEP: Gathering metrics 05/17/23 08:50:58.556
May 17 08:50:58.568: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
May 17 08:50:58.569: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.340124ms
May 17 08:50:58.569: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
May 17 08:50:58.569: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
May 17 08:50:58.615: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 17 08:50:58.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2038" for this suite. 05/17/23 08:50:58.617
------------------------------
â€¢ [1.094 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:50:57.526
    May 17 08:50:57.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename gc 05/17/23 08:50:57.527
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:57.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:57.534
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/17/23 08:50:57.536
    STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 08:50:57.538
    STEP: delete the deployment 05/17/23 08:50:58.044
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/17/23 08:50:58.046
    STEP: Gathering metrics 05/17/23 08:50:58.556
    May 17 08:50:58.568: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
    May 17 08:50:58.569: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.340124ms
    May 17 08:50:58.569: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
    May 17 08:50:58.569: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
    May 17 08:50:58.615: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 17 08:50:58.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2038" for this suite. 05/17/23 08:50:58.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:50:58.621
May 17 08:50:58.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replicaset 05/17/23 08:50:58.621
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:58.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:58.628
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May 17 08:50:58.630: INFO: Creating ReplicaSet my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31
May 17 08:50:58.633: INFO: Pod name my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31: Found 0 pods out of 1
May 17 08:51:03.636: INFO: Pod name my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31: Found 1 pods out of 1
May 17 08:51:03.636: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31" is running
May 17 08:51:03.636: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc" in namespace "replicaset-7780" to be "running"
May 17 08:51:03.637: INFO: Pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc": Phase="Running", Reason="", readiness=true. Elapsed: 1.726128ms
May 17 08:51:03.637: INFO: Pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc" satisfied condition "running"
May 17 08:51:03.637: INFO: Pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:50:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:51:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:51:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:50:58 +0000 UTC Reason: Message:}])
May 17 08:51:03.637: INFO: Trying to dial the pod
May 17 08:51:08.644: INFO: Controller my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31: Got expected result from replica 1 [my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc]: "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 17 08:51:08.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7780" for this suite. 05/17/23 08:51:08.646
------------------------------
â€¢ [SLOW TEST] [10.028 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:50:58.621
    May 17 08:50:58.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replicaset 05/17/23 08:50:58.621
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:50:58.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:50:58.628
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May 17 08:50:58.630: INFO: Creating ReplicaSet my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31
    May 17 08:50:58.633: INFO: Pod name my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31: Found 0 pods out of 1
    May 17 08:51:03.636: INFO: Pod name my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31: Found 1 pods out of 1
    May 17 08:51:03.636: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31" is running
    May 17 08:51:03.636: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc" in namespace "replicaset-7780" to be "running"
    May 17 08:51:03.637: INFO: Pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc": Phase="Running", Reason="", readiness=true. Elapsed: 1.726128ms
    May 17 08:51:03.637: INFO: Pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc" satisfied condition "running"
    May 17 08:51:03.637: INFO: Pod "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:50:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:51:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:51:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 08:50:58 +0000 UTC Reason: Message:}])
    May 17 08:51:03.637: INFO: Trying to dial the pod
    May 17 08:51:08.644: INFO: Controller my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31: Got expected result from replica 1 [my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc]: "my-hostname-basic-a0a8b394-ffa3-4aa4-b846-667a7718fc31-855gc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:08.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7780" for this suite. 05/17/23 08:51:08.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:08.649
May 17 08:51:08.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename kubectl 05/17/23 08:51:08.65
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:08.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:08.658
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:51:08.66
May 17 08:51:08.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 17 08:51:08.715: INFO: stderr: ""
May 17 08:51:08.716: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/17/23 08:51:08.716
STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 08:51:13.766
May 17 08:51:13.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 get pod e2e-test-httpd-pod -o json'
May 17 08:51:13.816: INFO: stderr: ""
May 17 08:51:13.816: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"dfca524f00cb631fd231aa6d2d17a1cf3339a900d6b2fdec3982ca25cad128f9\",\n            \"cni.projectcalico.org/podIP\": \"192.168.36.108/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.36.108/32\"\n        },\n        \"creationTimestamp\": \"2023-05-17T08:51:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4752\",\n        \"resourceVersion\": \"1214346\",\n        \"uid\": \"eafa692c-49dd-4d34-9b60-a2e393d26d9e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qz6sx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-node1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qz6sx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://553aff78b1bdd90127988fc2f4b2f4593b4299890606b2d6db0146eb437ed9a2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-17T08:51:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.79.210\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.36.108\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.36.108\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-17T08:51:08Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/17/23 08:51:13.816
May 17 08:51:13.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 replace -f -'
May 17 08:51:14.389: INFO: stderr: ""
May 17 08:51:14.389: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/17/23 08:51:14.389
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
May 17 08:51:14.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 delete pods e2e-test-httpd-pod'
May 17 08:51:16.188: INFO: stderr: ""
May 17 08:51:16.188: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 17 08:51:16.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4752" for this suite. 05/17/23 08:51:16.19
------------------------------
â€¢ [SLOW TEST] [7.544 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:08.649
    May 17 08:51:08.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename kubectl 05/17/23 08:51:08.65
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:08.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:08.658
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/17/23 08:51:08.66
    May 17 08:51:08.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May 17 08:51:08.715: INFO: stderr: ""
    May 17 08:51:08.716: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/17/23 08:51:08.716
    STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 08:51:13.766
    May 17 08:51:13.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 get pod e2e-test-httpd-pod -o json'
    May 17 08:51:13.816: INFO: stderr: ""
    May 17 08:51:13.816: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"dfca524f00cb631fd231aa6d2d17a1cf3339a900d6b2fdec3982ca25cad128f9\",\n            \"cni.projectcalico.org/podIP\": \"192.168.36.108/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.36.108/32\"\n        },\n        \"creationTimestamp\": \"2023-05-17T08:51:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4752\",\n        \"resourceVersion\": \"1214346\",\n        \"uid\": \"eafa692c-49dd-4d34-9b60-a2e393d26d9e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qz6sx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-node1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qz6sx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T08:51:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://553aff78b1bdd90127988fc2f4b2f4593b4299890606b2d6db0146eb437ed9a2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-17T08:51:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.79.210\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.36.108\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.36.108\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-17T08:51:08Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/17/23 08:51:13.816
    May 17 08:51:13.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 replace -f -'
    May 17 08:51:14.389: INFO: stderr: ""
    May 17 08:51:14.389: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/17/23 08:51:14.389
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    May 17 08:51:14.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=kubectl-4752 delete pods e2e-test-httpd-pod'
    May 17 08:51:16.188: INFO: stderr: ""
    May 17 08:51:16.188: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:16.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4752" for this suite. 05/17/23 08:51:16.19
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:16.194
May 17 08:51:16.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replicaset 05/17/23 08:51:16.194
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:16.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:16.201
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/17/23 08:51:16.202
May 17 08:51:16.207: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 08:51:21.210: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 08:51:21.21
STEP: getting scale subresource 05/17/23 08:51:21.21
STEP: updating a scale subresource 05/17/23 08:51:21.211
STEP: verifying the replicaset Spec.Replicas was modified 05/17/23 08:51:21.214
STEP: Patch a scale subresource 05/17/23 08:51:21.216
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 17 08:51:21.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5328" for this suite. 05/17/23 08:51:21.223
------------------------------
â€¢ [SLOW TEST] [5.032 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:16.194
    May 17 08:51:16.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replicaset 05/17/23 08:51:16.194
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:16.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:16.201
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/17/23 08:51:16.202
    May 17 08:51:16.207: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 08:51:21.210: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 08:51:21.21
    STEP: getting scale subresource 05/17/23 08:51:21.21
    STEP: updating a scale subresource 05/17/23 08:51:21.211
    STEP: verifying the replicaset Spec.Replicas was modified 05/17/23 08:51:21.214
    STEP: Patch a scale subresource 05/17/23 08:51:21.216
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:21.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5328" for this suite. 05/17/23 08:51:21.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:21.226
May 17 08:51:21.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename daemonsets 05/17/23 08:51:21.227
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:21.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:21.233
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
May 17 08:51:21.241: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:51:21.244
May 17 08:51:21.246: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:21.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:51:21.248: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:51:22.250: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:22.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:51:22.252: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 08:51:23.250: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:23.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 08:51:23.252: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 05/17/23 08:51:23.258
STEP: Check that daemon pods images are updated. 05/17/23 08:51:23.264
May 17 08:51:23.265: INFO: Wrong image for pod: daemon-set-47cks. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 17 08:51:23.265: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 17 08:51:23.267: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:24.270: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 17 08:51:24.272: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:25.270: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 17 08:51:25.272: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:26.269: INFO: Pod daemon-set-98nt7 is not available
May 17 08:51:26.269: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 17 08:51:26.271: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:27.269: INFO: Pod daemon-set-lrt4x is not available
May 17 08:51:27.271: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 05/17/23 08:51:27.271
May 17 08:51:27.272: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:27.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 08:51:27.274: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:51:28.276: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:51:28.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 08:51:28.278: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:51:28.284
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8942, will wait for the garbage collector to delete the pods 05/17/23 08:51:28.284
May 17 08:51:28.339: INFO: Deleting DaemonSet.extensions daemon-set took: 2.865532ms
May 17 08:51:28.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.759126ms
May 17 08:51:31.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:51:31.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 08:51:31.243: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1214578"},"items":null}

May 17 08:51:31.244: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1214578"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:51:31.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8942" for this suite. 05/17/23 08:51:31.25
------------------------------
â€¢ [SLOW TEST] [10.027 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:21.226
    May 17 08:51:21.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename daemonsets 05/17/23 08:51:21.227
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:21.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:21.233
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    May 17 08:51:21.241: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:51:21.244
    May 17 08:51:21.246: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:21.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:51:21.248: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:51:22.250: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:22.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:51:22.252: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 08:51:23.250: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:23.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 08:51:23.252: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 05/17/23 08:51:23.258
    STEP: Check that daemon pods images are updated. 05/17/23 08:51:23.264
    May 17 08:51:23.265: INFO: Wrong image for pod: daemon-set-47cks. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 17 08:51:23.265: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 17 08:51:23.267: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:24.270: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 17 08:51:24.272: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:25.270: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 17 08:51:25.272: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:26.269: INFO: Pod daemon-set-98nt7 is not available
    May 17 08:51:26.269: INFO: Wrong image for pod: daemon-set-vvpxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 17 08:51:26.271: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:27.269: INFO: Pod daemon-set-lrt4x is not available
    May 17 08:51:27.271: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 05/17/23 08:51:27.271
    May 17 08:51:27.272: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:27.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 08:51:27.274: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:51:28.276: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:51:28.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 08:51:28.278: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:51:28.284
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8942, will wait for the garbage collector to delete the pods 05/17/23 08:51:28.284
    May 17 08:51:28.339: INFO: Deleting DaemonSet.extensions daemon-set took: 2.865532ms
    May 17 08:51:28.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.759126ms
    May 17 08:51:31.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:51:31.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 08:51:31.243: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1214578"},"items":null}

    May 17 08:51:31.244: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1214578"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:31.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8942" for this suite. 05/17/23 08:51:31.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:31.254
May 17 08:51:31.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename watch 05/17/23 08:51:31.255
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:31.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:31.262
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/17/23 08:51:31.263
STEP: starting a background goroutine to produce watch events 05/17/23 08:51:31.265
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/17/23 08:51:31.265
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 17 08:51:34.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1222" for this suite. 05/17/23 08:51:34.108
------------------------------
â€¢ [2.908 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:31.254
    May 17 08:51:31.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename watch 05/17/23 08:51:31.255
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:31.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:31.262
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/17/23 08:51:31.263
    STEP: starting a background goroutine to produce watch events 05/17/23 08:51:31.265
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/17/23 08:51:31.265
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:34.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1222" for this suite. 05/17/23 08:51:34.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:34.162
May 17 08:51:34.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:51:34.163
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:34.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:34.17
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:51:34.172
May 17 08:51:34.175: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962" in namespace "projected-7405" to be "Succeeded or Failed"
May 17 08:51:34.177: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962": Phase="Pending", Reason="", readiness=false. Elapsed: 1.285391ms
May 17 08:51:36.180: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005044973s
May 17 08:51:38.180: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004348564s
STEP: Saw pod success 05/17/23 08:51:38.18
May 17 08:51:38.180: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962" satisfied condition "Succeeded or Failed"
May 17 08:51:38.181: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962 container client-container: <nil>
STEP: delete the pod 05/17/23 08:51:38.191
May 17 08:51:38.197: INFO: Waiting for pod downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962 to disappear
May 17 08:51:38.198: INFO: Pod downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 08:51:38.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7405" for this suite. 05/17/23 08:51:38.2
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:34.162
    May 17 08:51:34.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:51:34.163
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:34.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:34.17
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:51:34.172
    May 17 08:51:34.175: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962" in namespace "projected-7405" to be "Succeeded or Failed"
    May 17 08:51:34.177: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962": Phase="Pending", Reason="", readiness=false. Elapsed: 1.285391ms
    May 17 08:51:36.180: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005044973s
    May 17 08:51:38.180: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004348564s
    STEP: Saw pod success 05/17/23 08:51:38.18
    May 17 08:51:38.180: INFO: Pod "downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962" satisfied condition "Succeeded or Failed"
    May 17 08:51:38.181: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:51:38.191
    May 17 08:51:38.197: INFO: Waiting for pod downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962 to disappear
    May 17 08:51:38.198: INFO: Pod downwardapi-volume-d72b9cea-7566-45a0-a76e-ce1519811962 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:38.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7405" for this suite. 05/17/23 08:51:38.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:38.203
May 17 08:51:38.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename namespaces 05/17/23 08:51:38.204
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:38.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:38.212
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 05/17/23 08:51:38.213
STEP: patching the Namespace 05/17/23 08:51:38.219
STEP: get the Namespace and ensuring it has the label 05/17/23 08:51:38.221
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:51:38.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3118" for this suite. 05/17/23 08:51:38.224
STEP: Destroying namespace "nspatchtest-b90aebc4-96c7-49f9-a6b0-6dae0833f88a-9077" for this suite. 05/17/23 08:51:38.227
------------------------------
â€¢ [0.026 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:38.203
    May 17 08:51:38.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename namespaces 05/17/23 08:51:38.204
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:38.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:38.212
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 05/17/23 08:51:38.213
    STEP: patching the Namespace 05/17/23 08:51:38.219
    STEP: get the Namespace and ensuring it has the label 05/17/23 08:51:38.221
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:38.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3118" for this suite. 05/17/23 08:51:38.224
    STEP: Destroying namespace "nspatchtest-b90aebc4-96c7-49f9-a6b0-6dae0833f88a-9077" for this suite. 05/17/23 08:51:38.227
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:38.229
May 17 08:51:38.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename server-version 05/17/23 08:51:38.23
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:38.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:38.239
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/17/23 08:51:38.241
STEP: Confirm major version 05/17/23 08:51:38.241
May 17 08:51:38.241: INFO: Major version: 1
STEP: Confirm minor version 05/17/23 08:51:38.241
May 17 08:51:38.241: INFO: cleanMinorVersion: 26
May 17 08:51:38.241: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
May 17 08:51:38.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-3722" for this suite. 05/17/23 08:51:38.243
------------------------------
â€¢ [0.017 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:38.229
    May 17 08:51:38.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename server-version 05/17/23 08:51:38.23
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:38.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:38.239
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/17/23 08:51:38.241
    STEP: Confirm major version 05/17/23 08:51:38.241
    May 17 08:51:38.241: INFO: Major version: 1
    STEP: Confirm minor version 05/17/23 08:51:38.241
    May 17 08:51:38.241: INFO: cleanMinorVersion: 26
    May 17 08:51:38.241: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:38.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-3722" for this suite. 05/17/23 08:51:38.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:38.247
May 17 08:51:38.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:51:38.247
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:38.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:38.255
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
May 17 08:51:38.262: INFO: Waiting up to 5m0s for pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416" in namespace "svcaccounts-4495" to be "running"
May 17 08:51:38.264: INFO: Pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416": Phase="Pending", Reason="", readiness=false. Elapsed: 1.206601ms
May 17 08:51:40.267: INFO: Pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416": Phase="Running", Reason="", readiness=true. Elapsed: 2.004390676s
May 17 08:51:40.267: INFO: Pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416" satisfied condition "running"
STEP: reading a file in the container 05/17/23 08:51:40.267
May 17 08:51:40.267: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4495 pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/17/23 08:51:40.364
May 17 08:51:40.364: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4495 pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/17/23 08:51:40.461
May 17 08:51:40.461: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4495 pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May 17 08:51:40.559: INFO: Got root ca configmap in namespace "svcaccounts-4495"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 17 08:51:40.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4495" for this suite. 05/17/23 08:51:40.563
------------------------------
â€¢ [2.319 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:38.247
    May 17 08:51:38.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:51:38.247
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:38.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:38.255
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    May 17 08:51:38.262: INFO: Waiting up to 5m0s for pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416" in namespace "svcaccounts-4495" to be "running"
    May 17 08:51:38.264: INFO: Pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416": Phase="Pending", Reason="", readiness=false. Elapsed: 1.206601ms
    May 17 08:51:40.267: INFO: Pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416": Phase="Running", Reason="", readiness=true. Elapsed: 2.004390676s
    May 17 08:51:40.267: INFO: Pod "pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416" satisfied condition "running"
    STEP: reading a file in the container 05/17/23 08:51:40.267
    May 17 08:51:40.267: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4495 pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/17/23 08:51:40.364
    May 17 08:51:40.364: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4495 pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/17/23 08:51:40.461
    May 17 08:51:40.461: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4495 pod-service-account-8f28059c-1921-416c-8f8c-44dc04a4e416 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May 17 08:51:40.559: INFO: Got root ca configmap in namespace "svcaccounts-4495"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:40.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4495" for this suite. 05/17/23 08:51:40.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:40.574
May 17 08:51:40.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename dns 05/17/23 08:51:40.575
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:40.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:40.584
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/17/23 08:51:40.586
May 17 08:51:40.591: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3450  73a740b2-2886-4311-8068-b9cef54cde36 1214790 0 2023-05-17 08:51:40 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-17 08:51:40 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjzwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjzwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 08:51:40.591: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3450" to be "running and ready"
May 17 08:51:40.592: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451357ms
May 17 08:51:40.592: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 17 08:51:42.595: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004483582s
May 17 08:51:42.595: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May 17 08:51:42.595: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/17/23 08:51:42.595
May 17 08:51:42.596: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3450 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:51:42.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:51:42.596: INFO: ExecWithOptions: Clientset creation
May 17 08:51:42.596: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3450/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/17/23 08:51:42.648
May 17 08:51:42.648: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3450 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:51:42.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:51:42.648: INFO: ExecWithOptions: Clientset creation
May 17 08:51:42.648: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3450/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 08:51:42.699: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 17 08:51:42.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3450" for this suite. 05/17/23 08:51:42.708
------------------------------
â€¢ [2.137 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:40.574
    May 17 08:51:40.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename dns 05/17/23 08:51:40.575
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:40.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:40.584
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/17/23 08:51:40.586
    May 17 08:51:40.591: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3450  73a740b2-2886-4311-8068-b9cef54cde36 1214790 0 2023-05-17 08:51:40 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-17 08:51:40 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjzwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjzwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 08:51:40.591: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3450" to be "running and ready"
    May 17 08:51:40.592: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451357ms
    May 17 08:51:40.592: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:51:42.595: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004483582s
    May 17 08:51:42.595: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May 17 08:51:42.595: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/17/23 08:51:42.595
    May 17 08:51:42.596: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3450 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:51:42.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:51:42.596: INFO: ExecWithOptions: Clientset creation
    May 17 08:51:42.596: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3450/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/17/23 08:51:42.648
    May 17 08:51:42.648: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3450 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:51:42.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:51:42.648: INFO: ExecWithOptions: Clientset creation
    May 17 08:51:42.648: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3450/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 08:51:42.699: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:42.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3450" for this suite. 05/17/23 08:51:42.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:42.711
May 17 08:51:42.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:51:42.712
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:42.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:42.72
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:51:42.727
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:51:43.011
STEP: Deploying the webhook pod 05/17/23 08:51:43.015
STEP: Wait for the deployment to be ready 05/17/23 08:51:43.021
May 17 08:51:43.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:51:45.031
STEP: Verifying the service has paired with the endpoint 05/17/23 08:51:45.038
May 17 08:51:46.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 05/17/23 08:51:46.04
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/17/23 08:51:46.041
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 08:51:46.041
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/17/23 08:51:46.041
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/17/23 08:51:46.041
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 08:51:46.041
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 08:51:46.042
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:51:46.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7967" for this suite. 05/17/23 08:51:46.063
STEP: Destroying namespace "webhook-7967-markers" for this suite. 05/17/23 08:51:46.067
------------------------------
â€¢ [3.359 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:42.711
    May 17 08:51:42.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:51:42.712
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:42.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:42.72
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:51:42.727
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:51:43.011
    STEP: Deploying the webhook pod 05/17/23 08:51:43.015
    STEP: Wait for the deployment to be ready 05/17/23 08:51:43.021
    May 17 08:51:43.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:51:45.031
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:51:45.038
    May 17 08:51:46.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 05/17/23 08:51:46.04
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/17/23 08:51:46.041
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 08:51:46.041
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/17/23 08:51:46.041
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/17/23 08:51:46.041
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 08:51:46.041
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 08:51:46.042
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:46.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7967" for this suite. 05/17/23 08:51:46.063
    STEP: Destroying namespace "webhook-7967-markers" for this suite. 05/17/23 08:51:46.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:46.071
May 17 08:51:46.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename deployment 05/17/23 08:51:46.071
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:46.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:46.08
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May 17 08:51:46.085: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 17 08:51:51.087: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 08:51:51.087
May 17 08:51:51.087: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/17/23 08:51:51.091
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 08:51:53.101: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1157  b80daac2-ab3f-4830-a36b-664a9f910fcc 1214984 1 2023-05-17 08:51:51 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:51:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005880918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 08:51:51 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-05-17 08:51:52 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 08:51:53.102: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1157  0fa19e4b-2777-4171-80c2-ad31b17f6a52 1214974 1 2023-05-17 08:51:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b80daac2-ab3f-4830-a36b-664a9f910fcc 0xc005880ce7 0xc005880ce8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b80daac2-ab3f-4830-a36b-664a9f910fcc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:51:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005880d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 08:51:53.104: INFO: Pod "test-cleanup-deployment-7698ff6f6b-lwwsp" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-lwwsp test-cleanup-deployment-7698ff6f6b- deployment-1157  c8a63481-e3f7-440d-8084-e2cd139f2766 1214973 0 2023-05-17 08:51:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:bb82af27aca653f0d619d4703799ba45cf1af66713c0ea478f105e8acba6e606 cni.projectcalico.org/podIP:192.168.36.68/32 cni.projectcalico.org/podIPs:192.168.36.68/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 0fa19e4b-2777-4171-80c2-ad31b17f6a52 0xc005705937 0xc005705938}] [] [{calico Update v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0fa19e4b-2777-4171-80c2-ad31b17f6a52\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:51:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5b8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5b8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.68,StartTime:2023-05-17 08:51:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:51:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://9fe297e25b639b26b4b3902ab76a300090876be0a5646c4d00664dfd0119566f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 17 08:51:53.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1157" for this suite. 05/17/23 08:51:53.106
------------------------------
â€¢ [SLOW TEST] [7.039 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:46.071
    May 17 08:51:46.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename deployment 05/17/23 08:51:46.071
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:46.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:46.08
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May 17 08:51:46.085: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    May 17 08:51:51.087: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 08:51:51.087
    May 17 08:51:51.087: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/17/23 08:51:51.091
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 08:51:53.101: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1157  b80daac2-ab3f-4830-a36b-664a9f910fcc 1214984 1 2023-05-17 08:51:51 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:51:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005880918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 08:51:51 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-05-17 08:51:52 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 08:51:53.102: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1157  0fa19e4b-2777-4171-80c2-ad31b17f6a52 1214974 1 2023-05-17 08:51:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b80daac2-ab3f-4830-a36b-664a9f910fcc 0xc005880ce7 0xc005880ce8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b80daac2-ab3f-4830-a36b-664a9f910fcc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 08:51:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005880d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 08:51:53.104: INFO: Pod "test-cleanup-deployment-7698ff6f6b-lwwsp" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-lwwsp test-cleanup-deployment-7698ff6f6b- deployment-1157  c8a63481-e3f7-440d-8084-e2cd139f2766 1214973 0 2023-05-17 08:51:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:bb82af27aca653f0d619d4703799ba45cf1af66713c0ea478f105e8acba6e606 cni.projectcalico.org/podIP:192.168.36.68/32 cni.projectcalico.org/podIPs:192.168.36.68/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 0fa19e4b-2777-4171-80c2-ad31b17f6a52 0xc005705937 0xc005705938}] [] [{calico Update v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-17 08:51:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0fa19e4b-2777-4171-80c2-ad31b17f6a52\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 08:51:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.36.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5b8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5b8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 08:51:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.79.210,PodIP:192.168.36.68,StartTime:2023-05-17 08:51:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 08:51:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://9fe297e25b639b26b4b3902ab76a300090876be0a5646c4d00664dfd0119566f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:53.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1157" for this suite. 05/17/23 08:51:53.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:53.11
May 17 08:51:53.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replication-controller 05/17/23 08:51:53.111
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:53.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:53.119
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 05/17/23 08:51:53.121
STEP: When the matched label of one of its pods change 05/17/23 08:51:53.124
May 17 08:51:53.126: INFO: Pod name pod-release: Found 0 pods out of 1
May 17 08:51:58.128: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/17/23 08:51:58.133
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 17 08:51:59.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4599" for this suite. 05/17/23 08:51:59.14
------------------------------
â€¢ [SLOW TEST] [6.033 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:53.11
    May 17 08:51:53.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replication-controller 05/17/23 08:51:53.111
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:53.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:53.119
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 05/17/23 08:51:53.121
    STEP: When the matched label of one of its pods change 05/17/23 08:51:53.124
    May 17 08:51:53.126: INFO: Pod name pod-release: Found 0 pods out of 1
    May 17 08:51:58.128: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/17/23 08:51:58.133
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 17 08:51:59.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4599" for this suite. 05/17/23 08:51:59.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:51:59.144
May 17 08:51:59.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:51:59.145
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:59.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:59.153
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:51:59.155
May 17 08:51:59.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518" in namespace "downward-api-4803" to be "Succeeded or Failed"
May 17 08:51:59.160: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518": Phase="Pending", Reason="", readiness=false. Elapsed: 1.144182ms
May 17 08:52:01.162: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003334169s
May 17 08:52:03.162: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003714827s
STEP: Saw pod success 05/17/23 08:52:03.162
May 17 08:52:03.162: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518" satisfied condition "Succeeded or Failed"
May 17 08:52:03.164: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518 container client-container: <nil>
STEP: delete the pod 05/17/23 08:52:03.168
May 17 08:52:03.174: INFO: Waiting for pod downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518 to disappear
May 17 08:52:03.176: INFO: Pod downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:52:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4803" for this suite. 05/17/23 08:52:03.178
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:51:59.144
    May 17 08:51:59.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:51:59.145
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:51:59.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:51:59.153
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:51:59.155
    May 17 08:51:59.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518" in namespace "downward-api-4803" to be "Succeeded or Failed"
    May 17 08:51:59.160: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518": Phase="Pending", Reason="", readiness=false. Elapsed: 1.144182ms
    May 17 08:52:01.162: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003334169s
    May 17 08:52:03.162: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003714827s
    STEP: Saw pod success 05/17/23 08:52:03.162
    May 17 08:52:03.162: INFO: Pod "downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518" satisfied condition "Succeeded or Failed"
    May 17 08:52:03.164: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:52:03.168
    May 17 08:52:03.174: INFO: Waiting for pod downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518 to disappear
    May 17 08:52:03.176: INFO: Pod downwardapi-volume-659737bb-5ef7-4174-8625-97e7a3c73518 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:52:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4803" for this suite. 05/17/23 08:52:03.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:52:03.181
May 17 08:52:03.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename limitrange 05/17/23 08:52:03.182
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:52:03.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:52:03.19
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 05/17/23 08:52:03.191
STEP: Setting up watch 05/17/23 08:52:03.191
STEP: Submitting a LimitRange 05/17/23 08:52:03.293
STEP: Verifying LimitRange creation was observed 05/17/23 08:52:03.297
STEP: Fetching the LimitRange to ensure it has proper values 05/17/23 08:52:03.297
May 17 08:52:03.299: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 17 08:52:03.299: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/17/23 08:52:03.299
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/17/23 08:52:03.303
May 17 08:52:03.304: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 17 08:52:03.304: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/17/23 08:52:03.304
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/17/23 08:52:03.307
May 17 08:52:03.309: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 17 08:52:03.309: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/17/23 08:52:03.309
STEP: Failing to create a Pod with more than max resources 05/17/23 08:52:03.31
STEP: Updating a LimitRange 05/17/23 08:52:03.311
STEP: Verifying LimitRange updating is effective 05/17/23 08:52:03.315
STEP: Creating a Pod with less than former min resources 05/17/23 08:52:05.317
STEP: Failing to create a Pod with more than max resources 05/17/23 08:52:05.321
STEP: Deleting a LimitRange 05/17/23 08:52:05.323
STEP: Verifying the LimitRange was deleted 05/17/23 08:52:05.326
May 17 08:52:10.329: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/17/23 08:52:10.329
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May 17 08:52:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-588" for this suite. 05/17/23 08:52:10.338
------------------------------
â€¢ [SLOW TEST] [7.161 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:52:03.181
    May 17 08:52:03.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename limitrange 05/17/23 08:52:03.182
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:52:03.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:52:03.19
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 05/17/23 08:52:03.191
    STEP: Setting up watch 05/17/23 08:52:03.191
    STEP: Submitting a LimitRange 05/17/23 08:52:03.293
    STEP: Verifying LimitRange creation was observed 05/17/23 08:52:03.297
    STEP: Fetching the LimitRange to ensure it has proper values 05/17/23 08:52:03.297
    May 17 08:52:03.299: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May 17 08:52:03.299: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/17/23 08:52:03.299
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/17/23 08:52:03.303
    May 17 08:52:03.304: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May 17 08:52:03.304: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/17/23 08:52:03.304
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/17/23 08:52:03.307
    May 17 08:52:03.309: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May 17 08:52:03.309: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/17/23 08:52:03.309
    STEP: Failing to create a Pod with more than max resources 05/17/23 08:52:03.31
    STEP: Updating a LimitRange 05/17/23 08:52:03.311
    STEP: Verifying LimitRange updating is effective 05/17/23 08:52:03.315
    STEP: Creating a Pod with less than former min resources 05/17/23 08:52:05.317
    STEP: Failing to create a Pod with more than max resources 05/17/23 08:52:05.321
    STEP: Deleting a LimitRange 05/17/23 08:52:05.323
    STEP: Verifying the LimitRange was deleted 05/17/23 08:52:05.326
    May 17 08:52:10.329: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/17/23 08:52:10.329
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May 17 08:52:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-588" for this suite. 05/17/23 08:52:10.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:52:10.344
May 17 08:52:10.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:52:10.345
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:52:10.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:52:10.353
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:52:10.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-515" for this suite. 05/17/23 08:52:10.375
------------------------------
â€¢ [0.033 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:52:10.344
    May 17 08:52:10.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:52:10.345
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:52:10.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:52:10.353
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:52:10.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-515" for this suite. 05/17/23 08:52:10.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:52:10.377
May 17 08:52:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 08:52:10.378
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:52:10.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:52:10.385
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 05/17/23 08:52:10.387
May 17 08:52:10.392: INFO: Waiting up to 2m0s for pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" in namespace "var-expansion-765" to be "running"
May 17 08:52:10.394: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.7453ms
May 17 08:52:12.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004632682s
May 17 08:52:14.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004612021s
May 17 08:52:16.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004581517s
May 17 08:52:18.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004633787s
May 17 08:52:20.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004904042s
May 17 08:52:22.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005313498s
May 17 08:52:24.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004777626s
May 17 08:52:26.399: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006757621s
May 17 08:52:28.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005680089s
May 17 08:52:30.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005676038s
May 17 08:52:32.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004512948s
May 17 08:52:34.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006353682s
May 17 08:52:36.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006154438s
May 17 08:52:38.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005276629s
May 17 08:52:40.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006657116s
May 17 08:52:42.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005414522s
May 17 08:52:44.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006138661s
May 17 08:52:46.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005995961s
May 17 08:52:48.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004910668s
May 17 08:52:50.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006398883s
May 17 08:52:52.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005193237s
May 17 08:52:54.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005790585s
May 17 08:52:56.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006357619s
May 17 08:52:58.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004992435s
May 17 08:53:00.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005603881s
May 17 08:53:02.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005068879s
May 17 08:53:04.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005356383s
May 17 08:53:06.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004933613s
May 17 08:53:08.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0050541s
May 17 08:53:10.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005712285s
May 17 08:53:12.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004073403s
May 17 08:53:14.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004726529s
May 17 08:53:16.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00583661s
May 17 08:53:18.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005333621s
May 17 08:53:20.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005510694s
May 17 08:53:22.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004572349s
May 17 08:53:24.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005778584s
May 17 08:53:26.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005832645s
May 17 08:53:28.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005187244s
May 17 08:53:30.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005484571s
May 17 08:53:32.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004728894s
May 17 08:53:34.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.003964246s
May 17 08:53:36.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005246129s
May 17 08:53:38.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004290993s
May 17 08:53:40.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00441628s
May 17 08:53:42.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004749212s
May 17 08:53:44.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005246451s
May 17 08:53:46.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006092983s
May 17 08:53:48.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004293788s
May 17 08:53:50.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004115156s
May 17 08:53:52.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.003971038s
May 17 08:53:54.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004724209s
May 17 08:53:56.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005782145s
May 17 08:53:58.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004968208s
May 17 08:54:00.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005227852s
May 17 08:54:02.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004805341s
May 17 08:54:04.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005567603s
May 17 08:54:06.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006007512s
May 17 08:54:08.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004408632s
May 17 08:54:10.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006340717s
May 17 08:54:10.400: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007906483s
STEP: updating the pod 05/17/23 08:54:10.4
May 17 08:54:10.909: INFO: Successfully updated pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1"
STEP: waiting for pod running 05/17/23 08:54:10.909
May 17 08:54:10.909: INFO: Waiting up to 2m0s for pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" in namespace "var-expansion-765" to be "running"
May 17 08:54:10.910: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472453ms
May 17 08:54:12.913: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004494028s
May 17 08:54:12.913: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" satisfied condition "running"
STEP: deleting the pod gracefully 05/17/23 08:54:12.913
May 17 08:54:12.913: INFO: Deleting pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" in namespace "var-expansion-765"
May 17 08:54:12.918: INFO: Wait up to 5m0s for pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 08:54:44.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-765" for this suite. 05/17/23 08:54:44.928
------------------------------
â€¢ [SLOW TEST] [154.554 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:52:10.377
    May 17 08:52:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 08:52:10.378
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:52:10.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:52:10.385
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 05/17/23 08:52:10.387
    May 17 08:52:10.392: INFO: Waiting up to 2m0s for pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" in namespace "var-expansion-765" to be "running"
    May 17 08:52:10.394: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.7453ms
    May 17 08:52:12.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004632682s
    May 17 08:52:14.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004612021s
    May 17 08:52:16.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004581517s
    May 17 08:52:18.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004633787s
    May 17 08:52:20.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004904042s
    May 17 08:52:22.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005313498s
    May 17 08:52:24.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004777626s
    May 17 08:52:26.399: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006757621s
    May 17 08:52:28.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005680089s
    May 17 08:52:30.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005676038s
    May 17 08:52:32.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004512948s
    May 17 08:52:34.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006353682s
    May 17 08:52:36.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006154438s
    May 17 08:52:38.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005276629s
    May 17 08:52:40.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006657116s
    May 17 08:52:42.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005414522s
    May 17 08:52:44.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006138661s
    May 17 08:52:46.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005995961s
    May 17 08:52:48.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004910668s
    May 17 08:52:50.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006398883s
    May 17 08:52:52.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005193237s
    May 17 08:52:54.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005790585s
    May 17 08:52:56.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006357619s
    May 17 08:52:58.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004992435s
    May 17 08:53:00.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005603881s
    May 17 08:53:02.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005068879s
    May 17 08:53:04.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005356383s
    May 17 08:53:06.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004933613s
    May 17 08:53:08.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0050541s
    May 17 08:53:10.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005712285s
    May 17 08:53:12.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004073403s
    May 17 08:53:14.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004726529s
    May 17 08:53:16.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00583661s
    May 17 08:53:18.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005333621s
    May 17 08:53:20.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005510694s
    May 17 08:53:22.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004572349s
    May 17 08:53:24.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005778584s
    May 17 08:53:26.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005832645s
    May 17 08:53:28.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005187244s
    May 17 08:53:30.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005484571s
    May 17 08:53:32.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004728894s
    May 17 08:53:34.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.003964246s
    May 17 08:53:36.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005246129s
    May 17 08:53:38.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004290993s
    May 17 08:53:40.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00441628s
    May 17 08:53:42.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004749212s
    May 17 08:53:44.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005246451s
    May 17 08:53:46.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006092983s
    May 17 08:53:48.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004293788s
    May 17 08:53:50.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004115156s
    May 17 08:53:52.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.003971038s
    May 17 08:53:54.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004724209s
    May 17 08:53:56.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005782145s
    May 17 08:53:58.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004968208s
    May 17 08:54:00.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005227852s
    May 17 08:54:02.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004805341s
    May 17 08:54:04.397: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005567603s
    May 17 08:54:06.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006007512s
    May 17 08:54:08.396: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004408632s
    May 17 08:54:10.398: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006340717s
    May 17 08:54:10.400: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007906483s
    STEP: updating the pod 05/17/23 08:54:10.4
    May 17 08:54:10.909: INFO: Successfully updated pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1"
    STEP: waiting for pod running 05/17/23 08:54:10.909
    May 17 08:54:10.909: INFO: Waiting up to 2m0s for pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" in namespace "var-expansion-765" to be "running"
    May 17 08:54:10.910: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472453ms
    May 17 08:54:12.913: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004494028s
    May 17 08:54:12.913: INFO: Pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" satisfied condition "running"
    STEP: deleting the pod gracefully 05/17/23 08:54:12.913
    May 17 08:54:12.913: INFO: Deleting pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" in namespace "var-expansion-765"
    May 17 08:54:12.918: INFO: Wait up to 5m0s for pod "var-expansion-fd2deb02-730f-4b89-8229-277a88339cc1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 08:54:44.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-765" for this suite. 05/17/23 08:54:44.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:54:44.932
May 17 08:54:44.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 08:54:44.932
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:44.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:44.941
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
May 17 08:54:44.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: creating the pod 05/17/23 08:54:44.943
STEP: submitting the pod to kubernetes 05/17/23 08:54:44.943
May 17 08:54:44.948: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4" in namespace "pods-2305" to be "running and ready"
May 17 08:54:44.949: INFO: Pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524371ms
May 17 08:54:44.949: INFO: The phase of Pod pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4 is Pending, waiting for it to be Running (with Ready = true)
May 17 08:54:46.953: INFO: Pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00509994s
May 17 08:54:46.953: INFO: The phase of Pod pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4 is Running (Ready = true)
May 17 08:54:46.953: INFO: Pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 08:54:47.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2305" for this suite. 05/17/23 08:54:47.018
------------------------------
â€¢ [2.090 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:54:44.932
    May 17 08:54:44.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 08:54:44.932
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:44.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:44.941
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    May 17 08:54:44.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: creating the pod 05/17/23 08:54:44.943
    STEP: submitting the pod to kubernetes 05/17/23 08:54:44.943
    May 17 08:54:44.948: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4" in namespace "pods-2305" to be "running and ready"
    May 17 08:54:44.949: INFO: Pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524371ms
    May 17 08:54:44.949: INFO: The phase of Pod pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4 is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:54:46.953: INFO: Pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00509994s
    May 17 08:54:46.953: INFO: The phase of Pod pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4 is Running (Ready = true)
    May 17 08:54:46.953: INFO: Pod "pod-exec-websocket-578ea740-32e1-4a84-90a6-cac8604d4eb4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 08:54:47.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2305" for this suite. 05/17/23 08:54:47.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:54:47.022
May 17 08:54:47.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:54:47.023
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:47.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:47.033
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 05/17/23 08:54:47.035
May 17 08:54:47.039: INFO: Waiting up to 5m0s for pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0" in namespace "downward-api-2786" to be "Succeeded or Failed"
May 17 08:54:47.041: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.401722ms
May 17 08:54:49.044: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004312464s
May 17 08:54:51.045: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005169525s
STEP: Saw pod success 05/17/23 08:54:51.045
May 17 08:54:51.045: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0" satisfied condition "Succeeded or Failed"
May 17 08:54:51.046: INFO: Trying to get logs from node k8s-node1 pod downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0 container dapi-container: <nil>
STEP: delete the pod 05/17/23 08:54:51.056
May 17 08:54:51.063: INFO: Waiting for pod downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0 to disappear
May 17 08:54:51.064: INFO: Pod downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 17 08:54:51.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2786" for this suite. 05/17/23 08:54:51.065
------------------------------
â€¢ [4.046 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:54:47.022
    May 17 08:54:47.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:54:47.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:47.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:47.033
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 05/17/23 08:54:47.035
    May 17 08:54:47.039: INFO: Waiting up to 5m0s for pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0" in namespace "downward-api-2786" to be "Succeeded or Failed"
    May 17 08:54:47.041: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.401722ms
    May 17 08:54:49.044: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004312464s
    May 17 08:54:51.045: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005169525s
    STEP: Saw pod success 05/17/23 08:54:51.045
    May 17 08:54:51.045: INFO: Pod "downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0" satisfied condition "Succeeded or Failed"
    May 17 08:54:51.046: INFO: Trying to get logs from node k8s-node1 pod downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 08:54:51.056
    May 17 08:54:51.063: INFO: Waiting for pod downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0 to disappear
    May 17 08:54:51.064: INFO: Pod downward-api-50acdf8d-5139-4569-a6de-2aebd2d362a0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 17 08:54:51.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2786" for this suite. 05/17/23 08:54:51.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:54:51.068
May 17 08:54:51.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:54:51.069
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:51.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:51.077
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 08:54:51.079
May 17 08:54:51.083: INFO: Waiting up to 5m0s for pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544" in namespace "emptydir-723" to be "Succeeded or Failed"
May 17 08:54:51.084: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35519ms
May 17 08:54:53.088: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004615462s
May 17 08:54:55.087: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003896834s
STEP: Saw pod success 05/17/23 08:54:55.087
May 17 08:54:55.087: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544" satisfied condition "Succeeded or Failed"
May 17 08:54:55.088: INFO: Trying to get logs from node k8s-node1 pod pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544 container test-container: <nil>
STEP: delete the pod 05/17/23 08:54:55.092
May 17 08:54:55.100: INFO: Waiting for pod pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544 to disappear
May 17 08:54:55.101: INFO: Pod pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:54:55.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-723" for this suite. 05/17/23 08:54:55.103
------------------------------
â€¢ [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:54:51.068
    May 17 08:54:51.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:54:51.069
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:51.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:51.077
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 08:54:51.079
    May 17 08:54:51.083: INFO: Waiting up to 5m0s for pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544" in namespace "emptydir-723" to be "Succeeded or Failed"
    May 17 08:54:51.084: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35519ms
    May 17 08:54:53.088: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004615462s
    May 17 08:54:55.087: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003896834s
    STEP: Saw pod success 05/17/23 08:54:55.087
    May 17 08:54:55.087: INFO: Pod "pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544" satisfied condition "Succeeded or Failed"
    May 17 08:54:55.088: INFO: Trying to get logs from node k8s-node1 pod pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:54:55.092
    May 17 08:54:55.100: INFO: Waiting for pod pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544 to disappear
    May 17 08:54:55.101: INFO: Pod pod-61f0f6d1-db61-4e32-b1e9-8a42c30f1544 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:54:55.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-723" for this suite. 05/17/23 08:54:55.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:54:55.106
May 17 08:54:55.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:54:55.107
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:55.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:55.115
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-d289170d-39e6-4c0b-b5bc-d9573ba15da0 05/17/23 08:54:55.117
STEP: Creating a pod to test consume configMaps 05/17/23 08:54:55.119
May 17 08:54:55.124: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b" in namespace "projected-4443" to be "Succeeded or Failed"
May 17 08:54:55.125: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.367868ms
May 17 08:54:57.128: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004229717s
May 17 08:54:59.130: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005869404s
STEP: Saw pod success 05/17/23 08:54:59.13
May 17 08:54:59.130: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b" satisfied condition "Succeeded or Failed"
May 17 08:54:59.131: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b container agnhost-container: <nil>
STEP: delete the pod 05/17/23 08:54:59.134
May 17 08:54:59.143: INFO: Waiting for pod pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b to disappear
May 17 08:54:59.144: INFO: Pod pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 08:54:59.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4443" for this suite. 05/17/23 08:54:59.146
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:54:55.106
    May 17 08:54:55.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:54:55.107
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:55.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:55.115
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-d289170d-39e6-4c0b-b5bc-d9573ba15da0 05/17/23 08:54:55.117
    STEP: Creating a pod to test consume configMaps 05/17/23 08:54:55.119
    May 17 08:54:55.124: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b" in namespace "projected-4443" to be "Succeeded or Failed"
    May 17 08:54:55.125: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.367868ms
    May 17 08:54:57.128: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004229717s
    May 17 08:54:59.130: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005869404s
    STEP: Saw pod success 05/17/23 08:54:59.13
    May 17 08:54:59.130: INFO: Pod "pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b" satisfied condition "Succeeded or Failed"
    May 17 08:54:59.131: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 08:54:59.134
    May 17 08:54:59.143: INFO: Waiting for pod pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b to disappear
    May 17 08:54:59.144: INFO: Pod pod-projected-configmaps-f1101019-3e70-4cb3-86ac-0515dbaade4b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:54:59.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4443" for this suite. 05/17/23 08:54:59.146
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:54:59.149
May 17 08:54:59.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename init-container 05/17/23 08:54:59.15
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:59.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:59.159
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 05/17/23 08:54:59.16
May 17 08:54:59.160: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 17 08:55:02.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4477" for this suite. 05/17/23 08:55:02.649
------------------------------
â€¢ [3.503 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:54:59.149
    May 17 08:54:59.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename init-container 05/17/23 08:54:59.15
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:54:59.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:54:59.159
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 05/17/23 08:54:59.16
    May 17 08:54:59.160: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 17 08:55:02.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4477" for this suite. 05/17/23 08:55:02.649
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:55:02.653
May 17 08:55:02.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename security-context-test 05/17/23 08:55:02.653
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:55:02.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:55:02.663
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
May 17 08:55:02.669: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98" in namespace "security-context-test-7698" to be "Succeeded or Failed"
May 17 08:55:02.671: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495242ms
May 17 08:55:04.674: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004514093s
May 17 08:55:06.674: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005057328s
May 17 08:55:06.674: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 17 08:55:06.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7698" for this suite. 05/17/23 08:55:06.676
------------------------------
â€¢ [4.027 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:55:02.653
    May 17 08:55:02.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename security-context-test 05/17/23 08:55:02.653
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:55:02.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:55:02.663
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    May 17 08:55:02.669: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98" in namespace "security-context-test-7698" to be "Succeeded or Failed"
    May 17 08:55:02.671: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495242ms
    May 17 08:55:04.674: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004514093s
    May 17 08:55:06.674: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005057328s
    May 17 08:55:06.674: INFO: Pod "busybox-user-65534-7645c91f-7250-4dbb-9b86-7caeab732b98" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 17 08:55:06.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7698" for this suite. 05/17/23 08:55:06.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:55:06.68
May 17 08:55:06.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-runtime 05/17/23 08:55:06.681
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:55:06.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:55:06.69
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/17/23 08:55:06.696
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/17/23 08:55:24.748
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/17/23 08:55:24.75
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/17/23 08:55:24.753
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/17/23 08:55:24.753
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/17/23 08:55:24.763
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/17/23 08:55:26.77
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/17/23 08:55:28.775
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/17/23 08:55:28.778
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/17/23 08:55:28.778
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/17/23 08:55:28.79
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/17/23 08:55:29.796
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/17/23 08:55:33.304
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/17/23 08:55:33.307
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/17/23 08:55:33.307
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 17 08:55:33.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-868" for this suite. 05/17/23 08:55:33.32
------------------------------
â€¢ [SLOW TEST] [26.642 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:55:06.68
    May 17 08:55:06.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-runtime 05/17/23 08:55:06.681
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:55:06.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:55:06.69
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/17/23 08:55:06.696
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/17/23 08:55:24.748
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/17/23 08:55:24.75
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/17/23 08:55:24.753
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/17/23 08:55:24.753
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/17/23 08:55:24.763
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/17/23 08:55:26.77
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/17/23 08:55:28.775
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/17/23 08:55:28.778
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/17/23 08:55:28.778
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/17/23 08:55:28.79
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/17/23 08:55:29.796
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/17/23 08:55:33.304
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/17/23 08:55:33.307
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/17/23 08:55:33.307
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 17 08:55:33.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-868" for this suite. 05/17/23 08:55:33.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:55:33.323
May 17 08:55:33.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:55:33.324
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:55:33.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:55:33.331
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-daaff2c2-0670-45c1-ae73-61f65e51f90c 05/17/23 08:55:33.335
STEP: Creating secret with name s-test-opt-upd-481cf38a-9815-4608-a852-dff5c9585968 05/17/23 08:55:33.337
STEP: Creating the pod 05/17/23 08:55:33.339
May 17 08:55:33.344: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d" in namespace "projected-7599" to be "running and ready"
May 17 08:55:33.345: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31608ms
May 17 08:55:33.345: INFO: The phase of Pod pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d is Pending, waiting for it to be Running (with Ready = true)
May 17 08:55:35.349: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004775307s
May 17 08:55:35.349: INFO: The phase of Pod pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d is Pending, waiting for it to be Running (with Ready = true)
May 17 08:55:37.349: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d": Phase="Running", Reason="", readiness=true. Elapsed: 4.00455954s
May 17 08:55:37.349: INFO: The phase of Pod pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d is Running (Ready = true)
May 17 08:55:37.349: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-daaff2c2-0670-45c1-ae73-61f65e51f90c 05/17/23 08:55:37.36
STEP: Updating secret s-test-opt-upd-481cf38a-9815-4608-a852-dff5c9585968 05/17/23 08:55:37.363
STEP: Creating secret with name s-test-opt-create-a5e1b661-be25-48a2-8ecf-9e2d3c5281c0 05/17/23 08:55:37.366
STEP: waiting to observe update in volume 05/17/23 08:55:37.369
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 17 08:56:47.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7599" for this suite. 05/17/23 08:56:47.552
------------------------------
â€¢ [SLOW TEST] [74.232 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:55:33.323
    May 17 08:55:33.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:55:33.324
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:55:33.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:55:33.331
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-daaff2c2-0670-45c1-ae73-61f65e51f90c 05/17/23 08:55:33.335
    STEP: Creating secret with name s-test-opt-upd-481cf38a-9815-4608-a852-dff5c9585968 05/17/23 08:55:33.337
    STEP: Creating the pod 05/17/23 08:55:33.339
    May 17 08:55:33.344: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d" in namespace "projected-7599" to be "running and ready"
    May 17 08:55:33.345: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31608ms
    May 17 08:55:33.345: INFO: The phase of Pod pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:55:35.349: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004775307s
    May 17 08:55:35.349: INFO: The phase of Pod pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:55:37.349: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d": Phase="Running", Reason="", readiness=true. Elapsed: 4.00455954s
    May 17 08:55:37.349: INFO: The phase of Pod pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d is Running (Ready = true)
    May 17 08:55:37.349: INFO: Pod "pod-projected-secrets-b4eefe88-ec0a-4868-997d-344f9f7ade3d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-daaff2c2-0670-45c1-ae73-61f65e51f90c 05/17/23 08:55:37.36
    STEP: Updating secret s-test-opt-upd-481cf38a-9815-4608-a852-dff5c9585968 05/17/23 08:55:37.363
    STEP: Creating secret with name s-test-opt-create-a5e1b661-be25-48a2-8ecf-9e2d3c5281c0 05/17/23 08:55:37.366
    STEP: waiting to observe update in volume 05/17/23 08:55:37.369
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 17 08:56:47.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7599" for this suite. 05/17/23 08:56:47.552
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:56:47.555
May 17 08:56:47.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:56:47.556
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:56:47.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:56:47.564
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 08:56:47.565
May 17 08:56:47.570: INFO: Waiting up to 5m0s for pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37" in namespace "emptydir-7728" to be "Succeeded or Failed"
May 17 08:56:47.571: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37": Phase="Pending", Reason="", readiness=false. Elapsed: 1.357823ms
May 17 08:56:49.574: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004573336s
May 17 08:56:51.573: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003844684s
STEP: Saw pod success 05/17/23 08:56:51.573
May 17 08:56:51.574: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37" satisfied condition "Succeeded or Failed"
May 17 08:56:51.575: INFO: Trying to get logs from node k8s-node1 pod pod-73b764b3-ee3a-4a73-8e59-0f1031942f37 container test-container: <nil>
STEP: delete the pod 05/17/23 08:56:51.578
May 17 08:56:51.584: INFO: Waiting for pod pod-73b764b3-ee3a-4a73-8e59-0f1031942f37 to disappear
May 17 08:56:51.586: INFO: Pod pod-73b764b3-ee3a-4a73-8e59-0f1031942f37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:56:51.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7728" for this suite. 05/17/23 08:56:51.587
------------------------------
â€¢ [4.035 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:56:47.555
    May 17 08:56:47.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:56:47.556
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:56:47.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:56:47.564
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 08:56:47.565
    May 17 08:56:47.570: INFO: Waiting up to 5m0s for pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37" in namespace "emptydir-7728" to be "Succeeded or Failed"
    May 17 08:56:47.571: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37": Phase="Pending", Reason="", readiness=false. Elapsed: 1.357823ms
    May 17 08:56:49.574: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004573336s
    May 17 08:56:51.573: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003844684s
    STEP: Saw pod success 05/17/23 08:56:51.573
    May 17 08:56:51.574: INFO: Pod "pod-73b764b3-ee3a-4a73-8e59-0f1031942f37" satisfied condition "Succeeded or Failed"
    May 17 08:56:51.575: INFO: Trying to get logs from node k8s-node1 pod pod-73b764b3-ee3a-4a73-8e59-0f1031942f37 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:56:51.578
    May 17 08:56:51.584: INFO: Waiting for pod pod-73b764b3-ee3a-4a73-8e59-0f1031942f37 to disappear
    May 17 08:56:51.586: INFO: Pod pod-73b764b3-ee3a-4a73-8e59-0f1031942f37 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:56:51.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7728" for this suite. 05/17/23 08:56:51.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:56:51.591
May 17 08:56:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 08:56:51.591
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:56:51.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:56:51.598
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7936 05/17/23 08:56:51.599
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-7936 05/17/23 08:56:51.602
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7936 05/17/23 08:56:51.605
May 17 08:56:51.606: INFO: Found 0 stateful pods, waiting for 1
May 17 08:57:01.609: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/17/23 08:57:01.609
May 17 08:57:01.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:57:01.702: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:57:01.702: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:57:01.702: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:57:01.703: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 17 08:57:11.708: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:57:11.708: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:57:11.716: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
May 17 08:57:11.716: INFO: ss-0  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  }]
May 17 08:57:11.716: INFO: 
May 17 08:57:11.716: INFO: StatefulSet ss has not reached scale 3, at 1
May 17 08:57:12.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998375448s
May 17 08:57:13.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995483017s
May 17 08:57:14.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992831977s
May 17 08:57:15.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990255226s
May 17 08:57:16.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.987608656s
May 17 08:57:17.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.984171735s
May 17 08:57:18.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981500288s
May 17 08:57:19.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.978938982s
May 17 08:57:20.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.631899ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7936 05/17/23 08:57:21.741
May 17 08:57:21.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 08:57:21.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 08:57:21.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 08:57:21.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 08:57:21.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 08:57:21.928: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 17 08:57:21.928: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 08:57:21.928: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 08:57:21.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 08:57:22.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 17 08:57:22.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 08:57:22.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 08:57:22.026: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 08:57:22.026: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 08:57:22.026: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/17/23 08:57:22.026
May 17 08:57:22.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:57:22.132: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:57:22.132: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:57:22.132: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:57:22.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:57:22.224: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:57:22.224: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:57:22.224: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:57:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 08:57:22.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 08:57:22.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 08:57:22.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 08:57:22.317: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:57:22.319: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 17 08:57:32.323: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:57:32.323: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:57:32.323: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 17 08:57:32.329: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
May 17 08:57:32.329: INFO: ss-0  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  }]
May 17 08:57:32.329: INFO: ss-1  k8s-node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  }]
May 17 08:57:32.329: INFO: ss-2  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  }]
May 17 08:57:32.329: INFO: 
May 17 08:57:32.329: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 08:57:33.331: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.99834658s
May 17 08:57:34.333: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.996306362s
May 17 08:57:35.335: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.994250292s
May 17 08:57:36.338: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.991379726s
May 17 08:57:37.340: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.989785491s
May 17 08:57:38.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.987538438s
May 17 08:57:39.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.985661837s
May 17 08:57:40.346: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.983401618s
May 17 08:57:41.349: INFO: Verifying statefulset ss doesn't scale past 0 for another 980.86331ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7936 05/17/23 08:57:42.349
May 17 08:57:42.351: INFO: Scaling statefulset ss to 0
May 17 08:57:42.356: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 08:57:42.357: INFO: Deleting all statefulset in ns statefulset-7936
May 17 08:57:42.358: INFO: Scaling statefulset ss to 0
May 17 08:57:42.362: INFO: Waiting for statefulset status.replicas updated to 0
May 17 08:57:42.363: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 08:57:42.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7936" for this suite. 05/17/23 08:57:42.37
------------------------------
â€¢ [SLOW TEST] [50.782 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:56:51.591
    May 17 08:56:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 08:56:51.591
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:56:51.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:56:51.598
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7936 05/17/23 08:56:51.599
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-7936 05/17/23 08:56:51.602
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7936 05/17/23 08:56:51.605
    May 17 08:56:51.606: INFO: Found 0 stateful pods, waiting for 1
    May 17 08:57:01.609: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/17/23 08:57:01.609
    May 17 08:57:01.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:57:01.702: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:57:01.702: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:57:01.702: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:57:01.703: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May 17 08:57:11.708: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:57:11.708: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:57:11.716: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
    May 17 08:57:11.716: INFO: ss-0  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  }]
    May 17 08:57:11.716: INFO: 
    May 17 08:57:11.716: INFO: StatefulSet ss has not reached scale 3, at 1
    May 17 08:57:12.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998375448s
    May 17 08:57:13.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995483017s
    May 17 08:57:14.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992831977s
    May 17 08:57:15.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990255226s
    May 17 08:57:16.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.987608656s
    May 17 08:57:17.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.984171735s
    May 17 08:57:18.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981500288s
    May 17 08:57:19.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.978938982s
    May 17 08:57:20.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.631899ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7936 05/17/23 08:57:21.741
    May 17 08:57:21.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 08:57:21.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 08:57:21.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 08:57:21.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 08:57:21.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 08:57:21.928: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May 17 08:57:21.928: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 08:57:21.928: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 08:57:21.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 08:57:22.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May 17 08:57:22.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 08:57:22.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 08:57:22.026: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 08:57:22.026: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 08:57:22.026: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/17/23 08:57:22.026
    May 17 08:57:22.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:57:22.132: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:57:22.132: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:57:22.132: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:57:22.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:57:22.224: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:57:22.224: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:57:22.224: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:57:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=statefulset-7936 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 08:57:22.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 08:57:22.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 08:57:22.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 08:57:22.317: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:57:22.319: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May 17 08:57:32.323: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:57:32.323: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:57:32.323: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May 17 08:57:32.329: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
    May 17 08:57:32.329: INFO: ss-0  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:56:51 +0000 UTC  }]
    May 17 08:57:32.329: INFO: ss-1  k8s-node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  }]
    May 17 08:57:32.329: INFO: ss-2  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 08:57:11 +0000 UTC  }]
    May 17 08:57:32.329: INFO: 
    May 17 08:57:32.329: INFO: StatefulSet ss has not reached scale 0, at 3
    May 17 08:57:33.331: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.99834658s
    May 17 08:57:34.333: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.996306362s
    May 17 08:57:35.335: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.994250292s
    May 17 08:57:36.338: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.991379726s
    May 17 08:57:37.340: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.989785491s
    May 17 08:57:38.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.987538438s
    May 17 08:57:39.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.985661837s
    May 17 08:57:40.346: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.983401618s
    May 17 08:57:41.349: INFO: Verifying statefulset ss doesn't scale past 0 for another 980.86331ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7936 05/17/23 08:57:42.349
    May 17 08:57:42.351: INFO: Scaling statefulset ss to 0
    May 17 08:57:42.356: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 08:57:42.357: INFO: Deleting all statefulset in ns statefulset-7936
    May 17 08:57:42.358: INFO: Scaling statefulset ss to 0
    May 17 08:57:42.362: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 08:57:42.363: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:57:42.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7936" for this suite. 05/17/23 08:57:42.37
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:57:42.373
May 17 08:57:42.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename certificates 05/17/23 08:57:42.373
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:42.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:42.38
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/17/23 08:57:42.972
STEP: getting /apis/certificates.k8s.io 05/17/23 08:57:42.974
STEP: getting /apis/certificates.k8s.io/v1 05/17/23 08:57:42.975
STEP: creating 05/17/23 08:57:42.976
STEP: getting 05/17/23 08:57:42.984
STEP: listing 05/17/23 08:57:42.985
STEP: watching 05/17/23 08:57:42.989
May 17 08:57:42.989: INFO: starting watch
STEP: patching 05/17/23 08:57:42.99
STEP: updating 05/17/23 08:57:42.992
May 17 08:57:42.996: INFO: waiting for watch events with expected annotations
May 17 08:57:42.996: INFO: saw patched and updated annotations
STEP: getting /approval 05/17/23 08:57:42.996
STEP: patching /approval 05/17/23 08:57:42.997
STEP: updating /approval 05/17/23 08:57:43
STEP: getting /status 05/17/23 08:57:43.004
STEP: patching /status 05/17/23 08:57:43.005
STEP: updating /status 05/17/23 08:57:43.01
STEP: deleting 05/17/23 08:57:43.014
STEP: deleting a collection 05/17/23 08:57:43.019
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:57:43.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-6422" for this suite. 05/17/23 08:57:43.026
------------------------------
â€¢ [0.656 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:57:42.373
    May 17 08:57:42.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename certificates 05/17/23 08:57:42.373
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:42.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:42.38
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/17/23 08:57:42.972
    STEP: getting /apis/certificates.k8s.io 05/17/23 08:57:42.974
    STEP: getting /apis/certificates.k8s.io/v1 05/17/23 08:57:42.975
    STEP: creating 05/17/23 08:57:42.976
    STEP: getting 05/17/23 08:57:42.984
    STEP: listing 05/17/23 08:57:42.985
    STEP: watching 05/17/23 08:57:42.989
    May 17 08:57:42.989: INFO: starting watch
    STEP: patching 05/17/23 08:57:42.99
    STEP: updating 05/17/23 08:57:42.992
    May 17 08:57:42.996: INFO: waiting for watch events with expected annotations
    May 17 08:57:42.996: INFO: saw patched and updated annotations
    STEP: getting /approval 05/17/23 08:57:42.996
    STEP: patching /approval 05/17/23 08:57:42.997
    STEP: updating /approval 05/17/23 08:57:43
    STEP: getting /status 05/17/23 08:57:43.004
    STEP: patching /status 05/17/23 08:57:43.005
    STEP: updating /status 05/17/23 08:57:43.01
    STEP: deleting 05/17/23 08:57:43.014
    STEP: deleting a collection 05/17/23 08:57:43.019
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:57:43.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-6422" for this suite. 05/17/23 08:57:43.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:57:43.029
May 17 08:57:43.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename init-container 05/17/23 08:57:43.03
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:43.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:43.037
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 05/17/23 08:57:43.038
May 17 08:57:43.038: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 17 08:57:47.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8693" for this suite. 05/17/23 08:57:47.949
------------------------------
â€¢ [4.923 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:57:43.029
    May 17 08:57:43.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename init-container 05/17/23 08:57:43.03
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:43.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:43.037
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 05/17/23 08:57:43.038
    May 17 08:57:43.038: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 17 08:57:47.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8693" for this suite. 05/17/23 08:57:47.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:57:47.953
May 17 08:57:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename runtimeclass 05/17/23 08:57:47.954
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:47.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:47.962
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 17 08:57:47.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4592" for this suite. 05/17/23 08:57:47.968
------------------------------
â€¢ [0.017 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:57:47.953
    May 17 08:57:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 08:57:47.954
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:47.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:47.962
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 17 08:57:47.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4592" for this suite. 05/17/23 08:57:47.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:57:47.972
May 17 08:57:47.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replicaset 05/17/23 08:57:47.972
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:47.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:47.98
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/17/23 08:57:47.981
May 17 08:57:47.985: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4647" to be "running and ready"
May 17 08:57:47.987: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.212948ms
May 17 08:57:47.987: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 17 08:57:49.989: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.003587365s
May 17 08:57:49.989: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May 17 08:57:49.989: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/17/23 08:57:49.99
STEP: Then the orphan pod is adopted 05/17/23 08:57:49.993
STEP: When the matched label of one of its pods change 05/17/23 08:57:50.997
May 17 08:57:50.998: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/17/23 08:57:51.004
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 17 08:57:52.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4647" for this suite. 05/17/23 08:57:52.01
------------------------------
â€¢ [4.041 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:57:47.972
    May 17 08:57:47.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replicaset 05/17/23 08:57:47.972
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:47.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:47.98
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/17/23 08:57:47.981
    May 17 08:57:47.985: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4647" to be "running and ready"
    May 17 08:57:47.987: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.212948ms
    May 17 08:57:47.987: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May 17 08:57:49.989: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.003587365s
    May 17 08:57:49.989: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May 17 08:57:49.989: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/17/23 08:57:49.99
    STEP: Then the orphan pod is adopted 05/17/23 08:57:49.993
    STEP: When the matched label of one of its pods change 05/17/23 08:57:50.997
    May 17 08:57:50.998: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/17/23 08:57:51.004
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:57:52.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4647" for this suite. 05/17/23 08:57:52.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:57:52.013
May 17 08:57:52.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:57:52.014
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:52.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:52.022
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 05/17/23 08:57:52.024
May 17 08:57:52.028: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850" in namespace "emptydir-6838" to be "running"
May 17 08:57:52.029: INFO: Pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850": Phase="Pending", Reason="", readiness=false. Elapsed: 1.261841ms
May 17 08:57:54.033: INFO: Pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850": Phase="Running", Reason="", readiness=false. Elapsed: 2.0044768s
May 17 08:57:54.033: INFO: Pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/17/23 08:57:54.033
May 17 08:57:54.033: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6838 PodName:pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 08:57:54.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:57:54.033: INFO: ExecWithOptions: Clientset creation
May 17 08:57:54.033: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-6838/pods/pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May 17 08:57:54.078: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:57:54.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6838" for this suite. 05/17/23 08:57:54.08
------------------------------
â€¢ [2.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:57:52.013
    May 17 08:57:52.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:57:52.014
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:52.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:52.022
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 05/17/23 08:57:52.024
    May 17 08:57:52.028: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850" in namespace "emptydir-6838" to be "running"
    May 17 08:57:52.029: INFO: Pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850": Phase="Pending", Reason="", readiness=false. Elapsed: 1.261841ms
    May 17 08:57:54.033: INFO: Pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850": Phase="Running", Reason="", readiness=false. Elapsed: 2.0044768s
    May 17 08:57:54.033: INFO: Pod "pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/17/23 08:57:54.033
    May 17 08:57:54.033: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6838 PodName:pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 08:57:54.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:57:54.033: INFO: ExecWithOptions: Clientset creation
    May 17 08:57:54.033: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-6838/pods/pod-sharedvolume-8ba163dc-34c1-47b9-84de-5ffd72f22850/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May 17 08:57:54.078: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:57:54.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6838" for this suite. 05/17/23 08:57:54.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:57:54.083
May 17 08:57:54.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:57:54.083
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:54.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:54.092
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/17/23 08:57:54.093
May 17 08:57:54.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
May 17 08:57:55.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:58:00.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8450" for this suite. 05/17/23 08:58:00.765
------------------------------
â€¢ [SLOW TEST] [6.687 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:57:54.083
    May 17 08:57:54.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 08:57:54.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:57:54.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:57:54.092
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/17/23 08:57:54.093
    May 17 08:57:54.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    May 17 08:57:55.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:00.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8450" for this suite. 05/17/23 08:58:00.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:00.77
May 17 08:58:00.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:58:00.77
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:00.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:00.779
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:58:00.78
May 17 08:58:00.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59" in namespace "downward-api-2413" to be "Succeeded or Failed"
May 17 08:58:00.786: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275434ms
May 17 08:58:02.789: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00401182s
May 17 08:58:04.789: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004786973s
STEP: Saw pod success 05/17/23 08:58:04.79
May 17 08:58:04.790: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59" satisfied condition "Succeeded or Failed"
May 17 08:58:04.791: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59 container client-container: <nil>
STEP: delete the pod 05/17/23 08:58:04.795
May 17 08:58:04.802: INFO: Waiting for pod downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59 to disappear
May 17 08:58:04.804: INFO: Pod downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:58:04.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2413" for this suite. 05/17/23 08:58:04.805
------------------------------
â€¢ [4.038 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:00.77
    May 17 08:58:00.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:58:00.77
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:00.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:00.779
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:58:00.78
    May 17 08:58:00.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59" in namespace "downward-api-2413" to be "Succeeded or Failed"
    May 17 08:58:00.786: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275434ms
    May 17 08:58:02.789: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00401182s
    May 17 08:58:04.789: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004786973s
    STEP: Saw pod success 05/17/23 08:58:04.79
    May 17 08:58:04.790: INFO: Pod "downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59" satisfied condition "Succeeded or Failed"
    May 17 08:58:04.791: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:58:04.795
    May 17 08:58:04.802: INFO: Waiting for pod downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59 to disappear
    May 17 08:58:04.804: INFO: Pod downwardapi-volume-e4f60f47-4f4c-4c68-a451-4c75d2235c59 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:04.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2413" for this suite. 05/17/23 08:58:04.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:04.809
May 17 08:58:04.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename resourcequota 05/17/23 08:58:04.809
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:04.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:04.817
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 05/17/23 08:58:04.819
STEP: Counting existing ResourceQuota 05/17/23 08:58:09.82
STEP: Creating a ResourceQuota 05/17/23 08:58:14.824
STEP: Ensuring resource quota status is calculated 05/17/23 08:58:14.827
STEP: Creating a Secret 05/17/23 08:58:16.83
STEP: Ensuring resource quota status captures secret creation 05/17/23 08:58:16.837
STEP: Deleting a secret 05/17/23 08:58:18.84
STEP: Ensuring resource quota status released usage 05/17/23 08:58:18.843
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 17 08:58:20.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9564" for this suite. 05/17/23 08:58:20.847
------------------------------
â€¢ [SLOW TEST] [16.041 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:04.809
    May 17 08:58:04.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename resourcequota 05/17/23 08:58:04.809
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:04.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:04.817
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 05/17/23 08:58:04.819
    STEP: Counting existing ResourceQuota 05/17/23 08:58:09.82
    STEP: Creating a ResourceQuota 05/17/23 08:58:14.824
    STEP: Ensuring resource quota status is calculated 05/17/23 08:58:14.827
    STEP: Creating a Secret 05/17/23 08:58:16.83
    STEP: Ensuring resource quota status captures secret creation 05/17/23 08:58:16.837
    STEP: Deleting a secret 05/17/23 08:58:18.84
    STEP: Ensuring resource quota status released usage 05/17/23 08:58:18.843
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:20.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9564" for this suite. 05/17/23 08:58:20.847
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:20.85
May 17 08:58:20.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename endpointslicemirroring 05/17/23 08:58:20.851
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:20.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:20.859
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/17/23 08:58:20.866
May 17 08:58:20.869: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/17/23 08:58:22.871
May 17 08:58:22.876: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 05/17/23 08:58:24.878
May 17 08:58:24.882: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
May 17 08:58:26.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6234" for this suite. 05/17/23 08:58:26.886
------------------------------
â€¢ [SLOW TEST] [6.039 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:20.85
    May 17 08:58:20.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename endpointslicemirroring 05/17/23 08:58:20.851
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:20.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:20.859
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/17/23 08:58:20.866
    May 17 08:58:20.869: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/17/23 08:58:22.871
    May 17 08:58:22.876: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 05/17/23 08:58:24.878
    May 17 08:58:24.882: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:26.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6234" for this suite. 05/17/23 08:58:26.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:26.89
May 17 08:58:26.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 08:58:26.89
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:26.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:26.9
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/17/23 08:58:26.902
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/17/23 08:58:26.902
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 08:58:26.902
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/17/23 08:58:26.902
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/17/23 08:58:26.903
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 08:58:26.903
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 08:58:26.904
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:58:26.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5142" for this suite. 05/17/23 08:58:26.905
------------------------------
â€¢ [0.018 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:26.89
    May 17 08:58:26.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 08:58:26.89
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:26.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:26.9
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/17/23 08:58:26.902
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/17/23 08:58:26.902
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 08:58:26.902
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/17/23 08:58:26.902
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/17/23 08:58:26.903
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 08:58:26.903
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 08:58:26.904
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:26.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5142" for this suite. 05/17/23 08:58:26.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:26.909
May 17 08:58:26.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 08:58:26.91
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:26.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:26.916
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-c6b578c9-b845-4e10-8ece-fbe1e43de221 05/17/23 08:58:26.918
STEP: Creating a pod to test consume configMaps 05/17/23 08:58:26.92
May 17 08:58:26.923: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90" in namespace "configmap-8099" to be "Succeeded or Failed"
May 17 08:58:26.925: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19039ms
May 17 08:58:28.927: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003316762s
May 17 08:58:30.927: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003269253s
STEP: Saw pod success 05/17/23 08:58:30.927
May 17 08:58:30.927: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90" satisfied condition "Succeeded or Failed"
May 17 08:58:30.928: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-e6474803-34f1-4863-8719-097723d51b90 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 08:58:30.932
May 17 08:58:30.937: INFO: Waiting for pod pod-configmaps-e6474803-34f1-4863-8719-097723d51b90 to disappear
May 17 08:58:30.939: INFO: Pod pod-configmaps-e6474803-34f1-4863-8719-097723d51b90 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 08:58:30.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8099" for this suite. 05/17/23 08:58:30.94
------------------------------
â€¢ [4.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:26.909
    May 17 08:58:26.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 08:58:26.91
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:26.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:26.916
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-c6b578c9-b845-4e10-8ece-fbe1e43de221 05/17/23 08:58:26.918
    STEP: Creating a pod to test consume configMaps 05/17/23 08:58:26.92
    May 17 08:58:26.923: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90" in namespace "configmap-8099" to be "Succeeded or Failed"
    May 17 08:58:26.925: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19039ms
    May 17 08:58:28.927: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003316762s
    May 17 08:58:30.927: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003269253s
    STEP: Saw pod success 05/17/23 08:58:30.927
    May 17 08:58:30.927: INFO: Pod "pod-configmaps-e6474803-34f1-4863-8719-097723d51b90" satisfied condition "Succeeded or Failed"
    May 17 08:58:30.928: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-e6474803-34f1-4863-8719-097723d51b90 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 08:58:30.932
    May 17 08:58:30.937: INFO: Waiting for pod pod-configmaps-e6474803-34f1-4863-8719-097723d51b90 to disappear
    May 17 08:58:30.939: INFO: Pod pod-configmaps-e6474803-34f1-4863-8719-097723d51b90 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:30.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8099" for this suite. 05/17/23 08:58:30.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:30.944
May 17 08:58:30.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename discovery 05/17/23 08:58:30.945
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:30.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:30.953
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/17/23 08:58:30.955
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May 17 08:58:31.489: INFO: Checking APIGroup: apiregistration.k8s.io
May 17 08:58:31.490: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 17 08:58:31.490: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May 17 08:58:31.490: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 17 08:58:31.490: INFO: Checking APIGroup: apps
May 17 08:58:31.491: INFO: PreferredVersion.GroupVersion: apps/v1
May 17 08:58:31.491: INFO: Versions found [{apps/v1 v1}]
May 17 08:58:31.491: INFO: apps/v1 matches apps/v1
May 17 08:58:31.491: INFO: Checking APIGroup: events.k8s.io
May 17 08:58:31.491: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 17 08:58:31.491: INFO: Versions found [{events.k8s.io/v1 v1}]
May 17 08:58:31.491: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 17 08:58:31.491: INFO: Checking APIGroup: authentication.k8s.io
May 17 08:58:31.492: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 17 08:58:31.492: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May 17 08:58:31.492: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 17 08:58:31.492: INFO: Checking APIGroup: authorization.k8s.io
May 17 08:58:31.492: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 17 08:58:31.492: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May 17 08:58:31.492: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 17 08:58:31.493: INFO: Checking APIGroup: autoscaling
May 17 08:58:31.493: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May 17 08:58:31.493: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
May 17 08:58:31.493: INFO: autoscaling/v2 matches autoscaling/v2
May 17 08:58:31.493: INFO: Checking APIGroup: batch
May 17 08:58:31.493: INFO: PreferredVersion.GroupVersion: batch/v1
May 17 08:58:31.494: INFO: Versions found [{batch/v1 v1}]
May 17 08:58:31.494: INFO: batch/v1 matches batch/v1
May 17 08:58:31.494: INFO: Checking APIGroup: certificates.k8s.io
May 17 08:58:31.494: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 17 08:58:31.494: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May 17 08:58:31.494: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 17 08:58:31.494: INFO: Checking APIGroup: networking.k8s.io
May 17 08:58:31.495: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 17 08:58:31.495: INFO: Versions found [{networking.k8s.io/v1 v1}]
May 17 08:58:31.495: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 17 08:58:31.495: INFO: Checking APIGroup: policy
May 17 08:58:31.495: INFO: PreferredVersion.GroupVersion: policy/v1
May 17 08:58:31.495: INFO: Versions found [{policy/v1 v1}]
May 17 08:58:31.495: INFO: policy/v1 matches policy/v1
May 17 08:58:31.495: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 17 08:58:31.496: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 17 08:58:31.496: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May 17 08:58:31.496: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 17 08:58:31.496: INFO: Checking APIGroup: storage.k8s.io
May 17 08:58:31.496: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 17 08:58:31.496: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 17 08:58:31.496: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 17 08:58:31.496: INFO: Checking APIGroup: admissionregistration.k8s.io
May 17 08:58:31.497: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 17 08:58:31.497: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May 17 08:58:31.497: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 17 08:58:31.497: INFO: Checking APIGroup: apiextensions.k8s.io
May 17 08:58:31.497: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 17 08:58:31.497: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May 17 08:58:31.497: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 17 08:58:31.497: INFO: Checking APIGroup: scheduling.k8s.io
May 17 08:58:31.498: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 17 08:58:31.498: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May 17 08:58:31.498: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 17 08:58:31.498: INFO: Checking APIGroup: coordination.k8s.io
May 17 08:58:31.498: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 17 08:58:31.498: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May 17 08:58:31.498: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 17 08:58:31.498: INFO: Checking APIGroup: node.k8s.io
May 17 08:58:31.499: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 17 08:58:31.499: INFO: Versions found [{node.k8s.io/v1 v1}]
May 17 08:58:31.499: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 17 08:58:31.499: INFO: Checking APIGroup: discovery.k8s.io
May 17 08:58:31.499: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 17 08:58:31.499: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May 17 08:58:31.499: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 17 08:58:31.499: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 17 08:58:31.500: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
May 17 08:58:31.500: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
May 17 08:58:31.500: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
May 17 08:58:31.500: INFO: Checking APIGroup: crd.projectcalico.org
May 17 08:58:31.501: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May 17 08:58:31.501: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May 17 08:58:31.501: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
May 17 08:58:31.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-2179" for this suite. 05/17/23 08:58:31.503
------------------------------
â€¢ [0.561 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:30.944
    May 17 08:58:30.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename discovery 05/17/23 08:58:30.945
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:30.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:30.953
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/17/23 08:58:30.955
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May 17 08:58:31.489: INFO: Checking APIGroup: apiregistration.k8s.io
    May 17 08:58:31.490: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May 17 08:58:31.490: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May 17 08:58:31.490: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May 17 08:58:31.490: INFO: Checking APIGroup: apps
    May 17 08:58:31.491: INFO: PreferredVersion.GroupVersion: apps/v1
    May 17 08:58:31.491: INFO: Versions found [{apps/v1 v1}]
    May 17 08:58:31.491: INFO: apps/v1 matches apps/v1
    May 17 08:58:31.491: INFO: Checking APIGroup: events.k8s.io
    May 17 08:58:31.491: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May 17 08:58:31.491: INFO: Versions found [{events.k8s.io/v1 v1}]
    May 17 08:58:31.491: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May 17 08:58:31.491: INFO: Checking APIGroup: authentication.k8s.io
    May 17 08:58:31.492: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May 17 08:58:31.492: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May 17 08:58:31.492: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May 17 08:58:31.492: INFO: Checking APIGroup: authorization.k8s.io
    May 17 08:58:31.492: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May 17 08:58:31.492: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May 17 08:58:31.492: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May 17 08:58:31.493: INFO: Checking APIGroup: autoscaling
    May 17 08:58:31.493: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May 17 08:58:31.493: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    May 17 08:58:31.493: INFO: autoscaling/v2 matches autoscaling/v2
    May 17 08:58:31.493: INFO: Checking APIGroup: batch
    May 17 08:58:31.493: INFO: PreferredVersion.GroupVersion: batch/v1
    May 17 08:58:31.494: INFO: Versions found [{batch/v1 v1}]
    May 17 08:58:31.494: INFO: batch/v1 matches batch/v1
    May 17 08:58:31.494: INFO: Checking APIGroup: certificates.k8s.io
    May 17 08:58:31.494: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May 17 08:58:31.494: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May 17 08:58:31.494: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May 17 08:58:31.494: INFO: Checking APIGroup: networking.k8s.io
    May 17 08:58:31.495: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May 17 08:58:31.495: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May 17 08:58:31.495: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May 17 08:58:31.495: INFO: Checking APIGroup: policy
    May 17 08:58:31.495: INFO: PreferredVersion.GroupVersion: policy/v1
    May 17 08:58:31.495: INFO: Versions found [{policy/v1 v1}]
    May 17 08:58:31.495: INFO: policy/v1 matches policy/v1
    May 17 08:58:31.495: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May 17 08:58:31.496: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May 17 08:58:31.496: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May 17 08:58:31.496: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May 17 08:58:31.496: INFO: Checking APIGroup: storage.k8s.io
    May 17 08:58:31.496: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May 17 08:58:31.496: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May 17 08:58:31.496: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May 17 08:58:31.496: INFO: Checking APIGroup: admissionregistration.k8s.io
    May 17 08:58:31.497: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May 17 08:58:31.497: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May 17 08:58:31.497: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May 17 08:58:31.497: INFO: Checking APIGroup: apiextensions.k8s.io
    May 17 08:58:31.497: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May 17 08:58:31.497: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May 17 08:58:31.497: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May 17 08:58:31.497: INFO: Checking APIGroup: scheduling.k8s.io
    May 17 08:58:31.498: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May 17 08:58:31.498: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May 17 08:58:31.498: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May 17 08:58:31.498: INFO: Checking APIGroup: coordination.k8s.io
    May 17 08:58:31.498: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May 17 08:58:31.498: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May 17 08:58:31.498: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May 17 08:58:31.498: INFO: Checking APIGroup: node.k8s.io
    May 17 08:58:31.499: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May 17 08:58:31.499: INFO: Versions found [{node.k8s.io/v1 v1}]
    May 17 08:58:31.499: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May 17 08:58:31.499: INFO: Checking APIGroup: discovery.k8s.io
    May 17 08:58:31.499: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May 17 08:58:31.499: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May 17 08:58:31.499: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May 17 08:58:31.499: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May 17 08:58:31.500: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    May 17 08:58:31.500: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    May 17 08:58:31.500: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    May 17 08:58:31.500: INFO: Checking APIGroup: crd.projectcalico.org
    May 17 08:58:31.501: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    May 17 08:58:31.501: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    May 17 08:58:31.501: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:31.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-2179" for this suite. 05/17/23 08:58:31.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:31.506
May 17 08:58:31.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename statefulset 05/17/23 08:58:31.507
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:31.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:31.515
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8020 05/17/23 08:58:31.516
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
May 17 08:58:31.523: INFO: Found 0 stateful pods, waiting for 1
May 17 08:58:41.526: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/17/23 08:58:41.529
W0517 08:58:41.533932      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 17 08:58:41.537: INFO: Found 1 stateful pods, waiting for 2
May 17 08:58:51.540: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 08:58:51.540: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/17/23 08:58:51.544
STEP: Delete all of the StatefulSets 05/17/23 08:58:51.545
STEP: Verify that StatefulSets have been deleted 05/17/23 08:58:51.548
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 17 08:58:51.550: INFO: Deleting all statefulset in ns statefulset-8020
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 17 08:58:51.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8020" for this suite. 05/17/23 08:58:51.555
------------------------------
â€¢ [SLOW TEST] [20.053 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:31.506
    May 17 08:58:31.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename statefulset 05/17/23 08:58:31.507
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:31.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:31.515
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8020 05/17/23 08:58:31.516
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    May 17 08:58:31.523: INFO: Found 0 stateful pods, waiting for 1
    May 17 08:58:41.526: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/17/23 08:58:41.529
    W0517 08:58:41.533932      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 17 08:58:41.537: INFO: Found 1 stateful pods, waiting for 2
    May 17 08:58:51.540: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 08:58:51.540: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/17/23 08:58:51.544
    STEP: Delete all of the StatefulSets 05/17/23 08:58:51.545
    STEP: Verify that StatefulSets have been deleted 05/17/23 08:58:51.548
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 17 08:58:51.550: INFO: Deleting all statefulset in ns statefulset-8020
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:51.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8020" for this suite. 05/17/23 08:58:51.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:51.561
May 17 08:58:51.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename gc 05/17/23 08:58:51.562
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:51.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:51.57
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May 17 08:58:51.588: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"03418481-839a-41e1-a48b-8564a6b4d2ca", Controller:(*bool)(0xc004a5016a), BlockOwnerDeletion:(*bool)(0xc004a5016b)}}
May 17 08:58:51.591: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f364db44-e9c2-4a1f-ad07-abab17390438", Controller:(*bool)(0xc004a50392), BlockOwnerDeletion:(*bool)(0xc004a50393)}}
May 17 08:58:51.594: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1cdce833-0995-4cc6-93ba-7c3542a12639", Controller:(*bool)(0xc004a505c2), BlockOwnerDeletion:(*bool)(0xc004a505c3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 17 08:58:56.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5535" for this suite. 05/17/23 08:58:56.605
------------------------------
â€¢ [SLOW TEST] [5.046 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:51.561
    May 17 08:58:51.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename gc 05/17/23 08:58:51.562
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:51.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:51.57
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May 17 08:58:51.588: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"03418481-839a-41e1-a48b-8564a6b4d2ca", Controller:(*bool)(0xc004a5016a), BlockOwnerDeletion:(*bool)(0xc004a5016b)}}
    May 17 08:58:51.591: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f364db44-e9c2-4a1f-ad07-abab17390438", Controller:(*bool)(0xc004a50392), BlockOwnerDeletion:(*bool)(0xc004a50393)}}
    May 17 08:58:51.594: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1cdce833-0995-4cc6-93ba-7c3542a12639", Controller:(*bool)(0xc004a505c2), BlockOwnerDeletion:(*bool)(0xc004a505c3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 17 08:58:56.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5535" for this suite. 05/17/23 08:58:56.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:58:56.608
May 17 08:58:56.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 08:58:56.608
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:56.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:56.616
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 05/17/23 08:58:56.618
May 17 08:58:56.622: INFO: Waiting up to 5m0s for pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723" in namespace "var-expansion-2542" to be "Succeeded or Failed"
May 17 08:58:56.624: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723": Phase="Pending", Reason="", readiness=false. Elapsed: 1.545987ms
May 17 08:58:58.627: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004653946s
May 17 08:59:00.627: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004392055s
STEP: Saw pod success 05/17/23 08:59:00.627
May 17 08:59:00.627: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723" satisfied condition "Succeeded or Failed"
May 17 08:59:00.628: INFO: Trying to get logs from node k8s-node1 pod var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723 container dapi-container: <nil>
STEP: delete the pod 05/17/23 08:59:00.632
May 17 08:59:00.639: INFO: Waiting for pod var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723 to disappear
May 17 08:59:00.641: INFO: Pod var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 08:59:00.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2542" for this suite. 05/17/23 08:59:00.642
------------------------------
â€¢ [4.038 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:58:56.608
    May 17 08:58:56.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 08:58:56.608
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:58:56.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:58:56.616
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 05/17/23 08:58:56.618
    May 17 08:58:56.622: INFO: Waiting up to 5m0s for pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723" in namespace "var-expansion-2542" to be "Succeeded or Failed"
    May 17 08:58:56.624: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723": Phase="Pending", Reason="", readiness=false. Elapsed: 1.545987ms
    May 17 08:58:58.627: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004653946s
    May 17 08:59:00.627: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004392055s
    STEP: Saw pod success 05/17/23 08:59:00.627
    May 17 08:59:00.627: INFO: Pod "var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723" satisfied condition "Succeeded or Failed"
    May 17 08:59:00.628: INFO: Trying to get logs from node k8s-node1 pod var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 08:59:00.632
    May 17 08:59:00.639: INFO: Waiting for pod var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723 to disappear
    May 17 08:59:00.641: INFO: Pod var-expansion-09deaa36-1c00-4d43-a0d3-7fd8de824723 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:00.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2542" for this suite. 05/17/23 08:59:00.642
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:00.646
May 17 08:59:00.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename runtimeclass 05/17/23 08:59:00.646
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:00.656
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7653-delete-me 05/17/23 08:59:00.66
STEP: Waiting for the RuntimeClass to disappear 05/17/23 08:59:00.662
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 17 08:59:00.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7653" for this suite. 05/17/23 08:59:00.667
------------------------------
â€¢ [0.024 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:00.646
    May 17 08:59:00.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 08:59:00.646
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:00.656
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7653-delete-me 05/17/23 08:59:00.66
    STEP: Waiting for the RuntimeClass to disappear 05/17/23 08:59:00.662
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:00.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7653" for this suite. 05/17/23 08:59:00.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:00.671
May 17 08:59:00.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:59:00.672
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:00.688
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
May 17 08:59:00.698: INFO: created pod pod-service-account-defaultsa
May 17 08:59:00.698: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 17 08:59:00.700: INFO: created pod pod-service-account-mountsa
May 17 08:59:00.700: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 17 08:59:00.704: INFO: created pod pod-service-account-nomountsa
May 17 08:59:00.704: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 17 08:59:00.707: INFO: created pod pod-service-account-defaultsa-mountspec
May 17 08:59:00.707: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 17 08:59:00.710: INFO: created pod pod-service-account-mountsa-mountspec
May 17 08:59:00.710: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 17 08:59:00.713: INFO: created pod pod-service-account-nomountsa-mountspec
May 17 08:59:00.713: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 17 08:59:00.716: INFO: created pod pod-service-account-defaultsa-nomountspec
May 17 08:59:00.716: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 17 08:59:00.720: INFO: created pod pod-service-account-mountsa-nomountspec
May 17 08:59:00.720: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 17 08:59:00.723: INFO: created pod pod-service-account-nomountsa-nomountspec
May 17 08:59:00.723: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 17 08:59:00.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6534" for this suite. 05/17/23 08:59:00.726
------------------------------
â€¢ [0.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:00.671
    May 17 08:59:00.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 08:59:00.672
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:00.688
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    May 17 08:59:00.698: INFO: created pod pod-service-account-defaultsa
    May 17 08:59:00.698: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May 17 08:59:00.700: INFO: created pod pod-service-account-mountsa
    May 17 08:59:00.700: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May 17 08:59:00.704: INFO: created pod pod-service-account-nomountsa
    May 17 08:59:00.704: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May 17 08:59:00.707: INFO: created pod pod-service-account-defaultsa-mountspec
    May 17 08:59:00.707: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May 17 08:59:00.710: INFO: created pod pod-service-account-mountsa-mountspec
    May 17 08:59:00.710: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May 17 08:59:00.713: INFO: created pod pod-service-account-nomountsa-mountspec
    May 17 08:59:00.713: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May 17 08:59:00.716: INFO: created pod pod-service-account-defaultsa-nomountspec
    May 17 08:59:00.716: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May 17 08:59:00.720: INFO: created pod pod-service-account-mountsa-nomountspec
    May 17 08:59:00.720: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May 17 08:59:00.723: INFO: created pod pod-service-account-nomountsa-nomountspec
    May 17 08:59:00.723: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:00.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6534" for this suite. 05/17/23 08:59:00.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:00.732
May 17 08:59:00.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename namespaces 05/17/23 08:59:00.733
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:00.741
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 05/17/23 08:59:00.743
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.748
STEP: Creating a service in the namespace 05/17/23 08:59:00.749
STEP: Deleting the namespace 05/17/23 08:59:00.753
STEP: Waiting for the namespace to be removed. 05/17/23 08:59:00.757
STEP: Recreating the namespace 05/17/23 08:59:06.76
STEP: Verifying there is no service in the namespace 05/17/23 08:59:06.766
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:59:06.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-105" for this suite. 05/17/23 08:59:06.77
STEP: Destroying namespace "nsdeletetest-180" for this suite. 05/17/23 08:59:06.773
May 17 08:59:06.774: INFO: Namespace nsdeletetest-180 was already deleted
STEP: Destroying namespace "nsdeletetest-5957" for this suite. 05/17/23 08:59:06.774
------------------------------
â€¢ [SLOW TEST] [6.045 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:00.732
    May 17 08:59:00.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename namespaces 05/17/23 08:59:00.733
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:00.741
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 05/17/23 08:59:00.743
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:00.748
    STEP: Creating a service in the namespace 05/17/23 08:59:00.749
    STEP: Deleting the namespace 05/17/23 08:59:00.753
    STEP: Waiting for the namespace to be removed. 05/17/23 08:59:00.757
    STEP: Recreating the namespace 05/17/23 08:59:06.76
    STEP: Verifying there is no service in the namespace 05/17/23 08:59:06.766
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:06.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-105" for this suite. 05/17/23 08:59:06.77
    STEP: Destroying namespace "nsdeletetest-180" for this suite. 05/17/23 08:59:06.773
    May 17 08:59:06.774: INFO: Namespace nsdeletetest-180 was already deleted
    STEP: Destroying namespace "nsdeletetest-5957" for this suite. 05/17/23 08:59:06.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:06.778
May 17 08:59:06.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 08:59:06.779
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:06.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:06.785
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 05/17/23 08:59:06.787
May 17 08:59:06.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4" in namespace "downward-api-8630" to be "Succeeded or Failed"
May 17 08:59:06.792: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.289036ms
May 17 08:59:08.795: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004306719s
May 17 08:59:10.795: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004023939s
STEP: Saw pod success 05/17/23 08:59:10.795
May 17 08:59:10.795: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4" satisfied condition "Succeeded or Failed"
May 17 08:59:10.796: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4 container client-container: <nil>
STEP: delete the pod 05/17/23 08:59:10.806
May 17 08:59:10.813: INFO: Waiting for pod downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4 to disappear
May 17 08:59:10.814: INFO: Pod downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 17 08:59:10.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8630" for this suite. 05/17/23 08:59:10.816
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:06.778
    May 17 08:59:06.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 08:59:06.779
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:06.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:06.785
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 05/17/23 08:59:06.787
    May 17 08:59:06.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4" in namespace "downward-api-8630" to be "Succeeded or Failed"
    May 17 08:59:06.792: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.289036ms
    May 17 08:59:08.795: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004306719s
    May 17 08:59:10.795: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004023939s
    STEP: Saw pod success 05/17/23 08:59:10.795
    May 17 08:59:10.795: INFO: Pod "downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4" satisfied condition "Succeeded or Failed"
    May 17 08:59:10.796: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4 container client-container: <nil>
    STEP: delete the pod 05/17/23 08:59:10.806
    May 17 08:59:10.813: INFO: Waiting for pod downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4 to disappear
    May 17 08:59:10.814: INFO: Pod downwardapi-volume-519cc352-e801-4830-985f-69c472b426f4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:10.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8630" for this suite. 05/17/23 08:59:10.816
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:10.819
May 17 08:59:10.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:59:10.82
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:10.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:10.827
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-d545d396-38f4-499c-9347-a102a50a34a5 05/17/23 08:59:10.829
STEP: Creating a pod to test consume configMaps 05/17/23 08:59:10.832
May 17 08:59:10.836: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe" in namespace "projected-2826" to be "Succeeded or Failed"
May 17 08:59:10.837: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.418035ms
May 17 08:59:12.840: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004228764s
May 17 08:59:14.841: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004945441s
STEP: Saw pod success 05/17/23 08:59:14.841
May 17 08:59:14.841: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe" satisfied condition "Succeeded or Failed"
May 17 08:59:14.843: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe container agnhost-container: <nil>
STEP: delete the pod 05/17/23 08:59:14.846
May 17 08:59:14.852: INFO: Waiting for pod pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe to disappear
May 17 08:59:14.853: INFO: Pod pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 08:59:14.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2826" for this suite. 05/17/23 08:59:14.855
------------------------------
â€¢ [4.039 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:10.819
    May 17 08:59:10.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:59:10.82
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:10.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:10.827
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-d545d396-38f4-499c-9347-a102a50a34a5 05/17/23 08:59:10.829
    STEP: Creating a pod to test consume configMaps 05/17/23 08:59:10.832
    May 17 08:59:10.836: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe" in namespace "projected-2826" to be "Succeeded or Failed"
    May 17 08:59:10.837: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.418035ms
    May 17 08:59:12.840: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004228764s
    May 17 08:59:14.841: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004945441s
    STEP: Saw pod success 05/17/23 08:59:14.841
    May 17 08:59:14.841: INFO: Pod "pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe" satisfied condition "Succeeded or Failed"
    May 17 08:59:14.843: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 08:59:14.846
    May 17 08:59:14.852: INFO: Waiting for pod pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe to disappear
    May 17 08:59:14.853: INFO: Pod pod-projected-configmaps-e13f6b7d-4764-4726-ba43-e6507c1440fe no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:14.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2826" for this suite. 05/17/23 08:59:14.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:14.859
May 17 08:59:14.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename replication-controller 05/17/23 08:59:14.86
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:14.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:14.867
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-kzk9l" 05/17/23 08:59:14.868
May 17 08:59:14.871: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
May 17 08:59:15.872: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
May 17 08:59:15.873: INFO: Found 1 replicas for "e2e-rc-kzk9l" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-kzk9l" 05/17/23 08:59:15.874
STEP: Updating a scale subresource 05/17/23 08:59:15.875
STEP: Verifying replicas where modified for replication controller "e2e-rc-kzk9l" 05/17/23 08:59:15.878
May 17 08:59:15.878: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
May 17 08:59:16.880: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
May 17 08:59:16.882: INFO: Found 2 replicas for "e2e-rc-kzk9l" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 17 08:59:16.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1922" for this suite. 05/17/23 08:59:16.884
------------------------------
â€¢ [2.028 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:14.859
    May 17 08:59:14.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename replication-controller 05/17/23 08:59:14.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:14.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:14.867
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-kzk9l" 05/17/23 08:59:14.868
    May 17 08:59:14.871: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
    May 17 08:59:15.872: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
    May 17 08:59:15.873: INFO: Found 1 replicas for "e2e-rc-kzk9l" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-kzk9l" 05/17/23 08:59:15.874
    STEP: Updating a scale subresource 05/17/23 08:59:15.875
    STEP: Verifying replicas where modified for replication controller "e2e-rc-kzk9l" 05/17/23 08:59:15.878
    May 17 08:59:15.878: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
    May 17 08:59:16.880: INFO: Get Replication Controller "e2e-rc-kzk9l" to confirm replicas
    May 17 08:59:16.882: INFO: Found 2 replicas for "e2e-rc-kzk9l" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:16.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1922" for this suite. 05/17/23 08:59:16.884
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:16.887
May 17 08:59:16.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 08:59:16.888
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:16.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:16.897
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-3e91723b-7a4b-48fc-838d-19daa79b22a4 05/17/23 08:59:16.898
STEP: Creating a pod to test consume secrets 05/17/23 08:59:16.901
May 17 08:59:16.904: INFO: Waiting up to 5m0s for pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8" in namespace "secrets-9716" to be "Succeeded or Failed"
May 17 08:59:16.906: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.321289ms
May 17 08:59:18.909: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004271905s
May 17 08:59:20.909: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004440495s
STEP: Saw pod success 05/17/23 08:59:20.909
May 17 08:59:20.909: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8" satisfied condition "Succeeded or Failed"
May 17 08:59:20.910: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8 container secret-env-test: <nil>
STEP: delete the pod 05/17/23 08:59:20.914
May 17 08:59:20.918: INFO: Waiting for pod pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8 to disappear
May 17 08:59:20.919: INFO: Pod pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 08:59:20.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9716" for this suite. 05/17/23 08:59:20.921
------------------------------
â€¢ [4.037 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:16.887
    May 17 08:59:16.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 08:59:16.888
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:16.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:16.897
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-3e91723b-7a4b-48fc-838d-19daa79b22a4 05/17/23 08:59:16.898
    STEP: Creating a pod to test consume secrets 05/17/23 08:59:16.901
    May 17 08:59:16.904: INFO: Waiting up to 5m0s for pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8" in namespace "secrets-9716" to be "Succeeded or Failed"
    May 17 08:59:16.906: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.321289ms
    May 17 08:59:18.909: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004271905s
    May 17 08:59:20.909: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004440495s
    STEP: Saw pod success 05/17/23 08:59:20.909
    May 17 08:59:20.909: INFO: Pod "pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8" satisfied condition "Succeeded or Failed"
    May 17 08:59:20.910: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8 container secret-env-test: <nil>
    STEP: delete the pod 05/17/23 08:59:20.914
    May 17 08:59:20.918: INFO: Waiting for pod pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8 to disappear
    May 17 08:59:20.919: INFO: Pod pod-secrets-7b7c0f70-a35b-439f-991e-872c135e86b8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:20.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9716" for this suite. 05/17/23 08:59:20.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:20.925
May 17 08:59:20.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename emptydir 05/17/23 08:59:20.925
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:20.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:20.932
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 05/17/23 08:59:20.933
May 17 08:59:20.937: INFO: Waiting up to 5m0s for pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1" in namespace "emptydir-616" to be "Succeeded or Failed"
May 17 08:59:20.939: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36813ms
May 17 08:59:22.941: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004006549s
May 17 08:59:24.942: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004765426s
STEP: Saw pod success 05/17/23 08:59:24.942
May 17 08:59:24.942: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1" satisfied condition "Succeeded or Failed"
May 17 08:59:24.944: INFO: Trying to get logs from node k8s-node1 pod pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1 container test-container: <nil>
STEP: delete the pod 05/17/23 08:59:24.947
May 17 08:59:24.953: INFO: Waiting for pod pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1 to disappear
May 17 08:59:24.954: INFO: Pod pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 17 08:59:24.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-616" for this suite. 05/17/23 08:59:24.956
------------------------------
â€¢ [4.035 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:20.925
    May 17 08:59:20.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename emptydir 05/17/23 08:59:20.925
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:20.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:20.932
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/17/23 08:59:20.933
    May 17 08:59:20.937: INFO: Waiting up to 5m0s for pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1" in namespace "emptydir-616" to be "Succeeded or Failed"
    May 17 08:59:20.939: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36813ms
    May 17 08:59:22.941: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004006549s
    May 17 08:59:24.942: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004765426s
    STEP: Saw pod success 05/17/23 08:59:24.942
    May 17 08:59:24.942: INFO: Pod "pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1" satisfied condition "Succeeded or Failed"
    May 17 08:59:24.944: INFO: Trying to get logs from node k8s-node1 pod pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1 container test-container: <nil>
    STEP: delete the pod 05/17/23 08:59:24.947
    May 17 08:59:24.953: INFO: Waiting for pod pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1 to disappear
    May 17 08:59:24.954: INFO: Pod pod-f63e6f25-6885-49ed-88dc-0f510fed2ef1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:24.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-616" for this suite. 05/17/23 08:59:24.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:24.96
May 17 08:59:24.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 08:59:24.961
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:24.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:24.97
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 08:59:24.977
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:59:25.362
STEP: Deploying the webhook pod 05/17/23 08:59:25.367
STEP: Wait for the deployment to be ready 05/17/23 08:59:25.373
May 17 08:59:25.377: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 08:59:27.382
STEP: Verifying the service has paired with the endpoint 05/17/23 08:59:27.39
May 17 08:59:28.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 05/17/23 08:59:28.392
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:59:28.403
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/17/23 08:59:28.408
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:59:28.413
STEP: Patching a validating webhook configuration's rules to include the create operation 05/17/23 08:59:28.418
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:59:28.422
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 08:59:28.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1111" for this suite. 05/17/23 08:59:28.445
STEP: Destroying namespace "webhook-1111-markers" for this suite. 05/17/23 08:59:28.448
------------------------------
â€¢ [3.492 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:24.96
    May 17 08:59:24.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 08:59:24.961
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:24.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:24.97
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 08:59:24.977
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 08:59:25.362
    STEP: Deploying the webhook pod 05/17/23 08:59:25.367
    STEP: Wait for the deployment to be ready 05/17/23 08:59:25.373
    May 17 08:59:25.377: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 08:59:27.382
    STEP: Verifying the service has paired with the endpoint 05/17/23 08:59:27.39
    May 17 08:59:28.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 05/17/23 08:59:28.392
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:59:28.403
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/17/23 08:59:28.408
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:59:28.413
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/17/23 08:59:28.418
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 08:59:28.422
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:28.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1111" for this suite. 05/17/23 08:59:28.445
    STEP: Destroying namespace "webhook-1111-markers" for this suite. 05/17/23 08:59:28.448
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:28.452
May 17 08:59:28.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:59:28.453
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:28.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:28.461
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-7b2c7340-c590-44e3-99c8-05e2f75df161 05/17/23 08:59:28.463
STEP: Creating a pod to test consume secrets 05/17/23 08:59:28.465
May 17 08:59:28.471: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c" in namespace "projected-4829" to be "Succeeded or Failed"
May 17 08:59:28.472: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.164879ms
May 17 08:59:30.476: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004679172s
May 17 08:59:32.475: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004305283s
STEP: Saw pod success 05/17/23 08:59:32.475
May 17 08:59:32.476: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c" satisfied condition "Succeeded or Failed"
May 17 08:59:32.477: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 08:59:32.48
May 17 08:59:32.487: INFO: Waiting for pod pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c to disappear
May 17 08:59:32.488: INFO: Pod pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 17 08:59:32.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4829" for this suite. 05/17/23 08:59:32.49
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:28.452
    May 17 08:59:28.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:59:28.453
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:28.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:28.461
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-7b2c7340-c590-44e3-99c8-05e2f75df161 05/17/23 08:59:28.463
    STEP: Creating a pod to test consume secrets 05/17/23 08:59:28.465
    May 17 08:59:28.471: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c" in namespace "projected-4829" to be "Succeeded or Failed"
    May 17 08:59:28.472: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.164879ms
    May 17 08:59:30.476: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004679172s
    May 17 08:59:32.475: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004305283s
    STEP: Saw pod success 05/17/23 08:59:32.475
    May 17 08:59:32.476: INFO: Pod "pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c" satisfied condition "Succeeded or Failed"
    May 17 08:59:32.477: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 08:59:32.48
    May 17 08:59:32.487: INFO: Waiting for pod pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c to disappear
    May 17 08:59:32.488: INFO: Pod pod-projected-secrets-d5c4106b-0995-4ab2-8c96-b2b5d5e4cb7c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:32.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4829" for this suite. 05/17/23 08:59:32.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:32.494
May 17 08:59:32.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename daemonsets 05/17/23 08:59:32.495
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:32.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:32.504
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 05/17/23 08:59:32.514
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:59:32.518
May 17 08:59:32.520: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:59:32.521: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:59:32.521: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 08:59:33.524: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 08:59:33.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 08:59:33.525: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 05/17/23 08:59:33.527
May 17 08:59:33.529: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/17/23 08:59:33.529
May 17 08:59:33.534: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/17/23 08:59:33.534
May 17 08:59:33.535: INFO: Observed &DaemonSet event: ADDED
May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.535: INFO: Found daemon set daemon-set in namespace daemonsets-5090 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 08:59:33.535: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/17/23 08:59:33.535
STEP: watching for the daemon set status to be patched 05/17/23 08:59:33.54
May 17 08:59:33.541: INFO: Observed &DaemonSet event: ADDED
May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.541: INFO: Observed daemon set daemon-set in namespace daemonsets-5090 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
May 17 08:59:33.541: INFO: Found daemon set daemon-set in namespace daemonsets-5090 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May 17 08:59:33.541: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:59:33.543
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5090, will wait for the garbage collector to delete the pods 05/17/23 08:59:33.543
May 17 08:59:33.600: INFO: Deleting DaemonSet.extensions daemon-set took: 4.080952ms
May 17 08:59:33.700: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.38604ms
May 17 08:59:36.402: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 08:59:36.402: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 08:59:36.403: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1217526"},"items":null}

May 17 08:59:36.404: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1217526"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 08:59:36.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5090" for this suite. 05/17/23 08:59:36.411
------------------------------
â€¢ [3.920 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:32.494
    May 17 08:59:32.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename daemonsets 05/17/23 08:59:32.495
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:32.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:32.504
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 05/17/23 08:59:32.514
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 08:59:32.518
    May 17 08:59:32.520: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:59:32.521: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:59:32.521: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 08:59:33.524: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 08:59:33.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 08:59:33.525: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 05/17/23 08:59:33.527
    May 17 08:59:33.529: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/17/23 08:59:33.529
    May 17 08:59:33.534: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/17/23 08:59:33.534
    May 17 08:59:33.535: INFO: Observed &DaemonSet event: ADDED
    May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.535: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.535: INFO: Found daemon set daemon-set in namespace daemonsets-5090 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 08:59:33.535: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/17/23 08:59:33.535
    STEP: watching for the daemon set status to be patched 05/17/23 08:59:33.54
    May 17 08:59:33.541: INFO: Observed &DaemonSet event: ADDED
    May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.541: INFO: Observed daemon set daemon-set in namespace daemonsets-5090 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 08:59:33.541: INFO: Observed &DaemonSet event: MODIFIED
    May 17 08:59:33.541: INFO: Found daemon set daemon-set in namespace daemonsets-5090 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May 17 08:59:33.541: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 08:59:33.543
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5090, will wait for the garbage collector to delete the pods 05/17/23 08:59:33.543
    May 17 08:59:33.600: INFO: Deleting DaemonSet.extensions daemon-set took: 4.080952ms
    May 17 08:59:33.700: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.38604ms
    May 17 08:59:36.402: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 08:59:36.402: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 08:59:36.403: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1217526"},"items":null}

    May 17 08:59:36.404: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1217526"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:36.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5090" for this suite. 05/17/23 08:59:36.411
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:36.414
May 17 08:59:36.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename lease-test 05/17/23 08:59:36.415
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:36.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:36.423
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
May 17 08:59:36.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-1895" for this suite. 05/17/23 08:59:36.449
------------------------------
â€¢ [0.039 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:36.414
    May 17 08:59:36.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename lease-test 05/17/23 08:59:36.415
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:36.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:36.423
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:36.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-1895" for this suite. 05/17/23 08:59:36.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:36.453
May 17 08:59:36.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 08:59:36.454
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:36.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:36.461
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-b4f02c52-e9d5-4bac-9453-fdb864654691 05/17/23 08:59:36.463
STEP: Creating a pod to test consume configMaps 05/17/23 08:59:36.466
May 17 08:59:36.470: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec" in namespace "projected-488" to be "Succeeded or Failed"
May 17 08:59:36.472: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.330311ms
May 17 08:59:38.475: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004431583s
May 17 08:59:40.476: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005472282s
STEP: Saw pod success 05/17/23 08:59:40.476
May 17 08:59:40.476: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec" satisfied condition "Succeeded or Failed"
May 17 08:59:40.478: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec container agnhost-container: <nil>
STEP: delete the pod 05/17/23 08:59:40.481
May 17 08:59:40.489: INFO: Waiting for pod pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec to disappear
May 17 08:59:40.491: INFO: Pod pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 17 08:59:40.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-488" for this suite. 05/17/23 08:59:40.494
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:36.453
    May 17 08:59:36.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 08:59:36.454
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:36.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:36.461
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-b4f02c52-e9d5-4bac-9453-fdb864654691 05/17/23 08:59:36.463
    STEP: Creating a pod to test consume configMaps 05/17/23 08:59:36.466
    May 17 08:59:36.470: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec" in namespace "projected-488" to be "Succeeded or Failed"
    May 17 08:59:36.472: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.330311ms
    May 17 08:59:38.475: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004431583s
    May 17 08:59:40.476: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005472282s
    STEP: Saw pod success 05/17/23 08:59:40.476
    May 17 08:59:40.476: INFO: Pod "pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec" satisfied condition "Succeeded or Failed"
    May 17 08:59:40.478: INFO: Trying to get logs from node k8s-node1 pod pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 08:59:40.481
    May 17 08:59:40.489: INFO: Waiting for pod pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec to disappear
    May 17 08:59:40.491: INFO: Pod pod-projected-configmaps-b1c47bda-26a4-42c7-a820-8c0b8dee91ec no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 17 08:59:40.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-488" for this suite. 05/17/23 08:59:40.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 08:59:40.497
May 17 08:59:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 08:59:40.498
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:40.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:40.506
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437 in namespace container-probe-8203 05/17/23 08:59:40.508
May 17 08:59:40.512: INFO: Waiting up to 5m0s for pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437" in namespace "container-probe-8203" to be "not pending"
May 17 08:59:40.513: INFO: Pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299195ms
May 17 08:59:42.516: INFO: Pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437": Phase="Running", Reason="", readiness=true. Elapsed: 2.00404569s
May 17 08:59:42.516: INFO: Pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437" satisfied condition "not pending"
May 17 08:59:42.516: INFO: Started pod busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437 in namespace container-probe-8203
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:59:42.516
May 17 08:59:42.518: INFO: Initial restart count of pod busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437 is 0
STEP: deleting the pod 05/17/23 09:03:42.868
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 09:03:42.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8203" for this suite. 05/17/23 09:03:42.877
------------------------------
â€¢ [SLOW TEST] [242.383 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 08:59:40.497
    May 17 08:59:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 08:59:40.498
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 08:59:40.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 08:59:40.506
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437 in namespace container-probe-8203 05/17/23 08:59:40.508
    May 17 08:59:40.512: INFO: Waiting up to 5m0s for pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437" in namespace "container-probe-8203" to be "not pending"
    May 17 08:59:40.513: INFO: Pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299195ms
    May 17 08:59:42.516: INFO: Pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437": Phase="Running", Reason="", readiness=true. Elapsed: 2.00404569s
    May 17 08:59:42.516: INFO: Pod "busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437" satisfied condition "not pending"
    May 17 08:59:42.516: INFO: Started pod busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437 in namespace container-probe-8203
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 08:59:42.516
    May 17 08:59:42.518: INFO: Initial restart count of pod busybox-2cfc41db-49c5-4918-9cac-dd62bdd26437 is 0
    STEP: deleting the pod 05/17/23 09:03:42.868
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 09:03:42.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8203" for this suite. 05/17/23 09:03:42.877
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:03:42.88
May 17 09:03:42.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename csiinlinevolumes 05/17/23 09:03:42.881
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:42.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:42.891
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 05/17/23 09:03:42.892
STEP: getting 05/17/23 09:03:42.9
STEP: listing 05/17/23 09:03:42.902
STEP: deleting 05/17/23 09:03:42.903
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May 17 09:03:42.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8462" for this suite. 05/17/23 09:03:42.912
------------------------------
â€¢ [0.034 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:03:42.88
    May 17 09:03:42.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename csiinlinevolumes 05/17/23 09:03:42.881
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:42.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:42.891
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 05/17/23 09:03:42.892
    STEP: getting 05/17/23 09:03:42.9
    STEP: listing 05/17/23 09:03:42.902
    STEP: deleting 05/17/23 09:03:42.903
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May 17 09:03:42.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8462" for this suite. 05/17/23 09:03:42.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:03:42.915
May 17 09:03:42.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 09:03:42.916
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:42.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:42.922
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-20b1b30f-f28e-4b74-b0cf-591f788a7fd7 05/17/23 09:03:42.924
STEP: Creating a pod to test consume configMaps 05/17/23 09:03:42.926
May 17 09:03:42.930: INFO: Waiting up to 5m0s for pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80" in namespace "configmap-1231" to be "Succeeded or Failed"
May 17 09:03:42.931: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238876ms
May 17 09:03:44.934: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004186709s
May 17 09:03:46.935: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005151775s
STEP: Saw pod success 05/17/23 09:03:46.935
May 17 09:03:46.935: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80" satisfied condition "Succeeded or Failed"
May 17 09:03:46.936: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 09:03:46.947
May 17 09:03:46.952: INFO: Waiting for pod pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80 to disappear
May 17 09:03:46.953: INFO: Pod pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 09:03:46.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1231" for this suite. 05/17/23 09:03:46.955
------------------------------
â€¢ [4.043 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:03:42.915
    May 17 09:03:42.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 09:03:42.916
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:42.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:42.922
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-20b1b30f-f28e-4b74-b0cf-591f788a7fd7 05/17/23 09:03:42.924
    STEP: Creating a pod to test consume configMaps 05/17/23 09:03:42.926
    May 17 09:03:42.930: INFO: Waiting up to 5m0s for pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80" in namespace "configmap-1231" to be "Succeeded or Failed"
    May 17 09:03:42.931: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238876ms
    May 17 09:03:44.934: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004186709s
    May 17 09:03:46.935: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005151775s
    STEP: Saw pod success 05/17/23 09:03:46.935
    May 17 09:03:46.935: INFO: Pod "pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80" satisfied condition "Succeeded or Failed"
    May 17 09:03:46.936: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 09:03:46.947
    May 17 09:03:46.952: INFO: Waiting for pod pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80 to disappear
    May 17 09:03:46.953: INFO: Pod pod-configmaps-daa0bd75-a4d6-454c-ad21-6e7933e4fa80 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 09:03:46.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1231" for this suite. 05/17/23 09:03:46.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:03:46.958
May 17 09:03:46.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename var-expansion 05/17/23 09:03:46.959
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:46.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:46.97
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 05/17/23 09:03:46.972
May 17 09:03:46.976: INFO: Waiting up to 5m0s for pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c" in namespace "var-expansion-2666" to be "Succeeded or Failed"
May 17 09:03:46.977: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.398089ms
May 17 09:03:48.980: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c": Phase="Running", Reason="", readiness=false. Elapsed: 2.003850355s
May 17 09:03:50.980: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004810629s
STEP: Saw pod success 05/17/23 09:03:50.981
May 17 09:03:50.981: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c" satisfied condition "Succeeded or Failed"
May 17 09:03:50.982: INFO: Trying to get logs from node k8s-node1 pod var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c container dapi-container: <nil>
STEP: delete the pod 05/17/23 09:03:50.985
May 17 09:03:50.991: INFO: Waiting for pod var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c to disappear
May 17 09:03:50.992: INFO: Pod var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 17 09:03:50.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2666" for this suite. 05/17/23 09:03:50.994
------------------------------
â€¢ [4.038 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:03:46.958
    May 17 09:03:46.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename var-expansion 05/17/23 09:03:46.959
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:46.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:46.97
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 05/17/23 09:03:46.972
    May 17 09:03:46.976: INFO: Waiting up to 5m0s for pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c" in namespace "var-expansion-2666" to be "Succeeded or Failed"
    May 17 09:03:46.977: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.398089ms
    May 17 09:03:48.980: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c": Phase="Running", Reason="", readiness=false. Elapsed: 2.003850355s
    May 17 09:03:50.980: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004810629s
    STEP: Saw pod success 05/17/23 09:03:50.981
    May 17 09:03:50.981: INFO: Pod "var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c" satisfied condition "Succeeded or Failed"
    May 17 09:03:50.982: INFO: Trying to get logs from node k8s-node1 pod var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c container dapi-container: <nil>
    STEP: delete the pod 05/17/23 09:03:50.985
    May 17 09:03:50.991: INFO: Waiting for pod var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c to disappear
    May 17 09:03:50.992: INFO: Pod var-expansion-7058f561-71d2-4572-8093-d0a01ca4161c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 17 09:03:50.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2666" for this suite. 05/17/23 09:03:50.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:03:50.997
May 17 09:03:50.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename podtemplate 05/17/23 09:03:50.997
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:51.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:51.005
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May 17 09:03:51.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4785" for this suite. 05/17/23 09:03:51.021
------------------------------
â€¢ [0.026 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:03:50.997
    May 17 09:03:50.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename podtemplate 05/17/23 09:03:50.997
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:51.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:51.005
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May 17 09:03:51.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4785" for this suite. 05/17/23 09:03:51.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:03:51.024
May 17 09:03:51.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename downward-api 05/17/23 09:03:51.025
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:51.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:51.032
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 05/17/23 09:03:51.034
May 17 09:03:51.038: INFO: Waiting up to 5m0s for pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27" in namespace "downward-api-5712" to be "Succeeded or Failed"
May 17 09:03:51.039: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225385ms
May 17 09:03:53.042: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004064074s
May 17 09:03:55.043: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005033998s
STEP: Saw pod success 05/17/23 09:03:55.043
May 17 09:03:55.043: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27" satisfied condition "Succeeded or Failed"
May 17 09:03:55.044: INFO: Trying to get logs from node k8s-node1 pod downward-api-2779181b-95fb-457e-b602-8cd114486b27 container dapi-container: <nil>
STEP: delete the pod 05/17/23 09:03:55.047
May 17 09:03:55.053: INFO: Waiting for pod downward-api-2779181b-95fb-457e-b602-8cd114486b27 to disappear
May 17 09:03:55.055: INFO: Pod downward-api-2779181b-95fb-457e-b602-8cd114486b27 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 17 09:03:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5712" for this suite. 05/17/23 09:03:55.057
------------------------------
â€¢ [4.035 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:03:51.024
    May 17 09:03:51.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename downward-api 05/17/23 09:03:51.025
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:51.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:51.032
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 05/17/23 09:03:51.034
    May 17 09:03:51.038: INFO: Waiting up to 5m0s for pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27" in namespace "downward-api-5712" to be "Succeeded or Failed"
    May 17 09:03:51.039: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225385ms
    May 17 09:03:53.042: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004064074s
    May 17 09:03:55.043: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005033998s
    STEP: Saw pod success 05/17/23 09:03:55.043
    May 17 09:03:55.043: INFO: Pod "downward-api-2779181b-95fb-457e-b602-8cd114486b27" satisfied condition "Succeeded or Failed"
    May 17 09:03:55.044: INFO: Trying to get logs from node k8s-node1 pod downward-api-2779181b-95fb-457e-b602-8cd114486b27 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 09:03:55.047
    May 17 09:03:55.053: INFO: Waiting for pod downward-api-2779181b-95fb-457e-b602-8cd114486b27 to disappear
    May 17 09:03:55.055: INFO: Pod downward-api-2779181b-95fb-457e-b602-8cd114486b27 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 17 09:03:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5712" for this suite. 05/17/23 09:03:55.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:03:55.06
May 17 09:03:55.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 09:03:55.061
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:55.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:55.068
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 09:03:55.077
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 09:03:55.418
STEP: Deploying the webhook pod 05/17/23 09:03:55.422
STEP: Wait for the deployment to be ready 05/17/23 09:03:55.428
May 17 09:03:55.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 09:03:57.438
STEP: Verifying the service has paired with the endpoint 05/17/23 09:03:57.444
May 17 09:03:58.445: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
May 17 09:03:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8403-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 09:03:58.955
STEP: Creating a custom resource while v1 is storage version 05/17/23 09:03:58.966
STEP: Patching Custom Resource Definition to set v2 as storage 05/17/23 09:04:01.007
STEP: Patching the custom resource while v2 is storage version 05/17/23 09:04:01.011
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 09:04:01.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7013" for this suite. 05/17/23 09:04:01.575
STEP: Destroying namespace "webhook-7013-markers" for this suite. 05/17/23 09:04:01.58
------------------------------
â€¢ [SLOW TEST] [6.523 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:03:55.06
    May 17 09:03:55.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 09:03:55.061
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:03:55.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:03:55.068
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 09:03:55.077
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 09:03:55.418
    STEP: Deploying the webhook pod 05/17/23 09:03:55.422
    STEP: Wait for the deployment to be ready 05/17/23 09:03:55.428
    May 17 09:03:55.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 09:03:57.438
    STEP: Verifying the service has paired with the endpoint 05/17/23 09:03:57.444
    May 17 09:03:58.445: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    May 17 09:03:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8403-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 09:03:58.955
    STEP: Creating a custom resource while v1 is storage version 05/17/23 09:03:58.966
    STEP: Patching Custom Resource Definition to set v2 as storage 05/17/23 09:04:01.007
    STEP: Patching the custom resource while v2 is storage version 05/17/23 09:04:01.011
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 09:04:01.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7013" for this suite. 05/17/23 09:04:01.575
    STEP: Destroying namespace "webhook-7013-markers" for this suite. 05/17/23 09:04:01.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:04:01.584
May 17 09:04:01.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 09:04:01.585
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:01.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:01.593
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-342fae2a-65a4-4fad-8a44-c5bebda46f57 05/17/23 09:04:01.596
STEP: Creating configMap with name cm-test-opt-upd-d44bcf58-b2ea-44bd-aedb-2136966ba2b2 05/17/23 09:04:01.599
STEP: Creating the pod 05/17/23 09:04:01.601
May 17 09:04:01.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9" in namespace "configmap-7411" to be "running and ready"
May 17 09:04:01.607: INFO: Pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31356ms
May 17 09:04:01.607: INFO: The phase of Pod pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9 is Pending, waiting for it to be Running (with Ready = true)
May 17 09:04:03.610: INFO: Pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004808541s
May 17 09:04:03.610: INFO: The phase of Pod pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9 is Running (Ready = true)
May 17 09:04:03.610: INFO: Pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-342fae2a-65a4-4fad-8a44-c5bebda46f57 05/17/23 09:04:03.62
STEP: Updating configmap cm-test-opt-upd-d44bcf58-b2ea-44bd-aedb-2136966ba2b2 05/17/23 09:04:03.623
STEP: Creating configMap with name cm-test-opt-create-1d949dc5-5236-4e49-8efa-15242dfe12e5 05/17/23 09:04:03.625
STEP: waiting to observe update in volume 05/17/23 09:04:03.628
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 09:04:05.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7411" for this suite. 05/17/23 09:04:05.643
------------------------------
â€¢ [4.062 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:04:01.584
    May 17 09:04:01.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 09:04:01.585
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:01.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:01.593
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-342fae2a-65a4-4fad-8a44-c5bebda46f57 05/17/23 09:04:01.596
    STEP: Creating configMap with name cm-test-opt-upd-d44bcf58-b2ea-44bd-aedb-2136966ba2b2 05/17/23 09:04:01.599
    STEP: Creating the pod 05/17/23 09:04:01.601
    May 17 09:04:01.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9" in namespace "configmap-7411" to be "running and ready"
    May 17 09:04:01.607: INFO: Pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31356ms
    May 17 09:04:01.607: INFO: The phase of Pod pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9 is Pending, waiting for it to be Running (with Ready = true)
    May 17 09:04:03.610: INFO: Pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004808541s
    May 17 09:04:03.610: INFO: The phase of Pod pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9 is Running (Ready = true)
    May 17 09:04:03.610: INFO: Pod "pod-configmaps-b7ccf160-a67f-46b9-b5b5-68996da155e9" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-342fae2a-65a4-4fad-8a44-c5bebda46f57 05/17/23 09:04:03.62
    STEP: Updating configmap cm-test-opt-upd-d44bcf58-b2ea-44bd-aedb-2136966ba2b2 05/17/23 09:04:03.623
    STEP: Creating configMap with name cm-test-opt-create-1d949dc5-5236-4e49-8efa-15242dfe12e5 05/17/23 09:04:03.625
    STEP: waiting to observe update in volume 05/17/23 09:04:03.628
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 09:04:05.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7411" for this suite. 05/17/23 09:04:05.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:04:05.646
May 17 09:04:05.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename prestop 05/17/23 09:04:05.647
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:05.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:05.654
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1309 05/17/23 09:04:05.655
STEP: Waiting for pods to come up. 05/17/23 09:04:05.66
May 17 09:04:05.660: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1309" to be "running"
May 17 09:04:05.661: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461553ms
May 17 09:04:07.664: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.003903065s
May 17 09:04:07.664: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1309 05/17/23 09:04:07.665
May 17 09:04:07.669: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1309" to be "running"
May 17 09:04:07.670: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.375868ms
May 17 09:04:09.673: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004288212s
May 17 09:04:09.673: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/17/23 09:04:09.673
May 17 09:04:14.681: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/17/23 09:04:14.681
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
May 17 09:04:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-1309" for this suite. 05/17/23 09:04:14.69
------------------------------
â€¢ [SLOW TEST] [9.047 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:04:05.646
    May 17 09:04:05.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename prestop 05/17/23 09:04:05.647
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:05.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:05.654
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1309 05/17/23 09:04:05.655
    STEP: Waiting for pods to come up. 05/17/23 09:04:05.66
    May 17 09:04:05.660: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1309" to be "running"
    May 17 09:04:05.661: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461553ms
    May 17 09:04:07.664: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.003903065s
    May 17 09:04:07.664: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1309 05/17/23 09:04:07.665
    May 17 09:04:07.669: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1309" to be "running"
    May 17 09:04:07.670: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.375868ms
    May 17 09:04:09.673: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004288212s
    May 17 09:04:09.673: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/17/23 09:04:09.673
    May 17 09:04:14.681: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/17/23 09:04:14.681
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    May 17 09:04:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-1309" for this suite. 05/17/23 09:04:14.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:04:14.694
May 17 09:04:14.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename gc 05/17/23 09:04:14.695
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:14.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:14.703
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/17/23 09:04:14.705
STEP: delete the rc 05/17/23 09:04:19.711
STEP: wait for all pods to be garbage collected 05/17/23 09:04:19.714
STEP: Gathering metrics 05/17/23 09:04:24.718
May 17 09:04:24.730: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
May 17 09:04:24.731: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.390382ms
May 17 09:04:24.731: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
May 17 09:04:24.731: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
May 17 09:04:24.774: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 17 09:04:24.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3207" for this suite. 05/17/23 09:04:24.776
------------------------------
â€¢ [SLOW TEST] [10.086 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:04:14.694
    May 17 09:04:14.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename gc 05/17/23 09:04:14.695
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:14.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:14.703
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/17/23 09:04:14.705
    STEP: delete the rc 05/17/23 09:04:19.711
    STEP: wait for all pods to be garbage collected 05/17/23 09:04:19.714
    STEP: Gathering metrics 05/17/23 09:04:24.718
    May 17 09:04:24.730: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
    May 17 09:04:24.731: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.390382ms
    May 17 09:04:24.731: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
    May 17 09:04:24.731: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
    May 17 09:04:24.774: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 17 09:04:24.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3207" for this suite. 05/17/23 09:04:24.776
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:04:24.78
May 17 09:04:24.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename subpath 05/17/23 09:04:24.781
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:24.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:24.789
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 09:04:24.791
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-lcmc 05/17/23 09:04:24.795
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 09:04:24.795
May 17 09:04:24.800: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lcmc" in namespace "subpath-7101" to be "Succeeded or Failed"
May 17 09:04:24.801: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.187707ms
May 17 09:04:26.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003937351s
May 17 09:04:28.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 4.003582876s
May 17 09:04:30.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 6.003750444s
May 17 09:04:32.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 8.00339415s
May 17 09:04:34.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 10.0034775s
May 17 09:04:36.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 12.004552705s
May 17 09:04:38.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 14.003994038s
May 17 09:04:40.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 16.002988237s
May 17 09:04:42.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 18.003642301s
May 17 09:04:44.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 20.003796356s
May 17 09:04:46.806: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=false. Elapsed: 22.006257549s
May 17 09:04:48.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004637745s
STEP: Saw pod success 05/17/23 09:04:48.804
May 17 09:04:48.804: INFO: Pod "pod-subpath-test-secret-lcmc" satisfied condition "Succeeded or Failed"
May 17 09:04:48.806: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-secret-lcmc container test-container-subpath-secret-lcmc: <nil>
STEP: delete the pod 05/17/23 09:04:48.809
May 17 09:04:48.814: INFO: Waiting for pod pod-subpath-test-secret-lcmc to disappear
May 17 09:04:48.816: INFO: Pod pod-subpath-test-secret-lcmc no longer exists
STEP: Deleting pod pod-subpath-test-secret-lcmc 05/17/23 09:04:48.816
May 17 09:04:48.816: INFO: Deleting pod "pod-subpath-test-secret-lcmc" in namespace "subpath-7101"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 17 09:04:48.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7101" for this suite. 05/17/23 09:04:48.819
------------------------------
â€¢ [SLOW TEST] [24.042 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:04:24.78
    May 17 09:04:24.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename subpath 05/17/23 09:04:24.781
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:24.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:24.789
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 09:04:24.791
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-lcmc 05/17/23 09:04:24.795
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 09:04:24.795
    May 17 09:04:24.800: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lcmc" in namespace "subpath-7101" to be "Succeeded or Failed"
    May 17 09:04:24.801: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.187707ms
    May 17 09:04:26.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003937351s
    May 17 09:04:28.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 4.003582876s
    May 17 09:04:30.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 6.003750444s
    May 17 09:04:32.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 8.00339415s
    May 17 09:04:34.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 10.0034775s
    May 17 09:04:36.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 12.004552705s
    May 17 09:04:38.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 14.003994038s
    May 17 09:04:40.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 16.002988237s
    May 17 09:04:42.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 18.003642301s
    May 17 09:04:44.803: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=true. Elapsed: 20.003796356s
    May 17 09:04:46.806: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Running", Reason="", readiness=false. Elapsed: 22.006257549s
    May 17 09:04:48.804: INFO: Pod "pod-subpath-test-secret-lcmc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004637745s
    STEP: Saw pod success 05/17/23 09:04:48.804
    May 17 09:04:48.804: INFO: Pod "pod-subpath-test-secret-lcmc" satisfied condition "Succeeded or Failed"
    May 17 09:04:48.806: INFO: Trying to get logs from node k8s-node1 pod pod-subpath-test-secret-lcmc container test-container-subpath-secret-lcmc: <nil>
    STEP: delete the pod 05/17/23 09:04:48.809
    May 17 09:04:48.814: INFO: Waiting for pod pod-subpath-test-secret-lcmc to disappear
    May 17 09:04:48.816: INFO: Pod pod-subpath-test-secret-lcmc no longer exists
    STEP: Deleting pod pod-subpath-test-secret-lcmc 05/17/23 09:04:48.816
    May 17 09:04:48.816: INFO: Deleting pod "pod-subpath-test-secret-lcmc" in namespace "subpath-7101"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 17 09:04:48.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7101" for this suite. 05/17/23 09:04:48.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:04:48.822
May 17 09:04:48.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 09:04:48.823
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:48.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:48.83
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
May 17 09:04:48.837: INFO: Waiting up to 5m0s for pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b" in namespace "container-probe-2450" to be "running and ready"
May 17 09:04:48.838: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.138269ms
May 17 09:04:48.838: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Pending, waiting for it to be Running (with Ready = true)
May 17 09:04:50.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 2.003792301s
May 17 09:04:50.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:04:52.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 4.003916353s
May 17 09:04:52.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:04:54.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 6.004296835s
May 17 09:04:54.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:04:56.842: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 8.004530977s
May 17 09:04:56.842: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:04:58.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 10.004213118s
May 17 09:04:58.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:05:00.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 12.004239881s
May 17 09:05:00.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:05:02.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 14.003707009s
May 17 09:05:02.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:05:04.840: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 16.003441072s
May 17 09:05:04.840: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:05:06.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 18.003894182s
May 17 09:05:06.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:05:08.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 20.003590118s
May 17 09:05:08.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
May 17 09:05:10.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=true. Elapsed: 22.004183405s
May 17 09:05:10.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = true)
May 17 09:05:10.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b" satisfied condition "running and ready"
May 17 09:05:10.843: INFO: Container started at 2023-05-17 09:04:49 +0000 UTC, pod became ready at 2023-05-17 09:05:09 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 09:05:10.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2450" for this suite. 05/17/23 09:05:10.844
------------------------------
â€¢ [SLOW TEST] [22.025 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:04:48.822
    May 17 09:04:48.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 09:04:48.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:04:48.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:04:48.83
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    May 17 09:04:48.837: INFO: Waiting up to 5m0s for pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b" in namespace "container-probe-2450" to be "running and ready"
    May 17 09:04:48.838: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.138269ms
    May 17 09:04:48.838: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Pending, waiting for it to be Running (with Ready = true)
    May 17 09:04:50.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 2.003792301s
    May 17 09:04:50.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:04:52.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 4.003916353s
    May 17 09:04:52.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:04:54.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 6.004296835s
    May 17 09:04:54.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:04:56.842: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 8.004530977s
    May 17 09:04:56.842: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:04:58.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 10.004213118s
    May 17 09:04:58.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:05:00.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 12.004239881s
    May 17 09:05:00.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:05:02.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 14.003707009s
    May 17 09:05:02.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:05:04.840: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 16.003441072s
    May 17 09:05:04.840: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:05:06.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 18.003894182s
    May 17 09:05:06.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:05:08.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=false. Elapsed: 20.003590118s
    May 17 09:05:08.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = false)
    May 17 09:05:10.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b": Phase="Running", Reason="", readiness=true. Elapsed: 22.004183405s
    May 17 09:05:10.841: INFO: The phase of Pod test-webserver-b570023c-fbea-446b-b572-419373e9d79b is Running (Ready = true)
    May 17 09:05:10.841: INFO: Pod "test-webserver-b570023c-fbea-446b-b572-419373e9d79b" satisfied condition "running and ready"
    May 17 09:05:10.843: INFO: Container started at 2023-05-17 09:04:49 +0000 UTC, pod became ready at 2023-05-17 09:05:09 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:10.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2450" for this suite. 05/17/23 09:05:10.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:10.849
May 17 09:05:10.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename controllerrevisions 05/17/23 09:05:10.85
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:10.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:10.857
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-hxk47-daemon-set" 05/17/23 09:05:10.866
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 09:05:10.868
May 17 09:05:10.870: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 09:05:10.871: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 0
May 17 09:05:10.871: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
May 17 09:05:11.874: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 09:05:11.876: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 1
May 17 09:05:11.876: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
May 17 09:05:12.875: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 09:05:12.876: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 2
May 17 09:05:12.876: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-hxk47-daemon-set
STEP: Confirm DaemonSet "e2e-hxk47-daemon-set" successfully created with "daemonset-name=e2e-hxk47-daemon-set" label 05/17/23 09:05:12.877
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-hxk47-daemon-set" 05/17/23 09:05:12.88
May 17 09:05:12.881: INFO: Located ControllerRevision: "e2e-hxk47-daemon-set-6896bbcdb9"
STEP: Patching ControllerRevision "e2e-hxk47-daemon-set-6896bbcdb9" 05/17/23 09:05:12.883
May 17 09:05:12.885: INFO: e2e-hxk47-daemon-set-6896bbcdb9 has been patched
STEP: Create a new ControllerRevision 05/17/23 09:05:12.885
May 17 09:05:12.888: INFO: Created ControllerRevision: e2e-hxk47-daemon-set-5899b88cf7
STEP: Confirm that there are two ControllerRevisions 05/17/23 09:05:12.888
May 17 09:05:12.888: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 09:05:12.890: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-hxk47-daemon-set-6896bbcdb9" 05/17/23 09:05:12.89
STEP: Confirm that there is only one ControllerRevision 05/17/23 09:05:12.892
May 17 09:05:12.892: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 09:05:12.894: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-hxk47-daemon-set-5899b88cf7" 05/17/23 09:05:12.895
May 17 09:05:12.898: INFO: e2e-hxk47-daemon-set-5899b88cf7 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/17/23 09:05:12.898
W0517 09:05:12.902275      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/17/23 09:05:12.902
May 17 09:05:12.902: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 09:05:13.904: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 09:05:13.906: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-hxk47-daemon-set-5899b88cf7=updated" 05/17/23 09:05:13.906
STEP: Confirm that there is only one ControllerRevision 05/17/23 09:05:13.91
May 17 09:05:13.910: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 09:05:13.911: INFO: Found 1 ControllerRevisions
May 17 09:05:13.912: INFO: ControllerRevision "e2e-hxk47-daemon-set-79d558c444" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-hxk47-daemon-set" 05/17/23 09:05:13.914
STEP: deleting DaemonSet.extensions e2e-hxk47-daemon-set in namespace controllerrevisions-5398, will wait for the garbage collector to delete the pods 05/17/23 09:05:13.914
May 17 09:05:13.969: INFO: Deleting DaemonSet.extensions e2e-hxk47-daemon-set took: 3.398607ms
May 17 09:05:14.069: INFO: Terminating DaemonSet.extensions e2e-hxk47-daemon-set pods took: 100.319862ms
May 17 09:05:14.971: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 0
May 17 09:05:14.971: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-hxk47-daemon-set
May 17 09:05:14.973: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1218642"},"items":null}

May 17 09:05:14.974: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1218642"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
May 17 09:05:14.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-5398" for this suite. 05/17/23 09:05:14.98
------------------------------
â€¢ [4.134 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:10.849
    May 17 09:05:10.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename controllerrevisions 05/17/23 09:05:10.85
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:10.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:10.857
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-hxk47-daemon-set" 05/17/23 09:05:10.866
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 09:05:10.868
    May 17 09:05:10.870: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 09:05:10.871: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 0
    May 17 09:05:10.871: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
    May 17 09:05:11.874: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 09:05:11.876: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 1
    May 17 09:05:11.876: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
    May 17 09:05:12.875: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 17 09:05:12.876: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 2
    May 17 09:05:12.876: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-hxk47-daemon-set
    STEP: Confirm DaemonSet "e2e-hxk47-daemon-set" successfully created with "daemonset-name=e2e-hxk47-daemon-set" label 05/17/23 09:05:12.877
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-hxk47-daemon-set" 05/17/23 09:05:12.88
    May 17 09:05:12.881: INFO: Located ControllerRevision: "e2e-hxk47-daemon-set-6896bbcdb9"
    STEP: Patching ControllerRevision "e2e-hxk47-daemon-set-6896bbcdb9" 05/17/23 09:05:12.883
    May 17 09:05:12.885: INFO: e2e-hxk47-daemon-set-6896bbcdb9 has been patched
    STEP: Create a new ControllerRevision 05/17/23 09:05:12.885
    May 17 09:05:12.888: INFO: Created ControllerRevision: e2e-hxk47-daemon-set-5899b88cf7
    STEP: Confirm that there are two ControllerRevisions 05/17/23 09:05:12.888
    May 17 09:05:12.888: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 09:05:12.890: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-hxk47-daemon-set-6896bbcdb9" 05/17/23 09:05:12.89
    STEP: Confirm that there is only one ControllerRevision 05/17/23 09:05:12.892
    May 17 09:05:12.892: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 09:05:12.894: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-hxk47-daemon-set-5899b88cf7" 05/17/23 09:05:12.895
    May 17 09:05:12.898: INFO: e2e-hxk47-daemon-set-5899b88cf7 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/17/23 09:05:12.898
    W0517 09:05:12.902275      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/17/23 09:05:12.902
    May 17 09:05:12.902: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 09:05:13.904: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 09:05:13.906: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-hxk47-daemon-set-5899b88cf7=updated" 05/17/23 09:05:13.906
    STEP: Confirm that there is only one ControllerRevision 05/17/23 09:05:13.91
    May 17 09:05:13.910: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 09:05:13.911: INFO: Found 1 ControllerRevisions
    May 17 09:05:13.912: INFO: ControllerRevision "e2e-hxk47-daemon-set-79d558c444" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-hxk47-daemon-set" 05/17/23 09:05:13.914
    STEP: deleting DaemonSet.extensions e2e-hxk47-daemon-set in namespace controllerrevisions-5398, will wait for the garbage collector to delete the pods 05/17/23 09:05:13.914
    May 17 09:05:13.969: INFO: Deleting DaemonSet.extensions e2e-hxk47-daemon-set took: 3.398607ms
    May 17 09:05:14.069: INFO: Terminating DaemonSet.extensions e2e-hxk47-daemon-set pods took: 100.319862ms
    May 17 09:05:14.971: INFO: Number of nodes with available pods controlled by daemonset e2e-hxk47-daemon-set: 0
    May 17 09:05:14.971: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-hxk47-daemon-set
    May 17 09:05:14.973: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1218642"},"items":null}

    May 17 09:05:14.974: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1218642"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:14.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-5398" for this suite. 05/17/23 09:05:14.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:14.983
May 17 09:05:14.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 09:05:14.984
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:14.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:14.991
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-7371b579-60b2-4d5a-9f62-1e465d886f3e 05/17/23 09:05:14.992
STEP: Creating a pod to test consume secrets 05/17/23 09:05:14.995
May 17 09:05:14.998: INFO: Waiting up to 5m0s for pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e" in namespace "secrets-5442" to be "Succeeded or Failed"
May 17 09:05:15.000: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.315171ms
May 17 09:05:17.003: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004514071s
May 17 09:05:19.003: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004714004s
STEP: Saw pod success 05/17/23 09:05:19.003
May 17 09:05:19.003: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e" satisfied condition "Succeeded or Failed"
May 17 09:05:19.005: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 09:05:19.008
May 17 09:05:19.015: INFO: Waiting for pod pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e to disappear
May 17 09:05:19.016: INFO: Pod pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 09:05:19.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5442" for this suite. 05/17/23 09:05:19.018
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:14.983
    May 17 09:05:14.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 09:05:14.984
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:14.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:14.991
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-7371b579-60b2-4d5a-9f62-1e465d886f3e 05/17/23 09:05:14.992
    STEP: Creating a pod to test consume secrets 05/17/23 09:05:14.995
    May 17 09:05:14.998: INFO: Waiting up to 5m0s for pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e" in namespace "secrets-5442" to be "Succeeded or Failed"
    May 17 09:05:15.000: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.315171ms
    May 17 09:05:17.003: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004514071s
    May 17 09:05:19.003: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004714004s
    STEP: Saw pod success 05/17/23 09:05:19.003
    May 17 09:05:19.003: INFO: Pod "pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e" satisfied condition "Succeeded or Failed"
    May 17 09:05:19.005: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 09:05:19.008
    May 17 09:05:19.015: INFO: Waiting for pod pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e to disappear
    May 17 09:05:19.016: INFO: Pod pod-secrets-9506aa46-45fe-485e-a56e-6218a4a5ad2e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:19.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5442" for this suite. 05/17/23 09:05:19.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:19.021
May 17 09:05:19.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename projected 05/17/23 09:05:19.022
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:19.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:19.029
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 05/17/23 09:05:19.03
May 17 09:05:19.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80" in namespace "projected-7470" to be "Succeeded or Failed"
May 17 09:05:19.035: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1.212978ms
May 17 09:05:21.038: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003960984s
May 17 09:05:23.039: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004311225s
STEP: Saw pod success 05/17/23 09:05:23.039
May 17 09:05:23.039: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80" satisfied condition "Succeeded or Failed"
May 17 09:05:23.040: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80 container client-container: <nil>
STEP: delete the pod 05/17/23 09:05:23.043
May 17 09:05:23.049: INFO: Waiting for pod downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80 to disappear
May 17 09:05:23.050: INFO: Pod downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 17 09:05:23.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7470" for this suite. 05/17/23 09:05:23.051
------------------------------
â€¢ [4.032 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:19.021
    May 17 09:05:19.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename projected 05/17/23 09:05:19.022
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:19.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:19.029
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 05/17/23 09:05:19.03
    May 17 09:05:19.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80" in namespace "projected-7470" to be "Succeeded or Failed"
    May 17 09:05:19.035: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1.212978ms
    May 17 09:05:21.038: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003960984s
    May 17 09:05:23.039: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004311225s
    STEP: Saw pod success 05/17/23 09:05:23.039
    May 17 09:05:23.039: INFO: Pod "downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80" satisfied condition "Succeeded or Failed"
    May 17 09:05:23.040: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80 container client-container: <nil>
    STEP: delete the pod 05/17/23 09:05:23.043
    May 17 09:05:23.049: INFO: Waiting for pod downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80 to disappear
    May 17 09:05:23.050: INFO: Pod downwardapi-volume-4d1a0b53-024d-434c-bbe0-885afd8a9d80 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:23.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7470" for this suite. 05/17/23 09:05:23.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:23.054
May 17 09:05:23.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename configmap 05/17/23 09:05:23.055
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:23.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:23.062
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-c6c8b6dd-6555-443c-af27-fb708647b9ff 05/17/23 09:05:23.064
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 17 09:05:23.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8750" for this suite. 05/17/23 09:05:23.066
------------------------------
â€¢ [0.014 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:23.054
    May 17 09:05:23.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename configmap 05/17/23 09:05:23.055
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:23.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:23.062
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-c6c8b6dd-6555-443c-af27-fb708647b9ff 05/17/23 09:05:23.064
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:23.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8750" for this suite. 05/17/23 09:05:23.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:23.069
May 17 09:05:23.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename gc 05/17/23 09:05:23.069
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:23.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:23.076
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/17/23 09:05:23.078
STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 09:05:23.08
STEP: delete the deployment 05/17/23 09:05:23.584
STEP: wait for all rs to be garbage collected 05/17/23 09:05:23.586
STEP: expected 0 rs, got 1 rs 05/17/23 09:05:23.59
STEP: expected 0 pods, got 2 pods 05/17/23 09:05:23.591
STEP: Gathering metrics 05/17/23 09:05:24.095
May 17 09:05:24.101: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
May 17 09:05:24.102: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.320539ms
May 17 09:05:24.102: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
May 17 09:05:24.102: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
May 17 09:05:24.138: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 17 09:05:24.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1070" for this suite. 05/17/23 09:05:24.14
------------------------------
â€¢ [1.075 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:23.069
    May 17 09:05:23.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename gc 05/17/23 09:05:23.069
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:23.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:23.076
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/17/23 09:05:23.078
    STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 09:05:23.08
    STEP: delete the deployment 05/17/23 09:05:23.584
    STEP: wait for all rs to be garbage collected 05/17/23 09:05:23.586
    STEP: expected 0 rs, got 1 rs 05/17/23 09:05:23.59
    STEP: expected 0 pods, got 2 pods 05/17/23 09:05:23.591
    STEP: Gathering metrics 05/17/23 09:05:24.095
    May 17 09:05:24.101: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master" in namespace "kube-system" to be "running and ready"
    May 17 09:05:24.102: INFO: Pod "kube-controller-manager-k8s-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.320539ms
    May 17 09:05:24.102: INFO: The phase of Pod kube-controller-manager-k8s-master is Running (Ready = true)
    May 17 09:05:24.102: INFO: Pod "kube-controller-manager-k8s-master" satisfied condition "running and ready"
    May 17 09:05:24.138: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:24.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1070" for this suite. 05/17/23 09:05:24.14
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:24.144
May 17 09:05:24.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename pods 05/17/23 09:05:24.145
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:24.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:24.151
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
May 17 09:05:24.157: INFO: Waiting up to 5m0s for pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec" in namespace "pods-9681" to be "running and ready"
May 17 09:05:24.158: INFO: Pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.153769ms
May 17 09:05:24.158: INFO: The phase of Pod server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec is Pending, waiting for it to be Running (with Ready = true)
May 17 09:05:26.161: INFO: Pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.003812537s
May 17 09:05:26.161: INFO: The phase of Pod server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec is Running (Ready = true)
May 17 09:05:26.161: INFO: Pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec" satisfied condition "running and ready"
May 17 09:05:26.172: INFO: Waiting up to 5m0s for pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d" in namespace "pods-9681" to be "Succeeded or Failed"
May 17 09:05:26.174: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.398352ms
May 17 09:05:28.175: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003055725s
May 17 09:05:30.175: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003139213s
STEP: Saw pod success 05/17/23 09:05:30.175
May 17 09:05:30.176: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d" satisfied condition "Succeeded or Failed"
May 17 09:05:30.177: INFO: Trying to get logs from node k8s-node1 pod client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d container env3cont: <nil>
STEP: delete the pod 05/17/23 09:05:30.18
May 17 09:05:30.186: INFO: Waiting for pod client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d to disappear
May 17 09:05:30.187: INFO: Pod client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 17 09:05:30.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9681" for this suite. 05/17/23 09:05:30.189
------------------------------
â€¢ [SLOW TEST] [6.047 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:24.144
    May 17 09:05:24.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename pods 05/17/23 09:05:24.145
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:24.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:24.151
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    May 17 09:05:24.157: INFO: Waiting up to 5m0s for pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec" in namespace "pods-9681" to be "running and ready"
    May 17 09:05:24.158: INFO: Pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.153769ms
    May 17 09:05:24.158: INFO: The phase of Pod server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec is Pending, waiting for it to be Running (with Ready = true)
    May 17 09:05:26.161: INFO: Pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.003812537s
    May 17 09:05:26.161: INFO: The phase of Pod server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec is Running (Ready = true)
    May 17 09:05:26.161: INFO: Pod "server-envvars-ca92de6e-39fa-41f2-9c62-5add35f157ec" satisfied condition "running and ready"
    May 17 09:05:26.172: INFO: Waiting up to 5m0s for pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d" in namespace "pods-9681" to be "Succeeded or Failed"
    May 17 09:05:26.174: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.398352ms
    May 17 09:05:28.175: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003055725s
    May 17 09:05:30.175: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003139213s
    STEP: Saw pod success 05/17/23 09:05:30.175
    May 17 09:05:30.176: INFO: Pod "client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d" satisfied condition "Succeeded or Failed"
    May 17 09:05:30.177: INFO: Trying to get logs from node k8s-node1 pod client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d container env3cont: <nil>
    STEP: delete the pod 05/17/23 09:05:30.18
    May 17 09:05:30.186: INFO: Waiting for pod client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d to disappear
    May 17 09:05:30.187: INFO: Pod client-envvars-185f7ff2-19e5-442c-b83e-4d8ba5a7800d no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:30.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9681" for this suite. 05/17/23 09:05:30.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:30.192
May 17 09:05:30.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 09:05:30.192
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:30.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:30.2
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 09:05:30.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6122" for this suite. 05/17/23 09:05:30.204
------------------------------
â€¢ [0.015 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:30.192
    May 17 09:05:30.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 09:05:30.192
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:30.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:30.2
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:30.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6122" for this suite. 05/17/23 09:05:30.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:30.207
May 17 09:05:30.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename services 05/17/23 09:05:30.207
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:30.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:30.215
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-2310 05/17/23 09:05:30.216
STEP: creating service affinity-nodeport in namespace services-2310 05/17/23 09:05:30.216
STEP: creating replication controller affinity-nodeport in namespace services-2310 05/17/23 09:05:30.224
I0517 09:05:30.227222      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2310, replica count: 3
I0517 09:05:33.278389      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 09:05:33.283: INFO: Creating new exec pod
May 17 09:05:33.286: INFO: Waiting up to 5m0s for pod "execpod-affinitymrq4n" in namespace "services-2310" to be "running"
May 17 09:05:33.287: INFO: Pod "execpod-affinitymrq4n": Phase="Pending", Reason="", readiness=false. Elapsed: 1.250189ms
May 17 09:05:35.289: INFO: Pod "execpod-affinitymrq4n": Phase="Running", Reason="", readiness=true. Elapsed: 2.003236849s
May 17 09:05:35.289: INFO: Pod "execpod-affinitymrq4n" satisfied condition "running"
May 17 09:05:36.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
May 17 09:05:36.382: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 17 09:05:36.382: INFO: stdout: ""
May 17 09:05:36.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 10.105.4.1 80'
May 17 09:05:36.469: INFO: stderr: "+ nc -v -z -w 2 10.105.4.1 80\nConnection to 10.105.4.1 80 port [tcp/http] succeeded!\n"
May 17 09:05:36.469: INFO: stdout: ""
May 17 09:05:36.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 30163'
May 17 09:05:36.556: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 30163\nConnection to 10.0.79.210 30163 port [tcp/*] succeeded!\n"
May 17 09:05:36.556: INFO: stdout: ""
May 17 09:05:36.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 30163'
May 17 09:05:36.645: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 30163\nConnection to 10.0.79.211 30163 port [tcp/*] succeeded!\n"
May 17 09:05:36.645: INFO: stdout: ""
May 17 09:05:36.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.79.210:30163/ ; done'
May 17 09:05:36.784: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n"
May 17 09:05:36.784: INFO: stdout: "\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn"
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
May 17 09:05:36.784: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2310, will wait for the garbage collector to delete the pods 05/17/23 09:05:36.791
May 17 09:05:36.845: INFO: Deleting ReplicationController affinity-nodeport took: 2.591ms
May 17 09:05:36.946: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.866485ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 17 09:05:39.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2310" for this suite. 05/17/23 09:05:39.058
------------------------------
â€¢ [SLOW TEST] [8.854 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:30.207
    May 17 09:05:30.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename services 05/17/23 09:05:30.207
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:30.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:30.215
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-2310 05/17/23 09:05:30.216
    STEP: creating service affinity-nodeport in namespace services-2310 05/17/23 09:05:30.216
    STEP: creating replication controller affinity-nodeport in namespace services-2310 05/17/23 09:05:30.224
    I0517 09:05:30.227222      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2310, replica count: 3
    I0517 09:05:33.278389      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 09:05:33.283: INFO: Creating new exec pod
    May 17 09:05:33.286: INFO: Waiting up to 5m0s for pod "execpod-affinitymrq4n" in namespace "services-2310" to be "running"
    May 17 09:05:33.287: INFO: Pod "execpod-affinitymrq4n": Phase="Pending", Reason="", readiness=false. Elapsed: 1.250189ms
    May 17 09:05:35.289: INFO: Pod "execpod-affinitymrq4n": Phase="Running", Reason="", readiness=true. Elapsed: 2.003236849s
    May 17 09:05:35.289: INFO: Pod "execpod-affinitymrq4n" satisfied condition "running"
    May 17 09:05:36.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    May 17 09:05:36.382: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May 17 09:05:36.382: INFO: stdout: ""
    May 17 09:05:36.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 10.105.4.1 80'
    May 17 09:05:36.469: INFO: stderr: "+ nc -v -z -w 2 10.105.4.1 80\nConnection to 10.105.4.1 80 port [tcp/http] succeeded!\n"
    May 17 09:05:36.469: INFO: stdout: ""
    May 17 09:05:36.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 10.0.79.210 30163'
    May 17 09:05:36.556: INFO: stderr: "+ nc -v -z -w 2 10.0.79.210 30163\nConnection to 10.0.79.210 30163 port [tcp/*] succeeded!\n"
    May 17 09:05:36.556: INFO: stdout: ""
    May 17 09:05:36.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c nc -v -z -w 2 10.0.79.211 30163'
    May 17 09:05:36.645: INFO: stderr: "+ nc -v -z -w 2 10.0.79.211 30163\nConnection to 10.0.79.211 30163 port [tcp/*] succeeded!\n"
    May 17 09:05:36.645: INFO: stdout: ""
    May 17 09:05:36.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1934815320 --namespace=services-2310 exec execpod-affinitymrq4n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.79.210:30163/ ; done'
    May 17 09:05:36.784: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.79.210:30163/\n"
    May 17 09:05:36.784: INFO: stdout: "\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn\naffinity-nodeport-x5hvn"
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Received response from host: affinity-nodeport-x5hvn
    May 17 09:05:36.784: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2310, will wait for the garbage collector to delete the pods 05/17/23 09:05:36.791
    May 17 09:05:36.845: INFO: Deleting ReplicationController affinity-nodeport took: 2.591ms
    May 17 09:05:36.946: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.866485ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 17 09:05:39.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2310" for this suite. 05/17/23 09:05:39.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:05:39.061
May 17 09:05:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-probe 05/17/23 09:05:39.062
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:39.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:39.07
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 in namespace container-probe-4491 05/17/23 09:05:39.071
May 17 09:05:39.075: INFO: Waiting up to 5m0s for pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1" in namespace "container-probe-4491" to be "not pending"
May 17 09:05:39.077: INFO: Pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.223981ms
May 17 09:05:41.079: INFO: Pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003359065s
May 17 09:05:41.079: INFO: Pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1" satisfied condition "not pending"
May 17 09:05:41.079: INFO: Started pod liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 in namespace container-probe-4491
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 09:05:41.079
May 17 09:05:41.080: INFO: Initial restart count of pod liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is 0
May 17 09:06:01.108: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 1 (20.028291101s elapsed)
May 17 09:06:21.138: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 2 (40.057377856s elapsed)
May 17 09:06:41.167: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 3 (1m0.086689666s elapsed)
May 17 09:07:01.193: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 4 (1m20.112528578s elapsed)
May 17 09:08:11.297: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 5 (2m30.217272854s elapsed)
STEP: deleting the pod 05/17/23 09:08:11.297
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 17 09:08:11.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4491" for this suite. 05/17/23 09:08:11.306
------------------------------
â€¢ [SLOW TEST] [152.247 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:05:39.061
    May 17 09:05:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-probe 05/17/23 09:05:39.062
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:05:39.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:05:39.07
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 in namespace container-probe-4491 05/17/23 09:05:39.071
    May 17 09:05:39.075: INFO: Waiting up to 5m0s for pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1" in namespace "container-probe-4491" to be "not pending"
    May 17 09:05:39.077: INFO: Pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.223981ms
    May 17 09:05:41.079: INFO: Pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003359065s
    May 17 09:05:41.079: INFO: Pod "liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1" satisfied condition "not pending"
    May 17 09:05:41.079: INFO: Started pod liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 in namespace container-probe-4491
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 09:05:41.079
    May 17 09:05:41.080: INFO: Initial restart count of pod liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is 0
    May 17 09:06:01.108: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 1 (20.028291101s elapsed)
    May 17 09:06:21.138: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 2 (40.057377856s elapsed)
    May 17 09:06:41.167: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 3 (1m0.086689666s elapsed)
    May 17 09:07:01.193: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 4 (1m20.112528578s elapsed)
    May 17 09:08:11.297: INFO: Restart count of pod container-probe-4491/liveness-c7f7e641-92b8-4f1c-b23b-bd38411fb2b1 is now 5 (2m30.217272854s elapsed)
    STEP: deleting the pod 05/17/23 09:08:11.297
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 17 09:08:11.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4491" for this suite. 05/17/23 09:08:11.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:08:11.309
May 17 09:08:11.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename secrets 05/17/23 09:08:11.31
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:11.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:11.316
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-9bf6799b-a891-46a4-84be-6df50541ce58 05/17/23 09:08:11.321
STEP: Creating a pod to test consume secrets 05/17/23 09:08:11.324
May 17 09:08:11.328: INFO: Waiting up to 5m0s for pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa" in namespace "secrets-8033" to be "Succeeded or Failed"
May 17 09:08:11.329: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.325491ms
May 17 09:08:13.332: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003826214s
May 17 09:08:15.332: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003986814s
STEP: Saw pod success 05/17/23 09:08:15.332
May 17 09:08:15.332: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa" satisfied condition "Succeeded or Failed"
May 17 09:08:15.334: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 09:08:15.342
May 17 09:08:15.348: INFO: Waiting for pod pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa to disappear
May 17 09:08:15.350: INFO: Pod pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 17 09:08:15.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8033" for this suite. 05/17/23 09:08:15.351
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:08:11.309
    May 17 09:08:11.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename secrets 05/17/23 09:08:11.31
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:11.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:11.316
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-9bf6799b-a891-46a4-84be-6df50541ce58 05/17/23 09:08:11.321
    STEP: Creating a pod to test consume secrets 05/17/23 09:08:11.324
    May 17 09:08:11.328: INFO: Waiting up to 5m0s for pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa" in namespace "secrets-8033" to be "Succeeded or Failed"
    May 17 09:08:11.329: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.325491ms
    May 17 09:08:13.332: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003826214s
    May 17 09:08:15.332: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003986814s
    STEP: Saw pod success 05/17/23 09:08:15.332
    May 17 09:08:15.332: INFO: Pod "pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa" satisfied condition "Succeeded or Failed"
    May 17 09:08:15.334: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 09:08:15.342
    May 17 09:08:15.348: INFO: Waiting for pod pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa to disappear
    May 17 09:08:15.350: INFO: Pod pod-secrets-f250e1f8-3cbe-4da3-b0d3-20bf7ce744aa no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 17 09:08:15.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8033" for this suite. 05/17/23 09:08:15.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:08:15.354
May 17 09:08:15.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename webhook 05/17/23 09:08:15.355
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:15.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:15.363
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/17/23 09:08:15.371
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 09:08:15.654
STEP: Deploying the webhook pod 05/17/23 09:08:15.658
STEP: Wait for the deployment to be ready 05/17/23 09:08:15.664
May 17 09:08:15.668: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 09:08:17.673
STEP: Verifying the service has paired with the endpoint 05/17/23 09:08:17.68
May 17 09:08:18.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/17/23 09:08:18.682
STEP: create a namespace for the webhook 05/17/23 09:08:18.693
STEP: create a configmap should be unconditionally rejected by the webhook 05/17/23 09:08:18.697
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 17 09:08:18.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6235" for this suite. 05/17/23 09:08:18.729
STEP: Destroying namespace "webhook-6235-markers" for this suite. 05/17/23 09:08:18.732
------------------------------
â€¢ [3.381 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:08:15.354
    May 17 09:08:15.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename webhook 05/17/23 09:08:15.355
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:15.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:15.363
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/17/23 09:08:15.371
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 09:08:15.654
    STEP: Deploying the webhook pod 05/17/23 09:08:15.658
    STEP: Wait for the deployment to be ready 05/17/23 09:08:15.664
    May 17 09:08:15.668: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 09:08:17.673
    STEP: Verifying the service has paired with the endpoint 05/17/23 09:08:17.68
    May 17 09:08:18.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/17/23 09:08:18.682
    STEP: create a namespace for the webhook 05/17/23 09:08:18.693
    STEP: create a configmap should be unconditionally rejected by the webhook 05/17/23 09:08:18.697
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 17 09:08:18.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6235" for this suite. 05/17/23 09:08:18.729
    STEP: Destroying namespace "webhook-6235-markers" for this suite. 05/17/23 09:08:18.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:08:18.743
May 17 09:08:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 09:08:18.744
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:18.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:18.751
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/17/23 09:08:18.755
May 17 09:08:18.759: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5714" to be "running and ready"
May 17 09:08:18.760: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.272152ms
May 17 09:08:18.760: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 09:08:20.764: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005195276s
May 17 09:08:20.764: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 09:08:20.764: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 05/17/23 09:08:20.766
May 17 09:08:20.769: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5714" to be "running and ready"
May 17 09:08:20.771: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335368ms
May 17 09:08:20.771: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 09:08:22.773: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003806717s
May 17 09:08:22.773: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May 17 09:08:22.773: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/17/23 09:08:22.775
May 17 09:08:22.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 09:08:22.779: INFO: Pod pod-with-prestop-exec-hook still exists
May 17 09:08:24.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 09:08:24.781: INFO: Pod pod-with-prestop-exec-hook still exists
May 17 09:08:26.780: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 09:08:26.782: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/17/23 09:08:26.782
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 17 09:08:26.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5714" for this suite. 05/17/23 09:08:26.788
------------------------------
â€¢ [SLOW TEST] [8.047 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:08:18.743
    May 17 09:08:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 09:08:18.744
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:18.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:18.751
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 09:08:18.755
    May 17 09:08:18.759: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5714" to be "running and ready"
    May 17 09:08:18.760: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.272152ms
    May 17 09:08:18.760: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 09:08:20.764: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005195276s
    May 17 09:08:20.764: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 09:08:20.764: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 05/17/23 09:08:20.766
    May 17 09:08:20.769: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5714" to be "running and ready"
    May 17 09:08:20.771: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335368ms
    May 17 09:08:20.771: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 09:08:22.773: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003806717s
    May 17 09:08:22.773: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May 17 09:08:22.773: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/17/23 09:08:22.775
    May 17 09:08:22.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 17 09:08:22.779: INFO: Pod pod-with-prestop-exec-hook still exists
    May 17 09:08:24.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 17 09:08:24.781: INFO: Pod pod-with-prestop-exec-hook still exists
    May 17 09:08:26.780: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 17 09:08:26.782: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/17/23 09:08:26.782
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 17 09:08:26.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5714" for this suite. 05/17/23 09:08:26.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:08:26.791
May 17 09:08:26.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename sysctl 05/17/23 09:08:26.792
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:26.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:26.8
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/17/23 09:08:26.802
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 17 09:08:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5839" for this suite. 05/17/23 09:08:26.806
------------------------------
â€¢ [0.017 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:08:26.791
    May 17 09:08:26.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename sysctl 05/17/23 09:08:26.792
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:26.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:26.8
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/17/23 09:08:26.802
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 17 09:08:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5839" for this suite. 05/17/23 09:08:26.806
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/17/23 09:08:26.808
May 17 09:08:26.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
STEP: Building a namespace api object, basename init-container 05/17/23 09:08:26.809
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:26.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:26.816
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 05/17/23 09:08:26.818
May 17 09:08:26.818: INFO: PodSpec: initContainers in spec.initContainers
May 17 09:09:11.238: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cb48a8f4-d4d3-45ad-a8ab-54317e687af1", GenerateName:"", Namespace:"init-container-9218", SelfLink:"", UID:"2a865a4a-2523-43b7-9e3d-a281d86538ac", ResourceVersion:"1219655", Generation:0, CreationTimestamp:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"818539599"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a11f58de2391342ac3bc7c780f0725ac936ea5ac645833280ebfcf4958c32d3e", "cni.projectcalico.org/podIP":"192.168.36.76/32", "cni.projectcalico.org/podIPs":"192.168.36.76/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006ec6108), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 9, 8, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006ec6138), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 9, 9, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006ec6180), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-p4874", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005bcc000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4874", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4874", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4874", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0049ca508), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-node1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000b32000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049ca650)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049ca670)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0049ca678), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0049ca67c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001282020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.79.210", PodIP:"192.168.36.76", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.36.76"}}, StartTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b322a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b32310)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"sha256:d59c675982d8692814ec9e1486d4c645cd86ad825ef33975a5db196cf2801592", ContainerID:"containerd://40309cafc0ff54e1c7d133ad14d990c838e2aa5a8b2ffe01cbc51728275ecdf3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcc080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcc060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0049ca84f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 17 09:09:11.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9218" for this suite. 05/17/23 09:09:11.24
------------------------------
â€¢ [SLOW TEST] [44.435 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/17/23 09:08:26.808
    May 17 09:08:26.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1934815320
    STEP: Building a namespace api object, basename init-container 05/17/23 09:08:26.809
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 09:08:26.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 09:08:26.816
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 05/17/23 09:08:26.818
    May 17 09:08:26.818: INFO: PodSpec: initContainers in spec.initContainers
    May 17 09:09:11.238: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cb48a8f4-d4d3-45ad-a8ab-54317e687af1", GenerateName:"", Namespace:"init-container-9218", SelfLink:"", UID:"2a865a4a-2523-43b7-9e3d-a281d86538ac", ResourceVersion:"1219655", Generation:0, CreationTimestamp:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"818539599"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a11f58de2391342ac3bc7c780f0725ac936ea5ac645833280ebfcf4958c32d3e", "cni.projectcalico.org/podIP":"192.168.36.76/32", "cni.projectcalico.org/podIPs":"192.168.36.76/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006ec6108), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 9, 8, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006ec6138), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 9, 9, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006ec6180), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-p4874", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005bcc000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4874", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4874", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4874", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0049ca508), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-node1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000b32000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049ca650)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049ca670)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0049ca678), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0049ca67c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001282020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.79.210", PodIP:"192.168.36.76", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.36.76"}}, StartTime:time.Date(2023, time.May, 17, 9, 8, 26, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b322a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b32310)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"sha256:d59c675982d8692814ec9e1486d4c645cd86ad825ef33975a5db196cf2801592", ContainerID:"containerd://40309cafc0ff54e1c7d133ad14d990c838e2aa5a8b2ffe01cbc51728275ecdf3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcc080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcc060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0049ca84f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 17 09:09:11.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9218" for this suite. 05/17/23 09:09:11.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
May 17 09:09:11.245: INFO: Running AfterSuite actions on node 1
May 17 09:09:11.245: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    May 17 09:09:11.245: INFO: Running AfterSuite actions on node 1
    May 17 09:09:11.245: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.068 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5488.880 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h31m29.14195066s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

